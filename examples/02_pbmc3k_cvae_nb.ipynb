{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Conditional VAE on PBMC 3k with Negative Binomial Decoder\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Preprocessing PBMC 3k data (preserving raw counts)\n",
    "2. Training a cVAE with NB decoder on real scRNA-seq data\n",
    "3. Visualizing the latent space (UMAP)\n",
    "4. Evaluating via downstream cell type classification\n",
    "\n",
    "**Key principle**: We use raw counts + library size, not normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import scanpy as sc\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess PBMC 3k\n",
    "\n",
    "We download the PBMC 3k dataset and perform minimal preprocessing:\n",
    "- Filter low-quality cells and genes\n",
    "- Compute library size (total counts per cell)\n",
    "- Select highly variable genes\n",
    "- **Keep raw counts** (no normalization!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download PBMC 3k\n",
    "adata = sc.datasets.pbmc3k()\n",
    "adata.var_names_make_unique()\n",
    "print(f\"Raw data: {adata.n_obs} cells, {adata.n_vars} genes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC metrics\n",
    "adata.obs[\"library_size\"] = np.array(adata.X.sum(axis=1)).ravel()\n",
    "adata.obs[\"n_genes\"] = np.array((adata.X > 0).sum(axis=1)).ravel()\n",
    "adata.var[\"mt\"] = adata.var_names.str.startswith(\"MT-\")\n",
    "sc.pp.calculate_qc_metrics(adata, qc_vars=[\"mt\"], inplace=True)\n",
    "\n",
    "# Plot QC\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "axes[0].hist(adata.obs[\"library_size\"], bins=50)\n",
    "axes[0].set_xlabel(\"Library size\")\n",
    "axes[1].hist(adata.obs[\"n_genes\"], bins=50)\n",
    "axes[1].set_xlabel(\"Genes detected\")\n",
    "axes[2].hist(adata.obs[\"pct_counts_mt\"], bins=50)\n",
    "axes[2].set_xlabel(\"% mitochondrial\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter cells and genes\n",
    "sc.pp.filter_cells(adata, min_genes=200)\n",
    "adata = adata[adata.obs.pct_counts_mt < 20].copy()\n",
    "sc.pp.filter_genes(adata, min_cells=3)\n",
    "print(f\"After filtering: {adata.n_obs} cells, {adata.n_vars} genes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store raw counts BEFORE any normalization\n",
    "adata.layers[\"counts\"] = adata.X.copy()\n",
    "\n",
    "# Recompute library size after filtering\n",
    "adata.obs[\"library_size\"] = np.array(adata.X.sum(axis=1)).ravel()\n",
    "\n",
    "# HVG selection (uses temporary normalization internally)\n",
    "adata_norm = adata.copy()\n",
    "sc.pp.normalize_total(adata_norm, target_sum=1e4)\n",
    "sc.pp.log1p(adata_norm)\n",
    "sc.pp.highly_variable_genes(adata_norm, n_top_genes=2000)\n",
    "adata.var[\"highly_variable\"] = adata_norm.var[\"highly_variable\"]\n",
    "\n",
    "# Subset to HVGs\n",
    "adata = adata[:, adata.var.highly_variable].copy()\n",
    "print(f\"After HVG selection: {adata.n_obs} cells, {adata.n_vars} genes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cell type annotations (from processed version)\n",
    "# PBMC 3k has standard Louvain clusters that map to known cell types\n",
    "adata_processed = sc.datasets.pbmc3k_processed()\n",
    "\n",
    "# Match cells by barcode\n",
    "common_cells = adata.obs_names.intersection(adata_processed.obs_names)\n",
    "adata = adata[common_cells].copy()\n",
    "adata.obs[\"cell_type\"] = adata_processed.obs.loc[common_cells, \"louvain\"].astype(\"category\")\n",
    "\n",
    "print(f\"Final dataset: {adata.n_obs} cells, {adata.n_vars} genes\")\n",
    "print(f\"Cell types: {adata.obs.cell_type.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create PyTorch Dataset and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genailab.data.sc_dataset import SingleCellDataset, collate_sc_batch\n",
    "from genailab.data.loaders import split_dataset\n",
    "\n",
    "# Create dataset with raw counts and cell type as condition\n",
    "dataset = SingleCellDataset(\n",
    "    adata,\n",
    "    layer=\"counts\",\n",
    "    condition_keys=[\"cell_type\"],\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {len(dataset)} cells, {dataset.n_genes} genes\")\n",
    "print(f\"Conditions: {dataset.n_conditions}\")\n",
    "print(f\"Library size stats: {dataset.get_library_size_stats()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/val\n",
    "train_ds, val_ds = split_dataset(dataset, val_frac=0.15, seed=42)\n",
    "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_ds, \n",
    "    batch_size=128, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_sc_batch,\n",
    "    num_workers=0,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, \n",
    "    batch_size=256, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_sc_batch,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a batch\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"Batch keys: {batch.keys()}\")\n",
    "print(f\"x shape: {batch['x'].shape}\")\n",
    "print(f\"library_size shape: {batch['library_size'].shape}\")\n",
    "print(f\"cond keys: {batch['cond'].keys()}\")\n",
    "print(f\"cell_type shape: {batch['cond']['cell_type'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create and Train the cVAE with NB Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genailab.model.conditioning import ConditionSpec, ConditionEncoder\n",
    "from genailab.model.vae import CVAE_NB\n",
    "from genailab.objectives.losses import elbo_loss_nb\n",
    "\n",
    "# Create condition encoder\n",
    "spec = ConditionSpec(\n",
    "    n_cats={\"cell_type\": dataset.n_conditions[\"cell_type\"]},\n",
    "    emb_dim=32,\n",
    "    out_dim=64,\n",
    ")\n",
    "cond_encoder = ConditionEncoder(spec)\n",
    "\n",
    "# Create model\n",
    "model = CVAE_NB(\n",
    "    n_genes=dataset.n_genes,\n",
    "    z_dim=32,\n",
    "    cond_encoder=cond_encoder,\n",
    "    hidden=256,\n",
    "    n_layers=2,\n",
    "    dropout=0.1,\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, device, beta=1.0):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_recon, total_kl = 0, 0, 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        x = batch[\"x\"].to(device)\n",
    "        library_size = batch[\"library_size\"].to(device)\n",
    "        cond = {k: v.to(device) for k, v in batch[\"cond\"].items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(x, cond, library_size)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, parts = elbo_loss_nb(\n",
    "            x, out[\"mu\"], out[\"theta\"],\n",
    "            out[\"enc_mu\"], out[\"enc_logvar\"],\n",
    "            beta=beta,\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_recon += parts[\"recon\"].item()\n",
    "        total_kl += parts[\"kl\"].item()\n",
    "    \n",
    "    n = len(loader)\n",
    "    return total_loss / n, total_recon / n, total_kl / n\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, device, beta=1.0):\n",
    "    \"\"\"Evaluate for one epoch.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss, total_recon, total_kl = 0, 0, 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        x = batch[\"x\"].to(device)\n",
    "        library_size = batch[\"library_size\"].to(device)\n",
    "        cond = {k: v.to(device) for k, v in batch[\"cond\"].items()}\n",
    "        \n",
    "        out = model(x, cond, library_size)\n",
    "        loss, parts = elbo_loss_nb(\n",
    "            x, out[\"mu\"], out[\"theta\"],\n",
    "            out[\"enc_mu\"], out[\"enc_logvar\"],\n",
    "            beta=beta,\n",
    "        )\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_recon += parts[\"recon\"].item()\n",
    "        total_kl += parts[\"kl\"].item()\n",
    "    \n",
    "    n = len(loader)\n",
    "    return total_loss / n, total_recon / n, total_kl / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "\n",
    "n_epochs = 50\n",
    "beta = 0.5  # Start with lower beta to avoid posterior collapse\n",
    "\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"train_recon\": [], \"val_recon\": [], \"train_kl\": [], \"val_kl\": []}\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # Train\n",
    "    train_loss, train_recon, train_kl = train_epoch(model, train_loader, optimizer, device, beta=beta)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_recon, val_kl = eval_epoch(model, val_loader, device, beta=beta)\n",
    "    \n",
    "    # Record\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"train_recon\"].append(train_recon)\n",
    "    history[\"val_recon\"].append(val_recon)\n",
    "    history[\"train_kl\"].append(train_kl)\n",
    "    history[\"val_kl\"].append(val_kl)\n",
    "    \n",
    "    # LR scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save best\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_cvae_nb.pt\")\n",
    "    \n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:3d} | Train: loss={train_loss:.4f} recon={train_recon:.4f} kl={train_kl:.4f} | Val: loss={val_loss:.4f}\")\n",
    "\n",
    "print(f\"\\nBest validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "\n",
    "axes[0].plot(history[\"train_loss\"], label=\"Train\")\n",
    "axes[0].plot(history[\"val_loss\"], label=\"Val\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Total Loss\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(history[\"train_recon\"], label=\"Train\")\n",
    "axes[1].plot(history[\"val_recon\"], label=\"Val\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Reconstruction (NB NLL)\")\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].plot(history[\"train_kl\"], label=\"Train\")\n",
    "axes[2].plot(history[\"val_kl\"], label=\"Val\")\n",
    "axes[2].set_xlabel(\"Epoch\")\n",
    "axes[2].set_ylabel(\"KL Divergence\")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Latent Representations and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"best_cvae_nb.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Extract latents for all cells\n",
    "all_loader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=256, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_sc_batch,\n",
    ")\n",
    "\n",
    "latents = []\n",
    "cell_types = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in all_loader:\n",
    "        x = batch[\"x\"].to(device)\n",
    "        cond = {k: v.to(device) for k, v in batch[\"cond\"].items()}\n",
    "        \n",
    "        mu, _ = model.encode(x, cond)\n",
    "        latents.append(mu.cpu().numpy())\n",
    "        cell_types.append(batch[\"cond\"][\"cell_type\"].numpy())\n",
    "\n",
    "latents = np.concatenate(latents, axis=0)\n",
    "cell_types = np.concatenate(cell_types, axis=0)\n",
    "\n",
    "print(f\"Latent shape: {latents.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP visualization\n",
    "import umap\n",
    "\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "embedding = reducer.fit_transform(latents)\n",
    "\n",
    "# Get cell type names\n",
    "cell_type_names = adata.obs[\"cell_type\"].cat.categories.tolist()\n",
    "cell_type_labels = [cell_type_names[i] for i in cell_types]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, ct in enumerate(cell_type_names):\n",
    "    mask = np.array(cell_type_labels) == ct\n",
    "    plt.scatter(embedding[mask, 0], embedding[mask, 1], label=ct, s=5, alpha=0.7)\n",
    "\n",
    "plt.xlabel(\"UMAP 1\")\n",
    "plt.ylabel(\"UMAP 2\")\n",
    "plt.title(\"cVAE Latent Space (PBMC 3k)\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Downstream Evaluation: Cell Type Classification\n",
    "\n",
    "To evaluate the quality of the learned representations, we train a simple classifier on the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split latents\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    latents, cell_types, test_size=0.2, random_state=42, stratify=cell_types\n",
    ")\n",
    "\n",
    "# Train logistic regression\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Cell type classification accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=cell_type_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with PCA baseline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Get raw counts for comparison\n",
    "X_counts = dataset.X\n",
    "\n",
    "# Log-normalize for PCA (standard practice)\n",
    "X_norm = np.log1p(X_counts / X_counts.sum(axis=1, keepdims=True) * 1e4)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=32)\n",
    "X_pca = pca.fit_transform(X_norm)\n",
    "\n",
    "# Split and train\n",
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(\n",
    "    X_pca, cell_types, test_size=0.2, random_state=42, stratify=cell_types\n",
    ")\n",
    "\n",
    "clf_pca = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf_pca.fit(X_train_pca, y_train_pca)\n",
    "\n",
    "y_pred_pca = clf_pca.predict(X_test_pca)\n",
    "accuracy_pca = accuracy_score(y_test_pca, y_pred_pca)\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  cVAE latent accuracy: {accuracy:.4f}\")\n",
    "print(f\"  PCA baseline accuracy: {accuracy_pca:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Proper preprocessing** for generative models: raw counts + library size\n",
    "2. **Training a cVAE with NB decoder** on real scRNA-seq data\n",
    "3. **Latent space visualization** showing cell type separation\n",
    "4. **Downstream evaluation** via cell type classification\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try ZINB decoder for sparser data\n",
    "- Experiment with Î²-VAE for disentanglement\n",
    "- Add more conditions (batch, donor)\n",
    "- Implement counterfactual generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
