{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Models for Gene Expression: DDPM Tutorial\n",
    "\n",
    "**Goal**: Implement a denoising diffusion probabilistic model (DDPM) for generating gene expression profiles.\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Core DDPM mechanics (forward/reverse diffusion)\n",
    "2. Training a time-conditional score network on gene expression data\n",
    "3. Conditional generation (cell type → gene expression)\n",
    "4. Foundation for drug-response prediction (scPPDM approach)\n",
    "\n",
    "**Dataset**: PBMC 3k (small subset for fast iteration)\n",
    "\n",
    "**Next steps**: Extend to perturbation response (baseline + drug → perturbed expression)\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**Environment**: Make sure you're in the `genailab` conda environment:\n",
    "\n",
    "```bash\n",
    "mamba activate genailab\n",
    "```\n",
    "\n",
    "**Required packages**: torch, scanpy, numpy, matplotlib, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import scanpy as sc\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Gene Expression Data\n",
    "\n",
    "We'll use a small subset of PBMC 3k for fast iteration. For production, you'd use the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PBMC 3k data\n",
    "data_path = Path(\"../data/pbmc3k_raw.h5ad\")\n",
    "\n",
    "if data_path.exists():\n",
    "    adata = sc.read_h5ad(data_path)\n",
    "    print(f\"Loaded data: {adata.shape}\")\n",
    "else:\n",
    "    # Download if not available\n",
    "    adata = sc.datasets.pbmc3k()\n",
    "    print(f\"Downloaded PBMC 3k: {adata.shape}\")\n",
    "\n",
    "# Basic preprocessing\n",
    "sc.pp.filter_cells(adata, min_genes=200)\n",
    "sc.pp.filter_genes(adata, min_cells=3)\n",
    "\n",
    "# Normalize and log-transform for diffusion model\n",
    "# Note: For count-based models (like NB VAE), we'd use raw counts\n",
    "# For diffusion, we typically work with normalized continuous data\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "sc.pp.log1p(adata)\n",
    "\n",
    "# Select highly variable genes for faster training\n",
    "sc.pp.highly_variable_genes(adata, n_top_genes=500, flavor='seurat_v3')\n",
    "adata = adata[:, adata.var.highly_variable].copy()\n",
    "\n",
    "print(f\"After preprocessing: {adata.shape}\")\n",
    "print(f\"Gene expression range: [{adata.X.min():.2f}, {adata.X.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotate cell types for conditional generation\n",
    "sc.pp.neighbors(adata, n_neighbors=10)\n",
    "sc.tl.leiden(adata, resolution=0.5)\n",
    "\n",
    "# Store cell type labels\n",
    "cell_types = adata.obs['leiden'].values\n",
    "n_cell_types = len(np.unique(cell_types))\n",
    "\n",
    "print(f\"Found {n_cell_types} cell type clusters\")\n",
    "print(adata.obs['leiden'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create PyTorch Dataset\n",
    "\n",
    "We'll create a dataset that returns:\n",
    "- Gene expression vector (x)\n",
    "- Cell type label (condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneExpressionDataset(Dataset):\n",
    "    \"\"\"Dataset for gene expression with optional conditioning.\"\"\"\n",
    "    \n",
    "    def __init__(self, adata, condition_key=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            adata: AnnData object with preprocessed gene expression\n",
    "            condition_key: Key in adata.obs for conditioning (e.g., 'leiden', 'treatment')\n",
    "        \"\"\"\n",
    "        # Convert to dense array if sparse\n",
    "        if hasattr(adata.X, 'toarray'):\n",
    "            self.X = adata.X.toarray()\n",
    "        else:\n",
    "            self.X = adata.X\n",
    "        \n",
    "        self.X = torch.FloatTensor(self.X)\n",
    "        \n",
    "        # Extract conditions if provided\n",
    "        if condition_key is not None:\n",
    "            conditions = adata.obs[condition_key].values\n",
    "            # Convert to integer labels\n",
    "            unique_conditions = np.unique(conditions)\n",
    "            condition_to_idx = {c: i for i, c in enumerate(unique_conditions)}\n",
    "            self.conditions = torch.LongTensor([condition_to_idx[c] for c in conditions])\n",
    "            self.n_conditions = len(unique_conditions)\n",
    "        else:\n",
    "            self.conditions = None\n",
    "            self.n_conditions = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.conditions is not None:\n",
    "            return self.X[idx], self.conditions[idx]\n",
    "        return self.X[idx]\n",
    "\n",
    "# Create dataset\n",
    "dataset = GeneExpressionDataset(adata, condition_key='leiden')\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Gene dimension: {dataset.X.shape[1]}\")\n",
    "print(f\"Number of conditions: {dataset.n_conditions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement DDPM Components\n",
    "\n",
    "### 3.1 Noise Scheduler\n",
    "\n",
    "The noise scheduler defines how we add noise in the forward process:\n",
    "- $\\beta_t$: variance schedule (how much noise to add at each step)\n",
    "- $\\alpha_t = 1 - \\beta_t$\n",
    "- $\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i$ (cumulative product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseScheduler:\n",
    "    \"\"\"Linear noise schedule for DDPM.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_timesteps=1000, beta_start=1e-4, beta_end=0.02):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_timesteps: Number of diffusion steps (T)\n",
    "            beta_start: Starting noise variance\n",
    "            beta_end: Ending noise variance\n",
    "        \"\"\"\n",
    "        self.num_timesteps = num_timesteps\n",
    "        \n",
    "        # Linear schedule for beta\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
    "        \n",
    "        # Precompute useful quantities\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "        \n",
    "        # For sampling\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
    "        \n",
    "        # For posterior q(x_{t-1} | x_t, x_0)\n",
    "        self.posterior_variance = (\n",
    "            self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "    \n",
    "    def add_noise(self, x_0, t, noise=None):\n",
    "        \"\"\"\n",
    "        Forward diffusion: q(x_t | x_0) = N(x_t; sqrt(alpha_bar_t) * x_0, (1 - alpha_bar_t) * I)\n",
    "        \n",
    "        Args:\n",
    "            x_0: Original data [batch_size, dim]\n",
    "            t: Timestep [batch_size]\n",
    "            noise: Optional noise to add (for reproducibility)\n",
    "        \n",
    "        Returns:\n",
    "            x_t: Noisy data at timestep t\n",
    "            noise: The noise that was added\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0)\n",
    "        \n",
    "        # Get coefficients for this timestep\n",
    "        sqrt_alpha_prod = self.sqrt_alphas_cumprod[t].reshape(-1, 1)\n",
    "        sqrt_one_minus_alpha_prod = self.sqrt_one_minus_alphas_cumprod[t].reshape(-1, 1)\n",
    "        \n",
    "        # x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * noise\n",
    "        x_t = sqrt_alpha_prod * x_0 + sqrt_one_minus_alpha_prod * noise\n",
    "        \n",
    "        return x_t, noise\n",
    "\n",
    "# Test the scheduler\n",
    "scheduler = NoiseScheduler(num_timesteps=1000)\n",
    "\n",
    "# Visualize noise schedule\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(scheduler.betas.numpy())\n",
    "axes[0].set_title('Beta Schedule')\n",
    "axes[0].set_xlabel('Timestep')\n",
    "axes[0].set_ylabel('Beta')\n",
    "\n",
    "axes[1].plot(scheduler.alphas_cumprod.numpy())\n",
    "axes[1].set_title('Cumulative Alpha')\n",
    "axes[1].set_xlabel('Timestep')\n",
    "axes[1].set_ylabel('Alpha_bar')\n",
    "\n",
    "axes[2].plot(scheduler.sqrt_one_minus_alphas_cumprod.numpy())\n",
    "axes[2].set_title('Noise Coefficient')\n",
    "axes[2].set_xlabel('Timestep')\n",
    "axes[2].set_ylabel('sqrt(1 - alpha_bar)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualize Forward Diffusion Process\n",
    "\n",
    "Let's see how a gene expression vector gets progressively noisier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a sample gene expression vector\n",
    "x_0 = dataset.X[0:1]  # Shape: [1, n_genes]\n",
    "\n",
    "# Add noise at different timesteps\n",
    "timesteps = [0, 100, 250, 500, 750, 999]\n",
    "noisy_samples = []\n",
    "\n",
    "for t in timesteps:\n",
    "    t_tensor = torch.tensor([t])\n",
    "    x_t, _ = scheduler.add_noise(x_0, t_tensor)\n",
    "    noisy_samples.append(x_t[0].numpy())\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (t, x_t) in enumerate(zip(timesteps, noisy_samples)):\n",
    "    axes[i].hist(x_t, bins=50, alpha=0.7)\n",
    "    axes[i].set_title(f'Timestep t={t}')\n",
    "    axes[i].set_xlabel('Expression value')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].axvline(x=0, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.suptitle('Forward Diffusion: Gene Expression → Noise', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Original data mean: {x_0.mean():.3f}, std: {x_0.std():.3f}\")\n",
    "print(f\"Final noise mean: {noisy_samples[-1].mean():.3f}, std: {noisy_samples[-1].std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Time-Conditional Score Network\n",
    "\n",
    "The core of DDPM: a neural network that predicts the noise $\\epsilon_\\theta(x_t, t, c)$ given:\n",
    "- Noisy data $x_t$\n",
    "- Timestep $t$\n",
    "- Condition $c$ (e.g., cell type, drug)\n",
    "\n",
    "For gene expression (tabular data), we use an MLP with:\n",
    "- Sinusoidal time embeddings\n",
    "- Conditional embeddings\n",
    "- Residual connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    \"\"\"Sinusoidal embeddings for timesteps (like in Transformers).\"\"\"\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = np.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class MLPBlock(nn.Module):\n",
    "    \"\"\"MLP block with residual connection.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * 4, dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.norm(x + self.net(x))\n",
    "\n",
    "\n",
    "class ConditionalScoreNetwork(nn.Module):\n",
    "    \"\"\"Time and condition-conditional score network for gene expression.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        hidden_dim=256,\n",
    "        time_dim=64,\n",
    "        n_conditions=0,\n",
    "        condition_dim=32,\n",
    "        n_layers=4,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Gene expression dimension\n",
    "            hidden_dim: Hidden layer dimension\n",
    "            time_dim: Time embedding dimension\n",
    "            n_conditions: Number of condition classes (0 for unconditional)\n",
    "            condition_dim: Condition embedding dimension\n",
    "            n_layers: Number of MLP blocks\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.n_conditions = n_conditions\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_dim),\n",
    "            nn.Linear(time_dim, time_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim * 2, time_dim),\n",
    "        )\n",
    "        \n",
    "        # Condition embedding (if conditional)\n",
    "        if n_conditions > 0:\n",
    "            self.condition_embed = nn.Embedding(n_conditions, condition_dim)\n",
    "            total_input_dim = input_dim + time_dim + condition_dim\n",
    "        else:\n",
    "            self.condition_embed = None\n",
    "            total_input_dim = input_dim + time_dim\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(total_input_dim, hidden_dim)\n",
    "        \n",
    "        # MLP blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            MLPBlock(hidden_dim, dropout) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection (predict noise)\n",
    "        self.output_proj = nn.Linear(hidden_dim, input_dim)\n",
    "    \n",
    "    def forward(self, x, t, condition=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Noisy gene expression [batch_size, input_dim]\n",
    "            t: Timestep [batch_size]\n",
    "            condition: Condition labels [batch_size] (optional)\n",
    "        \n",
    "        Returns:\n",
    "            Predicted noise [batch_size, input_dim]\n",
    "        \"\"\"\n",
    "        # Time embedding\n",
    "        t_emb = self.time_mlp(t)\n",
    "        \n",
    "        # Concatenate inputs\n",
    "        if self.condition_embed is not None and condition is not None:\n",
    "            c_emb = self.condition_embed(condition)\n",
    "            h = torch.cat([x, t_emb, c_emb], dim=-1)\n",
    "        else:\n",
    "            h = torch.cat([x, t_emb], dim=-1)\n",
    "        \n",
    "        # Project to hidden dimension\n",
    "        h = self.input_proj(h)\n",
    "        \n",
    "        # Apply MLP blocks\n",
    "        for block in self.blocks:\n",
    "            h = block(h)\n",
    "        \n",
    "        # Predict noise\n",
    "        noise_pred = self.output_proj(h)\n",
    "        \n",
    "        return noise_pred\n",
    "\n",
    "# Test the network\n",
    "model = ConditionalScoreNetwork(\n",
    "    input_dim=dataset.X.shape[1],\n",
    "    hidden_dim=256,\n",
    "    n_conditions=dataset.n_conditions,\n",
    "    n_layers=4,\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "x_test, c_test = next(iter(dataloader))\n",
    "x_test, c_test = x_test.to(device), c_test.to(device)\n",
    "t_test = torch.randint(0, 1000, (x_test.shape[0],), device=device)\n",
    "\n",
    "noise_pred = model(x_test, t_test, c_test)\n",
    "print(f\"Input shape: {x_test.shape}\")\n",
    "print(f\"Output shape: {noise_pred.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop\n",
    "\n",
    "DDPM training is simple:\n",
    "1. Sample a batch of data $x_0$\n",
    "2. Sample random timesteps $t$\n",
    "3. Add noise: $x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon$\n",
    "4. Predict noise: $\\epsilon_\\theta(x_t, t, c)$\n",
    "5. Compute MSE loss: $\\|\\epsilon - \\epsilon_\\theta(x_t, t, c)\\|^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ddpm(model, dataloader, scheduler, num_epochs=100, lr=1e-4):\n",
    "    \"\"\"Train DDPM model.\"\"\"\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False):\n",
    "            if len(batch) == 2:\n",
    "                x_0, condition = batch\n",
    "                x_0 = x_0.to(device)\n",
    "                condition = condition.to(device)\n",
    "            else:\n",
    "                x_0 = batch.to(device)\n",
    "                condition = None\n",
    "            \n",
    "            batch_size = x_0.shape[0]\n",
    "            \n",
    "            # Sample random timesteps\n",
    "            t = torch.randint(0, scheduler.num_timesteps, (batch_size,), device=device)\n",
    "            \n",
    "            # Add noise\n",
    "            noise = torch.randn_like(x_0)\n",
    "            x_t, _ = scheduler.add_noise(x_0, t, noise)\n",
    "            \n",
    "            # Predict noise\n",
    "            noise_pred = model(x_t, t, condition)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "            \n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model (start with fewer epochs for testing)\n",
    "losses = train_ddpm(\n",
    "    model=model,\n",
    "    dataloader=dataloader,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=50,  # Increase to 200-500 for better results\n",
    "    lr=1e-4,\n",
    ")\n",
    "\n",
    "# Plot training curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('DDPM Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sampling (Reverse Diffusion)\n",
    "\n",
    "Generate new gene expression profiles by:\n",
    "1. Start with pure noise $x_T \\sim \\mathcal{N}(0, I)$\n",
    "2. Iteratively denoise: $x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t, c) \\right) + \\sigma_t z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_ddpm(model, scheduler, n_samples, condition=None, device='cpu'):\n",
    "    \"\"\"Sample from DDPM model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained score network\n",
    "        scheduler: Noise scheduler\n",
    "        n_samples: Number of samples to generate\n",
    "        condition: Condition labels [n_samples] (optional)\n",
    "        device: Device to run on\n",
    "    \n",
    "    Returns:\n",
    "        Generated samples [n_samples, input_dim]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Start from pure noise\n",
    "    x = torch.randn(n_samples, model.input_dim, device=device)\n",
    "    \n",
    "    if condition is not None:\n",
    "        condition = condition.to(device)\n",
    "    \n",
    "    # Reverse diffusion\n",
    "    for t in tqdm(reversed(range(scheduler.num_timesteps)), desc=\"Sampling\", total=scheduler.num_timesteps):\n",
    "        t_batch = torch.full((n_samples,), t, device=device, dtype=torch.long)\n",
    "        \n",
    "        # Predict noise\n",
    "        noise_pred = model(x, t_batch, condition)\n",
    "        \n",
    "        # Get scheduler coefficients\n",
    "        alpha_t = scheduler.alphas[t]\n",
    "        alpha_bar_t = scheduler.alphas_cumprod[t]\n",
    "        beta_t = scheduler.betas[t]\n",
    "        \n",
    "        # Compute mean\n",
    "        mean = (1 / torch.sqrt(alpha_t)) * (\n",
    "            x - (beta_t / torch.sqrt(1 - alpha_bar_t)) * noise_pred\n",
    "        )\n",
    "        \n",
    "        # Add noise (except at t=0)\n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(x)\n",
    "            sigma_t = torch.sqrt(scheduler.posterior_variance[t])\n",
    "            x = mean + sigma_t * noise\n",
    "        else:\n",
    "            x = mean\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples for each cell type\n",
    "n_samples_per_type = 50\n",
    "generated_samples = []\n",
    "generated_labels = []\n",
    "\n",
    "for cell_type_idx in range(dataset.n_conditions):\n",
    "    condition = torch.full((n_samples_per_type,), cell_type_idx, dtype=torch.long)\n",
    "    samples = sample_ddpm(model, scheduler, n_samples_per_type, condition, device=device)\n",
    "    generated_samples.append(samples.cpu())\n",
    "    generated_labels.extend([cell_type_idx] * n_samples_per_type)\n",
    "\n",
    "generated_samples = torch.cat(generated_samples, dim=0).numpy()\n",
    "generated_labels = np.array(generated_labels)\n",
    "\n",
    "print(f\"Generated {generated_samples.shape[0]} samples\")\n",
    "print(f\"Sample shape: {generated_samples.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "\n",
    "Compare generated vs real gene expression distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions\n",
    "real_data = dataset.X.numpy()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Overall distribution\n",
    "axes[0].hist(real_data.flatten(), bins=50, alpha=0.5, label='Real', density=True)\n",
    "axes[0].hist(generated_samples.flatten(), bins=50, alpha=0.5, label='Generated', density=True)\n",
    "axes[0].set_title('Overall Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# Mean expression per gene\n",
    "axes[1].scatter(real_data.mean(axis=0), generated_samples.mean(axis=0), alpha=0.3)\n",
    "axes[1].plot([real_data.mean(axis=0).min(), real_data.mean(axis=0).max()],\n",
    "             [real_data.mean(axis=0).min(), real_data.mean(axis=0).max()],\n",
    "             'r--', alpha=0.5)\n",
    "axes[1].set_xlabel('Real mean expression')\n",
    "axes[1].set_ylabel('Generated mean expression')\n",
    "axes[1].set_title('Mean Expression per Gene')\n",
    "\n",
    "# Std expression per gene\n",
    "axes[2].scatter(real_data.std(axis=0), generated_samples.std(axis=0), alpha=0.3)\n",
    "axes[2].plot([real_data.std(axis=0).min(), real_data.std(axis=0).max()],\n",
    "             [real_data.std(axis=0).min(), real_data.std(axis=0).max()],\n",
    "             'r--', alpha=0.5)\n",
    "axes[2].set_xlabel('Real std expression')\n",
    "axes[2].set_ylabel('Generated std expression')\n",
    "axes[2].set_title('Std Expression per Gene')\n",
    "\n",
    "# Sample a few genes and compare distributions\n",
    "for i, gene_idx in enumerate([0, 10, 50]):\n",
    "    axes[3 + i].hist(real_data[:, gene_idx], bins=30, alpha=0.5, label='Real', density=True)\n",
    "    axes[3 + i].hist(generated_samples[:, gene_idx], bins=30, alpha=0.5, label='Generated', density=True)\n",
    "    axes[3 + i].set_title(f'Gene {gene_idx}')\n",
    "    axes[3 + i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps: Extending to Drug-Response Prediction\n",
    "\n",
    "To implement the scPPDM approach for perturbation response:\n",
    "\n",
    "### Architecture Changes:\n",
    "1. **Input**: Concatenate baseline expression + drug embedding\n",
    "2. **Output**: Predict perturbed expression (not noise)\n",
    "3. **Conditioning**: Drug type + dose\n",
    "\n",
    "### Data Requirements:\n",
    "- Paired samples: (baseline, drug, dose, perturbed_expression)\n",
    "- Examples: Sci-Plex, LINCS L1000, Replogle et al. Perturb-seq\n",
    "\n",
    "### Modified Forward Process:\n",
    "```python\n",
    "# Instead of: x_t = sqrt(alpha_bar) * x_0 + sqrt(1 - alpha_bar) * noise\n",
    "# Use: x_t = sqrt(alpha_bar) * x_perturbed + sqrt(1 - alpha_bar) * noise\n",
    "# Condition on: [x_baseline, drug_embedding, dose]\n",
    "```\n",
    "\n",
    "### Training:\n",
    "```python\n",
    "# Predict perturbed expression from baseline + drug\n",
    "def forward(x_baseline, drug, dose, t):\n",
    "    # Encode drug\n",
    "    drug_emb = drug_encoder(drug, dose)\n",
    "    \n",
    "    # Concatenate baseline + drug info\n",
    "    condition = torch.cat([x_baseline, drug_emb], dim=-1)\n",
    "    \n",
    "    # Predict noise for perturbed expression\n",
    "    noise_pred = score_network(x_t, t, condition)\n",
    "    \n",
    "    return noise_pred\n",
    "```\n",
    "\n",
    "### Sampling:\n",
    "```python\n",
    "# Generate counterfactual response\n",
    "x_perturbed = sample_ddpm(\n",
    "    model,\n",
    "    scheduler,\n",
    "    condition={'baseline': x_baseline, 'drug': drug_id, 'dose': dose_value}\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've implemented:\n",
    "1. ✅ Noise scheduler (linear beta schedule)\n",
    "2. ✅ Forward diffusion (adding noise)\n",
    "3. ✅ Time-conditional score network (MLP for tabular data)\n",
    "4. ✅ Training loop (simple MSE loss)\n",
    "5. ✅ Sampling (reverse diffusion)\n",
    "6. ✅ Conditional generation (cell type → expression)\n",
    "\n",
    "**Next notebook**: Implement full scPPDM for drug-response prediction with perturbation datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
