{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"genai-lab","text":"<p>Generative AI for Computational Biology: Research into foundation models and generative methods for accelerating drug discovery, understanding treatment responses, and enabling in silico biological experimentation.</p>"},{"location":"#overview","title":"Overview","text":"<p>This project investigates generative modeling approaches across computational biology, inspired by emerging platforms such as:</p> <ul> <li>Gene Expression: Synthesize Bio (GEM-1), Deep Genomics (BigRNA)</li> <li>DNA Sequence: Arc Institute (Evo 2), InstaDeep (Nucleotide Transformer)</li> <li>Single-Cell: Geneformer, scGPT</li> <li>Gene Editing: Profluent (OpenCRISPR)</li> </ul> <p>Research Goals:</p> <ol> <li>Investigate state-of-the-art generative architectures (VAE, flows, diffusion, transformers) for biological sequences and multi-omics data</li> <li>Develop reusable, modular components for conditional generation and counterfactual simulation</li> <li>Explore causal inference methods for predicting treatment responses and perturbation effects</li> <li>Contribute to the growing field of generative biology with reproducible implementations and benchmarks</li> </ol> <p>See docs/INDUSTRY_LANDSCAPE.md for a comprehensive survey of companies and technologies in this space.</p>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>genai-lab/\n\u251c\u2500\u2500 src/genailab/\n\u2502   \u251c\u2500\u2500 foundation/     # \ud83c\udd95 Foundation model adaptation framework\n\u2502   \u2502   \u251c\u2500\u2500 configs/        # Resource-aware model configs (small/medium/large)\n\u2502   \u2502   \u251c\u2500\u2500 tuning/         # LoRA, adapters, freezing strategies\n\u2502   \u2502   \u251c\u2500\u2500 conditioning/   # FiLM, cross-attention, CFG (planned)\n\u2502   \u2502   \u2514\u2500\u2500 recipes/        # End-to-end pipelines (planned)\n\u2502   \u251c\u2500\u2500 data/           # Data loading, transforms, preprocessing\n\u2502   \u2502   \u251c\u2500\u2500 paths.py        # Standardized data path management\n\u2502   \u2502   \u251c\u2500\u2500 sc_preprocess.py    # scRNA-seq preprocessing (Scanpy)\n\u2502   \u2502   \u2514\u2500\u2500 bulk_preprocess.py  # Bulk RNA-seq preprocessing\n\u2502   \u251c\u2500\u2500 model/          # Encoders, decoders, VAE, diffusion architectures\n\u2502   \u2502   \u251c\u2500\u2500 vae.py          # CVAE, CVAE_NB, CVAE_ZINB\n\u2502   \u2502   \u251c\u2500\u2500 encoders.py     # ConditionEncoder, etc.\n\u2502   \u2502   \u251c\u2500\u2500 decoders.py     # Gaussian, NB, ZINB decoders\n\u2502   \u2502   \u2514\u2500\u2500 diffusion/      # Diffusion models (DDPM, score networks)\n\u2502   \u251c\u2500\u2500 objectives/     # Loss functions, regularizers\n\u2502   \u2502   \u2514\u2500\u2500 losses.py       # ELBO, NB, ZINB losses\n\u2502   \u251c\u2500\u2500 eval/           # Metrics, diagnostics, plotting\n\u2502   \u251c\u2500\u2500 workflows/      # Training, simulation, benchmarking\n\u2502   \u2514\u2500\u2500 utils/          # Config, reproducibility\n\u251c\u2500\u2500 docs/               # Theory documents and derivations\n\u2502   \u251c\u2500\u2500 foundation_models/  # \ud83c\udd95 Foundation model adaptation\n\u2502   \u251c\u2500\u2500 DiT/            # \ud83c\udd95 Diffusion Transformers\n\u2502   \u251c\u2500\u2500 JEPA/           # \ud83c\udd95 Joint Embedding Predictive Architecture\n\u2502   \u251c\u2500\u2500 latent_diffusion/   # \ud83c\udd95 Latent diffusion for biology\n\u2502   \u251c\u2500\u2500 DDPM/           # Denoising Diffusion Probabilistic Models\n\u2502   \u251c\u2500\u2500 VAE/            # VAE theory and derivations\n\u2502   \u251c\u2500\u2500 EBM/            # Energy-based models\n\u2502   \u251c\u2500\u2500 score_matching/ # Score matching and energy functions\n\u2502   \u251c\u2500\u2500 flow_matching/  # Flow matching &amp; rectified flow\n\u2502   \u2514\u2500\u2500 datasets/       # Data preparation guides\n\u251c\u2500\u2500 notebooks/          # Educational tutorials (interactive learning)\n\u2502   \u251c\u2500\u2500 foundation_models/  # \ud83c\udd95 Foundation adaptation tutorials\n\u2502   \u251c\u2500\u2500 diffusion/      # Diffusion models tutorials\n\u2502   \u251c\u2500\u2500 vae/            # VAE tutorials\n\u2502   \u2514\u2500\u2500 foundations/    # Mathematical foundations\n\u251c\u2500\u2500 examples/           # Production scripts (real-world applications)\n\u2502   \u251c\u2500\u2500 perturbation/   # Drug response, perturbation prediction\n\u2502   \u2514\u2500\u2500 utils/          # Helper modules for examples\n\u251c\u2500\u2500 scripts/            # Training scripts with CLI\n\u2502   \u2514\u2500\u2500 diffusion/      # Diffusion model training scripts\n\u251c\u2500\u2500 data/               # Local data storage (gitignored)\n\u251c\u2500\u2500 tests/\n\u2514\u2500\u2500 environment.yml     # Conda environment specification\n</code></pre>"},{"location":"#documentation-learning-resources","title":"Documentation &amp; Learning Resources","text":""},{"location":"#theory-documents-docs","title":"Theory Documents (<code>docs/</code>)","text":"<p>Detailed theory, derivations, and mathematical foundations:</p> Topic Description Start Here \ud83c\udd95 foundation_models Foundation model adaptation (LoRA, adapters, freezing) leveraging_foundation_models_v2.md \ud83c\udd95 DiT Diffusion Transformers (architecture, training, sampling) README.md \ud83c\udd95 JEPA Joint Embedding Predictive Architecture README.md \ud83c\udd95 latent_diffusion Latent diffusion with NB/ZINB decoders README.md DDPM Denoising Diffusion Probabilistic Models README.md VAE Variational Autoencoders (ELBO, inference, training) VAE-01-overview.md beta-VAE VAE with disentanglement (\u03b2 parameter) beta_vae.md EBM Energy-Based Models (Boltzmann, partition functions) README.md score_matching Score functions, Fisher vs Stein scores README.md flow_matching Flow matching &amp; rectified flow README.md datasets Datasets &amp; preprocessing pipelines README.md incubation Ideas under development README.md"},{"location":"#ideas-under-incubation-docsincubation","title":"Ideas Under Incubation (<code>docs/incubation/</code>)","text":"<p>Exploratory architectural proposals and application ideas not yet implemented:</p> Document Focus joint_latent_space_and_JEPA.md Joint latent spaces for static/dynamic data, JEPA for Perturb-seq generative-ai-for-gene-expression-prediction.md Diffusion/VAE/Flow for gene expression with uncertainty generative-ai-for-perturbation-modeling.md Generative approaches for scPerturb, beyond GEM-1 <p>Key insights from incubation:</p> <ul> <li>Joint latent spaces: Static (bulk RNA-seq) and dynamic (Perturb-seq) data can share the same manifold</li> <li>JEPA over reconstruction: Predicting embeddings is more robust for biology</li> <li>Hybrid predictive-generative: Combine GEM-1-style predictors with generative wrappers for uncertainty</li> </ul>"},{"location":"#interactive-tutorials-notebooks","title":"Interactive Tutorials (<code>notebooks/</code>)","text":"<p>Educational Jupyter notebooks for hands-on learning:</p> Topic Description Start Here \ud83c\udd95 foundation_models Foundation model adaptation (LoRA, adapters, resource management) README.md diffusion Diffusion models (DDPM, score-based, flow matching) 01_ddpm_basics.ipynb vae VAE tutorials (coming soon) - foundations Mathematical foundations (coming soon) - <p>See notebooks/README.md for learning paths and progression.</p>"},{"location":"#production-examples-examples","title":"Production Examples (<code>examples/</code>)","text":"<p>Ready-to-use Python scripts for real-world applications:</p> <ul> <li><code>01_bulk_cvae.ipynb</code> \u2014 Train CVAE on bulk RNA-seq</li> <li><code>02_pbmc3k_cvae_nb.ipynb</code> \u2014 Train CVAE with NB decoder on scRNA-seq</li> <li><code>perturbation/</code> \u2014 Drug response and perturbation prediction (coming soon)</li> </ul> <p>How to use:</p> <ul> <li>Learning: Start with <code>notebooks/</code> for interactive tutorials</li> <li>Theory: Reference <code>docs/</code> for detailed derivations</li> <li>Application: Use <code>examples/</code> for production workflows</li> <li>Follow the ROADMAP for structured progression</li> </ul>"},{"location":"#installation","title":"Installation","text":""},{"location":"#using-mamba-poetry-recommended","title":"Using mamba + poetry (recommended)","text":"<pre><code># Create conda environment\nmamba create -n genailab python=3.11 -y\nmamba activate genailab\n\n# Install poetry if not available\npip install poetry\n\n# Install package in editable mode\npoetry install\n\n# Optional: install bio dependencies (scanpy, anndata)\npoetry install --with bio\n\n# Optional: install dev dependencies\npoetry install --with dev\n</code></pre>"},{"location":"#quick-start","title":"Quick start","text":"<pre><code># Verify installation\npython -c \"import genailab; print(genailab.__version__)\"\n\n# Run toy training (once implemented)\ngenailab-train --config configs/cvae_toy.yaml\n</code></pre>"},{"location":"#milestones","title":"Milestones","text":""},{"location":"#stage-1-variational-autoencoders","title":"Stage 1: Variational Autoencoders \u2705","text":"<ul> <li> Core CVAE implementation with condition encoding</li> <li> Gaussian decoder (MSE reconstruction)</li> <li> Negative Binomial decoder for count data (<code>CVAE_NB</code>)</li> <li> Zero-Inflated Negative Binomial decoder (<code>CVAE_ZINB</code>)</li> <li> ELBO loss with KL annealing support</li> <li> Comprehensive documentation (VAE-01 through VAE-09)</li> <li> Unit tests for all model variants</li> </ul>"},{"location":"#stage-2-data-pipeline","title":"Stage 2: Data Pipeline \u2705","text":"<ul> <li> Standardized data path management (<code>genailab.data.paths</code>)</li> <li> scRNA-seq preprocessing with Scanpy</li> <li> Bulk RNA-seq preprocessing (Python + R/recount3)</li> <li> Environment setup (conda/mamba + Poetry)</li> <li> Data preparation documentation</li> </ul>"},{"location":"#stage-3-score-matching-energy-functions","title":"Stage 3: Score Matching &amp; Energy Functions \u2705","text":"<ul> <li> Score matching overview documentation</li> <li> Energy functions deep dive (Boltzmann, partition function)</li> <li> VP-SDE and VE-SDE formulations</li> <li> Denoising score matching loss</li> </ul>"},{"location":"#stage-4-diffusion-models","title":"Stage 4: Diffusion Models \u2705","text":"<ul> <li> Forward/reverse diffusion process (VP-SDE, VE-SDE)</li> <li> Score networks (MLP, TabularScoreNetwork, UNet2D, UNet3D)</li> <li> Medical imaging diffusion (synthetic X-rays)</li> <li> Training scripts with configurable model sizes</li> <li> RunPod setup documentation for GPU training</li> <li> Comprehensive DDPM documentation series</li> <li> Gene expression architectures (latent tokens, pathway tokens)</li> <li> Conditional generation with classifier-free guidance</li> <li> Flow matching implementation</li> </ul>"},{"location":"#stage-5-foundation-model-adaptation","title":"Stage 5: Foundation Model Adaptation \u2705","text":"<ul> <li> Resource-aware model configurations (small/medium/large)</li> <li> Auto-detection of hardware (M1 Mac, RunPod, Cloud)</li> <li> LoRA (Low-Rank Adaptation) implementation</li> <li> Comprehensive documentation (DiT, JEPA, Latent Diffusion)</li> <li> Adapters and freezing strategies</li> <li> Conditioning modules (FiLM, cross-attention, CFG)</li> <li> Tutorial notebooks for each adaptation pattern</li> <li> End-to-end recipes for gene expression tasks</li> </ul>"},{"location":"#stage-6-advanced-architectures","title":"Stage 6: Advanced Architectures \ud83d\udcdd","text":"<ul> <li> DiT (Diffusion Transformers) documentation</li> <li> JEPA (Joint Embedding Predictive Architecture) documentation</li> <li> Latent Diffusion documentation</li> <li> DiT implementation for gene expression</li> <li> JEPA implementation for Perturb-seq</li> <li> Flow matching implementation</li> </ul>"},{"location":"#stage-7-counterfactual-causal-planned","title":"Stage 7: Counterfactual &amp; Causal (Planned)","text":"<ul> <li> Counterfactual generation pipeline</li> <li> Deconfounding / SCM-flavored latent model</li> <li> Causal regularization via invariance</li> </ul>"},{"location":"#industry-landscape","title":"Industry Landscape","text":"<p>Companies and platforms pioneering generative AI for drug discovery and biological research:</p>"},{"location":"#gene-expression-multi-omics-foundation-models","title":"Gene Expression &amp; Multi-Omics Foundation Models","text":"Company Focus Key Technology Synthesize Bio Gene expression generation GEM-1 foundation model Ochre Bio Liver disease, RNA therapeutics Functional genomics + AI Deep Genomics RNA biology &amp; therapeutics BigRNA (~2B params) Helical DNA/RNA foundation models Helix-mRNA, open-source platform Noetik Cancer biology OCTO model for treatment prediction"},{"location":"#protein-structure-based-discovery","title":"Protein &amp; Structure-Based Discovery","text":"Company Focus Key Technology Isomorphic Labs Drug discovery (DeepMind spin-off) AlphaFold 3 EvolutionaryScale Protein design ESM3 generative model Generate:Biomedicines Protein therapeutics Generative Biology\u2122 platform Chai Discovery Molecular structure Chai-\u00bd (antibody design) Recursion Phenomics + drug discovery Phenom-Beta, BioHive-2"},{"location":"#clinical-treatment-response","title":"Clinical &amp; Treatment Response","text":"Company Focus Key Technology Insilico Medicine End-to-end drug discovery Pharma.AI, Precious3GPT Tempus Precision medicine AI-driven clinical insights Owkin Clinical trials, pathology Federated learning Retro Biosciences Cellular reprogramming GPT-4b micro (with OpenAI)"},{"location":"#other-notable-players","title":"Other Notable Players","text":"<ul> <li>BioMap \u2014 xTrimo (210B params, multi-modal)</li> <li>Ginkgo Bioworks \u2014 Synthetic biology + Google Cloud partnership</li> <li>Bioptimus \u2014 H-Optimus-0 pathology foundation model</li> <li>Atomic AI \u2014 RNA structure (ATOM-1, PARSE platform)</li> <li>Enveda Biosciences \u2014 PRISM for small molecule discovery</li> </ul>"},{"location":"#references","title":"References","text":""},{"location":"#academic","title":"Academic","text":"<ul> <li>Geneformer \u2014 Transfer learning for single-cell biology</li> <li>scVI \u2014 Probabilistic modeling of scRNA-seq</li> <li>CPA \u2014 Compositional Perturbation Autoencoder</li> </ul>"},{"location":"#industry","title":"Industry","text":"<ul> <li>Synthesize Bio Blog</li> <li>17 Companies Pioneering AI Foundation Models in Pharma</li> <li>NVIDIA BioNeMo Platform</li> </ul>"},{"location":"#related-projects","title":"Related Projects","text":""},{"location":"#causal-bio-lab-causal-aiml-for-computational-biology","title":"causal-bio-lab \u2014 Causal AI/ML for Computational Biology","text":"<p>Complementary Focus: While <code>genai-lab</code> focuses on modeling data-generating processes through generative models, <code>causal-bio-lab</code> focuses on uncovering causal structures and estimating causal effects from observational and interventional data.</p> <p>Synergy:</p> <ul> <li>Generative models (VAE, diffusion) can learn rich representations but may capture spurious correlations</li> <li>Causal methods (probabilistic graphical models, causal discovery, structural equations) ensure models capture true mechanisms, not just statistical patterns</li> <li>Together: Causal generative models combine the best of both worlds\u2014realistic simulation with causal guarantees</li> </ul> <p>Key Integration Points:</p> <ol> <li>Causal representation learning: Learn disentangled latent spaces that respect causal structure (causal VAEs, identifiable VAEs)</li> <li>Causal discovery for architecture: Use learned causal graphs to constrain generative model structure</li> <li>Counterfactual validation: Use causal inference methods (do-calculus, structural equations) to validate generated predictions</li> <li>Causal regularization: Apply invariance principles and interventional consistency losses for better generalization</li> </ol> <p>Example Workflow:</p> <pre><code>1. Train a CVAE on gene expression data (genai-lab)\n2. Discover causal gene regulatory network (causal-bio-lab)\n3. Constrain VAE latent space to respect causal structure\n4. Generate counterfactual perturbation responses with causal guarantees\n5. Estimate treatment effects using both generative and causal methods\n</code></pre> <p>Why This Matters for Computational Biology:</p> <ul> <li>Drug discovery: Generate realistic molecular perturbations while ensuring causal mechanisms are preserved</li> <li>Treatment response: Predict individual-level effects (counterfactuals) with uncertainty quantification</li> <li>Target identification: Discover causal drivers, not just biomarkers</li> <li>Combination therapy: Model synergistic effects through causal interaction terms</li> </ul> <p>See <code>causal-bio-lab</code> Milestone 0.5 (SCMs) and Milestone D (Causal Representation Learning) for integration work.</p>"},{"location":"#license","title":"License","text":"<p>MIT</p>"},{"location":"INDUSTRY_LANDSCAPE/","title":"Industry Landscape: Generative AI in Drug Discovery &amp; Computational Biology","text":"<p>This document surveys companies and platforms pioneering generative AI, foundation models, and machine learning approaches for drug discovery, treatment response prediction, and biological research.</p> <p>Last Updated: December 2024 Purpose: Track industry developments, identify research directions, and gather ideas for this project.</p>"},{"location":"INDUSTRY_LANDSCAPE/#table-of-contents","title":"Table of Contents","text":"<ol> <li>DNA Foundation Models &amp; Sequence Generation</li> <li>Single-Cell Foundation Models</li> <li>Gene Expression &amp; Multi-Omics</li> <li>Splicing &amp; RNA Processing</li> <li>Protein &amp; Structure-Based Discovery</li> <li>Gene Editing &amp; CRISPR</li> <li>Clinical &amp; Treatment Response</li> <li>AI-Driven Target Discovery</li> <li>Key Observations &amp; Research Directions</li> </ol>"},{"location":"INDUSTRY_LANDSCAPE/#dna-foundation-models-sequence-generation","title":"DNA Foundation Models &amp; Sequence Generation","text":"<p>Foundation models for DNA sequence understanding and generation \u2014 enabling synthetic genome design, variant effect prediction, and regulatory element discovery.</p>"},{"location":"INDUSTRY_LANDSCAPE/#evo-2-arc-institute","title":"Evo 2 (Arc Institute)","text":"Website arcinstitute.org/tools/evo Focus DNA foundation model for generalist prediction and design Key Technology Evo 2 (40B parameters, 1M context length) Partners NVIDIA <p>What They Do:</p> <ul> <li>Genomic foundation model for DNA, RNA, and protein tasks</li> <li>Single-nucleotide resolution with near-linear scaling</li> <li>Trained on 9+ trillion nucleotides from 128,000+ species (all domains of life)</li> <li>Generative design of synthetic DNA sequences</li> </ul> <p>Technical Details:</p> <ul> <li>40 billion parameters</li> <li>1 megabase (1M tokens) context length</li> <li>Frontier deep learning architecture</li> <li>Evo Designer tool for sequence generation</li> <li>Open source on GitHub and Hugging Face</li> </ul> <p>Relevance to This Project: \u2b50\u2b50\u2b50\u2b50\u2b50</p> <ul> <li>State-of-the-art in DNA generation</li> <li>Long-range context is critical for genomics</li> <li>Open source enables direct experimentation</li> </ul> <p>Resources:</p> <ul> <li>Evo 2 Preprint</li> <li>Evo 1 in Science (Nov 2024)</li> <li>GitHub</li> </ul>"},{"location":"INDUSTRY_LANDSCAPE/#nucleotide-transformer-instadeep","title":"Nucleotide Transformer (InstaDeep)","text":"Website instadeep.com Focus DNA foundation models for molecular phenotype prediction Key Technology Nucleotide Transformer (up to 2.5B parameters) Published Nature Methods (Nov 2024) <p>What They Do:</p> <ul> <li>Foundation models pre-trained on DNA sequences</li> <li>Transfer learning for genomic tasks with limited labeled data</li> <li>Multi-species genome understanding</li> </ul> <p>Technical Details:</p> <ul> <li>Models from 50M to 2.5B parameters</li> <li>Multispecies 2.5B outperforms single-species models</li> <li>18 benchmark tasks for evaluation</li> <li>Interactive leaderboard for comparison</li> </ul> <p>Relevance to This Project: \u2b50\u2b50\u2b50\u2b50</p> <ul> <li>Established benchmark for DNA foundation models</li> <li>Transfer learning approach applicable to expression</li> </ul> <p>Resources:</p> <ul> <li>GitHub</li> <li>Nature Methods Paper</li> </ul>"},{"location":"INDUSTRY_LANDSCAPE/#hyenadna-stanfordhazy-research","title":"HyenaDNA (Stanford/Hazy Research)","text":"Website GitHub Focus Long-range genomic sequence modeling Key Technology HyenaDNA (up to 1M context) Architecture Hyena (sub-quadratic attention alternative) <p>What They Do:</p> <ul> <li>Genomic foundation model with ultra-long context</li> <li>Single nucleotide resolution</li> <li>Pre-trained on human reference genome</li> </ul> <p>Technical Details:</p> <ul> <li>Up to 1 million token context (500x increase over dense attention)</li> <li>Hyena operator for efficient long-range modeling</li> <li>Fine-tunable for downstream tasks</li> </ul> <p>Relevance to This Project: \u2b50\u2b50\u2b50\u2b50</p> <ul> <li>Efficient architecture for long sequences</li> <li>Applicable to regulatory element prediction</li> </ul>"},{"location":"INDUSTRY_LANDSCAPE/#caduceus-cornell","title":"Caduceus (Cornell)","text":"Website caduceus-dna.github.io Focus Bi-directional DNA sequence modeling Key Technology Caduceus (BiMamba + RC equivariance) Architecture Mamba-based <p>What They Do:</p> <ul> <li>First family of RC (reverse complement) equivariant DNA models</li> <li>Bi-directional modeling for upstream/downstream context</li> <li>Long-range sequence modeling</li> </ul> <p>Technical Details:</p> <ul> <li>BiMamba: bi-directional Mamba block</li> <li>MambaDNA: RC equivariant extension</li> <li>Handles biological symmetry of DNA strands</li> </ul> <p>Relevance to This Project: \u2b50\u2b50\u2b50\u2b50</p> <ul> <li>Novel architecture addressing DNA-specific challenges</li> <li>Mamba efficiency for long sequences</li> </ul>"},{"location":"INDUSTRY_LANDSCAPE/#dnabert-dnabert-2","title":"DNABERT / DNABERT-2","text":"Website GitHub Focus Pre-trained bidirectional encoder for DNA Key Technology DNABERT-2 (ICLR 2024) Benchmark Genome Understanding Evaluation (GUE) <p>What They Do:</p> <ul> <li>BERT-style pre-training for DNA sequences</li> <li>Multi-species genome understanding</li> <li>DNABERT-S for DNA embeddings that cluster by genome</li> </ul> <p>Technical Details:</p> <ul> <li>Efficient foundation model</li> <li>28 datasets in GUE benchmark</li> <li>Transfer learning to downstream tasks</li> </ul> <p>Relevance to This Project: \u2b50\u2b50\u2b50</p> <ul> <li>Established baseline for DNA language models</li> <li>Comprehensive benchmark suite</li> </ul>"},{"location":"INDUSTRY_LANDSCAPE/#single-cell-foundation-models","title":"Single-Cell Foundation Models","text":"<p>Foundation models specifically designed for single-cell transcriptomics data.</p>"},{"location":"INDUSTRY_LANDSCAPE/#geneformer","title":"Geneformer","text":"Website Hugging Face Focus Transfer learning for single-cell biology Key Technology Geneformer-V2 (104M-316M parameters) Published Nature (2023) <p>What They Do:</p> <ul> <li>Foundation model for context-specific gene network predictions</li> <li>Pre-trained on ~104M human single-cell transcriptomes</li> <li>Zero-shot and fine-tuning capabilities</li> </ul> <p>Technical Details:</p> <ul> <li>Geneformer-V2-316M: latest model (Dec 2024)</li> <li>Input size: 4096 tokens</li> <li>Vocabulary: ~20K protein-coding genes</li> <li>Rank-value encoding of gene expression</li> </ul> <p>Relevance to This Project: \u2b50\u2b50\u2b50\u2b50\u2b50</p> <ul> <li>Primary inspiration for transformer-based expression modeling</li> <li>Representation learning that can be extended to generation</li> </ul> <p>Resources:</p> <ul> <li>Nature Paper</li> <li>Hugging Face</li> </ul>"},{"location":"INDUSTRY_LANDSCAPE/#scgpt","title":"scGPT","text":"Website GitHub Focus Generative pre-trained transformer for single-cell Key Technology scGPT Published Nature Methods (2024) <p>What They Do:</p> <ul> <li>Foundation model for single-cell multi-omics</li> <li>Generative capabilities for cell state prediction</li> <li>Pre-trained on 33+ million cells</li> </ul> <p>Technical Details:</p> <ul> <li>Generative pre-trained transformer architecture</li> <li>Multi-task learning: cell type annotation, perturbation prediction, multi-omics integration</li> <li>Attention-based gene-gene interaction modeling</li> </ul> <p>Relevance to This Project: \u2b50\u2b50\u2b50\u2b50\u2b50</p> <ul> <li>Directly relevant \u2014 generative model for single-cell</li> <li>Perturbation prediction aligns with counterfactual goals</li> </ul> <p>Resources:</p> <ul> <li>Nature Methods Paper</li> </ul>"},{"location":"INDUSTRY_LANDSCAPE/#gene-expression-multi-omics-foundation-models","title":"Gene Expression &amp; Multi-Omics Foundation Models","text":"<p>Companies building foundation models specifically for gene expression, transcriptomics, and multi-omics data.</p>"},{"location":"INDUSTRY_LANDSCAPE/#synthesize-bio","title":"Synthesize Bio","text":"Website https://www.synthesize.bio/ Focus Generative foundation models for gene expression Key Technology GEM-1 \u2014 Gene Expression Model Approach Generate biologically realistic gene expression data in silico <p>What They Do:</p> <ul> <li>Build generative models that can simulate gene expression profiles under various conditions</li> <li>Enable in silico experimentation that bridges wet lab and computation</li> <li>Multi-omics and public + private data harmonization</li> <li>Platform for drug discovery, hypothesis testing, and clinical decision support</li> </ul> <p>Relevance to This Project: \u2b50\u2b50\u2b50\u2b50\u2b50 (Primary inspiration) - Direct alignment with our cVAE and counterfactual simulation goals - Their GEM-1 represents the state-of-the-art we're studying</p> <p>Key Blog Posts:</p> <ul> <li>https://www.synthesize.bio/blog</li> </ul>"},{"location":"INDUSTRY_LANDSCAPE/#deep-genomics","title":"Deep Genomics","text":"Website https://www.deepgenomics.com/ Focus RNA biology and therapeutics Key Technology BigRNA (~2 billion parameters) Founded 2014 (Toronto) <p>What They Do:</p> <ul> <li>First transformer neural network engineered specifically for transcriptomics</li> <li>Predicts tissue-specific regulatory mechanisms of RNA expression</li> <li>Predicts binding sites of proteins and microRNAs</li> <li>Predicts effects of genetic variants and therapeutic candidates</li> </ul> <p>Technical Details:</p> <ul> <li>~2 billion adjustable parameters</li> <li>Trained on thousands of datasets (&gt;1 trillion genomic signals)</li> <li>Designed to understand complex RNA interactions</li> </ul> <p>Relevance to This Project: \u2b50\u2b50\u2b50\u2b50\u2b50 - BigRNA is a foundation model for RNA/transcriptomics \u2014 directly relevant - Their approach to predicting variant effects aligns with counterfactual reasoning</p>"},{"location":"INDUSTRY_LANDSCAPE/#helical","title":"Helical","text":"Website https://www.helical.ai/ Focus Open-source DNA/RNA foundation models Key Technology Helix-mRNA (hybrid foundation model) Founded 2023 (Luxembourg) Funding \u20ac2.2M seed (June 2024) <p>What They Do:</p> <ul> <li>First open-source platform dedicated to bio foundation models for DNA and RNA</li> <li>Democratize access to advanced AI tools for pharma/biotech</li> <li>Library of Bio AI Agents for tasks like biomarker discovery and target prediction</li> </ul> <p>Technical Details:</p> <ul> <li>Helix-mRNA: hybrid foundation model for mRNA therapeutics</li> <li>Outperforms prior methods in modeling UTRs and long-sequence regions</li> <li>Uses only ~10% of parameters of comparable models</li> <li>Available on AWS Marketplace</li> </ul> <p>Relevance to This Project: \u2b50\u2b50\u2b50\u2b50 - Open-source focus aligns with our goals - Could potentially integrate their models or learn from their architecture</p>"},{"location":"INDUSTRY_LANDSCAPE/#noetik","title":"Noetik","text":"Website https://www.noetik.ai/ Focus Cancer biology and treatment prediction Key Technology OCTO model Founded 2022 (San Francisco) Funding $40M Series A (2024) <p>What They Do:</p> <ul> <li>AI model that acts like a virtual lab for cancer research</li> <li>Predicts how different cancer treatments might play out in real patients</li> <li>Tests \"what if\" scenarios for treatment optimization</li> </ul> <p>Technical Details:</p> <ul> <li>OCTO trained on thousands of tumor samples</li> <li>Integrates gene expression, protein data, and cell images</li> <li>Predicts how tweaking a single gene could change protein levels across a tumor</li> <li>In vivo CRISPR Perturb-Map platform for validation</li> </ul> <p>Relevance to This Project: \u2b50\u2b50\u2b50\u2b50 - Their \"what if\" scenario testing is exactly counterfactual reasoning - Multi-modal integration (expression + protein + images) is advanced</p>"},{"location":"INDUSTRY_LANDSCAPE/#biomap","title":"BioMap","text":"Website https://www.biomap.com/ Focus Multi-modal biological foundation models Key Technology xTrimo (~210 billion parameters) Partnerships Sanofi (\\(10M upfront, &gt;\\)1B potential milestones) <p>What They Do:</p> <ul> <li>World's largest life science foundation model</li> <li>Supports DNA, RNA, protein, cellular, and systems-level modalities</li> <li>Designed to understand and predict biological behavior across multiple modalities</li> </ul> <p>Technical Details:</p> <ul> <li>~210 billion parameters (as of 2025)</li> <li>Cross-Modal Transformer Representation of Interactome and Multi-Omics</li> <li>GPU-accelerated deployment using multi-expert architectures and FP8 precision</li> </ul> <p>Relevance to This Project: \u2b50\u2b50\u2b50\u2b50 - Multi-modal approach is the future direction - Scale demonstrates what's possible with sufficient resources</p>"},{"location":"INDUSTRY_LANDSCAPE/#splicing-rna-processing","title":"Splicing &amp; RNA Processing","text":"<p>AI models for predicting and understanding alternative splicing, splice site recognition, and RNA processing \u2014 directly relevant to the meta-spliceai project.</p>"},{"location":"INDUSTRY_LANDSCAPE/#spliceai-illumina","title":"SpliceAI (Illumina)","text":"Website GitHub Focus Deep learning for splice site prediction Key Technology SpliceAI Published Cell (2019) <p>What They Do:</p> <ul> <li>Predict splicing alterations from DNA sequence</li> <li>Identify cryptic splice sites created by variants</li> <li>Score variant pathogenicity based on splicing impact</li> </ul> <p>Technical Details:</p> <ul> <li>Deep residual neural network</li> <li>10,000 nucleotide context window</li> <li>Predicts donor/acceptor gain/loss</li> <li>Pre-computed scores available for all SNVs</li> </ul> <p>Relevance to This Project: \u2b50\u2b50\u2b50\u2b50\u2b50</p> <ul> <li>Foundation for meta-spliceai project</li> <li>Demonstrates deep learning for splicing prediction</li> <li>Well-established benchmark</li> </ul> <p>Resources:</p> <ul> <li>GitHub</li> <li>Cell Paper</li> </ul>"},{"location":"INDUSTRY_LANDSCAPE/#splam-johns-hopkins","title":"Splam (Johns Hopkins)","text":"Website GitHub Focus Splice junction recognition Key Technology Splam Published 2024 <p>What They Do:</p> <ul> <li>Improved splice junction recognition over SpliceAI</li> <li>Better accuracy with less genomic data</li> <li>Cross-species generalization</li> </ul> <p>Technical Details:</p> <ul> <li>Deep learning model for splice sites</li> <li>Outperforms SpliceAI on benchmarks</li> <li>Generalizes across species</li> </ul> <p>Relevance to This Project: \u2b50\u2b50\u2b50\u2b50</p> <ul> <li>State-of-the-art in splice prediction</li> <li>Cross-species transfer learning</li> </ul>"},{"location":"INDUSTRY_LANDSCAPE/#pangolin","title":"Pangolin","text":"Focus Tissue-specific splicing prediction Key Technology Pangolin <p>What They Do:</p> <ul> <li>Predict tissue-specific alternative splicing</li> <li>Model splicing quantitative trait loci (sQTLs)</li> </ul> <p>Relevance to This Project: \u2b50\u2b50\u2b50\u2b50</p> <ul> <li>Tissue-specific conditioning aligns with our cVAE approach</li> <li>Splicing as a form of gene regulation</li> </ul>"},{"location":"INDUSTRY_LANDSCAPE/#gene-editing-crispr","title":"Gene Editing &amp; CRISPR","text":"<p>Generative AI for designing gene editors and optimizing CRISPR systems.</p>"},{"location":"INDUSTRY_LANDSCAPE/#profluent-bio","title":"Profluent Bio","text":"Website profluent.bio Focus AI-designed gene editors Key Technology OpenCRISPR-1 Published Nature (2025) <p>What They Do:</p> <ul> <li>First AI-designed gene editor to edit human genome</li> <li>Generative AI creates novel CRISPR proteins</li> <li>Open-source release of OpenCRISPR-1</li> </ul> <p>Technical Details:</p> <ul> <li>Large language models trained on CRISPR-Cas sequences</li> <li>Generated novel, functional genome editors</li> <li>Improved properties vs natural systems</li> <li>RNA-programmable with NGG PAM preference</li> </ul> <p>Relevance to This Project: \u2b50\u2b50\u2b50\u2b50\u2b50</p> <ul> <li>Demonstrates generative AI for protein design</li> <li>Open-source enables experimentation</li> <li>LLM approach to biological sequences</li> </ul> <p>Resources:</p> <ul> <li>GitHub</li> <li>Nature Paper</li> </ul>"},{"location":"INDUSTRY_LANDSCAPE/#protein-structure-based-discovery","title":"Protein &amp; Structure-Based Discovery","text":"<p>Companies focused on protein structure prediction, design, and protein-based therapeutics.</p>"},{"location":"INDUSTRY_LANDSCAPE/#isomorphic-labs","title":"Isomorphic Labs","text":"Website https://www.isomorphiclabs.com/ Focus AI-first drug discovery Key Technology AlphaFold 3 Parent Alphabet (DeepMind spin-off, 2021) <p>What They Do:</p> <ul> <li>Reimagining drug discovery from first principles with AI-first approach</li> <li>Expanded from small molecules to biologics</li> <li>Internal pipeline focused on oncology and immunology</li> </ul> <p>Technical Details:</p> <ul> <li>AlphaFold 3: predicts structure of proteins, DNA, RNA, ligands, and their interactions</li> <li>Released in collaboration with Google DeepMind</li> <li>Nobel Prize-winning foundation (AlphaFold 2)</li> </ul> <p>Relevance to This Project: \u2b50\u2b50\u2b50 - Structure prediction complements expression modeling - Their approach to \"AI-first\" drug discovery is instructive</p>"},{"location":"INDUSTRY_LANDSCAPE/#evolutionaryscale","title":"EvolutionaryScale","text":"Website https://www.evolutionaryscale.ai/ Focus Protein design and engineering Key Technology ESM3 (generative protein model) Founded 2024 (Meta FAIR spin-off) Funding $142M <p>What They Do:</p> <ul> <li>Programmable biology for protein engineering</li> <li>Target cancer cells, find alternatives to plastics, environmental mitigations</li> </ul> <p>Technical Details:</p> <ul> <li>ESM3: simultaneously reasons over sequence, structure, and function of proteins</li> <li>Third-generation ESM model</li> <li>Trained on NVIDIA H100 GPUs</li> <li>ESM Cambrian: parallel model family for protein understanding</li> </ul> <p>Relevance to This Project: \u2b50\u2b50\u2b50 - Generative approach to proteins parallels our approach to expression - Their training methodology is instructive</p>"},{"location":"INDUSTRY_LANDSCAPE/#generatebiomedicines","title":"Generate:Biomedicines","text":"Website https://generatebiomedicines.com/ Focus Protein therapeutics via generative AI Key Technology Generative Biology\u2122 platform <p>What They Do:</p> <ul> <li>Pioneer of \"Generative Biology\" \u2014 generating custom protein therapeutics</li> <li>From peptides to antibodies, enzymes, gene therapies</li> <li>Generate, build, measure, learn loop</li> </ul> <p>Clinical Progress:</p> <ul> <li>GB-0895: Phase 3 for severe asthma (anti-TSLP antibody)</li> <li>GB-0669: Phase 1 completed with positive results</li> </ul> <p>Relevance to This Project: \u2b50\u2b50\u2b50 - Their generate-build-measure-learn loop is a good framework - Demonstrates clinical translation of generative approaches</p>"},{"location":"INDUSTRY_LANDSCAPE/#chai-discovery","title":"Chai Discovery","text":"Website https://www.chaidiscovery.com/ Focus Molecular structure prediction and antibody design Key Technology Chai-1 (structure), Chai-2 (antibody design) Funding \\(100M total (\\)70M Series A, 2025) Investors Thrive Capital, OpenAI <p>What They Do:</p> <ul> <li>Open-source multi-modal foundation model for molecular structure</li> <li>Unifies predictions across proteins, small molecules, DNA, RNA, covalent modifications</li> </ul> <p>Technical Details:</p> <ul> <li>Chai-1: 77% success rate on PoseBusters (vs 76% AlphaFold3)</li> <li>Can operate without MSAs (reduces compute demands)</li> <li>Chai-2: ~16% hit rate for de novo antibody design across 52 novel antigens</li> </ul> <p>Relevance to This Project: \u2b50\u2b50\u2b50 - Open-source approach is valuable - Zero-shot antibody design is impressive generative capability</p>"},{"location":"INDUSTRY_LANDSCAPE/#recursion","title":"Recursion","text":"Website https://www.recursion.com/ Focus Phenomics + drug discovery Key Technology Phenom-Beta, BioHive-2 supercomputer <p>What They Do:</p> <ul> <li>Merge AI with massive biological datasets</li> <li>Process cellular microscopy images into general-purpose embeddings</li> <li>In-silico fluorescent staining from brightfield images</li> </ul> <p>Technical Details:</p> <ul> <li>Phenom-Beta: vision transformer (ViT) with masked autoencoders</li> <li>Trained on RxRx3 dataset (~2.2M images, ~17K knockouts, 1,674 chemicals)</li> <li>BioHive-2: 504 NVIDIA H100 GPUs</li> <li>Partnership with MIT for open-source protein co-folding model</li> </ul> <p>Relevance to This Project: \u2b50\u2b50\u2b50 - Phenomics (image-based) complements transcriptomics - Their self-supervised approach is relevant</p>"},{"location":"INDUSTRY_LANDSCAPE/#clinical-treatment-response","title":"Clinical &amp; Treatment Response","text":"<p>Companies focused on clinical trials, treatment optimization, and patient response prediction.</p>"},{"location":"INDUSTRY_LANDSCAPE/#insilico-medicine","title":"Insilico Medicine","text":"Website https://insilico.com/ Focus End-to-end AI drug discovery Key Technology Pharma.AI, Precious3GPT Founded 2014 Funding $110M Series E (2025) <p>What They Do:</p> <ul> <li>Fully integrated drug discovery suite</li> <li>PandaOmics: discover and prioritize novel targets</li> <li>Chemistry42: generate novel molecules</li> <li>InClinico: design and predict clinical trials</li> </ul> <p>Technical Details:</p> <ul> <li>Precious3GPT: multi-omics, cross-species foundation transformer for aging research</li> <li>Ingests data from rats, monkeys, humans across transcriptomics, proteomics, methylation</li> <li>Enables virtual experiments to forecast compound effects on aging hallmarks</li> <li>Available on Hugging Face</li> </ul> <p>Clinical Progress:</p> <ul> <li>Rentosertib (ISM001-055): AI-discovered drug, Phase 2a results published in Nature Medicine</li> <li>First AI-discovered drug to show clinical proof-of-concept</li> </ul> <p>Relevance to This Project: \u2b50\u2b50\u2b50\u2b50\u2b50 - Precious3GPT is directly relevant \u2014 multi-omics generative model - Their end-to-end approach shows the full pipeline - Clinical validation demonstrates real-world impact</p>"},{"location":"INDUSTRY_LANDSCAPE/#tempus","title":"Tempus","text":"Website https://www.tempus.com/ Focus Precision medicine with real-world data Key Technology Tempus One (AI platform) <p>What They Do:</p> <ul> <li>AI-enabled precision medicine</li> <li>Predict response to therapies with greater accuracy</li> <li>Uncover novel biomarkers from real-world data</li> </ul> <p>Technical Details:</p> <ul> <li>Integrates clinical data with AI-driven algorithms</li> <li>Neural-network-based high-throughput drug screening</li> <li>Generative AI capabilities for querying healthcare data</li> </ul> <p>Relevance to This Project: \u2b50\u2b50\u2b50 - Real-world data integration is important for validation - Treatment response prediction aligns with our goals</p>"},{"location":"INDUSTRY_LANDSCAPE/#owkin","title":"Owkin","text":"Website https://www.owkin.com/ Focus Clinical trials and digital pathology Key Technology Federated learning, SecureFedYJ <p>What They Do:</p> <ul> <li>AI models for drug discovery and clinical trial optimization</li> <li>Federated learning: train AI without centralizing data</li> <li>Digital pathology AI diagnostics</li> </ul> <p>Technical Details:</p> <ul> <li>Owkin Studio: federated learning platform (40% of revenue)</li> <li>Owkin Connect: AI models for drug discovery (35% of revenue)</li> <li>SecureFedYJ: secure federated learning algorithm</li> </ul> <p>Partnerships:</p> <ul> <li>Amgen: cardiovascular prediction</li> <li>AstraZeneca: AI tool for gBRCA mutation screening</li> </ul> <p>Relevance to This Project: \u2b50\u2b50\u2b50 - Federated learning is important for privacy-preserving multi-site studies - Clinical trial optimization is downstream application</p>"},{"location":"INDUSTRY_LANDSCAPE/#retro-biosciences","title":"Retro Biosciences","text":"Website https://www.retro.bio/ Focus Cellular reprogramming and longevity Key Technology GPT-4b micro (with OpenAI) Funding $1B (led by Sam Altman) <p>What They Do:</p> <ul> <li>Interventions to slow or reverse cellular aging</li> <li>Focus on neurodegeneration</li> <li>Combine wet-lab biology with computational methods</li> </ul> <p>Technical Details:</p> <ul> <li>GPT-4b micro: biology-specialized foundation model</li> <li>Trained on protein sequences, biological literature, tokenized 3D structural data</li> <li>Redesigned Yamanaka transcription factors (RetroSOX, RetroKLF)</li> <li>50-fold increases in pluripotency marker expression</li> </ul> <p>Relevance to This Project: \u2b50\u2b50\u2b50\u2b50 - Demonstrates LLM approach to biology - Reprogramming is a form of counterfactual intervention</p>"},{"location":"INDUSTRY_LANDSCAPE/#ai-driven-target-discovery","title":"AI-Driven Target Discovery","text":"<p>Companies using AI/ML for target identification and validation (not necessarily generative).</p>"},{"location":"INDUSTRY_LANDSCAPE/#ochre-bio","title":"Ochre Bio","text":"Website https://www.ochre-bio.com/ Focus RNA therapeutics for liver disease Key Technology OBELiX platform Headquarters Oxford, UK <p>What They Do:</p> <ul> <li>Developing RNA medicines for chronic liver diseases</li> <li>Built one of world's largest human liver functional genomics datasets (~120,000 samples)</li> <li>Combine machine learning with human validation models</li> </ul> <p>Technical Details:</p> <ul> <li>Proprietary gene perturbation atlases + patient disease atlases</li> <li>Make causal predictions about drug targets</li> <li>Human validation: perfused livers, diseased tissue slices, primary cells</li> <li>In-house RNA chemistry</li> </ul> <p>Partnerships:</p> <ul> <li>GSK: functional genomics and single-cell datasets</li> <li>Boehringer Ingelheim: chronic liver disease research</li> </ul> <p>Relevance to This Project: \u2b50\u2b50\u2b50 - Large-scale functional genomics data is valuable - Causal predictions align with our counterfactual goals - Note: Not building generative models, but using ML for target discovery</p>"},{"location":"INDUSTRY_LANDSCAPE/#atomic-ai","title":"Atomic AI","text":"Website https://atomic.ai/ Focus RNA structure and drug discovery Key Technology ATOM-1, PARSE platform Funding ~$42M (seed + Series A) <p>What They Do:</p> <ul> <li>AI-driven RNA drug discovery with atomic precision</li> <li>Predict RNA structural and functional properties</li> <li>Optimize RNA-targeted and RNA-based modalities</li> </ul> <p>Technical Details:</p> <ul> <li>ATOM-1: large language model for RNA structure prediction</li> <li>PARSE: Platform for AI-driven RNA Structure Exploration</li> <li>Combined foundation-model + wet-lab loop</li> </ul> <p>Relevance to This Project: \u2b50\u2b50\u2b50 - RNA structure is complementary to expression - Their foundation model + wet lab loop is a good paradigm</p>"},{"location":"INDUSTRY_LANDSCAPE/#enveda-biosciences","title":"Enveda Biosciences","text":"Website https://enveda.com/ Focus Natural product drug discovery Key Technology PRISM foundation model Funding $300M+ (Series C + D, unicorn valuation) Investors Sanofi <p>What They Do:</p> <ul> <li>Enhance molecular structure identification from natural products</li> <li>Self-supervised learning on mass spectrometry data</li> </ul> <p>Technical Details:</p> <ul> <li>PRISM: Pretrained Representations Informed by Spectral Masking</li> <li>Trained on 1.2 billion small molecule mass spectra</li> <li>Masked peak modeling (similar to masked LM in NLP)</li> </ul> <p>Relevance to This Project: \u2b50\u2b50 - Different modality (mass spec vs expression) - Self-supervised approach is transferable</p>"},{"location":"INDUSTRY_LANDSCAPE/#key-observations-research-directions","title":"Key Observations &amp; Research Directions","text":""},{"location":"INDUSTRY_LANDSCAPE/#trends-in-the-industry","title":"Trends in the Industry","text":"<ol> <li>Foundation Models Are Dominant</li> <li>Most companies are building large-scale foundation models</li> <li>Parameters range from millions to 210 billion (BioMap xTrimo)</li> <li> <p>Self-supervised pretraining is standard</p> </li> <li> <p>Multi-Modal Integration</p> </li> <li>Leading platforms integrate multiple data types</li> <li>Expression + protein + structure + images</li> <li> <p>Cross-species and cross-tissue modeling</p> </li> <li> <p>Generative vs. Predictive</p> </li> <li>Clear distinction between:<ul> <li>Generative: Synthesize Bio, Generate:Biomedicines, EvolutionaryScale</li> <li>Predictive: Ochre Bio, Tempus, Owkin</li> </ul> </li> <li> <p>Generative models enable counterfactual reasoning</p> </li> <li> <p>Clinical Translation</p> </li> <li>Insilico Medicine leads with AI-discovered drugs in clinic</li> <li>Validation in human systems is critical</li> <li> <p>Regulatory pathway is becoming clearer</p> </li> <li> <p>Open Source Movement</p> </li> <li>Helical, Chai Discovery pushing open-source</li> <li>Democratization of bio AI tools</li> <li>Opportunity for academic contribution</li> </ol>"},{"location":"INDUSTRY_LANDSCAPE/#research-directions-for-this-project","title":"Research Directions for This Project","text":"<p>Based on industry analysis, priority areas:</p> <ol> <li>Conditional Generation with Biological Constraints</li> <li>Tissue/disease/batch conditioning (current focus)</li> <li>Pathway-level constraints</li> <li> <p>Gene regulatory network priors</p> </li> <li> <p>Counterfactual Reasoning</p> </li> <li>Treatment response prediction</li> <li>Perturbation effect simulation</li> <li> <p>Causal inference integration</p> </li> <li> <p>Multi-Modal Extension</p> </li> <li>Expression + protein (pseudobulk bridging)</li> <li>Integration with structure predictions</li> <li> <p>Image-based phenomics</p> </li> <li> <p>Evaluation Frameworks</p> </li> <li>DE agreement metrics</li> <li>Pathway concordance</li> <li>Batch leakage tests</li> <li> <p>Clinical validation proxies</p> </li> <li> <p>Scalability</p> </li> <li>Efficient architectures (see Helical's 10% parameter efficiency)</li> <li>Latent diffusion for high-dimensional data</li> <li>Federated learning for multi-site data</li> </ol>"},{"location":"INDUSTRY_LANDSCAPE/#references","title":"References","text":"<ul> <li>17 Companies Pioneering AI Foundation Models in Pharma and Biotech</li> <li>NVIDIA BioNeMo Platform</li> <li>12 AI Drug Discovery Companies You Should Know</li> <li>Individual company websites and press releases</li> </ul>"},{"location":"ROADMAP/","title":"Generative AI Learning Roadmap","text":"<p>A systematic progression from classical latent-variable models to modern generative architectures, with implementation milestones for computational biology applications.</p>"},{"location":"ROADMAP/#overview","title":"Overview","text":"<pre><code>VAE \u2192 \u03b2-VAE \u2192 Score Matching \u2192 DDPM \u2192 Flow Matching \u2192 EBMs \u2192 JEPA \u2192 World Models\n \u2502                                \u2502\n \u2514\u2500\u2500 cVAE (conditional)           \u2514\u2500\u2500 Classifier-Free Guidance\n</code></pre>"},{"location":"ROADMAP/#stage-1-variational-autoencoders-vae","title":"Stage 1: Variational Autoencoders (VAE)","text":"<p>Status: \u2705 Implemented</p>"},{"location":"ROADMAP/#key-concepts","title":"Key Concepts","text":"<ul> <li>ELBO: Evidence Lower Bound as training objective</li> <li>Reparameterization trick: Making sampling differentiable</li> <li>KL regularization: Balancing reconstruction vs latent structure</li> <li>Amortized inference: Encoder learns approximate posterior</li> </ul>"},{"location":"ROADMAP/#core-equations","title":"Core Equations","text":"\\[ \\mathcal{L}_{\\text{ELBO}} = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - \\mathrm{KL}(q_\\phi(z|x) \\| p(z)) \\]"},{"location":"ROADMAP/#implementation","title":"Implementation","text":"<ul> <li><code>src/genailab/model/vae.py</code> \u2014 VAE architecture</li> <li><code>src/genailab/model/encoders.py</code> \u2014 Encoder networks</li> <li><code>src/genailab/model/decoders.py</code> \u2014 Decoder networks</li> <li><code>src/genailab/objectives/losses.py</code> \u2014 ELBO loss</li> </ul>"},{"location":"ROADMAP/#milestones","title":"Milestones","text":"<ul> <li> Basic VAE with MLP encoder/decoder</li> <li> Toy bulk expression dataset</li> <li> Training loop with validation</li> <li> Latent space visualization (UMAP/t-SNE)</li> <li> Interpolation demos</li> </ul>"},{"location":"ROADMAP/#documentation","title":"Documentation","text":"<ul> <li>VAE Theory</li> </ul>"},{"location":"ROADMAP/#stage-2-conditional-vae-cvae","title":"Stage 2: Conditional VAE (cVAE)","text":"<p>Status: \u2705 Implemented</p>"},{"location":"ROADMAP/#key-concepts_1","title":"Key Concepts","text":"<ul> <li>Conditional generation: \\(p_\\theta(x | z, y)\\)</li> <li>Label injection: Concatenating conditions to encoder/decoder</li> <li>Disentanglement: Separating content from style/condition</li> </ul>"},{"location":"ROADMAP/#core-equations_1","title":"Core Equations","text":"\\[ \\mathcal{L}_{\\text{cVAE}} = \\mathbb{E}_{q_\\phi(z|x,y)}[\\log p_\\theta(x|z,y)] - \\mathrm{KL}(q_\\phi(z|x,y) \\| p(z)) \\]"},{"location":"ROADMAP/#implementation_1","title":"Implementation","text":"<ul> <li><code>src/genailab/model/conditioning.py</code> \u2014 Condition embedding</li> <li><code>src/genailab/model/vae.py</code> \u2014 CVAE class</li> </ul>"},{"location":"ROADMAP/#milestones_1","title":"Milestones","text":"<ul> <li> Condition embedding (tissue, disease, batch)</li> <li> Conditional encoder/decoder</li> <li> Counterfactual generation (change condition, keep latent)</li> <li> Condition interpolation</li> </ul>"},{"location":"ROADMAP/#stage-3-vae-and-disentanglement","title":"Stage 3: \u03b2-VAE and Disentanglement","text":"<p>Status: \ud83d\udd32 Planned</p>"},{"location":"ROADMAP/#key-concepts_2","title":"Key Concepts","text":"<ul> <li>\u03b2 parameter: Trade-off between reconstruction and disentanglement</li> <li>Information bottleneck: Limiting mutual information \\(I(x; z)\\)</li> <li>Factor VAE: Encouraging factorial posterior</li> </ul>"},{"location":"ROADMAP/#core-equations_2","title":"Core Equations","text":"\\[ \\mathcal{L}_{\\beta\\text{-VAE}} = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - \\beta \\cdot \\mathrm{KL}(q(z|x) \\| p(z)) \\]"},{"location":"ROADMAP/#milestones_2","title":"Milestones","text":"<ul> <li> Implement \u03b2-VAE with configurable \u03b2</li> <li> Disentanglement metrics (DCI, MIG)</li> <li> Latent traversal visualization</li> <li> Compare \u03b2 values on gene expression</li> </ul>"},{"location":"ROADMAP/#references","title":"References","text":"<ul> <li>Higgins et al., \"\u03b2-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework\" (2017)</li> </ul>"},{"location":"ROADMAP/#stage-4-importance-weighted-autoencoders-iwae","title":"Stage 4: Importance Weighted Autoencoders (IWAE)","text":"<p>Status: \ud83d\udd32 Planned</p>"},{"location":"ROADMAP/#key-concepts_3","title":"Key Concepts","text":"<ul> <li>Tighter bound: Multiple samples for better gradient estimates</li> <li>Importance weighting: Reweighting samples by likelihood ratio</li> <li>Signal-to-noise: Trade-off with number of samples</li> </ul>"},{"location":"ROADMAP/#core-equations_3","title":"Core Equations","text":"\\[ \\mathcal{L}_{\\text{IWAE}} = \\mathbb{E}_{z_1, \\ldots, z_K \\sim q(z|x)} \\left[ \\log \\frac{1}{K} \\sum_{k=1}^{K} \\frac{p(x, z_k)}{q(z_k | x)} \\right] \\]"},{"location":"ROADMAP/#milestones_3","title":"Milestones","text":"<ul> <li> Implement IWAE with K samples</li> <li> Compare ELBO tightness vs VAE</li> <li> Analyze gradient variance</li> </ul>"},{"location":"ROADMAP/#references_1","title":"References","text":"<ul> <li>Burda et al., \"Importance Weighted Autoencoders\" (2016)</li> </ul>"},{"location":"ROADMAP/#stage-5-score-matching-denoising","title":"Stage 5: Score Matching &amp; Denoising","text":"<p>Status: \u2705 Implemented</p>"},{"location":"ROADMAP/#key-concepts_4","title":"Key Concepts","text":"<ul> <li>Score function: \\(\\nabla_x \\log p(x)\\)</li> <li>Denoising score matching: Learn score from noisy samples</li> <li>Langevin dynamics: Sampling via score-guided MCMC</li> </ul>"},{"location":"ROADMAP/#core-equations_4","title":"Core Equations","text":"\\[ \\mathcal{L}_{\\text{DSM}} = \\mathbb{E}_{x, \\tilde{x}} \\left[ \\| s_\\theta(\\tilde{x}) - \\nabla_{\\tilde{x}} \\log p(\\tilde{x} | x) \\|^2 \\right] \\]"},{"location":"ROADMAP/#implementation_2","title":"Implementation","text":"<ul> <li><code>src/genailab/diffusion/sde.py</code> \u2014 VP-SDE, VE-SDE with noise schedules</li> <li><code>src/genailab/diffusion/architectures.py</code> \u2014 Score networks (MLP, TabularScoreNetwork, UNet2D, UNet3D)</li> <li><code>src/genailab/diffusion/training.py</code> \u2014 Score matching training loops</li> </ul>"},{"location":"ROADMAP/#milestones_4","title":"Milestones","text":"<ul> <li> Implement score network (MLP, attention-based)</li> <li> Denoising score matching loss</li> <li> VP-SDE and VE-SDE formulations</li> <li> Noise schedules (linear, cosine)</li> <li> Langevin sampling (basic implementation exists)</li> <li> Noise-conditional score networks (NCSN)</li> </ul>"},{"location":"ROADMAP/#references_2","title":"References","text":"<ul> <li>Song &amp; Ermon, \"Generative Modeling by Estimating Gradients of the Data Distribution\" (2019)</li> <li><code>dev/references/Principles of diffusion models.pdf</code> \u2014 Section 2</li> </ul>"},{"location":"ROADMAP/#stage-6-denoising-diffusion-ddpm-sde-framework","title":"Stage 6: Denoising Diffusion (DDPM) &amp; SDE Framework","text":"<p>Status: \u2705 Implemented</p>"},{"location":"ROADMAP/#key-concepts_5","title":"Key Concepts","text":"<ul> <li>Forward process: Gradually add noise \\(q(x_t | x_{t-1})\\)</li> <li>Reverse process: Learn to denoise \\(p_\\theta(x_{t-1} | x_t)\\)</li> <li>Noise schedule: Linear, cosine, learned</li> <li>SDE formulation: Continuous-time diffusion via SDEs</li> </ul>"},{"location":"ROADMAP/#core-equations_5","title":"Core Equations","text":"<p>Forward:</p> \\[ q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I) \\] <p>Reverse:</p> \\[ p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_t^2 I) \\]"},{"location":"ROADMAP/#implementation_3","title":"Implementation","text":"<ul> <li><code>src/genailab/diffusion/</code> \u2014 Complete diffusion module</li> <li><code>sde.py</code> \u2014 VP-SDE, VE-SDE base classes</li> <li><code>architectures.py</code> \u2014 UNet2D, UNet3D, TabularScoreNetwork</li> <li><code>training.py</code> \u2014 <code>train_score_network</code>, <code>train_image_diffusion</code></li> <li><code>sampling.py</code> \u2014 Reverse SDE, probability flow ODE</li> <li><code>notebooks/diffusion/</code> \u2014 Educational tutorials (01-04)</li> <li><code>scripts/diffusion/</code> \u2014 Production training scripts</li> </ul>"},{"location":"ROADMAP/#milestones_5","title":"Milestones","text":"<ul> <li> Forward diffusion process (VP-SDE, VE-SDE)</li> <li> Score prediction network (U-Net for images, MLP+attention for tabular)</li> <li> Training loop with checkpointing</li> <li> Reverse SDE sampling</li> <li> Probability flow ODE sampling</li> <li> Medical imaging diffusion (synthetic X-rays)</li> <li> DDIM fast sampling</li> <li> Conditional diffusion (classifier-free guidance)</li> </ul>"},{"location":"ROADMAP/#references_3","title":"References","text":"<ul> <li>Ho et al., \"Denoising Diffusion Probabilistic Models\" (2020)</li> <li>Song et al., \"Score-Based Generative Modeling through Stochastic Differential Equations\" (2021)</li> <li><code>dev/references/Principles of diffusion models.pdf</code> \u2014 Sections 3-4</li> </ul>"},{"location":"ROADMAP/#stage-7-flow-matching-rectified-flow","title":"Stage 7: Flow Matching &amp; Rectified Flow","text":"<p>Status: \ud83d\udcdd Documented</p>"},{"location":"ROADMAP/#key-concepts_6","title":"Key Concepts","text":"<ul> <li>Flow matching: Learn velocity field via regression, not score matching</li> <li>Rectified flow: Linear interpolation paths (simplest flow matching)</li> <li>Deterministic sampling: ODE-based generation (no stochastic noise)</li> <li>Simulation-free training: No ODE solver during training</li> </ul>"},{"location":"ROADMAP/#core-equations_6","title":"Core Equations","text":"<p>Path (rectified flow):</p> \\[ x_t = (1 - t) \\cdot x_0 + t \\cdot x_1 \\] <p>Velocity:</p> \\[ \\frac{dx_t}{dt} = x_1 - x_0 \\] <p>Loss:</p> \\[ \\mathcal{L}_{\\text{RF}} = \\mathbb{E}_{x_0, x_1, t} \\left[ \\| v_\\theta(x_t, t) - (x_1 - x_0) \\|^2 \\right] \\]"},{"location":"ROADMAP/#comparison-score-matching-vs-flow-matching","title":"Comparison: Score Matching vs Flow Matching","text":"Aspect Score Matching Flow Matching What's learned Score: \\(\\nabla_x \\log p_t(x)\\) Velocity: \\(v_\\theta(x, t)\\) Forward process Stochastic (add noise) Deterministic (interpolate) Reverse process Stochastic SDE Deterministic ODE Sampling steps 100-1000 10-50"},{"location":"ROADMAP/#documentation_1","title":"Documentation","text":"<ul> <li>Rectified Flow Tutorial \u2014 From first principles</li> </ul>"},{"location":"ROADMAP/#milestones_6","title":"Milestones","text":"<ul> <li> Rectified flow theory documentation</li> <li> Implement flow matching loss</li> <li> Conditional flow matching</li> <li> Compare with DDPM on gene expression</li> </ul>"},{"location":"ROADMAP/#references_4","title":"References","text":"<ul> <li>Liu et al., \"Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow\" (2022)</li> <li>Lipman et al., \"Flow Matching for Generative Modeling\" (2023)</li> </ul>"},{"location":"ROADMAP/#stage-7b-diffusion-transformers-dit","title":"Stage 7b: Diffusion Transformers (DiT)","text":"<p>Status: \u2705 Documented</p>"},{"location":"ROADMAP/#key-concepts_7","title":"Key Concepts","text":"<ul> <li>Transformer backbone: Replace U-Net with Transformer for diffusion/flow models</li> <li>Patch tokenization: Convert images/data to token sequences</li> <li>Adaptive LayerNorm (AdaLN): Time and condition modulation via FiLM</li> <li>Architecture-objective separation: DiT works with score matching, noise prediction, or rectified flow</li> </ul>"},{"location":"ROADMAP/#architecture","title":"Architecture","text":"<pre><code>Input \u2192 Patch Embed \u2192 [Transformer Blocks with AdaLN] \u2192 Output Projection\n                              \u2191\n                    Time Embed + Condition Embed\n</code></pre>"},{"location":"ROADMAP/#why-dit-over-u-net","title":"Why DiT Over U-Net","text":"Aspect U-Net DiT Context Local \u2192 global via downsampling Global via attention Conditioning Architectural changes needed Add tokens or modulation Input shapes Fixed grid Variable (with masking) Scalability Limited Scales with compute"},{"location":"ROADMAP/#documentation_2","title":"Documentation","text":"<ul> <li>DiT Series \u2014 Complete documentation series</li> <li>00_dit_overview.md \u2014 Introduction</li> <li>01_dit_foundations.md \u2014 Architecture details</li> <li>02_dit_training.md \u2014 Training with rectified flow</li> <li>03_dit_sampling.md \u2014 Sampling strategies</li> <li>open_research_tokenization.md \u2014 Tokenization for biology</li> </ul>"},{"location":"ROADMAP/#milestones_7","title":"Milestones","text":"<ul> <li> DiT theory documentation (complete series)</li> <li> AdaLN/FiLM conditioning explanation</li> <li> Tokenization strategies for gene expression</li> <li> Minimal DiT implementation</li> <li> DiT + rectified flow for gene expression</li> </ul>"},{"location":"ROADMAP/#references_5","title":"References","text":"<ul> <li>Peebles &amp; Xie, \"Scalable Diffusion Models with Transformers\" (2023)</li> <li>Perez et al., \"FiLM: Visual Reasoning with a General Conditioning Layer\" (2018)</li> </ul>"},{"location":"ROADMAP/#stage-8-energy-based-models-ebms","title":"Stage 8: Energy-Based Models (EBMs)","text":"<p>Status: \ud83d\udd32 Planned</p>"},{"location":"ROADMAP/#key-concepts_8","title":"Key Concepts","text":"<ul> <li>Energy function: \\(E_\\theta(x)\\) defines unnormalized density</li> <li>Contrastive divergence: Approximate gradient of log-partition</li> <li>MCMC sampling: Langevin, HMC for generation</li> </ul>"},{"location":"ROADMAP/#core-equations_7","title":"Core Equations","text":"\\[ p_\\theta(x) = \\frac{\\exp(-E_\\theta(x))}{Z_\\theta} \\]"},{"location":"ROADMAP/#milestones_8","title":"Milestones","text":"<ul> <li> Energy network architecture</li> <li> Contrastive divergence training</li> <li> Langevin sampling</li> <li> Noise contrastive estimation (NCE)</li> </ul>"},{"location":"ROADMAP/#references_6","title":"References","text":"<ul> <li>LeCun et al., \"A Tutorial on Energy-Based Learning\" (2006)</li> </ul>"},{"location":"ROADMAP/#stage-8b-latent-diffusion-models","title":"Stage 8b: Latent Diffusion Models","text":"<p>Status: \u2705 Documented</p>"},{"location":"ROADMAP/#key-concepts_9","title":"Key Concepts","text":"<ul> <li>Two-stage training: VAE for compression + Diffusion in latent space</li> <li>Count-aware decoders: NB/ZINB decoders for gene expression</li> <li>Efficiency: Train diffusion on compressed representations</li> <li>Biological constraints: Preserve count structure and sparsity</li> </ul>"},{"location":"ROADMAP/#documentation_3","title":"Documentation","text":"<ul> <li>Latent Diffusion Series \u2014 Complete documentation</li> <li>00_latent_diffusion_overview.md</li> <li>01_latent_diffusion_foundations.md</li> <li>02_latent_diffusion_training.md</li> <li>03_latent_diffusion_applications.md</li> <li>04_latent_diffusion_combio.md</li> </ul>"},{"location":"ROADMAP/#milestones_9","title":"Milestones","text":"<ul> <li> Latent diffusion theory documentation</li> <li> VAE with NB/ZINB decoders</li> <li> DiT backbone for latent diffusion</li> <li> Conditioning mechanisms (FiLM, cross-attention)</li> <li> Implementation for gene expression</li> <li> End-to-end training pipeline</li> </ul>"},{"location":"ROADMAP/#stage-9-joint-embedding-predictive-architecture-jepa","title":"Stage 9: Joint Embedding Predictive Architecture (JEPA)","text":"<p>Status: \u2705 Documented</p>"},{"location":"ROADMAP/#key-concepts_10","title":"Key Concepts","text":"<ul> <li>Latent prediction: Predict in embedding space, not pixel space</li> <li>Self-supervised: No reconstruction, no contrastive negatives</li> <li>World models: Learn dynamics without generation</li> <li>Joint latent spaces: Static and dynamic data share the same manifold</li> </ul>"},{"location":"ROADMAP/#architecture_1","title":"Architecture","text":"<pre><code>x \u2192 Encoder \u2192 z_x\n              \u2193\n         Predictor \u2192 \u1e91_y\n              \u2191\ny \u2192 Encoder \u2192 z_y (target)\n</code></pre>"},{"location":"ROADMAP/#key-innovations-from-gokuv-jepa-2","title":"Key Innovations from Goku/V-JEPA 2","text":"<ul> <li>Joint VAE: Images (static) and videos (dynamic) share one latent space</li> <li>Rectified Flow: Direct velocity field instead of noise-based diffusion</li> <li>Patch n' Pack: Variable-length batching without padding</li> <li>Full Attention: No factorized spatial/temporal attention</li> </ul>"},{"location":"ROADMAP/#biological-parallels","title":"Biological Parallels","text":"Vision Domain Biology Domain Image (static) Bulk RNA-seq, baseline expression Video (dynamic) Time-series, Perturb-seq, lineage tracing Variable-length clips Single-cell snapshots across conditions"},{"location":"ROADMAP/#documentation_4","title":"Documentation","text":"<ul> <li>JEPA Series \u2014 Complete documentation</li> <li>00_jepa_overview.md</li> <li>01_jepa_foundations.md</li> <li>02_jepa_training.md</li> <li>03_jepa_applications.md</li> <li>04_jepa_perturbseq.md \u2014 Complete Perturb-seq implementation</li> <li>open_research_joint_latent.md</li> </ul>"},{"location":"ROADMAP/#milestones_10","title":"Milestones","text":"<ul> <li> JEPA theory documentation (complete series)</li> <li> VICReg regularization explanation</li> <li> Perturb-seq architecture and training</li> <li> Joint embedding architecture implementation</li> <li> Predictor network with perturbation conditioning</li> <li> Apply to gene expression time series</li> </ul>"},{"location":"ROADMAP/#references_7","title":"References","text":"<ul> <li>LeCun, \"A Path Towards Autonomous Machine Intelligence\" (2022)</li> <li>Assran et al., \"Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture\" (2023)</li> <li>Meta AI, \"V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction, and Planning\" (2025)</li> <li>ByteDance &amp; HKU, \"Goku: Native Joint Image-Video Generation\" (2024)</li> </ul>"},{"location":"ROADMAP/#stage-10-world-models","title":"Stage 10: World Models","text":"<p>Status: \ud83d\udd32 Planned</p>"},{"location":"ROADMAP/#key-concepts_11","title":"Key Concepts","text":"<ul> <li>Latent dynamics: \\(z_{t+1} = f_\\theta(z_t, a_t)\\)</li> <li>Imagination: Plan in latent space without environment</li> <li>Model-based RL: Use world model for policy learning</li> </ul>"},{"location":"ROADMAP/#applications-in-biology","title":"Applications in Biology","text":"<ul> <li>Perturbation prediction (action = drug/knockdown)</li> <li>Trajectory modeling (action = time step)</li> <li>Counterfactual reasoning</li> </ul>"},{"location":"ROADMAP/#milestones_11","title":"Milestones","text":"<ul> <li> Latent dynamics model</li> <li> Recurrent state-space model (RSSM)</li> <li> Dreamer-style imagination</li> <li> Apply to perturbation biology</li> </ul>"},{"location":"ROADMAP/#references_8","title":"References","text":"<ul> <li>Ha &amp; Schmidhuber, \"World Models\" (2018)</li> <li>Hafner et al., \"Dream to Control\" (2020)</li> </ul>"},{"location":"ROADMAP/#stage-11-foundation-model-adaptation","title":"Stage 11: Foundation Model Adaptation","text":"<p>Status: \u2705 Framework Implemented</p>"},{"location":"ROADMAP/#key-concepts_12","title":"Key Concepts","text":"<ul> <li>Resource-aware configurations: Small/medium/large model presets</li> <li>Parameter-efficient fine-tuning: LoRA, adapters, freezing strategies</li> <li>Hardware auto-detection: M1 Mac, RunPod, Cloud GPUs</li> <li>Modular design: Composable components for adaptation</li> </ul>"},{"location":"ROADMAP/#implementation_4","title":"Implementation","text":"<ul> <li><code>src/genailab/foundation/</code> \u2014 Complete package</li> <li><code>configs/model_configs.py</code> \u2014 Small/medium/large presets</li> <li><code>configs/resource_profiles.py</code> \u2014 Hardware detection</li> <li><code>tuning/lora.py</code> \u2014 LoRA implementation</li> </ul>"},{"location":"ROADMAP/#documentation_5","title":"Documentation","text":"<ul> <li>Foundation Models Series \u2014 Complete guides</li> <li>leveraging_foundation_models_v2.md</li> <li>data_shape_v2.md</li> <li>IMPLEMENTATION_GUIDE.md</li> <li>Package README</li> <li>Tutorial Roadmap</li> </ul>"},{"location":"ROADMAP/#milestones_12","title":"Milestones","text":"<ul> <li> Resource-aware model configurations</li> <li> Auto-detection of hardware resources</li> <li> LoRA implementation with save/load utilities</li> <li> Comprehensive documentation (foundation models, DiT, JEPA, latent diffusion)</li> <li> Adapters and freezing strategies</li> <li> Conditioning modules (FiLM, cross-attention, CFG)</li> <li> Tutorial notebooks</li> <li> End-to-end recipes for gene expression</li> </ul>"},{"location":"ROADMAP/#references_9","title":"References","text":"<ul> <li>Hu et al., \"LoRA: Low-Rank Adaptation of Large Language Models\" (2021)</li> <li>Houlsby et al., \"Parameter-Efficient Transfer Learning for NLP\" (2019)</li> <li>Perez et al., \"FiLM: Visual Reasoning with a General Conditioning Layer\" (2018)</li> </ul>"},{"location":"ROADMAP/#ideas-under-incubation","title":"Ideas Under Incubation","text":"<p>The <code>docs/incubation/</code> directory contains exploratory ideas and architectural proposals that may inform future development. These are not yet implemented but represent promising directions.</p>"},{"location":"ROADMAP/#current-incubation-documents","title":"Current Incubation Documents","text":"Document Focus Key Ideas <code>joint_latent_space_and_JEPA.md</code> Architecture Joint latent spaces for static/dynamic data, JEPA for Perturb-seq <code>generative-ai-for-gene-expression-prediction.md</code> Application Diffusion/VAE/Flow for gene expression with uncertainty <code>generative-ai-for-perturbation-modeling.md</code> Application Generative approaches for scPerturb, beyond GEM-1"},{"location":"ROADMAP/#key-architectural-insights","title":"Key Architectural Insights","text":"<ol> <li>Joint Latent Spaces: Static (bulk RNA-seq) and dynamic (time-series, Perturb-seq) data can share the same latent manifold, enabling mutual training</li> <li>JEPA over Reconstruction: Predicting embeddings (not pixels/counts) is more robust for biology where reconstruction is rarely the goal</li> <li>Hybrid Predictive-Generative: GEM-1-style predictive models + generative wrappers for uncertainty quantification</li> <li>Rectified Flow: May be preferable to diffusion for biology where \"noise semantics\" are unclear</li> </ol>"},{"location":"ROADMAP/#target-applications","title":"Target Applications","text":""},{"location":"ROADMAP/#application-1-gene-expression-prediction","title":"Application 1: Gene Expression Prediction","text":"<p>Goal: Predict gene expression from metadata with uncertainty quantification</p> <p>Current State: GEM-1 (Synthesize Bio) demonstrates supervised prediction at scale</p> <p>Generative AI Value-Add:</p> <ul> <li>Model full distribution \\(p(x \\mid \\text{metadata})\\), not just \\(\\mathbb{E}[x]\\)</li> <li>Uncertainty quantification for experimental planning</li> <li>Diverse synthetic data for augmentation</li> </ul> <p>Proposed Approach: Hybrid model (predictive foundation + diffusion on residuals)</p> <p>See: <code>docs/incubation/generative-ai-for-gene-expression-prediction.md</code></p>"},{"location":"ROADMAP/#application-2-perturbation-prediction-scperturb","title":"Application 2: Perturbation Prediction (scPerturb)","text":"<p>Goal: Predict cellular response to genetic/chemical perturbations</p> <p>Current State: scGen, CPA, GEARS use VAE/GNN approaches</p> <p>Generative AI Value-Add:</p> <ul> <li>Compositional generalization (unseen perturbation combinations)</li> <li>Cell-level heterogeneity modeling</li> <li>Counterfactual reasoning</li> </ul> <p>Proposed Approaches: 1. Conditional diffusion on scPerturb 2. Causal VAE with perturbation operators 3. JEPA for Perturb-seq (predict perturbed latent from baseline + perturbation)</p> <p>See: <code>docs/incubation/generative-ai-for-perturbation-modeling.md</code>, <code>docs/incubation/joint_latent_space_and_JEPA.md</code></p>"},{"location":"ROADMAP/#application-3-synthetic-biological-datasets","title":"Application 3: Synthetic Biological Datasets","text":"<p>Goal: Generate realistic synthetic datasets for drug/target discovery</p> <p>Use Cases:</p> <ul> <li>Data augmentation for rare conditions</li> <li>Privacy-preserving data sharing</li> <li>Benchmarking computational methods</li> <li>Training downstream classifiers</li> </ul> <p>Generative AI Value-Add:</p> <ul> <li>Diverse, realistic samples (not just mean predictions)</li> <li>Controllable generation (condition on disease, tissue, perturbation)</li> <li>Validation via biological consistency checks</li> </ul> <p>Proposed Approach: Conditional diffusion with metadata conditioning</p>"},{"location":"ROADMAP/#application-4-scppdm-single-cell-perturbation-prediction-via-diffusion-models","title":"Application 4: scPPDM (Single-cell Perturbation Prediction via Diffusion Models)","text":"<p>Goal: Implement and extend scPPDM methodology</p> <p>Status: Deferred (see <code>dev/references/scPPDM.pdf</code>)</p> <p>Key Ideas:</p> <ul> <li>Diffusion models for single-cell perturbation response prediction</li> <li>Conditional generation on perturbation identity</li> <li>Comparison with VAE-based methods (scGen, CPA)</li> </ul>"},{"location":"ROADMAP/#efficient-paths-to-applications","title":"Efficient Paths to Applications","text":""},{"location":"ROADMAP/#path-a-gene-expression-prediction-fastest","title":"Path A: Gene Expression Prediction (Fastest)","text":"<pre><code>Current State \u2192 Conditional VAE \u2192 Conditional Diffusion \u2192 Hybrid Model\n     \u2193              (2 weeks)        (3 weeks)           (2 weeks)\n  Stage 2          Add metadata     Add uncertainty      Combine with\n  (cVAE)           conditioning     quantification       predictive model\n</code></pre> <p>Dataset: GTEx or harmonized bulk RNA-seq</p>"},{"location":"ROADMAP/#path-b-perturbation-prediction-most-impactful","title":"Path B: Perturbation Prediction (Most Impactful)","text":"<pre><code>Current State \u2192 JEPA Prototype \u2192 Perturb-seq JEPA \u2192 Generative Wrapper\n     \u2193            (3 weeks)         (4 weeks)          (3 weeks)\n  Stage 6        Basic JEPA on     Add perturbation    Add diffusion\n  (Diffusion)    toy data          conditioning        for uncertainty\n</code></pre> <p>Dataset: Norman et al. 2019 (Perturb-seq, K562 cells)</p>"},{"location":"ROADMAP/#path-c-synthetic-data-generation-most-general","title":"Path C: Synthetic Data Generation (Most General)","text":"<pre><code>Current State \u2192 Conditional Diffusion \u2192 Multi-modal \u2192 Validation Pipeline\n     \u2193              (3 weeks)            (4 weeks)       (2 weeks)\n  Stage 6         Gene expression      Add metadata,    Biological\n  (Diffusion)     generation           perturbations    consistency checks\n</code></pre> <p>Dataset: scPerturb or CellxGene</p>"},{"location":"ROADMAP/#cross-cutting-themes","title":"Cross-Cutting Themes","text":""},{"location":"ROADMAP/#evaluation-metrics","title":"Evaluation Metrics","text":"Model Type Metrics VAE/cVAE ELBO, reconstruction MSE, KL, FID Diffusion FID, IS, likelihood bounds EBM Energy histograms, sample quality JEPA Downstream task performance Perturbation DEG recovery, pathway consistency, held-out perturbation accuracy Gene Expression Sample diversity, biological consistency, downstream task improvement"},{"location":"ROADMAP/#computational-biology-applications","title":"Computational Biology Applications","text":"<ol> <li>Gene expression generation: Conditional on cell type, disease</li> <li>Perturbation prediction: What happens if we knock out gene X?</li> <li>Trajectory inference: Developmental or disease progression</li> <li>Data augmentation: Generate synthetic training data</li> <li>Representation learning: Embeddings for downstream tasks</li> <li>Uncertainty quantification: Confidence intervals for predictions</li> <li>Counterfactual reasoning: \"What if\" scenarios for drug discovery</li> </ol>"},{"location":"ROADMAP/#reading-list","title":"Reading List","text":""},{"location":"ROADMAP/#foundational","title":"Foundational","text":"<ol> <li>Kingma &amp; Welling, \"Auto-Encoding Variational Bayes\" (2014)</li> <li>Rezende et al., \"Stochastic Backpropagation\" (2014)</li> <li><code>dev/references/Principles of diffusion models.pdf</code></li> </ol>"},{"location":"ROADMAP/#surveys","title":"Surveys","text":"<ol> <li><code>dev/references/Diffusion Models- A Comprehensive Survey of Methods and Applications.pdf</code></li> <li>Bond-Taylor et al., \"Deep Generative Modelling: A Comparative Review\" (2022)</li> </ol>"},{"location":"ROADMAP/#biology-specific","title":"Biology-Specific","text":"<ol> <li>Lopez et al., \"Deep generative modeling for single-cell transcriptomics\" (scVI, 2018)</li> <li>Lotfollahi et al., \"scGen: Predicting single-cell perturbation responses\" (2019)</li> <li>Bunne et al., \"Learning Single-Cell Perturbation Responses using Neural Optimal Transport\" (2023)</li> <li>scPPDM: \"Single-cell Perturbation Prediction via Diffusion Models\" \u2014 <code>dev/references/scPPDM.pdf</code></li> <li>Applies diffusion models to predict single-cell perturbation responses</li> <li>Key target for implementation in this project</li> </ol>"},{"location":"ROADMAP/#implementation-strategy","title":"Implementation Strategy","text":""},{"location":"ROADMAP/#phase-1-foundations-current","title":"Phase 1: Foundations (Current)","text":"<ul> <li>VAE, cVAE on toy data</li> <li>Establish training infrastructure</li> <li>Evaluation metrics</li> </ul>"},{"location":"ROADMAP/#phase-2-diffusion","title":"Phase 2: Diffusion","text":"<ul> <li>Score matching basics</li> <li>DDPM implementation</li> <li>Conditional generation</li> </ul>"},{"location":"ROADMAP/#phase-3-advanced","title":"Phase 3: Advanced","text":"<ul> <li>Flow matching</li> <li>EBMs</li> <li>JEPA for biology</li> </ul>"},{"location":"ROADMAP/#phase-4-applications","title":"Phase 4: Applications","text":"<ul> <li>Real single-cell data</li> <li>Perturbation prediction</li> <li>Benchmarking against published methods</li> </ul>"},{"location":"ROADMAP/#next-steps","title":"Next Steps","text":""},{"location":"ROADMAP/#immediate-this-week","title":"Immediate (This Week)","text":"<ol> <li>Test diffusion training on RunPod with large preset</li> <li>Add conditional diffusion (classifier-free guidance)</li> </ol>"},{"location":"ROADMAP/#short-term-2-4-weeks","title":"Short-term (2-4 Weeks)","text":"<ol> <li>Implement conditional VAE for gene expression prediction</li> <li>Begin JEPA prototype on toy biological data</li> <li>Download and preprocess Norman et al. Perturb-seq dataset</li> </ol>"},{"location":"ROADMAP/#medium-term-1-2-months","title":"Medium-term (1-2 Months)","text":"<ol> <li>Flow matching implementation</li> <li>JEPA for Perturb-seq with perturbation conditioning</li> <li>Benchmark against scGen/CPA on perturbation prediction</li> </ol>"},{"location":"ROADMAP/#long-term-3-months","title":"Long-term (3+ Months)","text":"<ol> <li>Hybrid predictive-generative model for gene expression</li> <li>Synthetic biological dataset generation pipeline</li> <li>Integration with real drug discovery workflows</li> </ol>"},{"location":"SETUP/","title":"GenAI-Lab Setup Guide","text":"<p>This guide explains how to properly install and configure the <code>genailab</code> package for development and notebook usage.</p>"},{"location":"SETUP/#quick-start","title":"Quick Start","text":""},{"location":"SETUP/#1-install-the-package-in-editable-mode","title":"1. Install the Package in Editable Mode","text":"<p>The <code>genailab</code> package uses Poetry for dependency management. Install it in editable mode so notebooks can import it directly without <code>sys.path</code> manipulation:</p> <pre><code># Activate the genailab conda environment\nmamba activate genailab\n\n# Install the package in editable mode\ncd /path/to/genai-lab\npoetry install\n\n# Or if you prefer pip\npip install -e .\n</code></pre>"},{"location":"SETUP/#2-verify-installation","title":"2. Verify Installation","text":"<pre><code># In Python or Jupyter notebook\nimport genailab\nfrom genailab import get_config, get_data_dir\n\nprint(f\"GenAI-Lab version: {genailab.__version__}\")\nprint(f\"Project root: {get_config().project_root}\")\nprint(f\"Data directory: {get_data_dir()}\")\n</code></pre> <p>You should see output like: <pre><code>GenAI-Lab version: 0.1.0\nProject root: /Users/yourname/work/genai-lab\nData directory: /Users/yourname/work/genai-lab/data\n</code></pre></p>"},{"location":"SETUP/#configuration-system","title":"Configuration System","text":""},{"location":"SETUP/#using-the-config-module","title":"Using the Config Module","text":"<p>The <code>genailab.config</code> module provides centralized configuration for paths, datasets, and models:</p> <pre><code>from genailab import get_config, get_data_dir, get_checkpoint_dir, get_device\n\n# Get configuration\nconfig = get_config()\n\n# Access paths\ndata_dir = get_data_dir()\ncheckpoint_dir = get_checkpoint_dir(\"diffusion/medical_imaging\")\ndevice = get_device()  # Auto-detects 'cuda', 'mps', or 'cpu'\n\n# Use in your code\ndata_path = data_dir / \"chest_xray\" / \"images\"\ncheckpoint_path = checkpoint_dir / \"model_epoch_1000.pt\"\n</code></pre>"},{"location":"SETUP/#directory-structure","title":"Directory Structure","text":"<p>After installation, the config module automatically creates:</p> <pre><code>genai-lab/\n\u251c\u2500\u2500 data/              # Datasets\n\u251c\u2500\u2500 checkpoints/       # Model checkpoints\n\u2502   \u2514\u2500\u2500 diffusion/\n\u2502       \u251c\u2500\u2500 medical_imaging/\n\u2502       \u2514\u2500\u2500 gene_expression/\n\u251c\u2500\u2500 results/           # Experiment results\n\u2514\u2500\u2500 .cache/            # Cached data\n</code></pre>"},{"location":"SETUP/#registering-datasets","title":"Registering Datasets","text":"<pre><code>from genailab import get_config\n\nconfig = get_config()\n\n# Register dataset paths\nconfig.register_dataset(\"chest_xray\", \"/path/to/chest_xray_dataset\")\nconfig.register_dataset(\"gene_expression\", \"/path/to/gene_data.h5\")\n\n# Retrieve later\nchest_xray_path = config.get_dataset_path(\"chest_xray\")\n</code></pre>"},{"location":"SETUP/#model-configuration-presets","title":"Model Configuration Presets","text":"<pre><code>from genailab import get_diffusion_config\n\n# Get preset configurations\nunet_config = get_diffusion_config(\"unet2d_medium\")\n# Returns: {'in_channels': 1, 'out_channels': 1, 'base_channels': 64, ...}\n\ngene_config = get_diffusion_config(\"tabular_gene_expression\")\n# Returns: {'hidden_dim': 512, 'num_layers': 8, ...}\n\n# Use in model initialization\nfrom genailab.diffusion import UNet2D\nmodel = UNet2D(**unet_config)\n</code></pre> <p>Available presets: - <code>unet2d_small</code>: 32 base channels, 3 levels - <code>unet2d_medium</code>: 64 base channels, 4 levels - <code>unet2d_large</code>: 128 base channels, 4 levels - <code>tabular_gene_expression</code>: MLP with attention for gene data</p>"},{"location":"SETUP/#notebook-best-practices","title":"Notebook Best Practices","text":""},{"location":"SETUP/#old-way-dont-do-this","title":"\u274c Old Way (Don't Do This)","text":"<pre><code># BAD: Hardcoded path manipulation\nimport sys\nfrom pathlib import Path\nsys.path.insert(0, str(Path('../../../src').resolve()))\n\nfrom genailab.diffusion import VPSDE\n</code></pre>"},{"location":"SETUP/#new-way-recommended","title":"\u2705 New Way (Recommended)","text":"<pre><code># GOOD: Direct import after editable install\nfrom genailab.diffusion import VPSDE\nfrom genailab import get_config, get_checkpoint_dir, get_device\n\n# Use configuration\nconfig = get_config()\ndevice = get_device()\ncheckpoint_dir = get_checkpoint_dir(\"my_experiment\")\n</code></pre>"},{"location":"SETUP/#example-notebook-setup-cell","title":"Example Notebook Setup Cell","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\n\n# Import genailab modules\nfrom genailab.diffusion import VPSDE, UNet2D, train_score_network\nfrom genailab import get_config, get_checkpoint_dir, get_device\n\n# Configuration\nconfig = get_config()\ndevice = get_device()\ncheckpoint_dir = get_checkpoint_dir(\"diffusion/medical_imaging\")\n\nprint(f\"Device: {device}\")\nprint(f\"Checkpoints: {checkpoint_dir}\")\n\n# Random seeds\nnp.random.seed(42)\ntorch.manual_seed(42)\n</code></pre>"},{"location":"SETUP/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"SETUP/#custom-configuration","title":"Custom Configuration","text":"<pre><code>from genailab.config import Config\nfrom pathlib import Path\n\n# Create custom configuration\ncustom_config = Config(\n    project_root=Path(\"/custom/path\"),\n    device=\"cuda\"\n)\n\n# Register custom datasets\ncustom_config.register_dataset(\"my_dataset\", \"/path/to/data\")\n\n# Register custom model configs\ncustom_config.register_model_config(\"my_unet\", {\n    \"in_channels\": 3,\n    \"base_channels\": 96,\n    \"channel_multipliers\": (1, 2, 4, 8, 16),\n})\n\n# Use globally\nfrom genailab.config import set_config\nset_config(custom_config)\n</code></pre>"},{"location":"SETUP/#environment-variables","title":"Environment Variables","text":"<p>You can override paths using environment variables:</p> <pre><code>export GENAILAB_DATA_DIR=/mnt/data/genailab\nexport GENAILAB_CHECKPOINT_DIR=/mnt/checkpoints\n</code></pre> <p>Then in Python: <pre><code>import os\nfrom genailab.config import Config\n\nconfig = Config(\n    data_dir=Path(os.getenv(\"GENAILAB_DATA_DIR\", config.data_dir)),\n    checkpoint_dir=Path(os.getenv(\"GENAILAB_CHECKPOINT_DIR\", config.checkpoint_dir)),\n)\n</code></pre></p>"},{"location":"SETUP/#troubleshooting","title":"Troubleshooting","text":""},{"location":"SETUP/#import-errors","title":"Import Errors","text":"<p>Problem: <code>ModuleNotFoundError: No module named 'genailab'</code></p> <p>Solution: Install the package in editable mode: <pre><code>cd /path/to/genai-lab\npoetry install\n# or\npip install -e .\n</code></pre></p> <p>Verify: <pre><code>import genailab\nprint(genailab.__file__)\n# Should show: /path/to/genai-lab/src/genailab/__init__.py\n</code></pre></p>"},{"location":"SETUP/#wrong-project-root","title":"Wrong Project Root","text":"<p>Problem: Config detects wrong project root</p> <p>Solution: Explicitly set project root: <pre><code>from genailab.config import Config\nfrom pathlib import Path\n\nconfig = Config(project_root=Path(\"/correct/path/to/genai-lab\"))\n</code></pre></p>"},{"location":"SETUP/#device-detection-issues","title":"Device Detection Issues","text":"<p>Problem: Wrong device detected (e.g., using CPU when GPU available)</p> <p>Solution: Manually set device: <pre><code>from genailab.config import Config\n\nconfig = Config(device=\"cuda\")  # or \"mps\" or \"cpu\"\n</code></pre></p>"},{"location":"SETUP/#migration-guide","title":"Migration Guide","text":""},{"location":"SETUP/#updating-existing-notebooks","title":"Updating Existing Notebooks","text":"<ol> <li> <p>Remove sys.path manipulation:    <pre><code># DELETE these lines\nimport sys\nfrom pathlib import Path\nsys.path.insert(0, str(Path('../../../src').resolve()))\n</code></pre></p> </li> <li> <p>Add configuration imports:    <pre><code># ADD these lines\nfrom genailab import get_config, get_checkpoint_dir, get_device\n</code></pre></p> </li> <li> <p>Update checkpoint paths:    <pre><code># OLD\ncheckpoint_dir = './checkpoints'\n\n# NEW\ncheckpoint_dir = get_checkpoint_dir(\"diffusion/medical_imaging\")\n</code></pre></p> </li> <li> <p>Update device detection:    <pre><code># OLD\ndevice = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n\n# NEW\ndevice = get_device()\n</code></pre></p> </li> </ol>"},{"location":"SETUP/#summary","title":"Summary","text":"<p>Key Benefits:</p> <ul> <li>\u2705 No more <code>sys.path</code> hacks in notebooks</li> <li>\u2705 Centralized configuration for paths and settings</li> <li>\u2705 Auto-detection of project root and device</li> <li>\u2705 Preset model configurations</li> <li>\u2705 Easy dataset registration and retrieval</li> <li>\u2705 Consistent directory structure across experiments</li> </ul> <p>Next Steps: 1. Install package: <code>poetry install</code> 2. Update notebooks to use config module 3. Register your datasets 4. Use preset model configurations</p>"},{"location":"DDPM/","title":"Denoising Diffusion Probabilistic Models (DDPM)","text":"<p>This directory contains comprehensive documentation on Denoising Diffusion Probabilistic Models (DDPM), the foundational discrete-time diffusion model introduced by Ho et al. (2020).</p>"},{"location":"DDPM/#overview","title":"Overview","text":"<p>DDPM is a class of generative models that learns to generate data by reversing a gradual noising process. It achieves state-of-the-art results across multiple domains and provides a principled probabilistic framework for generation.</p>"},{"location":"DDPM/#why-study-ddpm","title":"Why Study DDPM?","text":"<ol> <li>Foundational model: Understanding DDPM is essential for modern diffusion models</li> <li>Theoretical depth: Connects variational inference, score matching, and SDEs</li> <li>Practical success: State-of-the-art image generation, protein design, molecular generation</li> <li>Training simplicity: Simple MSE loss, no adversarial training</li> <li>Interpretability: Clear probabilistic interpretation via ELBO</li> </ol>"},{"location":"DDPM/#documents-in-this-directory","title":"Documents in This Directory","text":""},{"location":"DDPM/#core-theory","title":"Core Theory","text":"<ol> <li>01_ddpm_foundations.md \u2014 Mathematical Foundations \u2b50</li> <li>Forward and reverse processes</li> <li>Closed-form marginals</li> <li>Variational lower bound (ELBO)</li> <li>Noise prediction parameterization</li> <li>Connection to score matching</li> <li>Training and sampling algorithms</li> </ol>"},{"location":"DDPM/#practical-implementation","title":"Practical Implementation","text":"<ol> <li>02_ddpm_training.md \u2014 Training Details \u2b50 NEW</li> <li>Loss function variants (simple vs. weighted ELBO)</li> <li>Architecture choices (U-Net, MLP, DiT)</li> <li>Time embeddings and conditioning strategies</li> <li>Conditional generation methods</li> <li>Hyperparameter tuning</li> <li> <p>Training tips and common issues</p> </li> <li> <p>03_ddpm_sampling.md \u2014 Sampling Methods \u2b50 NEW</p> </li> <li>DDPM ancestral sampling (stochastic)</li> <li>DDIM deterministic sampling</li> <li>Fast sampling via step skipping</li> <li>Classifier-free guidance (see detailed guide below)</li> <li>Quality vs. speed trade-offs</li> </ol>"},{"location":"DDPM/#extensions-and-advanced-topics","title":"Extensions and Advanced Topics","text":"<ol> <li>Classifier-Free Guidance \u2014 Comprehensive Guide \u2b50</li> <li>Conditional generation without classifiers</li> <li>Training procedure (condition dropping)</li> <li>Guidance scale and its effects</li> <li>Implementation in both DDPM and SDE views</li> <li>Variants: dynamic guidance, negative prompting</li> </ol>"},{"location":"DDPM/#coming-soon","title":"Coming Soon","text":"<ol> <li>04_ddpm_extensions.md \u2014 Extensions and Variants (Planned)</li> <li>Improved DDPM (learned variance)</li> <li>Latent diffusion models</li> <li>Discrete diffusion</li> <li>Domain-specific adaptations</li> </ol>"},{"location":"DDPM/#quick-navigation","title":"Quick Navigation","text":""},{"location":"DDPM/#for-beginners","title":"For Beginners","text":"<p>Start here: DDPM Foundations</p> <p>This document provides a complete mathematical introduction from first principles, covering: - The forward noising process - The reverse denoising process - Training via ELBO - The simple noise prediction loss - Sampling algorithms</p> <p>Then:  1. Read DDPM Training for practical training details 2. Read DDPM Sampling for sampling algorithms 3. Work through the DDPM Basics Notebook for hands-on implementation</p>"},{"location":"DDPM/#for-deep-dive","title":"For Deep Dive","text":"<p>After understanding the foundations: 1. Study the SDE perspective to see DDPM as a discretization 2. Review the continuous limit to understand VP-SDE 3. Explore DDIM update coefficients for exact formulas 4. Read Reverse SDE &amp; Probability Flow ODE for sampling theory</p>"},{"location":"DDPM/#for-implementation","title":"For Implementation","text":"<ul> <li>Documentation: </li> <li>Training Details \u2014 Architectures, loss functions, hyperparameters</li> <li> <p>Sampling Methods \u2014 DDPM, DDIM, fast sampling, guidance</p> </li> <li> <p>Notebook: 01_ddpm_basics.ipynb</p> </li> <li>Complete PyTorch implementation</li> <li>Gene expression application</li> <li>Conditional generation</li> <li> <p>Training and sampling code</p> </li> <li> <p>Source code: <code>src/genailab/diffusion/</code></p> </li> <li>Production-ready implementations</li> <li>Modular components</li> <li>Reusable utilities</li> </ul>"},{"location":"DDPM/#key-concepts","title":"Key Concepts","text":""},{"location":"DDPM/#forward-process-data-noise","title":"Forward Process (Data \u2192 Noise)","text":"<p>Gradually add Gaussian noise over \\(T\\) steps:</p> \\[ q(x_t \\mid x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t I) \\] <p>Closed-form: Jump directly to any timestep:</p> \\[ x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I) \\]"},{"location":"DDPM/#reverse-process-noise-data","title":"Reverse Process (Noise \u2192 Data)","text":"<p>Learn to denoise step by step:</p> \\[ p_\\theta(x_{t-1} \\mid x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_t^2 I) \\]"},{"location":"DDPM/#training-objective","title":"Training Objective","text":"<p>Simple MSE loss on noise prediction:</p> \\[ L = \\mathbb{E}_{t, x_0, \\epsilon} \\left[\\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2\\right] \\]"},{"location":"DDPM/#sampling-algorithm","title":"Sampling Algorithm","text":"<pre><code>1. Start with x_T ~ N(0, I)\n2. For t = T, ..., 1:\n   - Predict noise: \u03b5_\u03b8(x_t, t)\n   - Compute mean: \u03bc_\u03b8\n   - Add noise: x_{t-1} = \u03bc_\u03b8 + \u03c3_t * z\n3. Return x_0\n</code></pre>"},{"location":"DDPM/#connections-to-other-frameworks","title":"Connections to Other Frameworks","text":""},{"location":"DDPM/#score-matching","title":"Score Matching","text":"<p>DDPM implicitly learns the score function:</p> \\[ \\nabla_{x_t} \\log q(x_t) = -\\frac{1}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon \\] <p>Predicting noise \u2248 predicting score (up to scaling).</p>"},{"location":"DDPM/#stochastic-differential-equations-sdes","title":"Stochastic Differential Equations (SDEs)","text":"<p>DDPM is a discretization of the VP-SDE:</p> \\[ dx = -\\frac{1}{2}\\beta(t) x\\,dt + \\sqrt{\\beta(t)}\\,dw \\] <p>See: VP-SDE to DDPM</p>"},{"location":"DDPM/#variational-autoencoders-vaes","title":"Variational Autoencoders (VAEs)","text":"<p>DDPM can be viewed as a hierarchical VAE with: - Markovian latent structure - Fixed encoder (forward process) - Learned decoder (reverse process)</p>"},{"location":"DDPM/#learning-path","title":"Learning Path","text":""},{"location":"DDPM/#recommended-order","title":"Recommended Order","text":"<ol> <li>Foundations (this directory)</li> <li>Start with DDPM Foundations</li> <li>Understand forward/reverse processes</li> <li> <p>Learn the training objective</p> </li> <li> <p>Practical Training (this directory)</p> </li> <li>Read DDPM Training</li> <li>Learn architecture choices</li> <li> <p>Understand hyperparameters and conditioning</p> </li> <li> <p>Sampling Methods (this directory)</p> </li> <li>Read DDPM Sampling</li> <li>Compare DDPM vs. DDIM</li> <li> <p>Learn fast sampling and guidance</p> </li> <li> <p>Implementation (notebooks)</p> </li> <li>Work through DDPM Basics</li> <li>Implement training loop</li> <li> <p>Generate samples</p> </li> <li> <p>Theory (SDE directory)</p> </li> <li>Study SDE View</li> <li>Understand continuous-time perspective</li> <li> <p>Connect discrete and continuous</p> </li> <li> <p>Advanced Topics</p> </li> <li>Latent diffusion models</li> <li>Domain-specific applications</li> <li>State-of-the-art techniques</li> </ol>"},{"location":"DDPM/#applications","title":"Applications","text":""},{"location":"DDPM/#image-generation","title":"Image Generation","text":"<ul> <li>Unconditional: Generate diverse images from noise</li> <li>Conditional: Text-to-image, class-conditional</li> <li>Inpainting: Fill missing regions</li> <li>Super-resolution: Upscale low-resolution images</li> </ul>"},{"location":"DDPM/#biological-data","title":"Biological Data","text":"<ul> <li>Gene expression: Generate cell states (see DDPM Basics)</li> <li>Protein design: Generate protein sequences and structures</li> <li>Drug response: Predict perturbation effects (scPPDM)</li> <li>Single-cell data: Generate realistic cell populations</li> </ul>"},{"location":"DDPM/#other-domains","title":"Other Domains","text":"<ul> <li>Audio: Speech synthesis, music generation</li> <li>Video: Frame prediction, video synthesis</li> <li>Molecular: Drug design, molecular generation</li> <li>3D: Point clouds, meshes, NeRF</li> </ul>"},{"location":"DDPM/#comparison-with-other-generative-models","title":"Comparison with Other Generative Models","text":"Model Training Sampling Quality Diversity Likelihood DDPM Stable Slow Excellent High Approximate GAN Unstable Fast Excellent Medium No VAE Stable Fast Good High Exact Flow Stable Fast Good High Exact <p>DDPM advantages:</p> <ul> <li>Training stability (no adversarial training)</li> <li>High sample quality</li> <li>Flexible conditioning</li> <li>Theoretical foundations</li> </ul> <p>DDPM disadvantages:</p> <ul> <li>Slow sampling (1000 steps)</li> <li>Approximate likelihood</li> <li>High computational cost</li> </ul>"},{"location":"DDPM/#historical-context","title":"Historical Context","text":""},{"location":"DDPM/#timeline","title":"Timeline","text":"<ul> <li>2015: Sohl-Dickstein et al. introduce diffusion models (ICML)</li> <li>2019: Song &amp; Ermon connect to score matching (NeurIPS)</li> <li>2020: Ho et al. introduce DDPM (NeurIPS) \u2014 breakthrough results</li> <li>2021: Nichol &amp; Dhariwal improve DDPM (ICML)</li> <li>2021: Song et al. introduce SDE framework (ICLR 2021)</li> <li>2021: Dhariwal &amp; Nichol beat GANs (NeurIPS)</li> <li>2022: Rombach et al. introduce Latent Diffusion (CVPR)</li> <li>2022: Ramesh et al. introduce DALL-E 2 (arXiv)</li> </ul>"},{"location":"DDPM/#key-papers","title":"Key Papers","text":"<ol> <li>Sohl-Dickstein et al. (2015): Deep Unsupervised Learning using Nonequilibrium Thermodynamics</li> <li>Ho et al. (2020): Denoising Diffusion Probabilistic Models</li> <li>Song et al. (2021): Score-Based Generative Modeling through SDEs</li> <li>Nichol &amp; Dhariwal (2021): Improved Denoising Diffusion Probabilistic Models</li> <li>Dhariwal &amp; Nichol (2021): Diffusion Models Beat GANs on Image Synthesis</li> </ol>"},{"location":"DDPM/#related-documentation","title":"Related Documentation","text":""},{"location":"DDPM/#in-this-repository","title":"In This Repository","text":"<ul> <li>SDE Formulation: docs/SDE/</li> <li>Continuous-time perspective</li> <li>Fokker-Planck equation</li> <li>Score matching connections</li> <li> <p>DDPM to VP-SDE</p> </li> <li> <p>Diffusion Models: docs/diffusion/</p> </li> <li>Historical Development \u2014 How DDPM, score-based, and flow-based models unified</li> <li>Classifier-Free Guidance \u2014 Conditional generation</li> <li>Brownian Motion Tutorial</li> <li> <p>General diffusion theory</p> </li> <li> <p>Notebooks: notebooks/diffusion/</p> </li> <li>DDPM basics implementation</li> <li>SDE formulation with code</li> <li>Advanced topics</li> </ul>"},{"location":"DDPM/#external-resources","title":"External Resources","text":"<ul> <li>Original papers: See references in 01_ddpm_foundations.md</li> <li>Tutorials: </li> <li>Lilian Weng's blog</li> <li>Yang Song's blog</li> <li>Code:</li> <li>Official DDPM repo</li> <li>Hugging Face Diffusers</li> </ul>"},{"location":"DDPM/#contributing","title":"Contributing","text":"<p>This documentation is part of the <code>genai-lab</code> project. To contribute:</p> <ol> <li>Follow the tutorial/blog style established in existing documents</li> <li>Use proper LaTeX notation (<code>$$...$$</code> for blocks, <code>$...$</code> for inline)</li> <li>Include intuition alongside mathematics</li> <li>Add examples and visualizations where helpful</li> <li>Link to related documents</li> </ol>"},{"location":"DDPM/#summary","title":"Summary","text":"<p>DDPM is a foundational generative model that: - Learns to reverse a gradual noising process - Trains via simple MSE loss on noise prediction - Achieves state-of-the-art sample quality - Connects to score matching and SDEs - Provides a principled probabilistic framework</p> <p>Start with: DDPM Foundations for a complete mathematical introduction.</p>"},{"location":"DDPM/01_ddpm_foundations/","title":"Denoising Diffusion Probabilistic Models (DDPM): Foundations","text":"<p>This document provides a comprehensive mathematical introduction to DDPM, the foundational discrete-time diffusion model introduced by Ho et al. (2020).</p>"},{"location":"DDPM/01_ddpm_foundations/#overview","title":"Overview","text":"<p>Denoising Diffusion Probabilistic Models (DDPM) are a class of generative models that learn to generate data by reversing a gradual noising process. They achieve state-of-the-art results in image generation and have been successfully applied to various domains including gene expression, protein design, and molecular generation.</p>"},{"location":"DDPM/01_ddpm_foundations/#key-idea","title":"Key Idea","text":"<ol> <li>Forward process: Gradually add Gaussian noise to data over \\(T\\) steps until it becomes pure noise</li> <li>Reverse process: Learn to denoise, step by step, starting from pure noise</li> <li>Training: Predict the noise added at each step (equivalently, predict the score)</li> </ol>"},{"location":"DDPM/01_ddpm_foundations/#why-ddpm-matters","title":"Why DDPM Matters","text":"<ul> <li>Theoretical foundation: Connects variational inference, score matching, and SDEs</li> <li>Training stability: Simple MSE loss, no adversarial training</li> <li>Sample quality: State-of-the-art FID scores on image generation</li> <li>Flexibility: Works for continuous, discrete, and structured data</li> <li>Interpretability: Clear probabilistic interpretation via ELBO</li> </ul>"},{"location":"DDPM/01_ddpm_foundations/#the-forward-process-data-noise","title":"The Forward Process (Data \u2192 Noise)","text":""},{"location":"DDPM/01_ddpm_foundations/#definition","title":"Definition","text":"<p>The forward process is a fixed Markov chain that gradually adds Gaussian noise to data \\(x_0 \\sim q(x_0)\\):</p> \\[ q(x_t \\mid x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t I) \\] <p>where:</p> <ul> <li>\\(t = 1, 2, \\ldots, T\\) (typically \\(T = 1000\\))</li> <li>\\(\\beta_t \\in (0, 1)\\) is the variance schedule (how much noise to add)</li> <li>\\(\\beta_1 &lt; \\beta_2 &lt; \\cdots &lt; \\beta_T\\) (increasing noise)</li> </ul>"},{"location":"DDPM/01_ddpm_foundations/#intuition","title":"Intuition","text":"<p>At each step: - Keep \\(\\sqrt{1 - \\beta_t}\\) of the previous signal - Add \\(\\sqrt{\\beta_t}\\) of fresh noise</p> <p>As \\(t \\to T\\), the signal becomes pure Gaussian noise.</p>"},{"location":"DDPM/01_ddpm_foundations/#reparameterization","title":"Reparameterization","text":"<p>Using the reparameterization trick:</p> \\[ x_t = \\sqrt{1 - \\beta_t} x_{t-1} + \\sqrt{\\beta_t} \\epsilon_{t-1}, \\quad \\epsilon_{t-1} \\sim \\mathcal{N}(0, I) \\]"},{"location":"DDPM/01_ddpm_foundations/#closed-form-forward-process","title":"Closed-Form Forward Process","text":""},{"location":"DDPM/01_ddpm_foundations/#key-insight","title":"Key Insight","text":"<p>Because each step is Gaussian and the process is Markovian, we can jump directly from \\(x_0\\) to any \\(x_t\\) without computing intermediate steps.</p>"},{"location":"DDPM/01_ddpm_foundations/#notation","title":"Notation","text":"<p>Define: - \\(\\alpha_t := 1 - \\beta_t\\) (signal retention coefficient) - \\(\\bar{\\alpha}_t := \\prod_{s=1}^t \\alpha_s\\) (cumulative product)</p>"},{"location":"DDPM/01_ddpm_foundations/#closed-form-marginal","title":"Closed-Form Marginal","text":"\\[ \\boxed{q(x_t \\mid x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I)} \\] <p>Reparameterization form:</p> \\[ x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I) \\]"},{"location":"DDPM/01_ddpm_foundations/#derivation","title":"Derivation","text":"<p>By induction:</p> <p>Base case (\\(t=1\\)):</p> \\[ x_1 = \\sqrt{\\alpha_1} x_0 + \\sqrt{1 - \\alpha_1} \\epsilon_0 = \\sqrt{\\bar{\\alpha}_1} x_0 + \\sqrt{1 - \\bar{\\alpha}_1} \\epsilon_0 \\] <p>Inductive step: Assume \\(x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\bar{\\epsilon}_t\\).</p> <p>Then:</p> \\[ \\begin{align} x_{t+1} &amp;= \\sqrt{\\alpha_{t+1}} x_t + \\sqrt{1 - \\alpha_{t+1}} \\epsilon_t \\\\ &amp;= \\sqrt{\\alpha_{t+1}} \\left(\\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\bar{\\epsilon}_t\\right) + \\sqrt{1 - \\alpha_{t+1}} \\epsilon_t \\\\ &amp;= \\sqrt{\\alpha_{t+1} \\bar{\\alpha}_t} x_0 + \\sqrt{\\alpha_{t+1}(1 - \\bar{\\alpha}_t)} \\bar{\\epsilon}_t + \\sqrt{1 - \\alpha_{t+1}} \\epsilon_t \\end{align} \\] <p>The two noise terms combine (sum of independent Gaussians):</p> \\[ \\sqrt{\\alpha_{t+1}(1 - \\bar{\\alpha}_t)} \\bar{\\epsilon}_t + \\sqrt{1 - \\alpha_{t+1}} \\epsilon_t \\sim \\mathcal{N}(0, [\\alpha_{t+1}(1 - \\bar{\\alpha}_t) + (1 - \\alpha_{t+1})] I) \\] <p>Simplify the variance:</p> \\[ \\alpha_{t+1}(1 - \\bar{\\alpha}_t) + (1 - \\alpha_{t+1}) = \\alpha_{t+1} - \\alpha_{t+1}\\bar{\\alpha}_t + 1 - \\alpha_{t+1} = 1 - \\alpha_{t+1}\\bar{\\alpha}_t = 1 - \\bar{\\alpha}_{t+1} \\] <p>Therefore:</p> \\[ x_{t+1} = \\sqrt{\\bar{\\alpha}_{t+1}} x_0 + \\sqrt{1 - \\bar{\\alpha}_{t+1}} \\epsilon \\]"},{"location":"DDPM/01_ddpm_foundations/#the-reverse-process-noise-data","title":"The Reverse Process (Noise \u2192 Data)","text":""},{"location":"DDPM/01_ddpm_foundations/#goal","title":"Goal","text":"<p>Learn to reverse the forward process: \\(p_\\theta(x_{t-1} \\mid x_t)\\).</p> <p>If we can sample \\(x_T \\sim \\mathcal{N}(0, I)\\) and iteratively apply the reverse process, we can generate new data.</p>"},{"location":"DDPM/01_ddpm_foundations/#parameterization","title":"Parameterization","text":"<p>Model the reverse process as a Markov chain:</p> \\[ p_\\theta(x_{0:T}) = p(x_T) \\prod_{t=1}^T p_\\theta(x_{t-1} \\mid x_t) \\] <p>where:</p> \\[ p_\\theta(x_{t-1} \\mid x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t)) \\]"},{"location":"DDPM/01_ddpm_foundations/#key-question","title":"Key Question","text":"<p>What should \\(\\mu_\\theta\\) and \\(\\Sigma_\\theta\\) be?</p> <p>Answer: We'll derive them from the posterior \\(q(x_{t-1} \\mid x_t, x_0)\\).</p>"},{"location":"DDPM/01_ddpm_foundations/#posterior-distribution","title":"Posterior Distribution","text":""},{"location":"DDPM/01_ddpm_foundations/#bayes-rule","title":"Bayes' Rule","text":"<p>Using Bayes' rule:</p> \\[ q(x_{t-1} \\mid x_t, x_0) = \\frac{q(x_t \\mid x_{t-1}, x_0) q(x_{t-1} \\mid x_0)}{q(x_t \\mid x_0)} \\] <p>Since the forward process is Markovian, \\(q(x_t \\mid x_{t-1}, x_0) = q(x_t \\mid x_{t-1})\\).</p>"},{"location":"DDPM/01_ddpm_foundations/#gaussian-posterior","title":"Gaussian Posterior","text":"<p>All three terms are Gaussian, so the posterior is also Gaussian. We can compute it in closed form.</p> <p>Result:</p> \\[ q(x_{t-1} \\mid x_t, x_0) = \\mathcal{N}(x_{t-1}; \\tilde{\\mu}_t(x_t, x_0), \\tilde{\\beta}_t I) \\] <p>where:</p> \\[ \\tilde{\\mu}_t(x_t, x_0) = \\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t}{1 - \\bar{\\alpha}_t} x_0 + \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} x_t \\] \\[ \\tilde{\\beta}_t = \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t \\]"},{"location":"DDPM/01_ddpm_foundations/#derivation_1","title":"Derivation","text":"<p>We have: - \\(q(x_t \\mid x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{\\alpha_t} x_{t-1}, \\beta_t I)\\) - \\(q(x_{t-1} \\mid x_0) = \\mathcal{N}(x_{t-1}; \\sqrt{\\bar{\\alpha}_{t-1}} x_0, (1 - \\bar{\\alpha}_{t-1}) I)\\) - \\(q(x_t \\mid x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I)\\)</p> <p>Using the formula for the product of Gaussians (completing the square), we get the result above.</p> <p>Key insight: The posterior mean is a weighted average of predictions from \\(x_t\\) and \\(x_0\\).</p>"},{"location":"DDPM/01_ddpm_foundations/#training-objective-the-elbo","title":"Training Objective: The ELBO","text":""},{"location":"DDPM/01_ddpm_foundations/#variational-lower-bound","title":"Variational Lower Bound","text":"<p>DDPM is trained by maximizing the evidence lower bound (ELBO):</p> \\[ \\log p_\\theta(x_0) \\geq \\mathbb{E}_{q(x_{1:T} \\mid x_0)} \\left[\\log \\frac{p_\\theta(x_{0:T})}{q(x_{1:T} \\mid x_0)}\\right] \\]"},{"location":"DDPM/01_ddpm_foundations/#decomposition","title":"Decomposition","text":"<p>The ELBO can be decomposed into three terms:</p> \\[ \\mathcal{L} = \\mathbb{E}_q \\left[\\underbrace{D_{KL}(q(x_T \\mid x_0) \\| p(x_T))}_{L_T} + \\sum_{t=2}^T \\underbrace{D_{KL}(q(x_{t-1} \\mid x_t, x_0) \\| p_\\theta(x_{t-1} \\mid x_t))}_{L_{t-1}} - \\underbrace{\\log p_\\theta(x_0 \\mid x_1)}_{L_0}\\right] \\] <p>Interpretation:</p> <ul> <li>\\(L_T\\): How close is \\(q(x_T \\mid x_0)\\) to \\(p(x_T) = \\mathcal{N}(0, I)\\)? (Usually negligible)</li> <li>\\(L_{t-1}\\): How well does \\(p_\\theta\\) match the true posterior \\(q\\)?</li> <li>\\(L_0\\): Reconstruction term (discrete decoder or continuous likelihood)</li> </ul>"},{"location":"DDPM/01_ddpm_foundations/#simplification","title":"Simplification","text":"<p>Since both \\(q(x_{t-1} \\mid x_t, x_0)\\) and \\(p_\\theta(x_{t-1} \\mid x_t)\\) are Gaussian, the KL divergence has a closed form:</p> \\[ L_{t-1} = \\mathbb{E}_{q(x_t, x_0)} \\left[\\frac{1}{2\\sigma_t^2} \\|\\tilde{\\mu}_t(x_t, x_0) - \\mu_\\theta(x_t, t)\\|^2\\right] + \\text{const} \\]"},{"location":"DDPM/01_ddpm_foundations/#noise-prediction-parameterization","title":"Noise Prediction Parameterization","text":""},{"location":"DDPM/01_ddpm_foundations/#key-insight_1","title":"Key Insight","text":"<p>Instead of directly predicting \\(\\mu_\\theta(x_t, t)\\), we can predict the noise \\(\\epsilon\\) that was added.</p>"},{"location":"DDPM/01_ddpm_foundations/#reparameterization_1","title":"Reparameterization","text":"<p>Recall:</p> \\[ x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon \\] <p>Solving for \\(x_0\\):</p> \\[ x_0 = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}} (x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon) \\] <p>Substituting into \\(\\tilde{\\mu}_t\\):</p> \\[ \\tilde{\\mu}_t(x_t, x_0) = \\frac{1}{\\sqrt{\\alpha_t}} \\left(x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon\\right) \\]"},{"location":"DDPM/01_ddpm_foundations/#noise-prediction-network","title":"Noise Prediction Network","text":"<p>Define:</p> \\[ \\mu_\\theta(x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left(x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t)\\right) \\] <p>where \\(\\epsilon_\\theta(x_t, t)\\) is a neural network that predicts the noise.</p>"},{"location":"DDPM/01_ddpm_foundations/#simplified-loss","title":"Simplified Loss","text":"<p>The ELBO simplifies to:</p> \\[ \\boxed{L_{\\text{simple}} = \\mathbb{E}_{t, x_0, \\epsilon} \\left[\\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2\\right]} \\] <p>where:</p> <ul> <li>\\(t \\sim \\text{Uniform}(\\{1, \\ldots, T\\})\\)</li> <li>\\(x_0 \\sim q(x_0)\\)</li> <li>\\(\\epsilon \\sim \\mathcal{N}(0, I)\\)</li> <li>\\(x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon\\)</li> </ul> <p>This is just MSE on noise prediction!</p>"},{"location":"DDPM/01_ddpm_foundations/#algorithm-summary","title":"Algorithm Summary","text":""},{"location":"DDPM/01_ddpm_foundations/#training","title":"Training","text":"<pre><code>1. Sample x_0 ~ q(x_0)\n2. Sample t ~ Uniform({1, ..., T})\n3. Sample \u03b5 ~ N(0, I)\n4. Compute x_t = sqrt(\u03b1_bar_t) * x_0 + sqrt(1 - \u03b1_bar_t) * \u03b5\n5. Compute loss = ||\u03b5 - \u03b5_\u03b8(x_t, t)||\u00b2\n6. Update \u03b8 via gradient descent\n</code></pre>"},{"location":"DDPM/01_ddpm_foundations/#sampling","title":"Sampling","text":"<pre><code>1. Sample x_T ~ N(0, I)\n2. For t = T, T-1, ..., 1:\n   a. Predict noise: \u03b5_\u03b8(x_t, t)\n   b. Compute mean: \u03bc_\u03b8 = (1/sqrt(\u03b1_t)) * (x_t - (\u03b2_t/sqrt(1-\u03b1_bar_t)) * \u03b5_\u03b8)\n   c. Sample: x_{t-1} = \u03bc_\u03b8 + \u03c3_t * z  (z ~ N(0,I) if t&gt;1, else z=0)\n3. Return x_0\n</code></pre>"},{"location":"DDPM/01_ddpm_foundations/#variance-schedule","title":"Variance Schedule","text":""},{"location":"DDPM/01_ddpm_foundations/#linear-schedule-original-ddpm","title":"Linear Schedule (Original DDPM)","text":"\\[ \\beta_t = \\beta_1 + \\frac{t-1}{T-1}(\\beta_T - \\beta_1) \\] <p>Typical values: \\(\\beta_1 = 10^{-4}\\), \\(\\beta_T = 0.02\\)</p>"},{"location":"DDPM/01_ddpm_foundations/#cosine-schedule-improved-ddpm","title":"Cosine Schedule (Improved DDPM)","text":"\\[ \\bar{\\alpha}_t = \\frac{f(t)}{f(0)}, \\quad f(t) = \\cos\\left(\\frac{t/T + s}{1 + s} \\cdot \\frac{\\pi}{2}\\right)^2 \\] <p>where \\(s = 0.008\\) is a small offset.</p> <p>Advantages:</p> <ul> <li>More uniform signal-to-noise ratio across timesteps</li> <li>Better sample quality</li> <li>Fewer steps needed</li> </ul>"},{"location":"DDPM/01_ddpm_foundations/#connection-to-score-matching","title":"Connection to Score Matching","text":""},{"location":"DDPM/01_ddpm_foundations/#score-function","title":"Score Function","text":"<p>The score is the gradient of the log density:</p> \\[ \\nabla_{x_t} \\log q(x_t) = -\\frac{1}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon \\]"},{"location":"DDPM/01_ddpm_foundations/#equivalence","title":"Equivalence","text":"<p>Predicting noise \\(\\epsilon\\) is equivalent to predicting the score (up to a constant):</p> \\[ \\epsilon_\\theta(x_t, t) \\approx \\epsilon \\quad \\Leftrightarrow \\quad -\\frac{1}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\approx \\nabla_{x_t} \\log q(x_t) \\] <p>Therefore: DDPM training is score matching with a specific noise schedule.</p>"},{"location":"DDPM/01_ddpm_foundations/#summary","title":"Summary","text":"<p>We derived DDPM from first principles:</p> <ol> <li>Forward process: Fixed Markov chain adding Gaussian noise</li> <li>Closed-form marginal: \\(x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon\\)</li> <li>Reverse process: Learned Gaussian transitions</li> <li>Training objective: Variational lower bound (ELBO)</li> <li>Simplified loss: MSE on noise prediction</li> <li>Connection to score matching: Noise prediction \u2248 score prediction</li> </ol> <p>Key insights:</p> <ul> <li>Training is simple: predict the noise that was added</li> <li>Sampling is iterative denoising</li> <li>The model learns the score function implicitly</li> <li>DDPM is a discrete-time approximation of continuous SDEs</li> </ul>"},{"location":"DDPM/01_ddpm_foundations/#related-documents","title":"Related Documents","text":"<ul> <li>DDPM Training Details \u2014 Loss functions, architectures, conditioning</li> <li>DDPM Sampling Methods \u2014 DDPM, DDIM, ancestral sampling</li> <li>From DDPM to VP-SDE \u2014 Continuous-time limit</li> <li>VP-SDE to DDPM \u2014 Discretization perspective</li> </ul>"},{"location":"DDPM/01_ddpm_foundations/#references","title":"References","text":"<ol> <li>Ho, J., Jain, A., &amp; Abbeel, P. (2020). Denoising Diffusion Probabilistic Models. NeurIPS.</li> <li>Sohl-Dickstein, J., et al. (2015). Deep Unsupervised Learning using Nonequilibrium Thermodynamics. ICML.</li> <li>Song, Y., &amp; Ermon, S. (2019). Generative Modeling by Estimating Gradients of the Data Distribution. NeurIPS.</li> <li>Nichol, A., &amp; Dhariwal, P. (2021). Improved Denoising Diffusion Probabilistic Models. ICML.</li> </ol>"},{"location":"DDPM/02_ddpm_training/","title":"DDPM Training: From Theory to Practice","text":"<p>This document bridges the mathematical foundations of DDPM to practical training considerations, covering loss functions, architectures, conditioning strategies, and training tips.</p>"},{"location":"DDPM/02_ddpm_training/#overview","title":"Overview","text":"<p>Training a DDPM involves:</p> <ol> <li>Loss function: Simple MSE vs. weighted ELBO</li> <li>Architecture: Choosing the right network for your data</li> <li>Conditioning: How to incorporate additional information</li> <li>Optimization: Hyperparameters and training strategies</li> </ol> <p>Goal: Understand the practical decisions that make DDPM training successful.</p>"},{"location":"DDPM/02_ddpm_training/#training-objective","title":"Training Objective","text":""},{"location":"DDPM/02_ddpm_training/#simple-loss-what-you-actually-use","title":"Simple Loss (What You Actually Use)","text":"<p>The simple loss from Ho et al. (2020):</p> \\[ L_{\\text{simple}} = \\mathbb{E}_{t, x_0, \\epsilon} \\left[\\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2\\right] \\] <p>Algorithm: <pre><code>1. Sample x_0 ~ q(x_0)           # Real data\n2. Sample t ~ Uniform({1,...,T})  # Random timestep\n3. Sample \u03b5 ~ N(0, I)             # Noise\n4. Compute x_t = sqrt(\u03b1\u0305_t) * x_0 + sqrt(1 - \u03b1\u0305_t) * \u03b5\n5. Predict \u03b5_\u03b8(x_t, t)\n6. Loss = ||\u03b5 - \u03b5_\u03b8(x_t, t)||\u00b2\n7. Update \u03b8 via gradient descent\n</code></pre></p>"},{"location":"DDPM/02_ddpm_training/#why-simple-loss-works","title":"Why Simple Loss Works","text":"<p>The simple loss ignores the time-dependent weighting in the full ELBO:</p> \\[ L_{\\text{ELBO}} = \\mathbb{E}_{t, x_0, \\epsilon} \\left[\\frac{1}{2\\sigma_t^2} \\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2\\right] \\] <p>Empirical finding (Ho et al., 2020): The simple loss produces better sample quality despite being theoretically less justified.</p> <p>Intuition: The simple loss gives equal weight to all timesteps, preventing the model from over-focusing on high-noise timesteps.</p>"},{"location":"DDPM/02_ddpm_training/#loss-function-variants","title":"Loss Function Variants","text":""},{"location":"DDPM/02_ddpm_training/#1-noise-prediction-standard","title":"1. Noise Prediction (Standard)","text":"\\[ L = \\mathbb{E}_{t, x_0, \\epsilon} \\left[\\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2\\right] \\] <p>Pros:</p> <ul> <li>Most common formulation</li> <li>Works well empirically</li> <li>Easy to implement</li> </ul> <p>When to use: Default choice for most applications</p>"},{"location":"DDPM/02_ddpm_training/#2-score-prediction","title":"2. Score Prediction","text":"\\[ L = \\mathbb{E}_{t, x_0, \\epsilon} \\left[\\left\\|\\nabla_{x_t} \\log q(x_t) - s_\\theta(x_t, t)\\right\\|^2\\right] \\] <p>Equivalent to noise prediction with scaling:</p> \\[ s_\\theta(x_t, t) = -\\frac{1}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\] <p>When to use: When connecting to score matching literature</p>"},{"location":"DDPM/02_ddpm_training/#3-x_0-prediction","title":"3. \\(x_0\\) Prediction","text":"\\[ L = \\mathbb{E}_{t, x_0, \\epsilon} \\left[\\|x_0 - \\hat{x}_0(x_t, t)\\|^2\\right] \\] <p>where:</p> \\[ \\hat{x}_0(x_t, t) = \\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_\\theta(x_t, t)}{\\sqrt{\\bar{\\alpha}_t}} \\] <p>Pros:</p> <ul> <li>Direct prediction of clean data</li> <li>Can be easier to interpret</li> </ul> <p>Cons:</p> <ul> <li>Can be less stable (predicting data vs. noise)</li> </ul> <p>When to use: When you want direct \\(x_0\\) estimates (e.g., for visualization)</p>"},{"location":"DDPM/02_ddpm_training/#4-velocity-prediction-rectified-flow","title":"4. Velocity Prediction (Rectified Flow)","text":"\\[ L = \\mathbb{E}_{t, x_0, x_1} \\left[\\|v_t - v_\\theta(x_t, t)\\|^2\\right] \\] <p>where \\(v_t = x_1 - x_0\\) is the \"velocity\" from noise to data.</p> <p>When to use: Rectified flow models, ODE-based sampling</p>"},{"location":"DDPM/02_ddpm_training/#architecture-choices","title":"Architecture Choices","text":""},{"location":"DDPM/02_ddpm_training/#for-images-u-net","title":"For Images: U-Net","text":"<p>Standard architecture for image diffusion models.</p> <p>Key components:</p> <ul> <li>Encoder-decoder structure: Downsampling \u2192 bottleneck \u2192 upsampling</li> <li>Skip connections: Preserve spatial information</li> <li>Attention blocks: Capture long-range dependencies</li> <li>Time conditioning: Via AdaGN (Adaptive Group Normalization)</li> </ul> <p>Example structure: <pre><code>Input (3, 256, 256)\n  \u2193 Conv + ResBlock + Attention\n(64, 128, 128)\n  \u2193 Downsample\n(128, 64, 64)\n  \u2193 Downsample\n(256, 32, 32)\n  \u2193 Bottleneck + Attention\n(256, 32, 32)\n  \u2193 Upsample + Skip\n(128, 64, 64)\n  \u2193 Upsample + Skip\n(64, 128, 128)\n  \u2193 Conv\nOutput (3, 256, 256)\n</code></pre></p> <p>When to use: Images, spatial data</p>"},{"location":"DDPM/02_ddpm_training/#for-tabular-data-mlp","title":"For Tabular Data: MLP","text":"<p>Simple architecture for non-spatial data (gene expression, tabular features).</p> <p>Key components:</p> <ul> <li>Residual MLP blocks: Prevent vanishing gradients</li> <li>Layer normalization: Stabilize training</li> <li>Time embeddings: Sinusoidal positional encodings</li> <li>Conditional embeddings: Concatenate or cross-attention</li> </ul> <p>Example from notebook: <pre><code>class ConditionalScoreNetwork(nn.Module):\n    def __init__(self, input_dim, hidden_dim=256, n_layers=4):\n        # Time embedding\n        self.time_mlp = SinusoidalPositionEmbeddings(64)\n\n        # Condition embedding\n        self.condition_embed = nn.Embedding(n_conditions, 32)\n\n        # MLP blocks with residual connections\n        self.blocks = nn.ModuleList([\n            MLPBlock(hidden_dim) for _ in range(n_layers)\n        ])\n</code></pre></p> <p>When to use: Gene expression, tabular data, point clouds</p>"},{"location":"DDPM/02_ddpm_training/#for-sequences-diffusion-transformers-dit","title":"For Sequences: Diffusion Transformers (DiT)","text":"<p>Transformer-based architecture for sequences and non-grid data.</p> <p>Key components:</p> <ul> <li>Token embeddings: Convert data to tokens</li> <li>Self-attention: Capture dependencies</li> <li>Time conditioning: Via AdaLN (Adaptive Layer Normalization)</li> <li>Positional encodings: For sequential data</li> </ul> <p>When to use: Sequences, non-grid structured data, biological sequences</p>"},{"location":"DDPM/02_ddpm_training/#time-conditioning","title":"Time Conditioning","text":""},{"location":"DDPM/02_ddpm_training/#sinusoidal-embeddings-standard","title":"Sinusoidal Embeddings (Standard)","text":"\\[ \\text{PE}(t, 2i) = \\sin\\left(\\frac{t}{10000^{2i/d}}\\right), \\quad \\text{PE}(t, 2i+1) = \\cos\\left(\\frac{t}{10000^{2i/d}}\\right) \\] <p>Pros:</p> <ul> <li>No learnable parameters</li> <li>Smooth interpolation</li> <li>Works well empirically</li> </ul> <p>Implementation: <pre><code>def sinusoidal_embedding(timesteps, dim):\n    half_dim = dim // 2\n    emb = np.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim) * -emb)\n    emb = timesteps[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n    return emb\n</code></pre></p>"},{"location":"DDPM/02_ddpm_training/#learned-embeddings","title":"Learned Embeddings","text":"\\[ t_{\\text{emb}} = \\text{Embedding}(t) \\] <p>Pros:</p> <ul> <li>Can learn task-specific representations</li> <li>More flexible</li> </ul> <p>Cons:</p> <ul> <li>Requires more parameters</li> <li>May overfit with limited data</li> </ul> <p>When to use: Large-scale models with lots of data</p>"},{"location":"DDPM/02_ddpm_training/#time-conditioning-mechanisms","title":"Time Conditioning Mechanisms","text":"<p>1. Concatenation (Simple): <pre><code>h = torch.cat([x, time_emb], dim=-1)\n</code></pre></p> <p>2. Additive (U-Net style): <pre><code>h = x + time_emb\n</code></pre></p> <p>3. Adaptive Normalization (AdaGN, AdaLN): <pre><code>scale, shift = time_mlp(time_emb).chunk(2, dim=-1)\nh = scale * normalize(x) + shift\n</code></pre></p> <p>Best practice: AdaGN/AdaLN for images, concatenation for tabular data</p>"},{"location":"DDPM/02_ddpm_training/#conditional-generation","title":"Conditional Generation","text":""},{"location":"DDPM/02_ddpm_training/#types-of-conditioning","title":"Types of Conditioning","text":"<ol> <li>Class-conditional: Generate specific categories (e.g., cell types)</li> <li>Text-conditional: Generate from text descriptions</li> <li>Image-conditional: Inpainting, super-resolution</li> <li>Continuous-conditional: Drug dose, physical parameters</li> </ol>"},{"location":"DDPM/02_ddpm_training/#conditioning-strategies","title":"Conditioning Strategies","text":""},{"location":"DDPM/02_ddpm_training/#1-concatenation-simple","title":"1. Concatenation (Simple)","text":"<pre><code>condition_emb = embedding(condition)\nh = torch.cat([x, time_emb, condition_emb], dim=-1)\n</code></pre> <p>Pros: Simple, works well for discrete conditions Cons: Limited flexibility</p>"},{"location":"DDPM/02_ddpm_training/#2-cross-attention-text-to-image","title":"2. Cross-Attention (Text-to-Image)","text":"<pre><code># Query from noisy image\nQ = linear_q(x)\n\n# Key, Value from text embedding\nK = linear_k(text_emb)\nV = linear_v(text_emb)\n\n# Attention\nattention = softmax(Q @ K.T / sqrt(d)) @ V\n</code></pre> <p>Pros: Flexible, captures complex relationships Cons: More parameters, slower</p> <p>When to use: Text-to-image, complex conditioning</p>"},{"location":"DDPM/02_ddpm_training/#3-adaptive-normalization-adagn","title":"3. Adaptive Normalization (AdaGN)","text":"<pre><code>scale, shift = condition_mlp(condition).chunk(2, dim=-1)\nh = scale * group_norm(x) + shift\n</code></pre> <p>Pros: Efficient, modulates features directly Cons: Less flexible than cross-attention</p> <p>When to use: Class-conditional, continuous conditioning</p>"},{"location":"DDPM/02_ddpm_training/#classifier-free-guidance","title":"Classifier-Free Guidance","text":"<p>Key idea: Train both conditional and unconditional models simultaneously.</p> <p>Training: <pre><code># Randomly drop condition with probability p (e.g., 0.1)\nif random() &lt; p:\n    condition = None  # Unconditional\n</code></pre></p> <p>Sampling: <pre><code># Interpolate between conditional and unconditional predictions\n\u03b5_pred = \u03b5_uncond + w * (\u03b5_cond - \u03b5_uncond)\n</code></pre></p> <p>where \\(w\\) is the guidance scale (typically 1-10).</p> <p>Effect: Higher \\(w\\) \u2192 stronger conditioning, less diversity</p>"},{"location":"DDPM/02_ddpm_training/#hyperparameters","title":"Hyperparameters","text":""},{"location":"DDPM/02_ddpm_training/#noise-schedule","title":"Noise Schedule","text":"<p>Linear schedule (original DDPM): <pre><code>betas = torch.linspace(1e-4, 0.02, T)\n</code></pre></p> <p>Cosine schedule (improved DDPM): <pre><code>def cosine_schedule(t, T, s=0.008):\n    f_t = np.cos((t/T + s) / (1 + s) * np.pi / 2) ** 2\n    alpha_bar_t = f_t / f(0)\n    return alpha_bar_t\n</code></pre></p> <p>Best practice: Cosine schedule for better sample quality</p>"},{"location":"DDPM/02_ddpm_training/#number-of-timesteps","title":"Number of Timesteps","text":"<ul> <li>Training: \\(T = 1000\\) (standard)</li> <li>Sampling: Can use fewer steps with DDIM (e.g., 50-100)</li> </ul> <p>Trade-off: More steps \u2192 better quality, slower sampling</p>"},{"location":"DDPM/02_ddpm_training/#learning-rate","title":"Learning Rate","text":"<ul> <li>Images: \\(1 \\times 10^{-4}\\) to \\(2 \\times 10^{-4}\\)</li> <li>Tabular: \\(1 \\times 10^{-4}\\) to \\(5 \\times 10^{-4}\\)</li> </ul> <p>Best practice: Use AdamW with weight decay \\(0.01\\)</p>"},{"location":"DDPM/02_ddpm_training/#batch-size","title":"Batch Size","text":"<ul> <li>Images: 128-256 (depends on GPU memory)</li> <li>Tabular: 128-512</li> </ul> <p>Best practice: Larger batch size \u2192 more stable training</p>"},{"location":"DDPM/02_ddpm_training/#training-tips","title":"Training Tips","text":""},{"location":"DDPM/02_ddpm_training/#1-ema-exponential-moving-average","title":"1. EMA (Exponential Moving Average)","text":"<p>Maintain a moving average of model weights:</p> <pre><code>ema_model = copy.deepcopy(model)\n\n# After each training step\nfor ema_param, param in zip(ema_model.parameters(), model.parameters()):\n    ema_param.data.mul_(0.999).add_(param.data, alpha=0.001)\n</code></pre> <p>Effect: Smoother samples, better quality</p>"},{"location":"DDPM/02_ddpm_training/#2-gradient-clipping","title":"2. Gradient Clipping","text":"<pre><code>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n</code></pre> <p>Effect: Prevents exploding gradients, stabilizes training</p>"},{"location":"DDPM/02_ddpm_training/#3-mixed-precision-training","title":"3. Mixed Precision Training","text":"<pre><code>from torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()\n\nwith autocast():\n    loss = compute_loss(...)\n\nscaler.scale(loss).backward()\nscaler.step(optimizer)\nscaler.update()\n</code></pre> <p>Effect: Faster training, lower memory usage</p>"},{"location":"DDPM/02_ddpm_training/#4-monitoring","title":"4. Monitoring","text":"<p>Key metrics to track:</p> <ul> <li>Training loss (should decrease steadily)</li> <li>Sample quality (visual inspection or FID)</li> <li>Gradient norms (should be stable)</li> </ul> <p>Best practice: Generate samples every N epochs to monitor quality</p>"},{"location":"DDPM/02_ddpm_training/#common-issues","title":"Common Issues","text":""},{"location":"DDPM/02_ddpm_training/#issue-1-model-predicts-constant-noise","title":"Issue 1: Model Predicts Constant Noise","text":"<p>Symptom: Generated samples are pure noise Cause: Model hasn't learned the score function Solution:</p> <ul> <li>Train longer</li> <li>Check learning rate (may be too high or too low)</li> <li>Verify data preprocessing</li> </ul>"},{"location":"DDPM/02_ddpm_training/#issue-2-mode-collapse","title":"Issue 2: Mode Collapse","text":"<p>Symptom: Model generates similar samples Cause: Insufficient model capacity or training Solution:</p> <ul> <li>Increase model size</li> <li>Train longer</li> <li>Use classifier-free guidance</li> </ul>"},{"location":"DDPM/02_ddpm_training/#issue-3-slow-convergence","title":"Issue 3: Slow Convergence","text":"<p>Symptom: Loss decreases very slowly Cause: Poor hyperparameters or architecture Solution:</p> <ul> <li>Increase learning rate</li> <li>Use cosine schedule instead of linear</li> <li>Add more layers or hidden dimensions</li> </ul>"},{"location":"DDPM/02_ddpm_training/#summary","title":"Summary","text":"<p>Key training decisions:</p> <ol> <li>Loss: Use simple MSE on noise prediction</li> <li>Architecture: U-Net for images, MLP for tabular, DiT for sequences</li> <li>Time conditioning: Sinusoidal embeddings with AdaGN/concatenation</li> <li>Conditioning: Concatenation for simple, cross-attention for complex</li> <li>Hyperparameters: Cosine schedule, \\(T=1000\\), lr=\\(10^{-4}\\)</li> <li>Training tips: Use EMA, gradient clipping, mixed precision</li> </ol> <p>Best practices:</p> <ul> <li>Start with simple loss and standard architecture</li> <li>Use cosine schedule for better quality</li> <li>Monitor sample quality during training</li> <li>Use EMA for final model</li> </ul>"},{"location":"DDPM/02_ddpm_training/#related-documents","title":"Related Documents","text":"<ul> <li>DDPM Foundations \u2014 Mathematical theory</li> <li>DDPM Sampling \u2014 Sampling algorithms</li> <li>DDPM Basics Notebook \u2014 Implementation</li> <li>SDE View \u2014 Continuous-time perspective</li> </ul>"},{"location":"DDPM/02_ddpm_training/#references","title":"References","text":"<ol> <li>Ho, J., Jain, A., &amp; Abbeel, P. (2020). Denoising Diffusion Probabilistic Models. NeurIPS.</li> <li>Nichol, A., &amp; Dhariwal, P. (2021). Improved Denoising Diffusion Probabilistic Models. ICML.</li> <li>Dhariwal, P., &amp; Nichol, A. (2021). Diffusion Models Beat GANs on Image Synthesis. NeurIPS.</li> <li>Ho, J., &amp; Salimans, T. (2022). Classifier-Free Diffusion Guidance. NeurIPS Workshop.</li> <li>Peebles, W., &amp; Xie, S. (2023). Scalable Diffusion Models with Transformers. ICCV.</li> </ol>"},{"location":"DDPM/02a_diffusion_arch_gene_expression/","title":"Architecture Choices for Gene Expression Data","text":"<p>This document explores different architectural approaches for diffusion models on gene expression data, moving beyond simple \"tabular MLP\" treatment to more sophisticated tokenization and modeling strategies.</p> <p>Key question: How should we represent and model gene expression data in diffusion models?</p>"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#the-problem-with-tabular-data-treatment","title":"The Problem with \"Tabular Data\" Treatment","text":""},{"location":"DDPM/02a_diffusion_arch_gene_expression/#why-gene-expression-isnt-just-tabular","title":"Why Gene Expression Isn't Just Tabular","text":"<p>Standard tabular approach (from <code>02_ddpm_training.md</code>): <pre><code># Treat as flat vector\nx = gene_expression  # (batch, 20000)\nx_noisy = add_noise(x, t)\nnoise_pred = mlp(x_noisy, t)  # Simple MLP\n</code></pre></p> <p>Problems: 1. Ignores structure: Genes aren't independent features 2. Ignores biology: Gene regulatory networks, pathways, modules 3. High dimensionality: 20K genes \u2192 huge parameter count 4. No inductive bias: Model must learn everything from scratch 5. Poor generalization: Doesn't transfer across datasets/conditions</p> <p>Reality: Gene expression has rich structure that should inform architecture.</p>"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#rethinking-tokenization-for-gene-expression","title":"Rethinking Tokenization for Gene Expression","text":"<p>Core insight: \"Tokenization\" = \"How we factor the object so attention has something meaningful to attend over\"</p> <p>Not just a preprocessing step \u2014 tokenization IS an architectural choice that determines: - What the model can learn - How efficiently it learns - How well it generalizes - How interpretable it is</p>"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#option-1-latent-tokens-recommended-default","title":"Option 1: Latent Tokens (Recommended Default)","text":""},{"location":"DDPM/02a_diffusion_arch_gene_expression/#concept","title":"Concept","text":"<p>Encoder-decoder architecture with learned latent representation:</p> <pre><code>Gene expression (20K) \u2192 Encoder \u2192 Latent tokens (m\u00d7d) \u2192 DiT \u2192 Decoder \u2192 Output\n</code></pre> <p>Key idea: Learn a compressed, structured representation where each token captures meaningful biological variation.</p>"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#architecture","title":"Architecture","text":"<pre><code>class LatentTokenDiffusion(nn.Module):\n    def __init__(\n        self,\n        num_genes=20000,\n        num_tokens=64,      # m tokens\n        token_dim=256,      # d dimensions per token\n        num_layers=12,\n        num_heads=8\n    ):\n        super().__init__()\n\n        # Encoder: gene expression \u2192 latent tokens\n        self.encoder = nn.Sequential(\n            nn.Linear(num_genes, 2048),\n            nn.LayerNorm(2048),\n            nn.GELU(),\n            nn.Linear(2048, num_tokens * token_dim)\n        )\n\n        # Reshape to tokens\n        self.num_tokens = num_tokens\n        self.token_dim = token_dim\n\n        # Positional encoding for tokens\n        self.pos_embed = nn.Parameter(torch.randn(1, num_tokens, token_dim))\n\n        # Transformer (DiT)\n        self.transformer = DiT(\n            embed_dim=token_dim,\n            depth=num_layers,\n            num_heads=num_heads\n        )\n\n        # Decoder: latent tokens \u2192 gene expression parameters\n        self.decoder = nn.Sequential(\n            nn.Linear(num_tokens * token_dim, 2048),\n            nn.LayerNorm(2048),\n            nn.GELU(),\n            nn.Linear(2048, num_genes * 2)  # Mean and variance for NB/ZINB\n        )\n\n    def encode(self, x):\n        \"\"\"Encode gene expression to latent tokens.\"\"\"\n        # x: (batch, num_genes)\n        z = self.encoder(x)  # (batch, num_tokens * token_dim)\n        z = z.view(-1, self.num_tokens, self.token_dim)  # (batch, num_tokens, token_dim)\n        z = z + self.pos_embed  # Add positional encoding\n        return z\n\n    def decode(self, z):\n        \"\"\"Decode latent tokens to gene expression parameters.\"\"\"\n        # z: (batch, num_tokens, token_dim)\n        z_flat = z.view(-1, self.num_tokens * self.token_dim)\n        params = self.decoder(z_flat)  # (batch, num_genes * 2)\n        mean, logvar = params.chunk(2, dim=-1)\n        return mean, logvar\n\n    def forward(self, x, t, condition=None):\n        \"\"\"\n        Forward pass for diffusion.\n\n        Args:\n            x: Gene expression (batch, num_genes)\n            t: Timesteps (batch,)\n            condition: Optional conditioning (perturbations, cell types, etc.)\n\n        Returns:\n            noise_pred: Predicted noise (batch, num_genes)\n        \"\"\"\n        # Encode to latent tokens\n        z = self.encode(x)  # (batch, num_tokens, token_dim)\n\n        # Run transformer on tokens\n        z_out = self.transformer(z, t, condition)  # (batch, num_tokens, token_dim)\n\n        # Decode to gene space\n        mean, logvar = self.decode(z_out)\n\n        return mean  # Or return distribution parameters\n</code></pre>"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#why-its-good","title":"Why It's Good","text":"<p>1. Learned, data-adaptive tokenization</p> <ul> <li>Model learns what biological variation to capture</li> <li>Tokens emerge from data, not imposed a priori</li> <li>Can discover novel gene modules/patterns</li> </ul> <p>2. Compute-friendly</p> <ul> <li>64 tokens &lt;&lt; 20K genes</li> <li>Attention is O(m\u00b2) where m=64, not O(20000\u00b2)</li> <li>Enables scaling to large models</li> </ul> <p>3. Plays nicely with LoRA/adapters <pre><code># Fine-tune on new dataset with small adapter\nclass LoRAAdapter(nn.Module):\n    def __init__(self, token_dim=256, rank=8):\n        super().__init__()\n        self.down = nn.Linear(token_dim, rank)\n        self.up = nn.Linear(rank, token_dim)\n\n    def forward(self, z):\n        return z + self.up(self.down(z))\n\n# Add adapter to frozen backbone\nmodel.transformer.requires_grad_(False)\nadapter = LoRAAdapter()\n</code></pre></p> <p>4. Flexible conditioning</p> <ul> <li>Easy to inject perturbation info at token level</li> <li>Can condition on cell type, time, etc.</li> </ul>"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#training-strategy","title":"Training Strategy","text":"<pre><code># Training loop\nfor x_0, condition in dataloader:\n    # Sample noise and timestep\n    t = torch.rand(batch_size)\n    noise = torch.randn_like(x_0)\n\n    # Add noise (in gene space or latent space)\n    x_t = sqrt(alpha_t) * x_0 + sqrt(1 - alpha_t) * noise\n\n    # Predict noise\n    noise_pred = model(x_t, t, condition)\n\n    # Loss\n    loss = F.mse_loss(noise_pred, noise)\n    loss.backward()\n    optimizer.step()\n</code></pre> <p>Alternative: Diffuse in latent space <pre><code># Encode to latent\nz_0 = model.encode(x_0)\n\n# Add noise in latent space\nz_t = sqrt(alpha_t) * z_0 + sqrt(1 - alpha_t) * noise\n\n# Predict in latent space\nnoise_pred = model.transformer(z_t, t, condition)\n\n# Decode\nx_pred = model.decode(noise_pred)\n</code></pre></p>"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#hyperparameters","title":"Hyperparameters","text":"Parameter Typical Value Notes <code>num_tokens</code> 32-128 Balance between capacity and compute <code>token_dim</code> 256-512 Higher for complex datasets <code>num_layers</code> 8-16 Deeper for better quality <code>num_heads</code> 8-16 Standard transformer setting"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#option-2-pathwaymodule-tokens-biologically-anchored","title":"Option 2: Pathway/Module Tokens (Biologically Anchored)","text":""},{"location":"DDPM/02a_diffusion_arch_gene_expression/#concept_1","title":"Concept","text":"<p>Use biological knowledge to define tokens as gene pathways or modules.</p> <pre><code>Genes \u2192 Group by pathway \u2192 Pathway embeddings \u2192 DiT \u2192 Output\n</code></pre> <p>Key idea: Each token represents a biological process (glycolysis, cell cycle, immune response, etc.)</p>"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#architecture_1","title":"Architecture","text":"<pre><code>class PathwayTokenDiffusion(nn.Module):\n    def __init__(\n        self,\n        num_genes=20000,\n        pathway_db='msigdb',  # MSigDB, Reactome, GO, etc.\n        token_dim=256,\n        num_layers=12\n    ):\n        super().__init__()\n\n        # Load pathway definitions\n        self.pathways = load_pathways(pathway_db)  # Dict: pathway_name \u2192 gene_indices\n        self.num_pathways = len(self.pathways)\n\n        # Gene-to-pathway mapping\n        self.gene_to_pathway = self._build_gene_pathway_matrix()  # (num_genes, num_pathways)\n\n        # Pathway embedding\n        self.pathway_embed = nn.ModuleDict({\n            name: nn.Linear(len(genes), token_dim)\n            for name, genes in self.pathways.items()\n        })\n\n        # Transformer on pathway tokens\n        self.transformer = DiT(\n            embed_dim=token_dim,\n            depth=num_layers,\n            num_heads=8\n        )\n\n        # Decoder: pathway tokens \u2192 gene expression\n        self.gene_decoder = nn.ModuleDict({\n            name: nn.Linear(token_dim, len(genes))\n            for name, genes in self.pathways.items()\n        })\n\n    def encode_pathways(self, x):\n        \"\"\"\n        Encode gene expression to pathway tokens.\n\n        Args:\n            x: Gene expression (batch, num_genes)\n\n        Returns:\n            pathway_tokens: (batch, num_pathways, token_dim)\n        \"\"\"\n        tokens = []\n\n        for pathway_name, gene_indices in self.pathways.items():\n            # Extract genes for this pathway\n            pathway_expr = x[:, gene_indices]  # (batch, num_genes_in_pathway)\n\n            # Embed to token\n            token = self.pathway_embed[pathway_name](pathway_expr)  # (batch, token_dim)\n            tokens.append(token)\n\n        # Stack tokens\n        pathway_tokens = torch.stack(tokens, dim=1)  # (batch, num_pathways, token_dim)\n\n        return pathway_tokens\n\n    def decode_pathways(self, pathway_tokens):\n        \"\"\"\n        Decode pathway tokens to gene expression.\n\n        Args:\n            pathway_tokens: (batch, num_pathways, token_dim)\n\n        Returns:\n            x_recon: Gene expression (batch, num_genes)\n        \"\"\"\n        gene_predictions = torch.zeros(pathway_tokens.shape[0], self.num_genes, device=pathway_tokens.device)\n        gene_counts = torch.zeros(self.num_genes, device=pathway_tokens.device)\n\n        for i, (pathway_name, gene_indices) in enumerate(self.pathways.items()):\n            # Decode token to genes\n            token = pathway_tokens[:, i, :]  # (batch, token_dim)\n            genes_pred = self.gene_decoder[pathway_name](token)  # (batch, num_genes_in_pathway)\n\n            # Accumulate predictions\n            gene_predictions[:, gene_indices] += genes_pred\n            gene_counts[gene_indices] += 1\n\n        # Average overlapping predictions\n        x_recon = gene_predictions / gene_counts.clamp(min=1)\n\n        return x_recon\n\n    def forward(self, x, t, condition=None):\n        # Encode to pathway tokens\n        tokens = self.encode_pathways(x)\n\n        # Transform with DiT\n        tokens_out = self.transformer(tokens, t, condition)\n\n        # Decode to genes\n        x_out = self.decode_pathways(tokens_out)\n\n        return x_out\n</code></pre>"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#why-its-good_1","title":"Why It's Good","text":"<p>1. Interpretability</p> <ul> <li>Each token has biological meaning</li> <li>Can explain predictions at pathway level</li> <li>Clinically legible (\"upregulated immune pathways\")</li> </ul> <p>2. Lower dimension</p> <ul> <li>~500 pathways vs 20K genes</li> <li>More tractable for analysis</li> </ul> <p>3. Transfer learning</p> <ul> <li>Pathways are consistent across datasets</li> <li>Model trained on one dataset can transfer to another</li> <li>Easier to align across species (conserved pathways)</li> </ul> <p>4. Inductive bias</p> <ul> <li>Encodes known biology</li> <li>Faster convergence</li> <li>Better generalization with limited data</li> </ul>"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#pathway-databases","title":"Pathway Databases","text":"Database # Pathways Coverage Use Case MSigDB Hallmark 50 Broad processes High-level analysis MSigDB C2 (KEGG) 186 Metabolic/signaling Mechanistic studies Reactome 2,500+ Detailed processes Fine-grained analysis GO Biological Process 10,000+ Very detailed Comprehensive coverage Data-driven modules Variable Dataset-specific Custom applications <p>Recommendation: Start with MSigDB Hallmark (50 pathways), scale to Reactome if needed.</p>"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#data-driven-module-discovery","title":"Data-Driven Module Discovery","text":"<p>Alternative: Learn modules from data instead of using databases.</p> <pre><code>from sklearn.decomposition import NMF\n\n# Learn gene modules via NMF\nnmf = NMF(n_components=100, random_state=42)\nW = nmf.fit_transform(gene_expression_matrix.T)  # (num_genes, 100)\nH = nmf.components_  # (100, num_samples)\n\n# W defines gene-to-module mapping\n# Use W to create pathway_embed layers\n</code></pre>"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#option-3-graph-structured-tokens-grn-aware-attention","title":"Option 3: Graph-Structured Tokens (GRN-Aware Attention)","text":""},{"location":"DDPM/02a_diffusion_arch_gene_expression/#concept_2","title":"Concept","text":"<p>Use gene regulatory networks to structure attention.</p> <pre><code>Genes as tokens + GRN structure \u2192 Sparse attention \u2192 Output\n</code></pre> <p>Key idea: Attention is constrained by known regulatory relationships, avoiding full O(n\u00b2) computation.</p>"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#architecture_2","title":"Architecture","text":"<pre><code>class GRNAwareDiffusion(nn.Module):\n    def __init__(\n        self,\n        num_genes=20000,\n        grn_adjacency=None,  # Sparse adjacency matrix\n        token_dim=256,\n        num_layers=8\n    ):\n        super().__init__()\n\n        # Gene embeddings\n        self.gene_embed = nn.Linear(1, token_dim)  # Each gene \u2192 token\n\n        # GRN structure\n        self.grn_adjacency = grn_adjacency  # (num_genes, num_genes) sparse\n\n        # Graph attention layers\n        self.gat_layers = nn.ModuleList([\n            GraphAttentionLayer(token_dim, num_heads=8, adjacency=grn_adjacency)\n            for _ in range(num_layers)\n        ])\n\n        # Time embedding\n        self.time_embed = TimestepEmbedding(token_dim)\n\n        # Output projection\n        self.output_proj = nn.Linear(token_dim, 1)\n\n    def forward(self, x, t, condition=None):\n        \"\"\"\n        Args:\n            x: Gene expression (batch, num_genes)\n            t: Timesteps (batch,)\n            condition: Optional conditioning\n\n        Returns:\n            x_out: Predicted expression (batch, num_genes)\n        \"\"\"\n        batch_size = x.shape[0]\n\n        # Embed genes to tokens\n        x_tokens = self.gene_embed(x.unsqueeze(-1))  # (batch, num_genes, token_dim)\n\n        # Time embedding\n        t_emb = self.time_embed(t)  # (batch, token_dim)\n        t_emb = t_emb.unsqueeze(1)  # (batch, 1, token_dim)\n\n        # Add time to all tokens\n        x_tokens = x_tokens + t_emb\n\n        # Graph attention layers (structured by GRN)\n        for layer in self.gat_layers:\n            x_tokens = layer(x_tokens)  # (batch, num_genes, token_dim)\n\n        # Project back to gene space\n        x_out = self.output_proj(x_tokens).squeeze(-1)  # (batch, num_genes)\n\n        return x_out\n\n\nclass GraphAttentionLayer(nn.Module):\n    def __init__(self, token_dim, num_heads=8, adjacency=None):\n        super().__init__()\n        self.num_heads = num_heads\n        self.head_dim = token_dim // num_heads\n        self.adjacency = adjacency  # Sparse mask\n\n        self.qkv = nn.Linear(token_dim, token_dim * 3)\n        self.proj = nn.Linear(token_dim, token_dim)\n        self.norm = nn.LayerNorm(token_dim)\n\n    def forward(self, x):\n        \"\"\"\n        Sparse attention based on GRN structure.\n\n        Args:\n            x: (batch, num_genes, token_dim)\n\n        Returns:\n            out: (batch, num_genes, token_dim)\n        \"\"\"\n        batch_size, num_genes, token_dim = x.shape\n\n        # QKV projection\n        qkv = self.qkv(x)  # (batch, num_genes, token_dim * 3)\n        q, k, v = qkv.chunk(3, dim=-1)\n\n        # Reshape for multi-head\n        q = q.view(batch_size, num_genes, self.num_heads, self.head_dim).transpose(1, 2)\n        k = k.view(batch_size, num_genes, self.num_heads, self.head_dim).transpose(1, 2)\n        v = v.view(batch_size, num_genes, self.num_heads, self.head_dim).transpose(1, 2)\n\n        # Attention scores\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n\n        # Apply GRN mask (only attend to neighbors)\n        if self.adjacency is not None:\n            mask = self.adjacency.unsqueeze(0).unsqueeze(0)  # (1, 1, num_genes, num_genes)\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n\n        # Softmax\n        attn = F.softmax(scores, dim=-1)\n\n        # Apply attention to values\n        out = torch.matmul(attn, v)  # (batch, num_heads, num_genes, head_dim)\n\n        # Concatenate heads\n        out = out.transpose(1, 2).contiguous().view(batch_size, num_genes, token_dim)\n\n        # Output projection\n        out = self.proj(out)\n\n        # Residual + norm\n        out = self.norm(x + out)\n\n        return out\n</code></pre>"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#why-its-good_2","title":"Why It's Good","text":"<p>1. Mechanistic flavor</p> <ul> <li>Respects known regulatory relationships</li> <li>More biologically plausible</li> <li>Better for causal reasoning</li> </ul> <p>2. Better inductive bias for perturbations</p> <ul> <li>Perturbations propagate through GRN</li> <li>Model learns regulatory logic</li> <li>More accurate predictions for unseen perturbations</li> </ul> <p>3. Computational efficiency</p> <ul> <li>Sparse attention: O(num_edges) instead of O(n\u00b2)</li> <li>Typical GRN: ~100K edges for 20K genes</li> <li>Much faster than full attention</li> </ul> <p>4. Interpretability</p> <ul> <li>Can trace predictions through regulatory paths</li> <li>Identify key regulators</li> <li>Explain perturbation effects mechanistically</li> </ul>"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#grn-sources","title":"GRN Sources","text":"Source Coverage Quality Use Case STRING Broad Moderate General purpose RegNetwork Human TFs High Transcriptional regulation SCENIC Data-driven Variable Dataset-specific ChIP-seq databases Validated High High-confidence edges Inferred from data Custom Variable Novel systems <p>Recommendation: Start with STRING or RegNetwork, refine with data-driven inference.</p>"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#option-4-rank-based-sequences-geneformer-style","title":"Option 4: Rank-Based Sequences (Geneformer Style)","text":""},{"location":"DDPM/02a_diffusion_arch_gene_expression/#concept_3","title":"Concept","text":"<p>Treat genes as sequence by ranking by expression level.</p> <pre><code>Gene expression \u2192 Rank genes \u2192 Sequence of gene tokens \u2192 Transformer \u2192 Output\n</code></pre> <p>Key idea: Order matters for transformers, so impose ordering via expression level.</p>"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#architecture_3","title":"Architecture","text":"<pre><code>class RankBasedDiffusion(nn.Module):\n    def __init__(\n        self,\n        num_genes=20000,\n        embed_dim=256,\n        num_layers=12,\n        num_heads=8,\n        max_seq_len=2000  # Truncate to top-k genes\n    ):\n        super().__init__()\n\n        # Gene embeddings (learned for each gene)\n        self.gene_embed = nn.Embedding(num_genes, embed_dim)\n\n        # Value embeddings (expression level)\n        self.value_embed = nn.Linear(1, embed_dim)\n\n        # Positional encoding (rank position)\n        self.pos_embed = nn.Parameter(torch.randn(1, max_seq_len, embed_dim))\n\n        # Transformer\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(embed_dim, num_heads, dim_feedforward=embed_dim*4),\n            num_layers=num_layers\n        )\n\n        # Output: predict expression for each gene\n        self.output_proj = nn.Linear(embed_dim, 1)\n\n        self.max_seq_len = max_seq_len\n\n    def forward(self, x, t, condition=None):\n        \"\"\"\n        Args:\n            x: Gene expression (batch, num_genes)\n            t: Timesteps (batch,)\n\n        Returns:\n            x_out: Predicted expression (batch, num_genes)\n        \"\"\"\n        batch_size, num_genes = x.shape\n\n        # Rank genes by expression\n        sorted_indices = torch.argsort(x, dim=1, descending=True)  # (batch, num_genes)\n        sorted_values = torch.gather(x, 1, sorted_indices)  # (batch, num_genes)\n\n        # Truncate to top-k\n        sorted_indices = sorted_indices[:, :self.max_seq_len]\n        sorted_values = sorted_values[:, :self.max_seq_len]\n\n        # Gene embeddings\n        gene_emb = self.gene_embed(sorted_indices)  # (batch, max_seq_len, embed_dim)\n\n        # Value embeddings\n        value_emb = self.value_embed(sorted_values.unsqueeze(-1))  # (batch, max_seq_len, embed_dim)\n\n        # Combine\n        tokens = gene_emb + value_emb + self.pos_embed  # (batch, max_seq_len, embed_dim)\n\n        # Transformer\n        tokens = tokens.transpose(0, 1)  # (max_seq_len, batch, embed_dim)\n        tokens_out = self.transformer(tokens)  # (max_seq_len, batch, embed_dim)\n        tokens_out = tokens_out.transpose(0, 1)  # (batch, max_seq_len, embed_dim)\n\n        # Predict expression\n        expr_pred = self.output_proj(tokens_out).squeeze(-1)  # (batch, max_seq_len)\n\n        # Unsort to original gene order\n        x_out = torch.zeros(batch_size, num_genes, device=x.device)\n        x_out.scatter_(1, sorted_indices, expr_pred)\n\n        return x_out\n</code></pre>"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#why-its-good-and-not-so-good","title":"Why It's Good (and Not So Good)","text":"<p>Pros:</p> <ul> <li>Works empirically (Geneformer shows this)</li> <li>Natural for transformers (sequence processing)</li> <li>Can capture expression-dependent relationships</li> </ul> <p>Cons:</p> <ul> <li>Ordering artifacts: Genes with same expression get arbitrary order</li> <li>Scaling issues: 20K sequence length is expensive</li> <li>Truncation: Top-k genes loses information</li> <li>Not biologically motivated: Ranking is artificial</li> </ul> <p>When to use: </p> <ul> <li>When you have lots of data (Geneformer trained on millions of cells)</li> <li>When interpretability is less important</li> <li>When other methods fail</li> </ul>"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#comparison-and-recommendations","title":"Comparison and Recommendations","text":""},{"location":"DDPM/02a_diffusion_arch_gene_expression/#summary-table","title":"Summary Table","text":"Approach Tokens Compute Interpretability Biology Transfer Best For Latent tokens 32-128 Low Moderate Learned Good Default choice Pathway tokens 50-500 Low High Strong Excellent Clinical applications GRN-aware 20K (sparse) Moderate High Strong Moderate Perturbation modeling Rank-based 2K-20K High Low Weak Poor Large-scale pretraining"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#decision-tree","title":"Decision Tree","text":"<pre><code>Do you need interpretability?\n\u251c\u2500 Yes \u2192 Pathway tokens or GRN-aware\n\u2502   \u251c\u2500 Clinical application \u2192 Pathway tokens\n\u2502   \u2514\u2500 Perturbation prediction \u2192 GRN-aware\n\u2514\u2500 No \u2192 Latent tokens or Rank-based\n    \u251c\u2500 Limited compute \u2192 Latent tokens\n    \u2514\u2500 Massive data \u2192 Rank-based\n</code></pre>"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#recommended-starting-point","title":"Recommended Starting Point","text":"<p>For most applications: 1. Start with latent tokens (Option 1) 2. Use 64 tokens, 256 dimensions 3. Train with rectified flow (simple objective) 4. Evaluate on your task</p> <p>If interpretability matters: 1. Use pathway tokens (Option 2) 2. Start with MSigDB Hallmark (50 pathways) 3. Scale to Reactome if needed 4. Validate pathway-level predictions</p> <p>If modeling perturbations: 1. Use GRN-aware (Option 3) 2. Start with STRING or RegNetwork 3. Refine with data-driven edges 4. Validate on held-out perturbations</p>"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#hybrid-approaches","title":"Hybrid Approaches","text":""},{"location":"DDPM/02a_diffusion_arch_gene_expression/#combining-multiple-strategies","title":"Combining Multiple Strategies","text":"<p>Latent + Pathway: <pre><code># Encoder produces pathway-structured latents\nencoder_pathway = PathwayEncoder(pathways)\nz_pathway = encoder_pathway(x)  # (batch, num_pathways, token_dim)\n\n# Transformer on pathway tokens\nz_out = transformer(z_pathway, t)\n\n# Decoder with pathway structure\nx_out = decoder_pathway(z_out)\n</code></pre></p> <p>GRN + Latent: <pre><code># Encode to latent\nz = encoder(x)  # (batch, num_tokens, token_dim)\n\n# Graph attention on latent tokens (with learned adjacency)\nz_out = graph_transformer(z, learned_adjacency)\n\n# Decode\nx_out = decoder(z_out)\n</code></pre></p>"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#implementation-recommendations","title":"Implementation Recommendations","text":""},{"location":"DDPM/02a_diffusion_arch_gene_expression/#training-tips","title":"Training Tips","text":"<p>1. Start simple <pre><code># Begin with latent tokens, small model\nmodel = LatentTokenDiffusion(\n    num_genes=20000,\n    num_tokens=64,\n    token_dim=256,\n    num_layers=8\n)\n</code></pre></p> <p>2. Validate tokenization quality <pre><code># Check reconstruction\nz = model.encode(x)\nx_recon = model.decode(z)\nrecon_error = F.mse_loss(x_recon, x)\nprint(f\"Reconstruction error: {recon_error:.4f}\")\n</code></pre></p> <p>3. Visualize learned tokens <pre><code># For latent tokens: PCA on token activations\nz = model.encode(x_batch)  # (batch, num_tokens, token_dim)\nz_flat = z.mean(dim=0)  # (num_tokens, token_dim)\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nz_pca = pca.fit_transform(z_flat.detach().cpu())\n\nplt.scatter(z_pca[:, 0], z_pca[:, 1])\nplt.title(\"Learned token structure\")\n</code></pre></p> <p>4. Compare to baselines <pre><code># Baseline: Simple MLP\nbaseline = SimpleMLP(num_genes=20000, hidden_dim=256)\n\n# Your model\nmodel = LatentTokenDiffusion(...)\n\n# Compare on held-out data\nbaseline_loss = evaluate(baseline, test_data)\nmodel_loss = evaluate(model, test_data)\nprint(f\"Improvement: {(baseline_loss - model_loss) / baseline_loss * 100:.1f}%\")\n</code></pre></p>"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#key-takeaways","title":"Key Takeaways","text":""},{"location":"DDPM/02a_diffusion_arch_gene_expression/#conceptual","title":"Conceptual","text":"<ol> <li>Gene expression isn't tabular \u2014 it has structure that should inform architecture</li> <li>Tokenization = architectural choice \u2014 determines what model can learn</li> <li>Multiple valid approaches \u2014 choose based on your goals</li> <li>Biology can help \u2014 incorporating known structure improves models</li> </ol>"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#practical","title":"Practical","text":"<ol> <li>Default: Latent tokens \u2014 learned, flexible, compute-efficient</li> <li>Interpretability: Pathway tokens \u2014 biologically meaningful, clinically legible</li> <li>Perturbations: GRN-aware \u2014 mechanistic, better generalization</li> <li>Large-scale: Rank-based \u2014 works but heavy and less principled</li> </ol>"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#research-directions","title":"Research Directions","text":"<ol> <li>Optimal number of tokens \u2014 how many latent tokens are needed?</li> <li>Token interpretability \u2014 can we make latent tokens biologically meaningful?</li> <li>Hybrid approaches \u2014 combining multiple tokenization strategies</li> <li>Transfer learning \u2014 do tokens transfer across datasets/species?</li> </ol>"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#related-documents","title":"Related Documents","text":"<ul> <li>02_ddpm_training.md \u2014 General DDPM training (includes simple MLP baseline)</li> <li>DiT Foundations \u2014 Transformer architecture details</li> <li>Tokenization Research \u2014 Deep dive on tokenization challenges</li> </ul>"},{"location":"DDPM/02a_diffusion_arch_gene_expression/#references","title":"References","text":"<p>Latent diffusion:</p> <ul> <li>Rombach et al. (2022): \"High-Resolution Image Synthesis with Latent Diffusion Models\"</li> </ul> <p>Gene expression models:</p> <ul> <li>Theodoris et al. (2023): \"Transfer learning enables predictions in network biology\" (Geneformer)</li> <li>Cui et al. (2024): \"scGPT: Toward Building a Foundation Model for Single-Cell Multi-omics\"</li> <li>Lotfollahi et al. (2023): \"Predicting cellular responses to novel drug combinations\"</li> </ul> <p>Pathway analysis:</p> <ul> <li>Subramanian et al. (2005): \"Gene set enrichment analysis\" (GSEA)</li> <li>Liberzon et al. (2015): \"The Molecular Signatures Database (MSigDB)\"</li> </ul> <p>Gene regulatory networks:</p> <ul> <li>Szklarczyk et al. (2021): \"The STRING database\" </li> <li>Aibar et al. (2017): \"SCENIC: single-cell regulatory network inference\"</li> </ul>"},{"location":"DDPM/02b_diffusion_arch_qa/","title":"Architecture Q&amp;A: Gene Expression Diffusion Models","text":"<p>This document addresses common questions about architectural choices for gene expression data in diffusion models, particularly around the latent token approach.</p> <p>Related: 02a_diffusion_arch_gene_expression.md \u2014 Main architecture document</p>"},{"location":"DDPM/02b_diffusion_arch_qa/#question-1-handling-thousands-of-samples","title":"Question 1: Handling Thousands of Samples","text":"<p>Question: The latent token architecture shows encoding gene expression to tokens. But how does this account for gene expression data where each gene is represented by thousands of samples with expression levels?</p>"},{"location":"DDPM/02b_diffusion_arch_qa/#answer-sample-by-sample-processing","title":"Answer: Sample-by-Sample Processing","text":"<p>The architecture processes gene expression sample-by-sample, not all samples at once. Let me clarify the dimensions:</p>"},{"location":"DDPM/02b_diffusion_arch_qa/#what-the-architecture-actually-does","title":"What the Architecture Actually Does","text":"<pre><code># ONE sample (single cell or bulk RNA-seq measurement)\nx = gene_expression  # Shape: (num_genes,) = (20000,)\n                     # This is ONE measurement: [gene1_count, gene2_count, ..., gene20000_count]\n\n# Encode to tokens\nz = encoder(x)  # Shape: (num_tokens, token_dim) = (64, 256)\n                # Creates 64 \"semantic tokens\" from the 20K gene values\n\n# In practice, we batch multiple samples\nx_batch = gene_expressions  # Shape: (batch_size, num_genes) = (32, 20000)\nz_batch = encoder(x_batch)  # Shape: (32, 64, 256)\n                            # 32 samples, each encoded to 64 tokens\n</code></pre>"},{"location":"DDPM/02b_diffusion_arch_qa/#data-flow-clarification","title":"Data Flow Clarification","text":"<pre><code>DATASET STRUCTURE:\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nYou have: N samples \u00d7 20,000 genes\n- Sample 1: [gene1=5, gene2=120, ..., gene20000=3]\n- Sample 2: [gene1=8, gene2=95, ..., gene20000=7]\n- ...\n- Sample N: [gene1=12, gene2=150, ..., gene20000=2]\n\nTRAINING:\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nEach training iteration:\n1. Sample a batch (e.g., 32 samples)\n2. Each sample processed independently through encoder\n   Sample 1 \u2192 Encoder \u2192 64 tokens of dim 256\n   Sample 2 \u2192 Encoder \u2192 64 tokens of dim 256\n   ...\n   Sample 32 \u2192 Encoder \u2192 64 tokens of dim 256\n\n3. Batch shape: (32, 64, 256)\n   - 32 samples (batch dimension)\n   - 64 tokens per sample (sequence dimension)\n   - 256 features per token (feature dimension)\n\n4. Transformer processes each sample's token sequence\n5. Decoder: tokens \u2192 gene expression prediction\n</code></pre>"},{"location":"DDPM/02b_diffusion_arch_qa/#the-key-insight","title":"The Key Insight","text":"<p>The encoder is NOT trying to encode all samples into one representation. Instead:</p> <ul> <li>Input: One gene expression profile (20K genes for one cell/sample)</li> <li>Output: A compressed representation as 64 tokens (each 256-dim)</li> <li>Batching: Process multiple samples in parallel (standard minibatch training)</li> </ul> <p>Think of it like processing images:</p> <pre><code># Images\nimages = (batch=32, channels=3, height=224, width=224)\n# Each image processed independently\n\n# Gene expression\ngene_expr = (batch=32, num_genes=20000)\n# Each sample processed independently \u2192 (batch=32, num_tokens=64, token_dim=256)\n</code></pre>"},{"location":"DDPM/02b_diffusion_arch_qa/#complete-training-example","title":"Complete Training Example","text":"<pre><code># \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# GENE EXPRESSION DATA STRUCTURE\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n# Dataset: Collection of samples\ndataset = {\n    'sample_1': [gene1=5, gene2=120, ..., gene20000=3],    # Cell 1\n    'sample_2': [gene1=8, gene2=95, ..., gene20000=7],     # Cell 2\n    'sample_3': [gene1=12, gene2=150, ..., gene20000=2],   # Cell 3\n    ...\n    'sample_N': [...]\n}\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# DURING TRAINING (Minibatch)\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n# Step 1: Sample a batch\nbatch = 32 samples\nx = (32, 20000)  # 32 samples, each with 20K gene counts\n\n# Step 2: Encode each sample to tokens\n# Each sample processed independently!\nz = encoder(x)  # (32, 64, 256)\n                # \u2191   \u2191   \u2191\n                # |   |   \u2514\u2500 Features per token\n                # |   \u2514\u2500\u2500\u2500\u2500\u2500 Tokens per sample\n                # \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Batch size\n\n# Step 3: Add positional encoding (per sample)\nz = z + pos_embed  # pos_embed: (1, 64, 256), broadcasts across batch\n\n# Step 4: Transformer processes each sample's token sequence\n# Attention operates WITHIN each sample's 64 tokens\n# (Can also attend across samples if desired, but typically within)\nt = timesteps           # (32,) - Diffusion timestep for each sample in batch\n                        #        e.g., [500, 732, 123, ..., 891]\n                        #        Controls noise level (high t = more noise)\n\ncondition = conditions  # (32, cond_dim) - Optional conditioning per sample\n                        # Examples:\n                        #   - Cell type: [CD4+, B_cell, NK, ..., Monocyte]\n                        #   - Perturbation: [CRISPR_gene1, drug_A, ..., control]\n                        #   - Disease state: [healthy, disease_A, ..., healthy]\n                        # Can be None for unconditional generation\n\nz_out = transformer(z, t, condition)  # (32, 64, 256)\n# Transformer uses:\n#   - z: Token sequences to process\n#   - t: Time conditioning (via AdaLN - modulates features based on noise level)\n#   - condition: Biological conditioning (affects what to generate)\n\n# Step 5: Decode back to gene space\n# Each sample's tokens \u2192 that sample's gene expression\nx_pred = decoder(z_out)  # (32, 20000)\n</code></pre>"},{"location":"DDPM/02b_diffusion_arch_qa/#understanding-the-transformer-inputs","title":"Understanding the Transformer Inputs","text":"<p>Let's break down what <code>transformer(z, t, condition)</code> means:</p>"},{"location":"DDPM/02b_diffusion_arch_qa/#1-token-sequences-z","title":"1. Token Sequences (z)","text":"<pre><code>z = (32, 64, 256)\n# Main input: The latent token sequences to process\n# - 32 samples in batch\n# - Each sample has 64 tokens\n# - Each token has 256 features\n\n# Think of each sample's 64 tokens as a \"sentence\"\n# where each token represents a semantic gene module\n</code></pre>"},{"location":"DDPM/02b_diffusion_arch_qa/#2-timestep-t","title":"2. Timestep (t)","text":"<pre><code>t = (32,)  # One timestep per sample in batch\n# Example values: [500, 732, 123, 891, ..., 445]\n\n# What does t mean?\n# - Diffusion timestep: ranges from 0 (clean) to T (pure noise)\n# - t=0: Nearly clean gene expression\n# - t=500: Medium noise level\n# - t=1000: Almost pure noise\n\n# How is t used?\n# - Embedded via sinusoidal encoding: t \u2192 time_embed (256-dim)\n# - Used for Adaptive LayerNorm (AdaLN):\n#     \u03b3, \u03b2 = MLP(time_embed)\n#     h_modulated = \u03b3 * LayerNorm(h) + \u03b2\n# - Tells model \"how much noise is present\"\n# - Model adjusts its behavior based on noise level\n\n# Why different t per sample?\n# - During training: randomly sample t for each sample\n# - Makes model learn to denoise at ALL noise levels\n# - More efficient training (diverse timesteps per batch)\n</code></pre>"},{"location":"DDPM/02b_diffusion_arch_qa/#3-condition-condition","title":"3. Condition (condition)","text":"<pre><code>condition = (32, cond_dim) or None\n# Optional conditioning information per sample\n\n# Examples of what condition could be:\n\n# A) Cell type conditioning\ncondition = ['CD4+ T cell', 'B cell', 'NK cell', ...]\n# After embedding: (32, 256)\n# Purpose: Generate expression for specific cell types\n\n# B) Perturbation conditioning\ncondition = {\n    'gene_knockout': ['FOXO1', 'TP53', None, ...],\n    'drug': ['drug_A', None, 'drug_B', ...],\n    'dose': [1.0, 0.0, 0.5, ...]\n}\n# After embedding: (32, 512)  # Combined embeddings\n# Purpose: Generate response to perturbations\n\n# C) Disease state conditioning\ncondition = ['healthy', 'diabetes', 'healthy', 'cancer', ...]\n# After embedding: (32, 256)\n# Purpose: Generate disease-specific expression\n\n# D) Multi-modal conditioning\ncondition = {\n    'cell_type': ['T_cell', ...],\n    'tissue': ['liver', ...],\n    'age': [45, ...],\n    'sex': ['M', ...]\n}\n# After embedding: (32, 1024)  # All concatenated\n# Purpose: Complex, multi-factor conditioning\n\n# E) No conditioning (unconditional generation)\ncondition = None\n# Purpose: Generate generic, diverse gene expression\n</code></pre>"},{"location":"DDPM/02b_diffusion_arch_qa/#how-to-represent-conditions-of-different-complexity","title":"How to Represent Conditions of Different Complexity","text":"<p>Key Question: How do we go from raw conditioning information (strings, dictionaries, numbers) to the tensor that the transformer uses?</p>"},{"location":"DDPM/02b_diffusion_arch_qa/#strategy-1-simple-categorical-conditions","title":"Strategy 1: Simple Categorical Conditions","text":"<p>Example: Cell type conditioning</p> <pre><code># Raw input\ncell_types = ['CD4+ T cell', 'B cell', 'NK cell', 'Monocyte', ...]  # (32,)\n\n# Step 1: Convert to indices\ncell_type_to_idx = {\n    'CD4+ T cell': 0,\n    'B cell': 1,\n    'NK cell': 2,\n    'Monocyte': 3,\n    ...\n}\nindices = [cell_type_to_idx[ct] for ct in cell_types]  # [0, 1, 2, 3, ...]\n\n# Step 2: Embed via embedding layer\nclass CellTypeEmbedder(nn.Module):\n    def __init__(self, num_cell_types=50, embed_dim=256):\n        super().__init__()\n        # Learned embedding for each cell type\n        self.embedding = nn.Embedding(num_cell_types, embed_dim)\n\n    def forward(self, cell_type_indices):\n        # Input: (batch,) - integer indices\n        # Output: (batch, embed_dim)\n        return self.embedding(cell_type_indices)\n\n# Result\nembedder = CellTypeEmbedder(num_cell_types=50, embed_dim=256)\ncondition_embed = embedder(torch.tensor(indices))  # (32, 256)\n</code></pre> <p>Why this works: </p> <ul> <li>Cell types are discrete categories</li> <li>Each gets a learnable vector representation</li> <li>Similar to word embeddings in NLP</li> </ul>"},{"location":"DDPM/02b_diffusion_arch_qa/#strategy-2-complex-multi-component-conditions","title":"Strategy 2: Complex Multi-Component Conditions","text":"<p>Example: Perturbation conditioning (gene knockout + drug + dose)</p> <pre><code># Raw input (different types of information)\nperturbations = {\n    'gene_knockout': ['FOXO1', 'TP53', None, 'MYC', ...],    # Categorical or None\n    'drug': ['drug_A', None, 'drug_B', 'drug_A', ...],       # Categorical or None\n    'dose': [1.0, 0.0, 0.5, 1.0, ...]                         # Continuous\n}\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# Approach A: Separate Embeddings + Concatenation\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nclass PerturbationEmbedder(nn.Module):\n    def __init__(\n        self,\n        num_genes=1000,       # Number of possible knockout targets\n        num_drugs=100,        # Number of drugs\n        embed_dim=128         # Embedding dimension per component\n    ):\n        super().__init__()\n\n        # Separate embeddings for each component\n        self.gene_embed = nn.Embedding(num_genes + 1, embed_dim)  # +1 for \"none\"\n        self.drug_embed = nn.Embedding(num_drugs + 1, embed_dim)  # +1 for \"none\"\n\n        # MLP for continuous dose\n        self.dose_encoder = nn.Sequential(\n            nn.Linear(1, embed_dim),\n            nn.SiLU(),\n            nn.Linear(embed_dim, embed_dim)\n        )\n\n        # Final projection (concatenated \u2192 combined)\n        self.combiner = nn.Sequential(\n            nn.Linear(3 * embed_dim, 512),\n            nn.SiLU(),\n            nn.Linear(512, 512)\n        )\n\n        # Special indices for \"none\"\n        self.gene_none_idx = num_genes\n        self.drug_none_idx = num_drugs\n\n    def forward(self, gene_indices, drug_indices, doses):\n        \"\"\"\n        Args:\n            gene_indices: (batch,) - indices of knockout genes (or none_idx)\n            drug_indices: (batch,) - indices of drugs (or none_idx)\n            doses: (batch,) - continuous dose values\n\n        Returns:\n            (batch, 512) - combined perturbation embedding\n        \"\"\"\n        # Embed each component separately\n        gene_emb = self.gene_embed(gene_indices)      # (batch, 128)\n        drug_emb = self.drug_embed(drug_indices)      # (batch, 128)\n        dose_emb = self.dose_encoder(doses[:, None])  # (batch, 128)\n\n        # Concatenate\n        combined = torch.cat([gene_emb, drug_emb, dose_emb], dim=-1)  # (batch, 384)\n\n        # Project to final dimension\n        condition_embed = self.combiner(combined)  # (batch, 512)\n\n        return condition_embed\n\n# Usage\ngene_to_idx = {'FOXO1': 0, 'TP53': 1, 'MYC': 2, None: embedder.gene_none_idx}\ndrug_to_idx = {'drug_A': 0, 'drug_B': 1, None: embedder.drug_none_idx}\n\ngene_indices = torch.tensor([gene_to_idx[g] for g in perturbations['gene_knockout']])\ndrug_indices = torch.tensor([drug_to_idx[d] for d in perturbations['drug']])\ndoses = torch.tensor(perturbations['dose'])\n\nembedder = PerturbationEmbedder()\ncondition_embed = embedder(gene_indices, drug_indices, doses)  # (32, 512)\n</code></pre> <p>Why this works:</p> <ul> <li>Each component embedded separately (preserves semantics)</li> <li>Concatenation combines all information</li> <li>MLP learns interactions between components</li> <li>Handles missing values naturally (None \u2192 special index)</li> </ul>"},{"location":"DDPM/02b_diffusion_arch_qa/#strategy-3-variable-length-complex-conditions","title":"Strategy 3: Variable-Length Complex Conditions","text":"<p>Example: Multiple perturbations per sample (variable number)</p> <pre><code># Raw input: Some samples have multiple perturbations\nperturbations = [\n    ['FOXO1_knockout', 'drug_A_0.5'],              # Sample 1: 2 perturbations\n    ['TP53_knockout'],                             # Sample 2: 1 perturbation\n    ['drug_B_1.0', 'drug_C_0.3', 'MYC_knockout'],  # Sample 3: 3 perturbations\n    [],                                             # Sample 4: No perturbation\n    ...\n]\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# Approach B: Set Embedding (Order-Invariant)\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nclass SetPerturbationEmbedder(nn.Module):\n    def __init__(self, num_perturbations=500, embed_dim=256, max_perts=10):\n        super().__init__()\n\n        # Embedding for each perturbation type\n        self.pert_embed = nn.Embedding(num_perturbations + 1, embed_dim)  # +1 for padding\n\n        # Transformer encoder (order-invariant aggregation)\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=4),\n            num_layers=2\n        )\n\n        # Pooling to fixed size\n        self.pool = nn.AdaptiveAvgPool1d(1)  # or attention pooling\n\n        self.pad_idx = num_perturbations\n        self.max_perts = max_perts\n\n    def forward(self, perturbation_indices, mask):\n        \"\"\"\n        Args:\n            perturbation_indices: (batch, max_perts) - padded perturbation indices\n            mask: (batch, max_perts) - True where valid, False for padding\n\n        Returns:\n            (batch, embed_dim) - aggregated perturbation embedding\n        \"\"\"\n        # Embed each perturbation\n        pert_embs = self.pert_embed(perturbation_indices)  # (batch, max_perts, embed_dim)\n\n        # Transformer (handles variable length via masking)\n        pert_embs = pert_embs.transpose(0, 1)  # (max_perts, batch, embed_dim)\n        encoded = self.transformer(pert_embs, src_key_padding_mask=~mask)\n        encoded = encoded.transpose(0, 1)  # (batch, max_perts, embed_dim)\n\n        # Pool to fixed size (mean over valid perturbations)\n        # Apply mask before pooling\n        encoded = encoded * mask.unsqueeze(-1)\n        condition_embed = encoded.sum(dim=1) / mask.sum(dim=1, keepdim=True)\n\n        return condition_embed  # (batch, embed_dim)\n\n# Usage: Convert to padded format\npert_to_idx = {'FOXO1_knockout': 0, 'drug_A_0.5': 1, ...}\n\ndef collate_perturbations(pert_lists, max_perts=10, pad_idx=500):\n    \"\"\"Convert variable-length lists to padded tensors\"\"\"\n    batch_size = len(pert_lists)\n\n    # Create padded tensor\n    indices = torch.full((batch_size, max_perts), pad_idx, dtype=torch.long)\n    mask = torch.zeros(batch_size, max_perts, dtype=torch.bool)\n\n    for i, perts in enumerate(pert_lists):\n        num_perts = min(len(perts), max_perts)\n        indices[i, :num_perts] = torch.tensor([pert_to_idx[p] for p in perts[:num_perts]])\n        mask[i, :num_perts] = True\n\n    return indices, mask\n\nindices, mask = collate_perturbations(perturbations)\nembedder = SetPerturbationEmbedder()\ncondition_embed = embedder(indices, mask)  # (32, 256)\n</code></pre> <p>Why this works:</p> <ul> <li>Handles variable number of perturbations</li> <li>Order-invariant (set semantics, not sequence)</li> <li>Masking handles different lengths</li> <li>Aggregation (mean/max/attention) gives fixed-size output</li> </ul>"},{"location":"DDPM/02b_diffusion_arch_qa/#strategy-4-hierarchical-conditions","title":"Strategy 4: Hierarchical Conditions","text":"<p>Example: Multi-level biological context (organism \u2192 tissue \u2192 cell type \u2192 state)</p> <pre><code># Raw input: Hierarchical structure\nconditions = {\n    'organism': ['human', 'human', 'mouse', ...],\n    'tissue': ['liver', 'brain', 'liver', ...],\n    'cell_type': ['hepatocyte', 'neuron', 'hepatocyte', ...],\n    'state': ['healthy', 'healthy', 'diseased', ...]\n}\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# Approach C: Hierarchical Embeddings\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nclass HierarchicalConditionEmbedder(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # Separate embeddings for each level\n        self.organism_embed = nn.Embedding(10, 64)   # Few organisms\n        self.tissue_embed = nn.Embedding(50, 128)    # More tissues\n        self.cell_type_embed = nn.Embedding(200, 256)  # Many cell types\n        self.state_embed = nn.Embedding(20, 128)     # Few states\n\n        # Hierarchical combination (bottom-up)\n        self.combine_tissue_cell = nn.Sequential(\n            nn.Linear(128 + 256, 256),\n            nn.SiLU()\n        )\n\n        self.combine_org_tissue_cell = nn.Sequential(\n            nn.Linear(64 + 256, 256),\n            nn.SiLU()\n        )\n\n        self.combine_all = nn.Sequential(\n            nn.Linear(256 + 128, 512),\n            nn.SiLU(),\n            nn.Linear(512, 512)\n        )\n\n    def forward(self, organism_idx, tissue_idx, cell_type_idx, state_idx):\n        \"\"\"Hierarchical composition: organism &gt; tissue &gt; cell_type, + state\"\"\"\n\n        # Embed each level\n        org_emb = self.organism_embed(organism_idx)      # (batch, 64)\n        tis_emb = self.tissue_embed(tissue_idx)          # (batch, 128)\n        cel_emb = self.cell_type_embed(cell_type_idx)    # (batch, 256)\n        sta_emb = self.state_embed(state_idx)            # (batch, 128)\n\n        # Hierarchical composition\n        # Level 1: Tissue + Cell type (cell exists in tissue)\n        tissue_cell = self.combine_tissue_cell(\n            torch.cat([tis_emb, cel_emb], dim=-1)\n        )  # (batch, 256)\n\n        # Level 2: Organism + (Tissue + Cell)\n        org_tissue_cell = self.combine_org_tissue_cell(\n            torch.cat([org_emb, tissue_cell], dim=-1)\n        )  # (batch, 256)\n\n        # Level 3: Add state (orthogonal to hierarchy)\n        final = self.combine_all(\n            torch.cat([org_tissue_cell, sta_emb], dim=-1)\n        )  # (batch, 512)\n\n        return final\n\n# Usage\norganism_to_idx = {'human': 0, 'mouse': 1, 'rat': 2}\ntissue_to_idx = {'liver': 0, 'brain': 1, ...}\n# ... similar for cell_type and state\n\nembedder = HierarchicalConditionEmbedder()\ncondition_embed = embedder(\n    torch.tensor([organism_to_idx[o] for o in conditions['organism']]),\n    torch.tensor([tissue_to_idx[t] for t in conditions['tissue']]),\n    torch.tensor([cell_type_to_idx[c] for c in conditions['cell_type']]),\n    torch.tensor([state_to_idx[s] for s in conditions['state']])\n)  # (32, 512)\n</code></pre> <p>Why this works:</p> <ul> <li>Respects biological hierarchy</li> <li>Bottom-up composition (cell \u2192 tissue \u2192 organism)</li> <li>Different embedding sizes reflect complexity</li> <li>Can incorporate prior knowledge about relationships</li> </ul>"},{"location":"DDPM/02b_diffusion_arch_qa/#comparison-of-strategies","title":"Comparison of Strategies","text":"Complexity Example Strategy Output Shape Best For Simple Categorical Cell type Embedding layer (batch, 256) Single discrete condition Multi-Component Gene KO + Drug + Dose Separate embeds + concat (batch, 512) Fixed set of heterogeneous conditions Variable-Length Multiple perturbations Set embedding + pooling (batch, 256) Variable number of conditions Hierarchical Organism \u2192 Tissue \u2192 Cell Hierarchical composition (batch, 512) Nested/structured conditions Very Complex Text descriptions Pre-trained encoder (CLIP, T5) (batch, 768) Natural language"},{"location":"DDPM/02b_diffusion_arch_qa/#practical-implementation-template","title":"Practical Implementation Template","text":"<pre><code>class UniversalConditionEmbedder(nn.Module):\n    \"\"\"Handles conditions of varying complexity\"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n        # Simple categorical\n        self.cell_type_embed = nn.Embedding(50, 256)\n\n        # Multi-component\n        self.perturbation_embed = PerturbationEmbedder()\n\n        # Continuous\n        self.dose_encoder = nn.Sequential(\n            nn.Linear(1, 128),\n            nn.SiLU(),\n            nn.Linear(128, 128)\n        )\n\n        # Combiner (if using multiple condition types)\n        self.combiner = nn.Linear(256 + 512 + 128, 512)  # Adjust based on what you use\n\n    def forward(self, batch):\n        \"\"\"\n        Flexible forward pass based on what conditions are present\n\n        Args:\n            batch: Dictionary containing various condition types\n\n        Returns:\n            Combined condition embedding\n        \"\"\"\n        embeds = []\n\n        # Handle cell type if present\n        if 'cell_type' in batch:\n            cell_emb = self.cell_type_embed(batch['cell_type'])\n            embeds.append(cell_emb)\n\n        # Handle perturbation if present\n        if 'perturbation' in batch:\n            pert_emb = self.perturbation_embed(\n                batch['perturbation']['gene'],\n                batch['perturbation']['drug'],\n                batch['perturbation']['dose']\n            )\n            embeds.append(pert_emb)\n\n        # Handle continuous dose if present\n        if 'dose' in batch:\n            dose_emb = self.dose_encoder(batch['dose'][:, None])\n            embeds.append(dose_emb)\n\n        # Combine all present conditions\n        if len(embeds) == 0:\n            return None  # Unconditional\n        elif len(embeds) == 1:\n            return embeds[0]\n        else:\n            combined = torch.cat(embeds, dim=-1)\n            return self.combiner(combined)\n</code></pre>"},{"location":"DDPM/02b_diffusion_arch_qa/#summary-from-raw-data-to-condition-tensor","title":"Summary: From Raw Data to Condition Tensor","text":"<p>The general pipeline:</p> <pre><code>Raw Condition Data\n    \u2193\n[Convert to appropriate format]\n    \u2193 (strings \u2192 indices, numbers \u2192 tensors, etc.)\nProcessable Format\n    \u2193\n[Embed each component]\n    \u2193 (embeddings, MLPs, encoders)\nComponent Embeddings\n    \u2193\n[Combine components]\n    \u2193 (concatenation, addition, hierarchical composition)\nFinal Condition Tensor\n    \u2193 (batch, condition_dim)\n[Input to Transformer]\n</code></pre> <p>Key principles: 1. Each component gets its own embedding strategy 2. Categorical \u2192 Embedding layers 3. Continuous \u2192 MLPs 4. Complex \u2192 Combination of above 5. Variable-length \u2192 Masking + pooling 6. Hierarchical \u2192 Compositional embeddings 7. Combine via concatenation, addition, or learned fusion</p> <p>The condition tensor shape <code>(batch, condition_dim)</code> is then used by the transformer via AdaLN, cross-attention, or other conditioning mechanisms.</p> <pre><code># How is condition used?\n# Option 1: AdaLN (like time)\n#   \u03b3_cond, \u03b2_cond = MLP(condition_embed)\n#   h = \u03b3_time * \u03b3_cond * LayerNorm(h) + \u03b2_time + \u03b2_cond\n#\n# Option 2: Cross-attention\n#   h_out = CrossAttention(query=h, key=condition, value=condition)\n#\n# Option 3: Concatenation\n#   combined = concat([time_embed, condition_embed])\n#   \u03b3, \u03b2 = MLP(combined)\n</code></pre>"},{"location":"DDPM/02b_diffusion_arch_qa/#complete-example-with-all-inputs","title":"Complete Example with All Inputs","text":"<pre><code># Training step\nfor batch in dataloader:\n    # Load data\n    x_0 = batch['expression']      # (32, 20000) - Clean gene expression\n    cell_types = batch['cell_type'] # (32,) - ['T_cell', 'B_cell', ...]\n\n    # Sample random timesteps (different for each sample)\n    t = torch.randint(0, 1000, (32,))  # e.g., [234, 789, 12, 901, ...]\n\n    # Add noise according to timestep\n    noise = torch.randn_like(x_0)\n    x_t = sqrt(alpha_bar[t]) * x_0 + sqrt(1 - alpha_bar[t]) * noise\n    # x_t: (32, 20000) - Noisy gene expression\n\n    # Embed conditions\n    condition = cell_type_embedder(cell_types)  # (32, 256)\n\n    # Forward pass\n    z = encoder(x_t)                          # (32, 64, 256) - Encode to tokens\n    z_out = transformer(z, t, condition)      # (32, 64, 256) - Process with context\n    noise_pred = decoder(z_out)               # (32, 20000) - Predict noise\n\n    # Loss\n    loss = F.mse_loss(noise_pred, noise)\n</code></pre>"},{"location":"DDPM/02b_diffusion_arch_qa/#question-2-is-this-like-a-black-and-white-image","title":"Question 2: Is This Like a \"Black and White Image\"?","text":"<p>Question: Is the token representation <code>(num_tokens, token_dim)</code> almost like a \"black and white image\" with only 1 channel of size <code>(num_tokens, token_dim)</code>?</p> <p>Reference: The positional embedding line in the architecture: <pre><code>self.pos_embed = nn.Parameter(torch.randn(1, num_tokens, token_dim))\n</code></pre></p>"},{"location":"DDPM/02b_diffusion_arch_qa/#answer-no-its-a-sequence-not-an-image","title":"Answer: No, It's a Sequence, Not an Image","text":"<p>Great intuition, but not quite! Let me explain the dimensionality:</p>"},{"location":"DDPM/02b_diffusion_arch_qa/#image-vs-token-representation","title":"Image vs Token Representation","text":"<pre><code># GRAYSCALE IMAGE (what you're thinking of)\nimage = (batch, 1, height, width)\n      = (32, 1, 224, 224)\n# - 1 channel (grayscale)\n# - Spatial dimensions: height \u00d7 width\n# - Still has 2D spatial structure\n\n# LATENT TOKENS (what we have)\ntokens = (batch, num_tokens, token_dim)\n       = (32, 64, 256)\n# - 64 tokens (like 64 \"patches\")\n# - 256 features per token\n# - NO spatial structure! Just a SEQUENCE of tokens\n</code></pre>"},{"location":"DDPM/02b_diffusion_arch_qa/#better-analogy-sequence-not-image","title":"Better Analogy: Sequence, Not Image","text":"<p>The token representation is more like:</p> <pre><code>TEXT SEQUENCE (NLP):\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nsentence = [\"The\", \"cat\", \"sat\", \"on\", \"mat\"]\nembeddings = (batch, seq_len, embed_dim)\n           = (32, 5, 768)\n# - 5 words in sequence\n# - 768-dimensional embedding per word\n\nGENE EXPRESSION TOKENS:\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ngene_profile \u2192 [token1, token2, ..., token64]\ntokens = (batch, num_tokens, token_dim)\n       = (32, 64, 256)\n# - 64 tokens in sequence\n# - 256-dimensional features per token\n# - Each token represents some \"semantic cluster\" of genes\n</code></pre>"},{"location":"DDPM/02b_diffusion_arch_qa/#visual-comparison","title":"Visual Comparison","text":"<pre><code>IMAGE REPRESENTATION:\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 \u2502  Channel 1 (R)\n     \u2502 \u2591\u2591\u2591\u2593\u2593\u2591\u2591\u2591\u2591\u2591\u2591 \u2502\n     \u2502 \u2591\u2591\u2593\u2593\u2593\u2593\u2591\u2591\u2591\u2591\u2591 \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 \u2502  Channel 2 (G)\n     \u2502 \u2591\u2591\u2591\u2593\u2593\u2591\u2591\u2591\u2591\u2591\u2591 \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 \u2502  Channel 3 (B)\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nShape: (height, width, channels)\nStructure: 2D spatial grid\n\n\nTOKEN REPRESENTATION:\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nToken 1: [0.5, -0.2, 0.8, ..., 0.3]  \u2190 256 features\nToken 2: [0.1, 0.7, -0.4, ..., 0.9]\nToken 3: [-0.3, 0.2, 0.6, ..., -0.1]\n...\nToken 64: [0.4, -0.5, 0.1, ..., 0.7]\n\nShape: (num_tokens, token_dim)\nStructure: 1D sequence (like words in a sentence)\n</code></pre>"},{"location":"DDPM/02b_diffusion_arch_qa/#what-each-token-might-represent","title":"What Each Token Might Represent","text":"<p>Unlike pixels in an image, each token captures semantic information:</p> <pre><code># Hypothetical learned representation\nToken 1 \u2192 \"Cell cycle genes\" (high for proliferating cells)\nToken 2 \u2192 \"Immune response genes\" (high in activated immune cells)  \nToken 3 \u2192 \"Metabolic genes\" (high in metabolically active cells)\nToken 4 \u2192 \"Housekeeping genes\" (stable across conditions)\n...\nToken 64 \u2192 \"Rare pathway genes\"\n\n# Each token is 256-dimensional, encoding complex patterns\nToken 1 = [0.5, -0.2, 0.8, ..., 0.3]\n          \u2191    \u2191    \u2191        \u2191\n          Features capturing different aspects of \"cell cycle-ness\"\n</code></pre>"},{"location":"DDPM/02b_diffusion_arch_qa/#dimensionality-breakdown","title":"Dimensionality Breakdown","text":""},{"location":"DDPM/02b_diffusion_arch_qa/#comparison-table","title":"Comparison Table","text":"Dimension Name Size Meaning Batch <code>batch_size</code> 32 Number of samples processed together Tokens <code>num_tokens</code> 64 Number of semantic \"chunks\" per sample Features <code>token_dim</code> 256 Features describing each token"},{"location":"DDPM/02b_diffusion_arch_qa/#cross-domain-comparison","title":"Cross-Domain Comparison","text":"Data Type Dimension 1 Dimension 2 Dimension 3 Dimension 4 Image batch=32 channels=3 height=224 width=224 Tokens batch=32 num_tokens=64 token_dim=256 - Text batch=32 seq_len=50 embed_dim=768 - <p>Key difference: </p> <ul> <li>Images have 2D spatial structure (height \u00d7 width)</li> <li>Tokens have 1D sequence structure (just num_tokens)</li> <li>The positional encoding provides ordering info (like in NLP)</li> </ul>"},{"location":"DDPM/02b_diffusion_arch_qa/#why-not-actually-like-an-image","title":"Why Not Actually Like an Image?","text":"<p>If we tried to make it image-like:</p> <pre><code># \u274c WRONG: Treating as image\ntokens_as_image = (batch, channels=1, height=8, width=8)\n                = (32, 1, 8, 8)  # 64 \"pixels\" arranged spatially\n\n# \u2705 CORRECT: Treating as sequence\ntokens_as_sequence = (batch, seq_len=64, features=256)\n                   = (32, 64, 256)  # 64 tokens with 256 features each\n</code></pre>"},{"location":"DDPM/02b_diffusion_arch_qa/#why-sequence-is-better","title":"Why Sequence is Better","text":"<p>1. No inherent spatial structure in gene expression</p> <ul> <li>Gene order in genome \u2260 meaningful for expression patterns</li> <li>Unlike pixels, where neighbors have spatial meaning</li> </ul> <p>2. Transformers work on sequences</p> <ul> <li>Self-attention doesn't assume spatial locality</li> <li>Can capture any gene-gene interactions</li> </ul> <p>3. More like NLP than vision</p> <ul> <li>Genes are like \"words\" in a biological \"sentence\"</li> <li>Tokens are semantic clusters, not spatial patches</li> </ul>"},{"location":"DDPM/02b_diffusion_arch_qa/#example-what-spatial-structure-would-mean","title":"Example: What Spatial Structure Would Mean","text":"<pre><code># If we treated as 2D image (8\u00d78 grid of tokens)\n# \u274c This would imply:\n# - Token at position (0,0) is \"near\" token at (0,1)\n# - Token at position (0,0) is \"far\" from token at (7,7)\n# - Spatial locality matters\n\n# But in gene expression:\n# - Token 1 (cell cycle) might interact strongly with Token 50 (DNA repair)\n# - No reason to assume nearby tokens are more related\n# - Attention should be free to connect any tokens\n</code></pre>"},{"location":"DDPM/02b_diffusion_arch_qa/#practical-implications","title":"Practical Implications","text":""},{"location":"DDPM/02b_diffusion_arch_qa/#for-model-design","title":"For Model Design","text":"<p>1. Use sequence models, not convolutional models <pre><code># \u2705 Good: Transformer (no spatial bias)\ntransformer = nn.TransformerEncoder(...)\n\n# \u274c Bad: CNN (assumes spatial locality)\ncnn = nn.Conv2d(...)  # Wrong inductive bias for gene expression\n</code></pre></p> <p>2. Positional encoding is flexible <pre><code># Can use learned or sinusoidal\nself.pos_embed = nn.Parameter(torch.randn(1, num_tokens, token_dim))\n\n# Or sinusoidal (like BERT)\nself.pos_embed = SinusoidalPositionEmbeddings(token_dim)\n</code></pre></p> <p>3. Attention is unrestricted <pre><code># Each token can attend to all other tokens\n# No spatial locality assumption\nattn_scores = Q @ K.T  # (num_tokens, num_tokens)\n# All-to-all attention\n</code></pre></p>"},{"location":"DDPM/02b_diffusion_arch_qa/#for-interpretation","title":"For Interpretation","text":"<p>1. Tokens are semantic, not spatial</p> <ul> <li>Analyze what biological patterns each token captures</li> <li>Use gene loadings to interpret tokens</li> <li>Compare to known pathways/modules</li> </ul> <p>2. Token order doesn't matter (much)</p> <ul> <li>Positional encoding adds ordering info</li> <li>But tokens aren't inherently ordered like pixels</li> <li>Could potentially shuffle and retrain</li> </ul> <p>3. Visualization differs from images <pre><code># For images: Show as 2D grid\nplt.imshow(image)\n\n# For tokens: Show as heatmap or t-SNE\nplt.imshow(tokens.T)  # (token_dim, num_tokens)\n# Or project to 2D\ntsne = TSNE(n_components=2)\ntokens_2d = tsne.fit_transform(tokens)\nplt.scatter(tokens_2d[:, 0], tokens_2d[:, 1])\n</code></pre></p>"},{"location":"DDPM/02b_diffusion_arch_qa/#summary","title":"Summary","text":""},{"location":"DDPM/02b_diffusion_arch_qa/#question-1-handling-thousands-of-samples_1","title":"Question 1: Handling Thousands of Samples","text":"<p>The architecture processes one sample at a time (with batching for efficiency). The \"thousands of samples\" are your dataset, processed in minibatches during training, just like images in computer vision.</p> <p>Key points:</p> <ul> <li>Each sample: 20K genes \u2192 64 tokens (256-dim each)</li> <li>Batching: Process 32 samples in parallel</li> <li>Training: Iterate through dataset in minibatches</li> <li>Same as standard deep learning practice</li> </ul>"},{"location":"DDPM/02b_diffusion_arch_qa/#question-2-is-this-like-a-black-and-white-image_1","title":"Question 2: Is This Like a Black-and-White Image?","text":"<p>The token representation is NOT like a black-and-white image. It's more like:</p> <p>Text embeddings: A sequence of semantic tokens - Each token is a 256-dimensional feature vector - No 2D spatial structure, just 1D sequence - Position info added via positional encoding</p> <p>Key intuition: Think of it as compressing a 20,000-dimensional gene expression vector into a sequence of 64 semantic tokens (each 256-dim), where each token represents some learned biological pattern/module.</p>"},{"location":"DDPM/02b_diffusion_arch_qa/#correct-mental-model","title":"Correct Mental Model","text":"<pre><code>Gene Expression \u2192 Encoder \u2192 Sequence of Semantic Tokens \u2192 Transformer \u2192 Decoder \u2192 Prediction\n\nNOT:\nGene Expression \u2192 Encoder \u2192 2D Image \u2192 CNN \u2192 Decoder \u2192 Prediction\n</code></pre> <p>Think: NLP (BERT, GPT) not Computer Vision (ResNet, ViT)</p>"},{"location":"DDPM/02b_diffusion_arch_qa/#related-questions","title":"Related Questions","text":""},{"location":"DDPM/02b_diffusion_arch_qa/#q-why-64-tokens-specifically","title":"Q: Why 64 tokens specifically?","text":"<p>A: Hyperparameter choice balancing: - Fewer tokens (e.g., 32): Faster, but less capacity - More tokens (e.g., 128): More capacity, but slower - 64 tokens: Sweet spot for most applications</p> <p>Experiment to find optimal for your data.</p>"},{"location":"DDPM/02b_diffusion_arch_qa/#q-can-tokens-attend-across-samples-in-a-batch","title":"Q: Can tokens attend across samples in a batch?","text":"<p>A: Typically no, but possible:</p> <pre><code># Standard: Within-sample attention\n# Each sample's 64 tokens attend to each other\nattn_mask = None  # Full attention within sample\n\n# Advanced: Cross-sample attention\n# Tokens can attend across samples (rare for gene expression)\n# Useful for: batch effects, sample relationships\n</code></pre>"},{"location":"DDPM/02b_diffusion_arch_qa/#q-how-do-i-interpret-learned-tokens","title":"Q: How do I interpret learned tokens?","text":"<p>A: Several approaches:</p> <pre><code># 1. Gene loadings\n# Which genes contribute most to each token?\nencoder_weights = model.encoder[0].weight  # (2048, 20000)\n# Analyze top genes per token\n\n# 2. Activation patterns\n# When is each token active?\nz = model.encode(x_batch)  # (batch, 64, 256)\ntoken_activations = z.mean(dim=-1)  # (batch, 64)\n# Correlate with cell types, conditions\n\n# 3. Pathway enrichment\n# Do tokens align with known pathways?\n# Use GSEA on gene loadings per token\n</code></pre>"},{"location":"DDPM/02b_diffusion_arch_qa/#related-documents","title":"Related Documents","text":"<ul> <li>02a_diffusion_arch_gene_expression.md \u2014 Architecture options</li> <li>02_ddpm_training.md \u2014 Training strategies</li> <li>../DiT/01_dit_foundations.md \u2014 Transformer details</li> <li>../DiT/open_research_tokenization.md \u2014 Tokenization deep dive</li> </ul>"},{"location":"DDPM/02b_diffusion_arch_qa/#references","title":"References","text":"<p>Sequence models for biology:</p> <ul> <li>Theodoris et al. (2023): \"Transfer learning enables predictions in network biology\" (Geneformer)</li> <li>Cui et al. (2024): \"scGPT: Toward Building a Foundation Model for Single-Cell Multi-omics\"</li> </ul> <p>Transformers and attention:</p> <ul> <li>Vaswani et al. (2017): \"Attention Is All You Need\"</li> <li>Devlin et al. (2019): \"BERT: Pre-training of Deep Bidirectional Transformers\"</li> </ul> <p>Latent representations:</p> <ul> <li>Rombach et al. (2022): \"High-Resolution Image Synthesis with Latent Diffusion Models\"</li> <li>Kingma &amp; Welling (2014): \"Auto-Encoding Variational Bayes\"</li> </ul>"},{"location":"DDPM/03_ddpm_sampling/","title":"DDPM Sampling: From Noise to Data","text":"<p>This document covers sampling algorithms for diffusion models, including DDPM ancestral sampling, DDIM deterministic sampling, fast sampling techniques, and guidance methods.</p>"},{"location":"DDPM/03_ddpm_sampling/#overview","title":"Overview","text":"<p>Sampling from a trained DDPM means reversing the diffusion process: starting from pure noise and iteratively denoising to generate data.</p> <p>Key sampling methods: 1. DDPM (Ancestral): Stochastic, adds noise at each step 2. DDIM: Deterministic, no added noise 3. Fast sampling: Fewer steps via step skipping 4. Guidance: Conditional generation with control</p> <p>Goal: Understand the trade-offs between quality, speed, and diversity.</p>"},{"location":"DDPM/03_ddpm_sampling/#ddpm-ancestral-sampling","title":"DDPM Ancestral Sampling","text":""},{"location":"DDPM/03_ddpm_sampling/#the-algorithm","title":"The Algorithm","text":"<p>DDPM sampling (Ho et al., 2020) is the original stochastic sampling procedure.</p> <p>Algorithm: <pre><code>1. Sample x_T ~ N(0, I)\n2. For t = T, T-1, ..., 1:\n   a. Predict noise: \u03b5_\u03b8(x_t, t)\n   b. Compute mean: \u03bc_\u03b8\n   c. Compute variance: \u03c3_t\u00b2\n   d. Sample: x_{t-1} = \u03bc_\u03b8 + \u03c3_t * z\n3. Return x_0\n</code></pre></p>"},{"location":"DDPM/03_ddpm_sampling/#the-update-formula","title":"The Update Formula","text":"\\[ x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left(x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t)\\right) + \\sigma_t z \\] <p>where \\(z \\sim \\mathcal{N}(0, I)\\) is fresh noise at each step.</p>"},{"location":"DDPM/03_ddpm_sampling/#properties","title":"Properties","text":"<p>Pros:</p> <ul> <li>Theoretically grounded</li> <li>High sample quality</li> <li>Diverse samples</li> </ul> <p>Cons:</p> <ul> <li>Slow (1000 steps)</li> <li>Stochastic</li> </ul> <p>When to use: Highest quality and diversity needed</p>"},{"location":"DDPM/03_ddpm_sampling/#ddim-deterministic-sampling","title":"DDIM: Deterministic Sampling","text":""},{"location":"DDPM/03_ddpm_sampling/#the-key-insight","title":"The Key Insight","text":"<p>DDIM (Song et al., 2021) follows a deterministic ODE instead of a stochastic SDE.</p>"},{"location":"DDPM/03_ddpm_sampling/#the-update-formula_1","title":"The Update Formula","text":"\\[ x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}} \\hat{x}_0 + \\sqrt{1 - \\bar{\\alpha}_{t-1}} \\epsilon_\\theta(x_t, t) \\] <p>where \\(\\hat{x}_0 = \\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_\\theta(x_t, t)}{\\sqrt{\\bar{\\alpha}_t}}\\)</p>"},{"location":"DDPM/03_ddpm_sampling/#properties_1","title":"Properties","text":"<p>Pros:</p> <ul> <li>Deterministic</li> <li>Fast (can skip steps)</li> <li>Smooth interpolation</li> </ul> <p>Cons:</p> <ul> <li>Slightly lower diversity</li> </ul> <p>When to use: Speed, reproducibility, or interpolation needed</p>"},{"location":"DDPM/03_ddpm_sampling/#fast-sampling","title":"Fast Sampling","text":""},{"location":"DDPM/03_ddpm_sampling/#step-skipping","title":"Step Skipping","text":"<p>Use a subsequence of timesteps: \\(\\{t_S, t_{S-1}, \\ldots, t_0\\}\\) where \\(S \\ll T\\).</p> <p>Quality vs. Speed:</p> <ul> <li>1000 steps: Best quality</li> <li>250 steps: Excellent</li> <li>50 steps: Very good</li> <li>10 steps: Good for previews</li> </ul> <p>Best practice: Start with 50 steps for DDIM</p>"},{"location":"DDPM/03_ddpm_sampling/#guidance-methods","title":"Guidance Methods","text":""},{"location":"DDPM/03_ddpm_sampling/#classifier-free-guidance","title":"Classifier-Free Guidance","text":"<p>Enables high-quality conditional generation without a separate classifier.</p> <p>Training: Randomly drop conditioning with probability \\(p\\) (e.g., 0.1)</p> <p>Sampling: Interpolate predictions:</p> \\[ \\tilde{\\epsilon}_\\theta = \\epsilon_\\theta(x_t, t, \\emptyset) + w \\cdot (\\epsilon_\\theta(x_t, t, c) - \\epsilon_\\theta(x_t, t, \\emptyset)) \\] <p>where \\(w\\) is the guidance scale (typically 3-10).</p> <p>Effect:</p> <ul> <li>\\(w = 1\\): Standard conditional</li> <li>\\(w &gt; 1\\): Stronger conditioning, less diversity</li> </ul> <p>Best practice: \\(w = 7.5\\) for text-to-image, \\(w = 3-5\\) for class-conditional</p> <p>Comprehensive guide: For detailed implementation with code examples, training procedures, and troubleshooting, see 04_classifier_free_guidance.md. For theoretical foundations, see classifier_free_guidance.md.</p>"},{"location":"DDPM/03_ddpm_sampling/#summary","title":"Summary","text":"<p>Key sampling methods:</p> <ol> <li>DDPM: Stochastic, 1000 steps, highest quality</li> <li>DDIM: Deterministic, 50-250 steps, very good quality</li> <li>Fast DDIM: 10-50 steps, good quality</li> <li>With guidance: Better conditioning control</li> </ol> <p>Recommendations:</p> <ul> <li>Default: DDIM with 50 steps</li> <li>High quality: DDPM with 1000 steps</li> <li>Fast: DDIM with 10-20 steps</li> <li>Conditional: Add classifier-free guidance</li> </ul>"},{"location":"DDPM/03_ddpm_sampling/#related-documents","title":"Related Documents","text":"<ul> <li>DDPM Foundations \u2014 Mathematical theory</li> <li>DDPM Training \u2014 Training details</li> <li>DDIM Update Coefficients \u2014 Exact formulas</li> <li>Reverse SDE &amp; Probability Flow ODE \u2014 Theoretical foundation</li> </ul>"},{"location":"DDPM/03_ddpm_sampling/#references","title":"References","text":"<ol> <li>Ho, J., Jain, A., &amp; Abbeel, P. (2020). Denoising Diffusion Probabilistic Models. NeurIPS.</li> <li>Song, J., Meng, C., &amp; Ermon, S. (2021). Denoising Diffusion Implicit Models. ICLR.</li> <li>Ho, J., &amp; Salimans, T. (2022). Classifier-Free Diffusion Guidance. NeurIPS Workshop.</li> <li>Lu, C., et al. (2022). DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling. NeurIPS.</li> <li>Salimans, T., &amp; Ho, J. (2022). Progressive Distillation for Fast Sampling of Diffusion Models. ICLR.</li> </ol>"},{"location":"DDPM/04_classifier_free_guidance/","title":"Classifier-Free Guidance for DDPM: Implementation Guide","text":"<p>Related: For theoretical foundations and general diffusion context, see classifier_free_guidance.md. This document focuses on practical implementation in DDPM.</p>"},{"location":"DDPM/04_classifier_free_guidance/#overview","title":"Overview","text":"<p>Classifier-free guidance enables high-quality conditional generation in DDPM without needing a separate classifier network. This document covers:</p> <ol> <li>How to modify DDPM training for guidance</li> <li>How to sample with guidance</li> <li>Practical implementation details</li> <li>Hyperparameter tuning</li> <li>Common pitfalls and solutions</li> </ol> <p>Key idea: Train one model that handles both conditional and unconditional generation, then blend their predictions at sampling time.</p>"},{"location":"DDPM/04_classifier_free_guidance/#quick-reference","title":"Quick Reference","text":""},{"location":"DDPM/04_classifier_free_guidance/#training","title":"Training","text":"<pre><code># Randomly drop condition with probability p_uncond (typically 0.1)\nif random.random() &lt; p_uncond:\n    c = null_token  # Unconditional\nelse:\n    c = condition   # Conditional\n\n# Train as normal\nepsilon_pred = model(x_t, t, c)\nloss = MSE(epsilon, epsilon_pred)\n</code></pre>"},{"location":"DDPM/04_classifier_free_guidance/#sampling","title":"Sampling","text":"<pre><code># Two forward passes per step\nepsilon_uncond = model(x_t, t, null_token)\nepsilon_cond = model(x_t, t, condition)\n\n# Blend with guidance scale w\nepsilon_guided = epsilon_uncond + w * (epsilon_cond - epsilon_uncond)\n\n# Use guided prediction for denoising\nx_{t-1} = denoise_step(x_t, epsilon_guided, t)\n</code></pre>"},{"location":"DDPM/04_classifier_free_guidance/#part-1-modifying-ddpm-training","title":"Part 1: Modifying DDPM Training","text":""},{"location":"DDPM/04_classifier_free_guidance/#standard-ddpm-training-unconditional","title":"Standard DDPM Training (Unconditional)","text":"<p>Recall the standard DDPM training algorithm:</p> <pre><code># Standard unconditional DDPM\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        x_0 = batch['image']\n\n        # Sample timestep and noise\n        t = random.randint(1, T)\n        epsilon = torch.randn_like(x_0)\n\n        # Forward diffusion\n        x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * epsilon\n\n        # Predict noise (unconditional)\n        epsilon_pred = model(x_t, t)\n\n        # Loss\n        loss = F.mse_loss(epsilon_pred, epsilon)\n        loss.backward()\n        optimizer.step()\n</code></pre>"},{"location":"DDPM/04_classifier_free_guidance/#modified-training-for-classifier-free-guidance","title":"Modified Training for Classifier-Free Guidance","text":"<p>Key changes: 1. Dataset includes conditions: <code>(x, c)</code> pairs 2. Randomly replace condition with null token 3. Model takes condition as input</p> <pre><code># Classifier-free guidance training\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        x_0 = batch['image']      # Data\n        c = batch['condition']    # Condition (class, text, etc.)\n\n        # Sample timestep and noise\n        t = random.randint(1, T)\n        epsilon = torch.randn_like(x_0)\n\n        # Forward diffusion (same as before)\n        x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * epsilon\n\n        # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        # NEW: Randomly drop condition\n        # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        if random.random() &lt; p_uncond:  # Typically p_uncond = 0.1\n            c = null_token  # Use null/empty condition\n\n        # Predict noise (now conditional)\n        epsilon_pred = model(x_t, t, c)\n\n        # Loss (same as before)\n        loss = F.mse_loss(epsilon_pred, epsilon)\n        loss.backward()\n        optimizer.step()\n</code></pre>"},{"location":"DDPM/04_classifier_free_guidance/#implementation-details","title":"Implementation Details","text":""},{"location":"DDPM/04_classifier_free_guidance/#1-null-token-representation","title":"1. Null Token Representation","text":"<p>Different ways to represent \"no condition\":</p> <pre><code># Option 1: Zero vector (simplest)\nnull_token = torch.zeros(condition_dim)\n\n# Option 2: Learnable embedding\nclass Model(nn.Module):\n    def __init__(self):\n        self.null_embedding = nn.Parameter(torch.randn(condition_dim))\n\n    def get_null_token(self):\n        return self.null_embedding\n\n# Option 3: Special token index (for discrete conditions)\nnull_token = -1  # or num_classes + 1\n\n# Option 4: Mask flag (most explicit)\nuse_condition = False  # Boolean flag to model\n</code></pre> <p>Recommendation: </p> <ul> <li>Discrete conditions (class labels): Use special index</li> <li>Continuous conditions (embeddings): Use zero vector or learnable embedding</li> <li>Text conditions: Use empty string \"\" or padding tokens</li> </ul>"},{"location":"DDPM/04_classifier_free_guidance/#2-condition-dropout-probability","title":"2. Condition Dropout Probability","text":"<pre><code>p_uncond = 0.1  # Typical value\n\n# Higher values (0.2): Better unconditional generation\n# Lower values (0.05): Better conditional generation\n# Trade-off: unconditional quality vs conditional quality\n</code></pre> <p>Guidelines:</p> <ul> <li><code>p_uncond = 0.1</code>: Standard choice for most applications</li> <li><code>p_uncond = 0.2</code>: If unconditional quality matters (text-to-image)</li> <li><code>p_uncond = 0.05</code>: If you always want conditioned output</li> </ul>"},{"location":"DDPM/04_classifier_free_guidance/#3-model-architecture-changes","title":"3. Model Architecture Changes","text":"<p>Your model must accept the condition:</p> <pre><code>class DDPMWithGuidance(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.time_embed = TimeEmbedding(256)\n        self.class_embed = nn.Embedding(num_classes + 1, 256)  # +1 for null\n\n        # Combine time and class embeddings\n        self.combine = nn.Linear(512, 512)\n\n        # U-Net or other architecture\n        self.unet = UNet(...)\n\n    def forward(self, x_t, t, c):\n        \"\"\"\n        Args:\n            x_t: Noisy input (batch, channels, height, width)\n            t: Timesteps (batch,)\n            c: Conditions (batch,) - class indices\n\n        Returns:\n            epsilon_pred: Predicted noise (batch, channels, height, width)\n        \"\"\"\n        # Embed time\n        t_emb = self.time_embed(t)  # (batch, 256)\n\n        # Embed condition\n        c_emb = self.class_embed(c)  # (batch, 256)\n\n        # Combine\n        conditioning = self.combine(torch.cat([t_emb, c_emb], dim=1))\n\n        # U-Net with conditioning\n        epsilon_pred = self.unet(x_t, conditioning)\n\n        return epsilon_pred\n</code></pre> <p>For Transformers (DiT):</p> <pre><code>class DiTWithGuidance(nn.Module):\n    def forward(self, x_t, t, c):\n        # Create embeddings\n        time_embed = self.time_embed(t)\n        class_embed = self.class_embed(c)\n\n        # Combine for AdaLN\n        combined = time_embed + class_embed\n        gamma, beta = self.adaln_mlp(combined)\n\n        # Standard DiT processing\n        h = self.patchify(x_t)\n        for block in self.blocks:\n            h = block(h, gamma, beta)\n\n        return self.unpatchify(h)\n</code></pre>"},{"location":"DDPM/04_classifier_free_guidance/#part-2-sampling-with-guidance","title":"Part 2: Sampling with Guidance","text":""},{"location":"DDPM/04_classifier_free_guidance/#standard-ddpm-sampling-review","title":"Standard DDPM Sampling (Review)","text":"<pre><code>def ddpm_sample(model, shape, T=1000):\n    \"\"\"Standard unconditional DDPM sampling\"\"\"\n    x = torch.randn(shape)  # Start from noise\n\n    for t in reversed(range(1, T+1)):\n        # Predict noise\n        epsilon_pred = model(x, t)\n\n        # Compute mean\n        alpha_t = get_alpha(t)\n        alpha_bar_t = get_alpha_bar(t)\n        mean = (1 / sqrt(alpha_t)) * (\n            x - ((1 - alpha_t) / sqrt(1 - alpha_bar_t)) * epsilon_pred\n        )\n\n        # Add noise (except last step)\n        if t &gt; 1:\n            sigma = get_sigma(t)\n            z = torch.randn_like(x)\n            x = mean + sigma * z\n        else:\n            x = mean\n\n    return x\n</code></pre>"},{"location":"DDPM/04_classifier_free_guidance/#guided-sampling-two-forward-passes","title":"Guided Sampling (Two Forward Passes)","text":"<pre><code>def guided_sample(model, shape, condition, guidance_scale=7.5, T=1000):\n    \"\"\"\n    Classifier-free guided DDPM sampling\n\n    Args:\n        model: Trained model with condition input\n        shape: Output shape\n        condition: Condition to use (class, text embedding, etc.)\n        guidance_scale: w, typically 1.0-10.0\n        T: Number of diffusion steps\n\n    Returns:\n        Generated sample\n    \"\"\"\n    x = torch.randn(shape)  # Start from noise\n    null_token = get_null_token()  # Unconditional token\n\n    for t in reversed(range(1, T+1)):\n        # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        # KEY: Two forward passes\n        # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n        # 1. Unconditional prediction\n        epsilon_uncond = model(x, t, null_token)\n\n        # 2. Conditional prediction\n        epsilon_cond = model(x, t, condition)\n\n        # 3. Blend with guidance scale\n        epsilon_guided = epsilon_uncond + guidance_scale * (\n            epsilon_cond - epsilon_uncond\n        )\n\n        # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        # Standard DDPM update (same as before)\n        # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n        alpha_t = get_alpha(t)\n        alpha_bar_t = get_alpha_bar(t)\n\n        # Compute mean using GUIDED noise prediction\n        mean = (1 / sqrt(alpha_t)) * (\n            x - ((1 - alpha_t) / sqrt(1 - alpha_bar_t)) * epsilon_guided\n        )\n\n        # Add noise (except last step)\n        if t &gt; 1:\n            sigma = get_sigma(t)\n            z = torch.randn_like(x)\n            x = mean + sigma * z\n        else:\n            x = mean\n\n    return x\n</code></pre>"},{"location":"DDPM/04_classifier_free_guidance/#alternative-single-forward-pass-training-with-guidance-embedding","title":"Alternative: Single Forward Pass (Training with Guidance Embedding)","text":"<p>Some implementations embed the guidance scale during training:</p> <pre><code># Training with guidance scale as input\nepsilon_pred = model(x_t, t, c, guidance_scale)\n\n# Sampling (single forward pass)\nepsilon_guided = model(x_t, t, c, w)\n</code></pre> <p>Trade-off: </p> <ul> <li>\u2705 Faster sampling (1 pass instead of 2)</li> <li>\u274c Less flexible (can't change w without retraining)</li> <li>\u274c More complex training</li> </ul> <p>Recommendation: Use two-pass approach for flexibility.</p>"},{"location":"DDPM/04_classifier_free_guidance/#part-3-guidance-scale-selection","title":"Part 3: Guidance Scale Selection","text":""},{"location":"DDPM/04_classifier_free_guidance/#effect-of-guidance-scale","title":"Effect of Guidance Scale","text":"<pre><code>w = 0.0   # Pure unconditional (ignores condition)\nw = 1.0   # Standard conditional (no guidance)\nw = 3.0   # Mild guidance\nw = 7.5   # Strong guidance (common for text-to-image)\nw = 15.0  # Very strong guidance (may cause artifacts)\n</code></pre>"},{"location":"DDPM/04_classifier_free_guidance/#empirical-guidelines","title":"Empirical Guidelines","text":"Application Typical w Notes Class-conditional images 3-5 Lower values work well Text-to-image 7-10 Higher values needed Image inpainting 5-7 Balance coherence and diversity Super-resolution 1-3 Lower to preserve details"},{"location":"DDPM/04_classifier_free_guidance/#trade-offs","title":"Trade-offs","text":"<pre><code>Guidance Scale     Fidelity to Condition     Diversity     Quality\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nw = 1              \u25cb Weak                    \u2713 High        \u25cb Moderate\nw = 3-5            \u2713 Good                    \u2713 Good        \u2713 Good\nw = 7-10           \u2713\u2713 Strong                 \u25cb Lower       \u2713 Good\nw &gt; 15             \u2713\u2713\u2713 Very Strong           \u2717 Very Low    \u2717 Artifacts\n</code></pre>"},{"location":"DDPM/04_classifier_free_guidance/#adaptive-guidance","title":"Adaptive Guidance","text":"<p>You can vary guidance scale during sampling:</p> <pre><code>def adaptive_guided_sample(model, shape, condition, T=1000):\n    x = torch.randn(shape)\n    null_token = get_null_token()\n\n    for t in reversed(range(1, T+1)):\n        # Adaptive guidance scale\n        if t &gt; 800:  # Early steps: high noise\n            w = 10.0  # Strong guidance for structure\n        elif t &gt; 200:  # Middle steps\n            w = 7.5   # Moderate guidance\n        else:  # Final steps: low noise\n            w = 3.0   # Weak guidance for details\n\n        epsilon_uncond = model(x, t, null_token)\n        epsilon_cond = model(x, t, condition)\n        epsilon_guided = epsilon_uncond + w * (epsilon_cond - epsilon_uncond)\n\n        # Standard update\n        x = denoise_step(x, epsilon_guided, t)\n\n    return x\n</code></pre>"},{"location":"DDPM/04_classifier_free_guidance/#part-4-implementation-examples","title":"Part 4: Implementation Examples","text":""},{"location":"DDPM/04_classifier_free_guidance/#example-1-class-conditional-cifar-10","title":"Example 1: Class-Conditional CIFAR-10","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ClassConditionalDDPM(nn.Module):\n    def __init__(self, num_classes=10, img_channels=3, base_channels=128):\n        super().__init__()\n\n        # Time embedding\n        self.time_embed = nn.Sequential(\n            SinusoidalEmbedding(base_channels),\n            nn.Linear(base_channels, base_channels * 4),\n            nn.SiLU(),\n            nn.Linear(base_channels * 4, base_channels * 4),\n        )\n\n        # Class embedding\n        self.class_embed = nn.Embedding(\n            num_classes + 1,  # +1 for null token\n            base_channels * 4\n        )\n\n        # U-Net architecture (simplified)\n        self.encoder = UNetEncoder(img_channels, base_channels)\n        self.bottleneck = UNetBottleneck(base_channels * 8)\n        self.decoder = UNetDecoder(base_channels, img_channels)\n\n        self.null_class_idx = num_classes  # Index for unconditional\n\n    def forward(self, x_t, t, c):\n        \"\"\"\n        Args:\n            x_t: (batch, 3, 32, 32) - noisy images\n            t: (batch,) - timesteps\n            c: (batch,) - class indices (or null_class_idx)\n        \"\"\"\n        # Embeddings\n        t_emb = self.time_embed(t)  # (batch, 512)\n        c_emb = self.class_embed(c)  # (batch, 512)\n\n        # Combine (simple addition)\n        conditioning = t_emb + c_emb  # (batch, 512)\n\n        # U-Net forward\n        features = self.encoder(x_t)\n        bottleneck = self.bottleneck(features[-1], conditioning)\n        epsilon_pred = self.decoder(bottleneck, features)\n\n        return epsilon_pred\n\n    def get_null_token(self, batch_size, device):\n        \"\"\"Return null token for unconditional generation\"\"\"\n        return torch.full((batch_size,), self.null_class_idx, \n                         dtype=torch.long, device=device)\n\n\n# Training\ndef train_with_guidance(model, dataloader, p_uncond=0.1):\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n    for epoch in range(num_epochs):\n        for batch in dataloader:\n            x_0 = batch['image']  # (batch, 3, 32, 32)\n            c = batch['label']    # (batch,) class indices 0-9\n\n            # Sample timestep and noise\n            t = torch.randint(1, T + 1, (x_0.shape[0],))\n            epsilon = torch.randn_like(x_0)\n\n            # Forward diffusion\n            alpha_bar_t = get_alpha_bar(t)\n            x_t = torch.sqrt(alpha_bar_t)[:, None, None, None] * x_0 + \\\n                  torch.sqrt(1 - alpha_bar_t)[:, None, None, None] * epsilon\n\n            # Randomly drop condition\n            mask = torch.rand(x_0.shape[0]) &lt; p_uncond\n            c = torch.where(mask, model.null_class_idx, c)\n\n            # Predict noise\n            epsilon_pred = model(x_t, t, c)\n\n            # Loss\n            loss = F.mse_loss(epsilon_pred, epsilon)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n\n# Sampling\n@torch.no_grad()\ndef sample_with_guidance(model, class_idx, guidance_scale=5.0, \n                        num_samples=4, device='cuda'):\n    \"\"\"Generate samples for a specific class\"\"\"\n    model.eval()\n\n    # Initialize from noise\n    x = torch.randn(num_samples, 3, 32, 32, device=device)\n\n    # Prepare conditions\n    condition = torch.full((num_samples,), class_idx, \n                          dtype=torch.long, device=device)\n    null_token = model.get_null_token(num_samples, device)\n\n    # Reverse diffusion\n    for t in reversed(range(1, T + 1)):\n        t_batch = torch.full((num_samples,), t, device=device)\n\n        # Two forward passes\n        epsilon_uncond = model(x, t_batch, null_token)\n        epsilon_cond = model(x, t_batch, condition)\n\n        # Guided prediction\n        epsilon_guided = epsilon_uncond + guidance_scale * (\n            epsilon_cond - epsilon_uncond\n        )\n\n        # DDPM update\n        alpha_t = get_alpha(t)\n        alpha_bar_t = get_alpha_bar(t)\n        beta_t = 1 - alpha_t\n\n        mean = (1 / torch.sqrt(alpha_t)) * (\n            x - (beta_t / torch.sqrt(1 - alpha_bar_t)) * epsilon_guided\n        )\n\n        if t &gt; 1:\n            sigma = torch.sqrt(beta_t)\n            z = torch.randn_like(x)\n            x = mean + sigma * z\n        else:\n            x = mean\n\n    return x\n</code></pre>"},{"location":"DDPM/04_classifier_free_guidance/#example-2-text-conditional-generation","title":"Example 2: Text-Conditional Generation","text":"<pre><code>class TextConditionalDDPM(nn.Module):\n    def __init__(self, text_embed_dim=512):\n        super().__init__()\n\n        self.time_embed = TimeEmbedding(256)\n\n        # Text encoder (e.g., pre-trained CLIP or T5)\n        self.text_encoder = CLIPTextEncoder()\n\n        # Cross-attention U-Net\n        self.unet = CrossAttentionUNet(\n            text_dim=text_embed_dim,\n            time_dim=256\n        )\n\n        # Null text embedding (learnable)\n        self.null_text_embed = nn.Parameter(\n            torch.randn(text_embed_dim)\n        )\n\n    def forward(self, x_t, t, text_embedding):\n        \"\"\"\n        Args:\n            x_t: Noisy images\n            t: Timesteps\n            text_embedding: Text embeddings (batch, seq_len, 512)\n                           Or null_text_embed for unconditional\n        \"\"\"\n        t_emb = self.time_embed(t)\n        epsilon_pred = self.unet(x_t, t_emb, text_embedding)\n        return epsilon_pred\n\n    def encode_text(self, text_prompts):\n        \"\"\"Encode text prompts to embeddings\"\"\"\n        return self.text_encoder(text_prompts)\n\n    def get_null_token(self, batch_size):\n        \"\"\"Return null token for unconditional generation\"\"\"\n        # Expand to match text sequence shape if needed\n        return self.null_text_embed.unsqueeze(0).expand(\n            batch_size, -1, -1\n        )\n\n\n# Training\ndef train_text_conditional(model, dataloader, p_uncond=0.1):\n    for batch in dataloader:\n        x_0 = batch['image']\n        text = batch['caption']\n\n        # Encode text\n        text_embed = model.encode_text(text)\n\n        # Standard diffusion forward\n        t = torch.randint(1, T + 1, (x_0.shape[0],))\n        epsilon = torch.randn_like(x_0)\n        x_t = add_noise(x_0, t, epsilon)\n\n        # Randomly use null token\n        mask = torch.rand(x_0.shape[0]) &lt; p_uncond\n        null_embed = model.get_null_token(x_0.shape[0])\n        text_embed = torch.where(\n            mask[:, None, None], null_embed, text_embed\n        )\n\n        # Predict and train\n        epsilon_pred = model(x_t, t, text_embed)\n        loss = F.mse_loss(epsilon_pred, epsilon)\n        # ... backprop\n</code></pre>"},{"location":"DDPM/04_classifier_free_guidance/#part-5-common-pitfalls-and-solutions","title":"Part 5: Common Pitfalls and Solutions","text":""},{"location":"DDPM/04_classifier_free_guidance/#pitfall-1-forgetting-to-train-on-unconditional","title":"Pitfall 1: Forgetting to Train on Unconditional","text":"<p>Problem: Only training with conditions, forgetting to drop them</p> <pre><code># \u274c WRONG: Never drops condition\nepsilon_pred = model(x_t, t, c)  # Always has c\nloss = F.mse_loss(epsilon_pred, epsilon)\n</code></pre> <p>Solution: Always implement condition dropout</p> <pre><code># \u2705 CORRECT: Randomly drop condition\nif random.random() &lt; p_uncond:\n    c = null_token\nepsilon_pred = model(x_t, t, c)\n</code></pre>"},{"location":"DDPM/04_classifier_free_guidance/#pitfall-2-using-wrong-null-token","title":"Pitfall 2: Using Wrong Null Token","text":"<p>Problem: Inconsistent null token between training and sampling</p> <pre><code># Training: uses zeros\nif random.random() &lt; p_uncond:\n    c = torch.zeros_like(c)\n\n# Sampling: uses -1\nnull_token = torch.full_like(c, -1)  # \u274c Mismatch!\n</code></pre> <p>Solution: Use the same null representation</p> <pre><code># Define once, use everywhere\nNULL_CLASS_IDX = num_classes  # e.g., 10 for CIFAR-10\n\n# Training\nif random.random() &lt; p_uncond:\n    c = torch.full_like(c, NULL_CLASS_IDX)\n\n# Sampling\nnull_token = torch.full((batch_size,), NULL_CLASS_IDX)\n</code></pre>"},{"location":"DDPM/04_classifier_free_guidance/#pitfall-3-guidance-scale-too-high","title":"Pitfall 3: Guidance Scale Too High","text":"<p>Problem: Over-guidance causes artifacts</p> <pre><code># \u274c Too high: w=20\nepsilon_guided = epsilon_uncond + 20 * (epsilon_cond - epsilon_uncond)\n# Result: Oversaturated, unrealistic images\n</code></pre> <p>Solution: Start low and increase gradually</p> <pre><code># \u2705 Start with moderate values\nfor w in [1.0, 3.0, 5.0, 7.5, 10.0]:\n    samples = sample_with_guidance(model, condition, w)\n    evaluate(samples)  # Find best w\n</code></pre>"},{"location":"DDPM/04_classifier_free_guidance/#pitfall-4-not-caching-unconditional-predictions","title":"Pitfall 4: Not Caching Unconditional Predictions","text":"<p>Problem: Computing unconditional prediction every step is wasteful</p> <pre><code># Inefficient: compute unconditional every time\nfor t in range(T, 0, -1):\n    epsilon_uncond = model(x, t, null_token)  # Same for all conditions!\n    epsilon_cond = model(x, t, condition)\n</code></pre> <p>Solution: Batch multiple conditions</p> <pre><code># Better: batch process if generating multiple samples with same x_t\n# (Not always applicable, but useful for parallel generation)\nbatch_conditions = [cond1, cond2, ..., null_token]\nepsilon_all = model(x.repeat(len(batch_conditions), 1, 1, 1), \n                   t, batch_conditions)\n</code></pre>"},{"location":"DDPM/04_classifier_free_guidance/#pitfall-5-numerical-instability-with-high-guidance","title":"Pitfall 5: Numerical Instability with High Guidance","text":"<p>Problem: Extremely large guidance scales cause NaNs</p> <pre><code># Can cause numerical issues\nepsilon_guided = epsilon_uncond + 100 * (epsilon_cond - epsilon_uncond)\n</code></pre> <p>Solution: Clip guided predictions</p> <pre><code># Clip to reasonable range\nepsilon_guided = epsilon_uncond + w * (epsilon_cond - epsilon_uncond)\nepsilon_guided = torch.clamp(epsilon_guided, -10, 10)  # Prevent extremes\n</code></pre>"},{"location":"DDPM/04_classifier_free_guidance/#part-6-advanced-techniques","title":"Part 6: Advanced Techniques","text":""},{"location":"DDPM/04_classifier_free_guidance/#1-dynamic-guidance-schedules","title":"1. Dynamic Guidance Schedules","text":"<p>Vary guidance strength throughout sampling:</p> <pre><code>def get_dynamic_guidance(t, T):\n    \"\"\"\n    Higher guidance early (structure)\n    Lower guidance late (details)\n    \"\"\"\n    progress = t / T  # 1.0 \u2192 0.0\n\n    if progress &gt; 0.8:  # Very noisy\n        return 10.0\n    elif progress &gt; 0.5:  # Moderately noisy\n        return 7.5\n    elif progress &gt; 0.2:  # Less noisy\n        return 5.0\n    else:  # Almost clean\n        return 3.0\n</code></pre>"},{"location":"DDPM/04_classifier_free_guidance/#2-guidance-with-multiple-conditions","title":"2. Guidance with Multiple Conditions","text":"<pre><code># Multiple conditions: class + style + color\nepsilon_guided = (\n    epsilon_uncond + \n    w_class * (epsilon_class - epsilon_uncond) +\n    w_style * (epsilon_style - epsilon_uncond) +\n    w_color * (epsilon_color - epsilon_uncond)\n)\n</code></pre>"},{"location":"DDPM/04_classifier_free_guidance/#3-negative-guidance","title":"3. Negative Guidance","text":"<p>Push away from unwanted conditions:</p> <pre><code># Generate \"not a dog\"\nepsilon_guided = epsilon_uncond - w_negative * (\n    epsilon_dog - epsilon_uncond\n)\n</code></pre>"},{"location":"DDPM/04_classifier_free_guidance/#4-self-guidance-unconditional-only","title":"4. Self-Guidance (Unconditional Only)","text":"<p>Use guidance even without conditions:</p> <pre><code># Split noise prediction into components\nepsilon_mean = epsilon_pred.mean()\nepsilon_guided = epsilon_mean + w * (epsilon_pred - epsilon_mean)\n</code></pre>"},{"location":"DDPM/04_classifier_free_guidance/#part-7-evaluation-and-debugging","title":"Part 7: Evaluation and Debugging","text":""},{"location":"DDPM/04_classifier_free_guidance/#metrics-to-track","title":"Metrics to Track","text":""},{"location":"DDPM/04_classifier_free_guidance/#1-condition-fidelity","title":"1. Condition Fidelity","text":"<p>How well do samples match the condition?</p> <pre><code># For class-conditional\nclassifier_accuracy = pretrained_classifier(samples)\n\n# For text-to-image\nclip_score = compute_clip_score(samples, text_prompts)\n</code></pre>"},{"location":"DDPM/04_classifier_free_guidance/#2-sample-quality","title":"2. Sample Quality","text":"<pre><code># FID (Fr\u00e9chet Inception Distance)\nfid_score = compute_fid(generated_samples, real_samples)\n\n# Inception Score\nis_score = compute_inception_score(generated_samples)\n</code></pre>"},{"location":"DDPM/04_classifier_free_guidance/#3-diversity","title":"3. Diversity","text":"<pre><code># Intra-class diversity\ndiversity = compute_pairwise_distance(samples_same_class)\n\n# Should decrease with higher guidance\n</code></pre>"},{"location":"DDPM/04_classifier_free_guidance/#debugging-checklist","title":"Debugging Checklist","text":"<pre><code># 1. Check unconditional generation works\nsamples_uncond = sample_with_guidance(model, condition, w=0.0)\n# Should produce diverse, realistic (but generic) samples\n\n# 2. Check conditional generation works\nsamples_cond = sample_with_guidance(model, condition, w=1.0)\n# Should produce samples matching condition\n\n# 3. Check guidance improves fidelity\nfor w in [1.0, 3.0, 5.0, 7.5]:\n    samples = sample_with_guidance(model, condition, w)\n    fidelity = measure_condition_fidelity(samples, condition)\n    print(f\"w={w}: fidelity={fidelity}\")\n# Fidelity should increase with w\n\n# 4. Check for mode collapse\nsamples = [sample_with_guidance(model, condition, w=7.5) \n           for _ in range(100)]\ndiversity = compute_diversity(samples)\n# Diversity should be reasonable (not all identical)\n\n# 5. Visualize guidance scale sweep\nvisualize_guidance_sweep(model, condition, w_values=[0, 1, 3, 5, 7, 10, 15])\n</code></pre>"},{"location":"DDPM/04_classifier_free_guidance/#common-issues-and-fixes","title":"Common Issues and Fixes","text":"Symptom Likely Cause Solution Unconditional (w=0) is poor Not enough uncond training Increase p_uncond High w causes artifacts Guidance too strong Lower w or clip predictions All samples look identical Mode collapse Lower w, check diversity loss Condition ignored even at w=10 Model didn't learn conditioning Check condition embedding, increase training NaN during sampling Numerical instability Clip predictions, lower w"},{"location":"DDPM/04_classifier_free_guidance/#part-8-practical-tips","title":"Part 8: Practical Tips","text":""},{"location":"DDPM/04_classifier_free_guidance/#training_1","title":"Training","text":"<ol> <li>Start with unconditional: Train a good unconditional model first, then add guidance</li> <li>Use moderate p_uncond: 0.1 is safe, 0.2 if unconditional quality matters</li> <li>Monitor both: Track unconditional and conditional losses separately</li> <li>Longer training: Guidance requires more iterations to converge</li> </ol>"},{"location":"DDPM/04_classifier_free_guidance/#sampling_1","title":"Sampling","text":"<ol> <li>Start low: Begin with w=1-3 and increase if needed</li> <li>Application-specific: Text-to-image needs higher w than class-conditional</li> <li>Quality &gt; adherence: Don't sacrifice quality for perfect condition matching</li> <li>Use dynamic schedules: High w early, low w late often works well</li> </ol>"},{"location":"DDPM/04_classifier_free_guidance/#hyperparameters","title":"Hyperparameters","text":"<pre><code># Recommended starting points\nCONFIG = {\n    'p_uncond': 0.1,           # Unconditional probability\n    'guidance_scale': 7.5,      # Default w (tune for your task)\n    'min_guidance': 1.0,        # Minimum w\n    'max_guidance': 15.0,       # Maximum w\n    'clip_epsilon': 10.0,       # Clip range for predictions\n}\n</code></pre>"},{"location":"DDPM/04_classifier_free_guidance/#part-9-complete-working-example","title":"Part 9: Complete Working Example","text":"<p>Here's a minimal, complete implementation:</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SimpleDDPM(nn.Module):\n    \"\"\"Minimal DDPM with classifier-free guidance\"\"\"\n\n    def __init__(self, num_classes=10):\n        super().__init__()\n        # Simple architecture for demonstration\n        self.class_embed = nn.Embedding(num_classes + 1, 128)\n        self.time_embed = SinusoidalEmbedding(128)\n        self.net = SimpleUNet(in_channels=3, time_dim=128, class_dim=128)\n        self.null_class = num_classes\n\n    def forward(self, x, t, c):\n        t_emb = self.time_embed(t)\n        c_emb = self.class_embed(c)\n        return self.net(x, t_emb + c_emb)\n\n# Training\ndef train(model, dataloader, T=1000, p_uncond=0.1, num_epochs=100):\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n    for epoch in range(num_epochs):\n        for images, labels in dataloader:\n            # Sample t and noise\n            t = torch.randint(1, T+1, (images.shape[0],))\n            noise = torch.randn_like(images)\n\n            # Add noise\n            alpha_bar = get_alpha_bar(t)\n            x_t = torch.sqrt(alpha_bar[:, None, None, None]) * images + \\\n                  torch.sqrt(1 - alpha_bar[:, None, None, None]) * noise\n\n            # Drop condition randomly\n            mask = torch.rand(images.shape[0]) &lt; p_uncond\n            labels = torch.where(mask, model.null_class, labels)\n\n            # Predict and loss\n            noise_pred = model(x_t, t, labels)\n            loss = F.mse_loss(noise_pred, noise)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n\n# Sampling\n@torch.no_grad()\ndef sample(model, class_idx, w=7.5, T=1000):\n    model.eval()\n    x = torch.randn(1, 3, 32, 32)\n\n    for t in reversed(range(1, T+1)):\n        # Two forward passes\n        t_tensor = torch.tensor([t])\n        null = torch.tensor([model.null_class])\n        cond = torch.tensor([class_idx])\n\n        noise_uncond = model(x, t_tensor, null)\n        noise_cond = model(x, t_tensor, cond)\n\n        # Guided prediction\n        noise_guided = noise_uncond + w * (noise_cond - noise_uncond)\n\n        # DDPM step\n        alpha = get_alpha(t)\n        alpha_bar = get_alpha_bar(t)\n        beta = 1 - alpha\n\n        mean = (x - (beta / torch.sqrt(1 - alpha_bar)) * noise_guided) / torch.sqrt(alpha)\n\n        if t &gt; 1:\n            x = mean + torch.sqrt(beta) * torch.randn_like(x)\n        else:\n            x = mean\n\n    return x\n\n# Usage\nmodel = SimpleDDPM(num_classes=10)\ntrain(model, train_loader)\nsample_image = sample(model, class_idx=3, w=7.5)  # Generate class 3\n</code></pre>"},{"location":"DDPM/04_classifier_free_guidance/#summary","title":"Summary","text":"<p>Classifier-free guidance enables high-quality conditional generation in DDPM:</p>"},{"location":"DDPM/04_classifier_free_guidance/#key-points","title":"Key Points","text":"<ol> <li>Training: Randomly drop conditions (10-20% of time) with null token</li> <li>Sampling: Two forward passes per step (unconditional + conditional)</li> <li>Guidance: Blend predictions with scale w (typically 3-10)</li> <li>Trade-off: Higher w \u2192 better condition matching, lower diversity</li> </ol>"},{"location":"DDPM/04_classifier_free_guidance/#implementation-checklist","title":"Implementation Checklist","text":"<ul> <li> Model accepts condition input</li> <li> Define null token consistently</li> <li> Implement condition dropout in training (p_uncond \u2248 0.1)</li> <li> Two forward passes during sampling</li> <li> Start with moderate guidance scale (w=5-7)</li> <li> Monitor both conditional and unconditional quality</li> <li> Clip predictions if numerical issues arise</li> </ul>"},{"location":"DDPM/04_classifier_free_guidance/#when-to-use","title":"When to Use","text":"<p>\u2705 Use classifier-free guidance when: - Need high-quality conditional generation - Want control over condition strength - Don't want to train a separate classifier - Text-to-image, class-conditional, or any conditional task</p> <p>\u274c Don't use when: - Unconditional generation only - Inference speed is critical (2x slower) - Limited training data (needs both conditional and unconditional)</p> <p>Next: Advanced Sampling Techniques | Back to DDPM Overview</p> <p>Related: Theoretical Foundations | DiT with Guidance</p>"},{"location":"DiT/","title":"Diffusion Transformers (DiT) + Rectified Flow","text":"<p>This directory contains comprehensive documentation on Diffusion Transformers (DiT) combined with rectified flow \u2014 the modern architecture for scalable, flexible generative modeling.</p> <p>DiT represents the shift from convolutional U-Nets to Transformers, enabling better scaling, flexible conditioning, and modality-agnostic generation.</p>"},{"location":"DiT/#core-documentation-series","title":"Core Documentation Series","text":"<p>This series follows the same structure as DDPM, SDE, and flow matching documentation.</p> Document Description 00_dit_overview.md Overview: Why DiT matters, key concepts, modern stack 01_dit_foundations.md Foundations: Architecture details, components, design choices 02_dit_training.md Training: How to train DiT + rectified flow models 03_dit_sampling.md Sampling: How to generate samples efficiently"},{"location":"DiT/#supplementary-documents","title":"Supplementary Documents","text":"<p>Deep dives on specific topics (located in <code>docs/diffusion/DiT/</code>):</p> Document Description diffusion_transformer.md Comprehensive tutorial with biology applications time_embeddings_explained.md Deep dive on time conditioning mechanisms"},{"location":"DiT/#quick-navigation","title":"Quick Navigation","text":""},{"location":"DiT/#for-beginners","title":"For Beginners","text":"<p>Start with the overview to understand the big picture, then move through foundations and training.</p> <p>Path: Overview \u2192 Foundations \u2192 Training</p>"},{"location":"DiT/#for-implementation","title":"For Implementation","text":"<p>Focus on the practical training and sampling guides with code examples.</p> <p>Path: Training \u2192 Sampling \u2192 Supplementary docs</p>"},{"location":"DiT/#for-theory-deep-dive","title":"For Theory Deep Dive","text":"<p>Understand the architectural choices and mathematical foundations.</p> <p>Path: Foundations \u2192 Supplementary docs \u2192 Flow matching theory</p>"},{"location":"DiT/#key-concepts","title":"Key Concepts","text":""},{"location":"DiT/#the-modern-generative-stack","title":"The Modern Generative Stack","text":"<pre><code>Rectified Flow (objective) + DiT (architecture) + AdaLN (conditioning)\n</code></pre> <p>Rectified Flow: Simple regression target $$ \\mathcal{L} = \\mathbb{E}{x_0, x_1, t} \\left[ \\left| v\\theta(x_t, t) - (x_1 - x_0) \\right|^2 \\right] $$</p> <p>DiT: Transformer-based architecture - Tokenization: Input \u2192 patches \u2192 tokens - Self-attention: Global dependencies - AdaLN: Time/condition modulation</p> <p>Result: Fast, scalable, flexible generation</p>"},{"location":"DiT/#dit-vs-u-net","title":"DiT vs U-Net","text":"Aspect U-Net DiT Architecture Convolutional Transformer Receptive field Local \u2192 Global Global from start Input format Fixed grids Flexible tokens Conditioning Architectural changes Built-in (AdaLN) Scaling Limited Excellent Best for Images, fixed size Any modality"},{"location":"DiT/#core-components","title":"Core Components","text":"<p>1. Tokenization <pre><code>Image/Data \u2192 Patches \u2192 Flatten \u2192 Embed \u2192 Tokens\n</code></pre></p> <p>2. Time Conditioning (AdaLN) <pre><code>t \u2192 TimeEmbed(t) \u2192 MLP \u2192 (\u03b3, \u03b2) \u2192 Modulate features\n</code></pre></p> <p>3. Transformer Blocks <pre><code>Tokens \u2192 Self-Attention \u2192 MLP \u2192 Updated Tokens\n</code></pre></p> <p>4. Output Projection <pre><code>Tokens \u2192 Linear \u2192 Velocity Field\n</code></pre></p>"},{"location":"DiT/#training-overview","title":"Training Overview","text":""},{"location":"DiT/#rectified-flow-loss","title":"Rectified Flow Loss","text":"<p>Simple regression:</p> \\[ \\mathcal{L} = \\mathbb{E}_{x_0, x_1, t} \\left[ \\left\\| v_\\theta(x_t, t) - (x_1 - x_0) \\right\\|^2 \\right] \\] <p>where:</p> <ul> <li>\\(x_0 \\sim p_{\\text{data}}\\) (real data)</li> <li>\\(x_1 \\sim \\mathcal{N}(0, I)\\) (noise)</li> <li>\\(x_t = t x_1 + (1-t) x_0\\) (linear interpolation)</li> </ul> <p>Key advantages:</p> <ul> <li>No noise schedules</li> <li>No variance parameterization</li> <li>Direct regression target</li> <li>Stable training</li> </ul>"},{"location":"DiT/#training-algorithm","title":"Training Algorithm","text":"<pre><code>for batch in dataloader:\n    x_0 = batch  # Real data\n    x_1 = torch.randn_like(x_0)  # Noise\n    t = torch.rand(batch_size)  # Random time\n\n    # Linear interpolation\n    x_t = t * x_1 + (1 - t) * x_0\n\n    # Predict velocity\n    v_pred = model(x_t, t)\n\n    # Compute loss\n    target = x_1 - x_0\n    loss = F.mse_loss(v_pred, target)\n\n    # Update\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"DiT/#sampling-overview","title":"Sampling Overview","text":""},{"location":"DiT/#ode-integration","title":"ODE Integration","text":"<p>Forward ODE (noise \u2192 data):</p> \\[ \\frac{dx}{dt} = v_\\theta(x, t) \\] <p>Euler discretization: <pre><code>x = torch.randn(shape)  # Start from noise\ndt = 1.0 / num_steps\n\nfor k in range(num_steps):\n    t = k * dt\n    v = model(x, t)\n    x = x + v * dt\n\nreturn x  # Generated sample\n</code></pre></p> <p>Properties:</p> <ul> <li>Deterministic (same noise \u2192 same output)</li> <li>Fast (20-50 steps)</li> <li>Straight paths (rectified flow)</li> </ul>"},{"location":"DiT/#applications","title":"Applications","text":""},{"location":"DiT/#vision","title":"Vision","text":"<ul> <li>Images: Stable Diffusion 3, DALL-E 3</li> <li>Videos: Sora, Goku</li> <li>3D: Point clouds, meshes</li> </ul>"},{"location":"DiT/#audio","title":"Audio","text":"<ul> <li>Music: MusicGen</li> <li>Speech: AudioLDM</li> <li>Sound effects: Foley generation</li> </ul>"},{"location":"DiT/#biology","title":"Biology","text":"<ul> <li>Gene expression: Cell state generation</li> <li>Perturbations: Predict intervention effects</li> <li>Trajectories: Developmental paths</li> <li>Molecules: Protein structure</li> </ul>"},{"location":"DiT/#other","title":"Other","text":"<ul> <li>Robotics: Trajectory planning</li> <li>Physics: Simulation</li> <li>Design: CAD, architecture</li> </ul>"},{"location":"DiT/#why-dit-for-biology","title":"Why DiT for Biology?","text":""},{"location":"DiT/#challenges-with-traditional-approaches","title":"Challenges with Traditional Approaches","text":"<p>Gene expression data:</p> <ul> <li>High-dimensional (10K-30K genes)</li> <li>Unordered (no natural sequence)</li> <li>Sparse (many zeros)</li> <li>Compositional (relative values matter)</li> </ul> <p>U-Net limitations:</p> <ul> <li>Assumes spatial structure</li> <li>Fixed input sizes</li> <li>Hard to condition on perturbations</li> </ul>"},{"location":"DiT/#dit-advantages","title":"DiT Advantages","text":"<p>Flexibility:</p> <ul> <li>Genes/cells/regions as tokens</li> <li>Variable-length sequences</li> <li>Natural conditioning on perturbations</li> </ul> <p>Global interactions:</p> <ul> <li>Self-attention captures gene-gene dependencies</li> <li>No locality bias</li> <li>Learn regulatory networks</li> </ul> <p>Scalability:</p> <ul> <li>Handle large gene panels</li> <li>Batch different experiments</li> <li>Scale to billions of parameters</li> </ul>"},{"location":"DiT/#open-questions","title":"Open Questions","text":"<ol> <li>Tokenization: How to represent genes as tokens?</li> <li>Rank by expression? (Geneformer approach)</li> <li>Gene embeddings? (learned representations)</li> <li> <p>Set-based? (permutation invariant)</p> </li> <li> <p>Latent space: Better to work in latent space?</p> </li> <li>Encode expression \u2192 latent \u2192 diffusion</li> <li>Avoids sparsity issues</li> <li> <p>More stable training</p> </li> <li> <p>Architecture: DiT vs alternatives?</p> </li> <li>State-space models (Mamba, S4)</li> <li>Hyena (long convolutions)</li> <li>Hybrid approaches</li> </ol> <p>See: Supplementary documents for deeper exploration.</p>"},{"location":"DiT/#learning-path","title":"Learning Path","text":""},{"location":"DiT/#conceptual-understanding","title":"Conceptual Understanding","text":"<ol> <li>DiT Overview \u2014 Why DiT matters</li> <li>Architectural shift from U-Net</li> <li>Modern generative stack</li> <li> <p>Key advantages</p> </li> <li> <p>Flow Matching Basics \u2014 Rectified flow theory</p> </li> <li>Velocity fields</li> <li>Linear interpolation</li> <li> <p>ODE sampling</p> </li> <li> <p>DiT Foundations \u2014 Architecture details</p> </li> <li>Tokenization strategies</li> <li>Transformer blocks</li> <li>Time conditioning</li> </ol>"},{"location":"DiT/#practical-implementation","title":"Practical Implementation","text":"<ol> <li>DiT Training \u2014 Training pipeline</li> <li>Data preparation</li> <li>Model architecture</li> <li>Training loop</li> <li> <p>Hyperparameters</p> </li> <li> <p>DiT Sampling \u2014 Generation strategies</p> </li> <li>ODE solvers</li> <li>Conditional generation</li> <li>Quality vs speed</li> </ol>"},{"location":"DiT/#advanced-topics","title":"Advanced Topics","text":"<ol> <li>Comprehensive Tutorial \u2014 Deep dive</li> <li>Alternative backbones</li> <li>Biology applications</li> <li> <p>State-space models</p> </li> <li> <p>Time Embeddings \u2014 Conditioning mechanisms</p> </li> <li>Sinusoidal embeddings</li> <li>AdaLN details</li> <li>FiLM modulation</li> </ol>"},{"location":"DiT/#comparison-with-other-methods","title":"Comparison with Other Methods","text":""},{"location":"DiT/#dit-vs-ddpm","title":"DiT vs DDPM","text":"Aspect DDPM DiT + Rectified Flow Architecture U-Net Transformer Objective Noise prediction Velocity prediction Training Noise schedule needed Simple regression Sampling 1000 steps (SDE) 20-50 steps (ODE) Conditioning Concatenation/FiLM AdaLN/Cross-attention Flexibility Images mainly Any modality"},{"location":"DiT/#dit-vs-flow-matching-u-net","title":"DiT vs Flow Matching (U-Net)","text":"Aspect Flow Matching + U-Net DiT + Rectified Flow Objective Same (velocity) Same (velocity) Architecture Convolutional Transformer Scaling Limited Excellent Conditioning Moderate Excellent Speed Fast convolutions Slower attention <p>Key insight: DiT is an architectural choice, orthogonal to the training objective.</p>"},{"location":"DiT/#key-takeaways","title":"Key Takeaways","text":""},{"location":"DiT/#conceptual","title":"Conceptual","text":"<ol> <li>DiT = Transformer architecture for diffusion/flow models</li> <li>Rectified flow = simple objective (velocity regression)</li> <li>Together = modern stack for state-of-the-art generation</li> <li>Tokenization enables modality-agnostic modeling</li> </ol>"},{"location":"DiT/#practical","title":"Practical","text":"<ol> <li>Training is simple: Regression on \\(v = x_1 - x_0\\)</li> <li>Sampling is fast: 20-50 ODE steps</li> <li>Conditioning is easy: Tokens or AdaLN</li> <li>Scales well: Proven to billions of parameters</li> </ol>"},{"location":"DiT/#for-biology","title":"For Biology","text":"<ol> <li>Flexible representation: Genes, cells, perturbations</li> <li>Global interactions: Attention captures dependencies</li> <li>Conditional generation: Model interventions</li> <li>Active research: Best practices still emerging</li> </ol>"},{"location":"DiT/#related-documentation","title":"Related Documentation","text":""},{"location":"DiT/#prerequisites","title":"Prerequisites","text":"<ul> <li>Flow Matching \u2014 Rectified flow theory</li> <li>DDPM \u2014 Discrete diffusion models</li> <li>SDE \u2014 Continuous-time perspective</li> </ul>"},{"location":"DiT/#advanced-topics_1","title":"Advanced Topics","text":"<ul> <li>Latent Diffusion \u2014 Diffusion in latent space</li> <li>JEPA \u2014 Joint-embedding predictive architectures</li> </ul>"},{"location":"DiT/#code-examples","title":"Code Examples","text":"<ul> <li><code>notebooks/diffusion/</code> \u2014 Interactive tutorials</li> <li><code>examples/</code> \u2014 Production scripts</li> </ul>"},{"location":"DiT/#references","title":"References","text":""},{"location":"DiT/#key-papers","title":"Key Papers","text":"<p>DiT:</p> <ul> <li>Peebles &amp; Xie (2023): \"Scalable Diffusion Models with Transformers\"</li> </ul> <p>Rectified Flow:</p> <ul> <li>Liu et al. (2022): \"Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow\"</li> <li>Liu et al. (2023): \"InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation\"</li> </ul> <p>Transformers:</p> <ul> <li>Dosovitskiy et al. (2020): \"An Image is Worth 16x16 Words\" (ViT)</li> <li>Vaswani et al. (2017): \"Attention is All You Need\"</li> </ul> <p>Conditioning:</p> <ul> <li>Perez et al. (2018): \"FiLM: Visual Reasoning with a General Conditioning Layer\"</li> </ul>"},{"location":"DiT/#modern-implementations","title":"Modern Implementations","text":"<ul> <li>Stable Diffusion 3: DiT-based text-to-image</li> <li>Sora: DiT for video generation</li> <li>Hugging Face Diffusers: DiT implementations</li> <li>OpenAI: DALL-E 3</li> </ul>"},{"location":"DiT/#summary","title":"Summary","text":"<p>Diffusion Transformers (DiT) combined with rectified flow represent the modern approach to generative modeling:</p> <p>Architecture: Transformers replace U-Nets - Global attention from the start - Flexible tokenization - First-class conditioning</p> <p>Objective: Rectified flow simplifies training - Direct velocity regression - No noise schedules - Fast ODE sampling</p> <p>Result: State-of-the-art generation - Images, video, audio - Scalable to billions of parameters - Emerging applications in biology</p> <p>The modern stack: <pre><code>Rectified Flow + DiT + AdaLN = Powerful, flexible generation\n</code></pre></p> <p>This combination has become the foundation for cutting-edge generative models and is particularly promising for computational biology applications.</p>"},{"location":"DiT/00_dit_overview/","title":"Diffusion Transformers (DiT): Overview","text":"<p>This document provides a high-level introduction to Diffusion Transformers (DiT) \u2014 the architectural shift from convolutional U-Nets to Transformers for generative modeling, particularly when combined with rectified flow.</p>"},{"location":"DiT/00_dit_overview/#what-is-dit","title":"What is DiT?","text":"<p>Diffusion Transformer (DiT) is an architectural choice, not a new diffusion theory.</p> <p>DiT uses a Transformer to parameterize the function learned in diffusion or flow-based models:</p> \\[ v_\\theta(x, t, c) \\quad \\text{(velocity for flow matching)} \\] \\[ \\epsilon_\\theta(x, t, c) \\quad \\text{(noise for DDPM)} \\] \\[ s_\\theta(x, t) \\quad \\text{(score for score matching)} \\] <p>Key insight: The objective (what to learn) and the architecture (how to learn it) are orthogonal design choices.</p>"},{"location":"DiT/00_dit_overview/#why-dit-matters","title":"Why DiT Matters","text":""},{"location":"DiT/00_dit_overview/#the-architectural-evolution","title":"The Architectural Evolution","text":"<p>Historical progression: 1. U-Net era (2020-2022): Convolutional architectures dominated 2. DiT era (2023+): Transformers became the standard 3. Modern stack: DiT + Rectified Flow</p>"},{"location":"DiT/00_dit_overview/#what-changed","title":"What Changed","text":"Aspect U-Net DiT Inductive bias Spatial locality Global attention Input format Fixed grids Flexible tokens Conditioning Architectural changes needed First-class via modulation Scaling Limited Excellent Flexibility Images only Any modality"},{"location":"DiT/00_dit_overview/#the-core-idea-grids-tokens","title":"The Core Idea: Grids \u2192 Tokens","text":"<p>U-Net thinking: Process images as spatial grids with local convolutions</p> <p>DiT thinking: Represent inputs as sequences of tokens, process with global attention</p> <p>For images: 1. Split image into patches (e.g., 16\u00d716 pixels) 2. Flatten each patch into a vector 3. Embed into token space 4. Process with Transformer 5. Project back to image space</p> <p>For other domains:</p> <ul> <li>Genes, cells, regions \u2192 tokens</li> <li>Time series \u2192 temporal tokens</li> <li>Latent representations \u2192 abstract tokens</li> </ul>"},{"location":"DiT/00_dit_overview/#dit-rectified-flow-the-modern-stack","title":"DiT + Rectified Flow: The Modern Stack","text":""},{"location":"DiT/00_dit_overview/#why-this-combination-works","title":"Why This Combination Works","text":"<p>Rectified Flow provides: - Simple regression target: \\(v = x_1 - x_0\\) - Straight paths in data space - Fast ODE sampling - No density assumptions</p> <p>DiT provides: - Global context via self-attention - Flexible conditioning via modulation - Scalability to large models - Modality-agnostic architecture</p> <p>Together: <pre><code>Simple objective + Powerful architecture = State-of-the-art generation\n</code></pre></p>"},{"location":"DiT/00_dit_overview/#key-components","title":"Key Components","text":"<p>1. Tokenization: Convert input to sequence <pre><code>Image \u2192 Patches \u2192 Tokens\n</code></pre></p> <p>2. Time Conditioning: Adaptive LayerNorm (AdaLN) <pre><code>t \u2192 TimeEmbed \u2192 (\u03b3, \u03b2) \u2192 Modulate features\n</code></pre></p> <p>3. Self-Attention: Global dependencies <pre><code>Tokens \u2192 Attention \u2192 Updated tokens\n</code></pre></p> <p>4. Output Projection: Map back to target space <pre><code>Tokens \u2192 Linear \u2192 Velocity field\n</code></pre></p>"},{"location":"DiT/00_dit_overview/#training-objective","title":"Training Objective","text":""},{"location":"DiT/00_dit_overview/#rectified-flow-loss","title":"Rectified Flow Loss","text":"<p>Standard form:</p> \\[ \\mathcal{L} = \\mathbb{E}_{x_0, x_1, t} \\left[ \\left\\| v_\\theta(x_t, t) - (x_1 - x_0) \\right\\|^2 \\right] \\] <p>where:</p> <ul> <li>\\(x_0 \\sim p_{\\text{data}}\\) (real data)</li> <li>\\(x_1 \\sim \\mathcal{N}(0, I)\\) (noise)</li> <li>\\(x_t = t x_1 + (1-t) x_0\\) (linear interpolation)</li> <li>\\(v_\\theta\\) is the DiT network</li> </ul> <p>With conditioning:</p> <p>$$</p> <p>\\mathcal{L} = \\mathbb{E}{x_0, x_1, t, c} \\left[ \\left| v\\theta(x_t, t, c) - (x_1 - x_0) \\right|^2 \\right] $$</p>"},{"location":"DiT/00_dit_overview/#why-this-is-simple","title":"Why This is Simple","text":"<p>Compared to DDPM:</p> <ul> <li>No noise schedules to tune</li> <li>No variance parameterization</li> <li>Direct regression target</li> </ul> <p>Compared to score matching:</p> <ul> <li>No score function computation</li> <li>No Langevin dynamics</li> <li>Deterministic sampling via ODE</li> </ul>"},{"location":"DiT/00_dit_overview/#sampling-process","title":"Sampling Process","text":""},{"location":"DiT/00_dit_overview/#ode-integration","title":"ODE Integration","text":"<p>Forward ODE (noise \u2192 data):</p> \\[ \\frac{dx}{dt} = v_\\theta(x, t, c) \\] <p>Discretization (Euler method): <pre><code>x = torch.randn(shape)  # Start from noise\ndt = 1.0 / num_steps\n\nfor k in range(num_steps):\n    t = k * dt\n    v = model(x, t, condition)\n    x = x + v * dt\n\nreturn x  # Generated sample\n</code></pre></p> <p>Properties:</p> <ul> <li>Deterministic (same noise \u2192 same output)</li> <li>Fast (20-50 steps typical)</li> <li>Straight paths (rectified flow)</li> </ul>"},{"location":"DiT/00_dit_overview/#why-dit-scales-better","title":"Why DiT Scales Better","text":""},{"location":"DiT/00_dit_overview/#1-global-context-is-native","title":"1. Global Context is Native","text":"<p>U-Net: Needs deep pyramids to propagate information - Downsample \u2192 process \u2192 upsample - Limited receptive field at each layer - Information bottleneck</p> <p>DiT: Self-attention is global by default - Every token attends to every other token - No information bottleneck - Direct long-range dependencies</p>"},{"location":"DiT/00_dit_overview/#2-flexible-input-shapes","title":"2. Flexible Input Shapes","text":"<p>U-Net: Fixed grid sizes - Must pad/crop to specific resolutions - Awkward for variable-size inputs - Hard to batch different sizes</p> <p>DiT: Variable-length sequences - Different number of tokens per sample - Batch with masking/packing - Natural for heterogeneous data</p>"},{"location":"DiT/00_dit_overview/#3-first-class-conditioning","title":"3. First-Class Conditioning","text":"<p>U-Net: Conditioning requires architectural changes - Concatenate channels - Add FiLM layers - Modify skip connections</p> <p>DiT: Conditioning is built-in - Add condition tokens (cross-attention) - Modulate with AdaLN - No architectural surgery</p>"},{"location":"DiT/00_dit_overview/#applications","title":"Applications","text":""},{"location":"DiT/00_dit_overview/#images","title":"Images","text":"<ul> <li>Stable Diffusion 3: DiT backbone</li> <li>DALL-E 3: Transformer-based</li> <li>Imagen: Cascaded DiT</li> </ul>"},{"location":"DiT/00_dit_overview/#videos","title":"Videos","text":"<ul> <li>Sora: DiT for video generation</li> <li>Goku: Efficient video DiT</li> </ul>"},{"location":"DiT/00_dit_overview/#beyond-vision","title":"Beyond Vision","text":"<ul> <li>Audio: AudioLDM, MusicGen</li> <li>Molecules: Protein structure generation</li> <li>Robotics: Trajectory generation</li> <li>Biology: Gene expression, cell states</li> </ul>"},{"location":"DiT/00_dit_overview/#key-advantages","title":"Key Advantages","text":""},{"location":"DiT/00_dit_overview/#theoretical","title":"Theoretical","text":"<ol> <li>Unified framework: Same architecture for all modalities</li> <li>Scalability: Proven to scale to billions of parameters</li> <li>Interpretability: Attention maps show what model focuses on</li> </ol>"},{"location":"DiT/00_dit_overview/#practical","title":"Practical","text":"<ol> <li>Training stability: Rectified flow is well-behaved</li> <li>Fast sampling: 20-50 steps vs 1000 for DDPM</li> <li>Easy conditioning: Add tokens or modulation</li> <li>Transfer learning: Pretrained transformers can be adapted</li> </ol>"},{"location":"DiT/00_dit_overview/#engineering","title":"Engineering","text":"<ol> <li>Infrastructure: Leverage existing Transformer tools</li> <li>Optimization: Well-understood training dynamics</li> <li>Debugging: Attention visualization helps</li> <li>Deployment: Standard Transformer serving</li> </ol>"},{"location":"DiT/00_dit_overview/#comparison-u-net-vs-dit","title":"Comparison: U-Net vs DiT","text":"Aspect U-Net DiT Architecture Convolutional Transformer Receptive field Local \u2192 Global Global from start Input format Fixed grids Flexible tokens Conditioning Concatenation/FiLM AdaLN/Cross-attention Scaling Limited Excellent Speed Fast convolutions Slower attention Memory Moderate Higher Best for Images, fixed size Any modality, variable size <p>Modern trend: DiT is becoming the default for new models.</p>"},{"location":"DiT/00_dit_overview/#dit-for-computational-biology","title":"DiT for Computational Biology","text":""},{"location":"DiT/00_dit_overview/#why-dit-is-promising-for-biology","title":"Why DiT is Promising for Biology","text":"<p>Traditional challenges:</p> <ul> <li>Gene expression: High-dimensional, unordered</li> <li>Cell states: Continuous, compositional</li> <li>Perturbations: Need flexible conditioning</li> <li>Time series: Variable-length trajectories</li> </ul> <p>DiT solutions:</p> <ul> <li>Tokens: Genes, cells, regions, timepoints</li> <li>Attention: Capture gene-gene interactions</li> <li>Conditioning: Perturbations, cell types, experimental conditions</li> <li>Flexibility: Handle variable numbers of cells/genes</li> </ul>"},{"location":"DiT/00_dit_overview/#potential-applications","title":"Potential Applications","text":"<ol> <li>Perturb-seq modeling: Predict perturbation effects</li> <li>Cell state generation: Sample from cell type distributions</li> <li>Trajectory inference: Model developmental paths</li> <li>Counterfactual generation: \"What if\" scenarios</li> </ol>"},{"location":"DiT/00_dit_overview/#open-questions","title":"Open Questions","text":"<ol> <li>Tokenization: How to represent gene expression as tokens?</li> <li>Ordering: Genes have no natural sequence \u2014 use set-based attention?</li> <li>Sparsity: Many genes have zero expression \u2014 special handling?</li> <li>Latent space: Better to work in latent space than raw expression?</li> </ol> <p>See: Advanced topics in supplementary documents for deeper exploration.</p>"},{"location":"DiT/00_dit_overview/#document-organization","title":"Document Organization","text":"<p>This DiT documentation is organized as follows:</p>"},{"location":"DiT/00_dit_overview/#core-series-practical-workflow","title":"Core Series (Practical Workflow)","text":"<ol> <li>00_dit_overview.md (this document) \u2014 High-level introduction</li> <li>01_dit_foundations.md \u2014 Architecture details, components</li> <li>02_dit_training.md \u2014 How to train DiT + rectified flow</li> <li>03_dit_sampling.md \u2014 How to generate samples</li> </ol>"},{"location":"DiT/00_dit_overview/#supplementary-documents-deep-dives","title":"Supplementary Documents (Deep Dives)","text":"<p>Located in <code>docs/diffusion/DiT/</code>: - diffusion_transformer.md \u2014 Comprehensive tutorial with biology focus - time_embeddings_explained.md \u2014 Deep dive on time conditioning - Additional topics as needed</p>"},{"location":"DiT/00_dit_overview/#learning-path","title":"Learning Path","text":""},{"location":"DiT/00_dit_overview/#for-beginners","title":"For Beginners","text":"<ol> <li>Start here (00_dit_overview.md) \u2014 Understand the big picture</li> <li>Flow matching basics (docs/flow_matching/) \u2014 Learn rectified flow</li> <li>DiT foundations (01_dit_foundations.md) \u2014 Architecture details</li> <li>Training guide (02_dit_training.md) \u2014 Practical implementation</li> </ol>"},{"location":"DiT/00_dit_overview/#for-implementers","title":"For Implementers","text":"<ol> <li>Training guide (02_dit_training.md) \u2014 Complete training pipeline</li> <li>Sampling guide (03_dit_sampling.md) \u2014 Generation strategies</li> <li>Supplementary docs \u2014 Advanced techniques</li> <li>Code examples \u2014 See <code>examples/</code> and <code>notebooks/</code></li> </ol>"},{"location":"DiT/00_dit_overview/#for-theorists","title":"For Theorists","text":"<ol> <li>Flow matching theory (docs/flow_matching/) \u2014 Mathematical foundations</li> <li>DiT paper (Peebles &amp; Xie 2023) \u2014 Original architecture</li> <li>Supplementary docs \u2014 Deep dives on specific topics</li> <li>SDE view (docs/SDE/) \u2014 Continuous-time perspective</li> </ol>"},{"location":"DiT/00_dit_overview/#key-takeaways","title":"Key Takeaways","text":""},{"location":"DiT/00_dit_overview/#conceptual","title":"Conceptual","text":"<ol> <li>DiT is an architecture, not a new diffusion method</li> <li>Transformers replace U-Nets for better scaling and flexibility</li> <li>Rectified flow + DiT is the modern generative stack</li> <li>Tokenization enables modality-agnostic generation</li> </ol>"},{"location":"DiT/00_dit_overview/#practical_1","title":"Practical","text":"<ol> <li>Training is simple: Regression on velocity field</li> <li>Sampling is fast: 20-50 ODE steps</li> <li>Conditioning is easy: Add tokens or modulation</li> <li>Scales well: Proven to billions of parameters</li> </ol>"},{"location":"DiT/00_dit_overview/#for-biology","title":"For Biology","text":"<ol> <li>Flexible representation: Genes, cells, perturbations as tokens</li> <li>Global interactions: Attention captures dependencies</li> <li>Conditional generation: Model perturbation effects</li> <li>Open research: Best tokenization strategies still being explored</li> </ol>"},{"location":"DiT/00_dit_overview/#next-steps","title":"Next Steps","text":"<p>Continue to:</p> <ul> <li>01_dit_foundations.md \u2014 Detailed architecture</li> <li>02_dit_training.md \u2014 Training pipeline</li> <li>03_dit_sampling.md \u2014 Sampling strategies</li> </ul> <p>Related documentation:</p> <ul> <li>Flow Matching \u2014 Rectified flow theory</li> <li>DDPM \u2014 Discrete diffusion models</li> <li>SDE \u2014 Continuous-time perspective</li> </ul> <p>Supplementary deep dives:</p> <ul> <li>diffusion_transformer.md \u2014 Comprehensive tutorial</li> <li>time_embeddings_explained.md \u2014 Time conditioning</li> </ul>"},{"location":"DiT/00_dit_overview/#references","title":"References","text":""},{"location":"DiT/00_dit_overview/#key-papers","title":"Key Papers","text":"<ul> <li>Peebles &amp; Xie (2023): \"Scalable Diffusion Models with Transformers\" (DiT)</li> <li>Liu et al. (2022): \"Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow\"</li> <li>Dosovitskiy et al. (2020): \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" (ViT)</li> <li>Perez et al. (2018): \"FiLM: Visual Reasoning with a General Conditioning Layer\"</li> </ul>"},{"location":"DiT/00_dit_overview/#modern-implementations","title":"Modern Implementations","text":"<ul> <li>Stable Diffusion 3: DiT-based text-to-image</li> <li>Sora: DiT for video generation</li> <li>Hugging Face Diffusers: DiT implementations</li> </ul>"},{"location":"DiT/00_dit_overview/#summary","title":"Summary","text":"<p>Diffusion Transformers (DiT) represent the modern approach to generative modeling:</p> <ul> <li>Replace U-Nets with Transformers for better scaling</li> <li>Combine with rectified flow for simple, fast training</li> <li>Enable flexible conditioning via tokens and modulation</li> <li>Scale to any modality through tokenization</li> </ul> <p>The modern generative stack: <pre><code>Rectified Flow (objective) + DiT (architecture) + AdaLN (conditioning)\n</code></pre></p> <p>This combination has become the foundation for state-of-the-art generative models across images, video, audio, and emerging applications in biology.</p>"},{"location":"DiT/01_dit_foundations/","title":"DiT Foundations: Architecture and Components","text":"<p>This document provides detailed coverage of the Diffusion Transformer (DiT) architecture, explaining each component and design choice.</p> <p>Prerequisites: Understanding of rectified flow and basic Transformer architecture.</p>"},{"location":"DiT/01_dit_foundations/#overview","title":"Overview","text":"<p>DiT replaces convolutional U-Nets with Transformers for diffusion/flow-based generative modeling. The key architectural shift:</p> <pre><code>U-Net: Spatial grids + Local convolutions + Hierarchical downsampling\nDiT:   Token sequences + Global attention + Flat architecture\n</code></pre> <p>Core components: 1. Tokenization (input \u2192 tokens) 2. Positional encoding (preserve structure) 3. Time conditioning (AdaLN) 4. Transformer blocks (attention + MLP) 5. Output projection (tokens \u2192 predictions)</p>"},{"location":"DiT/01_dit_foundations/#1-input-tokenization","title":"1. Input Tokenization","text":""},{"location":"DiT/01_dit_foundations/#11-patchification-images","title":"1.1 Patchification (Images)","text":"<p>Goal: Convert image to sequence of tokens</p> <p>Process: <pre><code># Input: image of shape (B, C, H, W)\n# Example: (batch_size, 3, 256, 256)\n\n# Step 1: Split into patches\npatch_size = 16\nnum_patches = (H // patch_size) * (W // patch_size)  # 256 patches for 256\u00d7256\n\n# Step 2: Reshape\n# (B, C, H, W) \u2192 (B, num_patches, patch_size\u00b2, C)\npatches = image.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\npatches = patches.contiguous().view(B, C, num_patches, -1)\npatches = patches.permute(0, 2, 3, 1)  # (B, num_patches, patch_dim, C)\n\n# Step 3: Flatten each patch\n# (B, num_patches, patch_size * patch_size * C)\npatches = patches.reshape(B, num_patches, -1)\n</code></pre></p> <p>Result: Image \u2192 sequence of 256 tokens (for 16\u00d716 patches on 256\u00d7256 image)</p>"},{"location":"DiT/01_dit_foundations/#12-patch-embedding","title":"1.2 Patch Embedding","text":"<p>Linear projection to model dimension:</p> <pre><code>class PatchEmbed(nn.Module):\n    def __init__(self, img_size=256, patch_size=16, in_channels=3, embed_dim=768):\n        super().__init__()\n        self.num_patches = (img_size // patch_size) ** 2\n        self.patch_size = patch_size\n\n        # Linear projection\n        patch_dim = patch_size * patch_size * in_channels\n        self.proj = nn.Linear(patch_dim, embed_dim)\n\n    def forward(self, x):\n        # x: (B, C, H, W)\n        B, C, H, W = x.shape\n\n        # Patchify\n        x = x.unfold(2, self.patch_size, self.patch_size)\n        x = x.unfold(3, self.patch_size, self.patch_size)\n        x = x.contiguous().view(B, C, -1, self.patch_size * self.patch_size)\n        x = x.permute(0, 2, 3, 1).reshape(B, -1, self.patch_size * self.patch_size * C)\n\n        # Project to embedding dimension\n        x = self.proj(x)  # (B, num_patches, embed_dim)\n        return x\n</code></pre> <p>Dimensions:</p> <ul> <li>Input: <code>(B, 3, 256, 256)</code></li> <li>After patchify: <code>(B, 256, 768)</code> where 768 = 16\u00d716\u00d73</li> <li>After projection: <code>(B, 256, embed_dim)</code></li> </ul>"},{"location":"DiT/01_dit_foundations/#13-tokenization-for-other-modalities","title":"1.3 Tokenization for Other Modalities","text":"<p>Gene expression: <pre><code># Option 1: Direct embedding (no explicit tokens)\nz = nn.Linear(num_genes, embed_dim)(gene_expression)\n\n# Option 2: Gene-level tokens\ngene_tokens = [embed(gene_i) for gene_i in genes]\n\n# Option 3: Module-level tokens\nmodule_tokens = [embed(module_j) for module_j in pathways]\n</code></pre></p> <p>See: open_research_tokenization.md for detailed discussion.</p>"},{"location":"DiT/01_dit_foundations/#2-positional-encoding","title":"2. Positional Encoding","text":""},{"location":"DiT/01_dit_foundations/#21-why-positional-encoding","title":"2.1 Why Positional Encoding?","text":"<p>Transformers are permutation-invariant \u2014 they treat input as a set, not a sequence.</p> <p>For images, spatial structure matters: - Top-left patch vs bottom-right patch - Neighboring patches are related - Absolute and relative positions</p> <p>Solution: Add positional information to patch embeddings.</p>"},{"location":"DiT/01_dit_foundations/#22-types-of-positional-encoding","title":"2.2 Types of Positional Encoding","text":"<p>Learned Positional Embeddings (most common for DiT):</p> <pre><code>class DiT(nn.Module):\n    def __init__(self, num_patches=256, embed_dim=768):\n        super().__init__()\n        # Learnable position embeddings\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n\n    def forward(self, x):\n        # x: (B, num_patches, embed_dim)\n        x = x + self.pos_embed  # Broadcast across batch\n        return x\n</code></pre> <p>Sinusoidal Positional Encoding (from original Transformer):</p> <pre><code>def sinusoidal_position_embedding(num_patches, embed_dim):\n    \"\"\"Generate sinusoidal position embeddings.\"\"\"\n    position = torch.arange(num_patches).unsqueeze(1)\n    div_term = torch.exp(torch.arange(0, embed_dim, 2) * \n                         -(math.log(10000.0) / embed_dim))\n\n    pos_embed = torch.zeros(num_patches, embed_dim)\n    pos_embed[:, 0::2] = torch.sin(position * div_term)\n    pos_embed[:, 1::2] = torch.cos(position * div_term)\n\n    return pos_embed\n</code></pre> <p>2D Positional Encoding (for images):</p> <pre><code>def get_2d_sincos_pos_embed(embed_dim, grid_size):\n    \"\"\"\n    Generate 2D sinusoidal position embeddings.\n\n    Args:\n        embed_dim: Embedding dimension\n        grid_size: Image size in patches (e.g., 16 for 256\u00d7256 with 16\u00d716 patches)\n    \"\"\"\n    grid_h = torch.arange(grid_size, dtype=torch.float32)\n    grid_w = torch.arange(grid_size, dtype=torch.float32)\n    grid = torch.meshgrid(grid_w, grid_h, indexing='xy')\n    grid = torch.stack(grid, dim=0)  # (2, grid_size, grid_size)\n\n    grid = grid.reshape([2, 1, grid_size, grid_size])\n    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n    return pos_embed\n</code></pre> <p>Trade-offs:</p> Type Pros Cons Learned Flexible, adapts to data Doesn't generalize to different sizes Sinusoidal Generalizes to longer sequences Fixed pattern 2D Respects image structure More complex <p>DiT standard: Learned positional embeddings.</p>"},{"location":"DiT/01_dit_foundations/#3-time-conditioning-via-adaln","title":"3. Time Conditioning via AdaLN","text":""},{"location":"DiT/01_dit_foundations/#31-the-time-conditioning-problem","title":"3.1 The Time Conditioning Problem","text":"<p>Diffusion models are time-dependent: behavior must change based on noise level.</p> <p>At t=0 (clean data): Refine details At t=1 (pure noise): Generate coarse structure</p> <p>Challenge: How to inject time information into every layer?</p>"},{"location":"DiT/01_dit_foundations/#32-adaptive-layer-normalization-adaln","title":"3.2 Adaptive Layer Normalization (AdaLN)","text":"<p>Standard LayerNorm:</p> \\[ \\text{LN}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sigma} + \\beta \\] <p>where \\(\\gamma\\), \\(\\beta\\) are learned parameters (same for all timesteps).</p> <p>Adaptive LayerNorm:</p> <p>$$</p> <p>\\text{AdaLN}(x, t) = \\gamma(t) \\cdot \\frac{x - \\mu}{\\sigma} + \\beta(t) $$</p> <p>where \\(\\gamma(t)\\), \\(\\beta(t)\\) are functions of time.</p> <p>Key insight: Time controls the normalization parameters at every layer.</p>"},{"location":"DiT/01_dit_foundations/#33-implementation","title":"3.3 Implementation","text":"<pre><code>class AdaLN(nn.Module):\n    def __init__(self, embed_dim, time_embed_dim):\n        super().__init__()\n        self.norm = nn.LayerNorm(embed_dim, elementwise_affine=False)\n\n        # MLP to produce scale and shift from time embedding\n        self.adaLN_modulation = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(time_embed_dim, 2 * embed_dim)\n        )\n\n    def forward(self, x, t_emb):\n        # x: (B, N, embed_dim) - token features\n        # t_emb: (B, time_embed_dim) - time embedding\n\n        # Normalize\n        x_norm = self.norm(x)\n\n        # Get scale and shift from time\n        scale, shift = self.adaLN_modulation(t_emb).chunk(2, dim=-1)\n        scale = scale.unsqueeze(1)  # (B, 1, embed_dim)\n        shift = shift.unsqueeze(1)  # (B, 1, embed_dim)\n\n        # Modulate\n        x_modulated = scale * x_norm + shift\n\n        return x_modulated\n</code></pre> <p>Flow: <pre><code>Time t \u2192 TimeEmbed(t) \u2192 MLP \u2192 (\u03b3(t), \u03b2(t)) \u2192 Modulate features\n</code></pre></p>"},{"location":"DiT/01_dit_foundations/#34-time-embedding","title":"3.4 Time Embedding","text":"<p>Sinusoidal time embedding (similar to positional encoding):</p> <pre><code>class TimestepEmbedding(nn.Module):\n    def __init__(self, time_embed_dim=256):\n        super().__init__()\n        self.time_embed_dim = time_embed_dim\n\n        # MLP to process sinusoidal features\n        self.mlp = nn.Sequential(\n            nn.Linear(time_embed_dim, time_embed_dim * 4),\n            nn.SiLU(),\n            nn.Linear(time_embed_dim * 4, time_embed_dim)\n        )\n\n    def forward(self, t):\n        # t: (B,) - timesteps\n\n        # Sinusoidal encoding\n        half_dim = self.time_embed_dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n        emb = t[:, None] * emb[None, :]\n        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n\n        # Process through MLP\n        emb = self.mlp(emb)\n\n        return emb\n</code></pre> <p>Why sinusoidal?</p> <ul> <li>Smooth, continuous representation</li> <li>Similar timesteps \u2192 similar embeddings</li> <li>Well-studied in Transformers</li> </ul> <p>See: time_embeddings_explained.md for detailed explanation.</p>"},{"location":"DiT/01_dit_foundations/#4-transformer-blocks","title":"4. Transformer Blocks","text":""},{"location":"DiT/01_dit_foundations/#41-dit-block-architecture","title":"4.1 DiT Block Architecture","text":"<p>Standard Transformer block: <pre><code>x \u2192 LayerNorm \u2192 Self-Attention \u2192 Add \u2192 LayerNorm \u2192 MLP \u2192 Add \u2192 output\n</code></pre></p> <p>DiT block with AdaLN: <pre><code>x \u2192 AdaLN(\u00b7, t) \u2192 Self-Attention \u2192 Add \u2192 AdaLN(\u00b7, t) \u2192 MLP \u2192 Add \u2192 output\n</code></pre></p> <p>Key difference: Time-dependent normalization at every step.</p>"},{"location":"DiT/01_dit_foundations/#42-implementation","title":"4.2 Implementation","text":"<pre><code>class DiTBlock(nn.Module):\n    def __init__(self, embed_dim=768, num_heads=12, mlp_ratio=4.0):\n        super().__init__()\n\n        # Attention\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n\n        # MLP\n        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, mlp_hidden_dim),\n            nn.GELU(),\n            nn.Linear(mlp_hidden_dim, embed_dim)\n        )\n\n        # AdaLN for attention\n        self.adaLN_attn = AdaLN(embed_dim, time_embed_dim=256)\n\n        # AdaLN for MLP\n        self.adaLN_mlp = AdaLN(embed_dim, time_embed_dim=256)\n\n    def forward(self, x, t_emb):\n        # x: (B, N, embed_dim)\n        # t_emb: (B, time_embed_dim)\n\n        # Attention block\n        x_norm = self.adaLN_attn(x, t_emb)\n        attn_out, _ = self.attn(x_norm, x_norm, x_norm)\n        x = x + attn_out\n\n        # MLP block\n        x_norm = self.adaLN_mlp(x, t_emb)\n        mlp_out = self.mlp(x_norm)\n        x = x + mlp_out\n\n        return x\n</code></pre>"},{"location":"DiT/01_dit_foundations/#43-self-attention-mechanism","title":"4.3 Self-Attention Mechanism","text":"<p>Multi-head self-attention:</p> \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\] <p>where:</p> <ul> <li>\\(Q = XW_Q\\) (queries)</li> <li>\\(K = XW_K\\) (keys)</li> <li>\\(V = XW_V\\) (values)</li> </ul> <p>Multi-head: <pre><code># Split into multiple heads\nQ = Q.view(B, N, num_heads, head_dim).transpose(1, 2)  # (B, num_heads, N, head_dim)\nK = K.view(B, N, num_heads, head_dim).transpose(1, 2)\nV = V.view(B, N, num_heads, head_dim).transpose(1, 2)\n\n# Attention scores\nscores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(head_dim)\nattn = F.softmax(scores, dim=-1)\n\n# Apply attention to values\nout = torch.matmul(attn, V)  # (B, num_heads, N, head_dim)\n\n# Concatenate heads\nout = out.transpose(1, 2).contiguous().view(B, N, embed_dim)\n</code></pre></p> <p>Complexity: \\(O(N^2 \\cdot d)\\) where \\(N\\) is sequence length, \\(d\\) is dimension.</p> <p>For images: \\(N = (H/p)^2\\) where \\(p\\) is patch size.</p>"},{"location":"DiT/01_dit_foundations/#44-mlp-feed-forward-network","title":"4.4 MLP (Feed-Forward Network)","text":"<p>Standard two-layer MLP:</p> <pre><code>class MLP(nn.Module):\n    def __init__(self, in_features, hidden_features, out_features):\n        super().__init__()\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.fc2(x)\n        return x\n</code></pre> <p>Typical ratio: <code>hidden_features = 4 \u00d7 in_features</code></p> <p>Purpose: Add non-linearity and capacity after attention.</p>"},{"location":"DiT/01_dit_foundations/#5-conditioning-mechanisms","title":"5. Conditioning Mechanisms","text":""},{"location":"DiT/01_dit_foundations/#51-time-conditioning-covered-above","title":"5.1 Time Conditioning (Covered Above)","text":"<p>Via AdaLN: \\(\\gamma(t)\\), \\(\\beta(t)\\) modulate features.</p>"},{"location":"DiT/01_dit_foundations/#52-class-conditioning","title":"5.2 Class Conditioning","text":"<p>For class-conditional generation (e.g., ImageNet):</p> <pre><code>class DiTWithClassConditioning(nn.Module):\n    def __init__(self, num_classes=1000, embed_dim=768):\n        super().__init__()\n\n        # Class embedding\n        self.class_embed = nn.Embedding(num_classes, embed_dim)\n\n        # Combine with time embedding\n        self.time_class_mlp = nn.Sequential(\n            nn.Linear(embed_dim * 2, embed_dim * 4),\n            nn.SiLU(),\n            nn.Linear(embed_dim * 4, embed_dim)\n        )\n\n    def forward(self, x, t, class_labels):\n        # Time embedding\n        t_emb = self.time_embed(t)\n\n        # Class embedding\n        c_emb = self.class_embed(class_labels)\n\n        # Combine\n        tc_emb = torch.cat([t_emb, c_emb], dim=-1)\n        tc_emb = self.time_class_mlp(tc_emb)\n\n        # Use tc_emb for AdaLN\n        for block in self.blocks:\n            x = block(x, tc_emb)\n\n        return x\n</code></pre>"},{"location":"DiT/01_dit_foundations/#53-cross-attention-conditioning","title":"5.3 Cross-Attention Conditioning","text":"<p>For text-to-image (like Stable Diffusion):</p> <pre><code>class DiTBlockWithCrossAttention(nn.Module):\n    def __init__(self, embed_dim=768, num_heads=12):\n        super().__init__()\n\n        # Self-attention\n        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads)\n\n        # Cross-attention (attend to text)\n        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads)\n\n        # MLP\n        self.mlp = MLP(embed_dim, embed_dim * 4, embed_dim)\n\n        # AdaLN\n        self.adaLN_self = AdaLN(embed_dim, time_embed_dim=256)\n        self.adaLN_cross = AdaLN(embed_dim, time_embed_dim=256)\n        self.adaLN_mlp = AdaLN(embed_dim, time_embed_dim=256)\n\n    def forward(self, x, t_emb, context):\n        # x: image tokens\n        # context: text tokens\n\n        # Self-attention\n        x_norm = self.adaLN_self(x, t_emb)\n        x = x + self.self_attn(x_norm, x_norm, x_norm)[0]\n\n        # Cross-attention\n        x_norm = self.adaLN_cross(x, t_emb)\n        x = x + self.cross_attn(x_norm, context, context)[0]\n\n        # MLP\n        x_norm = self.adaLN_mlp(x, t_emb)\n        x = x + self.mlp(x_norm)\n\n        return x\n</code></pre> <p>Use cases:</p> <ul> <li>Text-to-image: Text tokens as context</li> <li>Perturbation modeling: Perturbation embeddings as context</li> <li>Multi-modal: Any conditioning modality</li> </ul>"},{"location":"DiT/01_dit_foundations/#6-output-projection","title":"6. Output Projection","text":""},{"location":"DiT/01_dit_foundations/#61-from-tokens-to-predictions","title":"6.1 From Tokens to Predictions","text":"<p>Goal: Map token representations back to target space.</p> <p>For rectified flow (velocity prediction):</p> <pre><code>class DiTOutput(nn.Module):\n    def __init__(self, embed_dim=768, patch_size=16, out_channels=3):\n        super().__init__()\n\n        # Linear projection\n        self.proj = nn.Linear(embed_dim, patch_size * patch_size * out_channels)\n\n        self.patch_size = patch_size\n        self.out_channels = out_channels\n\n    def forward(self, x):\n        # x: (B, num_patches, embed_dim)\n\n        # Project to patch space\n        x = self.proj(x)  # (B, num_patches, patch_size\u00b2 * out_channels)\n\n        # Reshape to image\n        B, N, _ = x.shape\n        H = W = int(math.sqrt(N))\n\n        x = x.reshape(B, H, W, self.patch_size, self.patch_size, self.out_channels)\n        x = x.permute(0, 5, 1, 3, 2, 4)  # (B, C, H, p, W, p)\n        x = x.reshape(B, self.out_channels, H * self.patch_size, W * self.patch_size)\n\n        return x\n</code></pre> <p>Dimensions:</p> <ul> <li>Input: <code>(B, 256, 768)</code> (256 tokens, 768-dim)</li> <li>After projection: <code>(B, 256, 768)</code> where 768 = 16\u00d716\u00d73</li> <li>After reshape: <code>(B, 3, 256, 256)</code> (image)</li> </ul>"},{"location":"DiT/01_dit_foundations/#62-final-layer-normalization","title":"6.2 Final Layer Normalization","text":"<p>Optional: Apply final AdaLN before output projection:</p> <pre><code>def forward(self, x, t_emb):\n    # Process through transformer blocks\n    for block in self.blocks:\n        x = block(x, t_emb)\n\n    # Final AdaLN\n    x = self.final_adaLN(x, t_emb)\n\n    # Output projection\n    x = self.output_proj(x)\n\n    return x\n</code></pre>"},{"location":"DiT/01_dit_foundations/#7-complete-dit-architecture","title":"7. Complete DiT Architecture","text":""},{"location":"DiT/01_dit_foundations/#71-full-model","title":"7.1 Full Model","text":"<pre><code>class DiT(nn.Module):\n    def __init__(\n        self,\n        img_size=256,\n        patch_size=16,\n        in_channels=3,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4.0,\n        num_classes=None\n    ):\n        super().__init__()\n\n        # Patch embedding\n        self.patch_embed = PatchEmbed(img_size, patch_size, in_channels, embed_dim)\n        num_patches = self.patch_embed.num_patches\n\n        # Positional embedding\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n\n        # Time embedding\n        self.time_embed = TimestepEmbedding(embed_dim)\n\n        # Class embedding (optional)\n        if num_classes is not None:\n            self.class_embed = nn.Embedding(num_classes, embed_dim)\n        else:\n            self.class_embed = None\n\n        # Transformer blocks\n        self.blocks = nn.ModuleList([\n            DiTBlock(embed_dim, num_heads, mlp_ratio)\n            for _ in range(depth)\n        ])\n\n        # Final layer\n        self.final_adaLN = AdaLN(embed_dim, embed_dim)\n\n        # Output projection\n        self.output_proj = DiTOutput(embed_dim, patch_size, in_channels)\n\n        # Initialize weights\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        # Initialize positional embedding\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n\n        # Initialize patch embedding\n        w = self.patch_embed.proj.weight.data\n        nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n\n        # Initialize transformer blocks\n        for block in self.blocks:\n            nn.init.constant_(block.adaLN_attn.adaLN_modulation[-1].weight, 0)\n            nn.init.constant_(block.adaLN_attn.adaLN_modulation[-1].bias, 0)\n\n    def forward(self, x, t, y=None):\n        # x: (B, C, H, W) - noisy images\n        # t: (B,) - timesteps\n        # y: (B,) - class labels (optional)\n\n        # Patch embedding\n        x = self.patch_embed(x)  # (B, N, embed_dim)\n\n        # Add positional embedding\n        x = x + self.pos_embed\n\n        # Time embedding\n        t_emb = self.time_embed(t)\n\n        # Class embedding (if provided)\n        if y is not None and self.class_embed is not None:\n            c_emb = self.class_embed(y)\n            t_emb = t_emb + c_emb\n\n        # Transformer blocks\n        for block in self.blocks:\n            x = block(x, t_emb)\n\n        # Final layer\n        x = self.final_adaLN(x, t_emb)\n\n        # Output projection\n        x = self.output_proj(x)\n\n        return x\n</code></pre>"},{"location":"DiT/01_dit_foundations/#72-model-sizes","title":"7.2 Model Sizes","text":"<p>DiT variants (following ViT naming):</p> Model Depth Hidden Dim Heads Params DiT-S/2 12 384 6 33M DiT-B/2 12 768 12 130M DiT-L/2 24 1024 16 458M DiT-XL/2 28 1152 16 675M <p>Notation: DiT-{Size}/{Patch size} - DiT-XL/2: Extra-large model, 2\u00d72 patches - DiT-XL/8: Extra-large model, 8\u00d78 patches</p> <p>Trade-off: Smaller patches = more tokens = more compute, but better quality.</p>"},{"location":"DiT/01_dit_foundations/#8-design-choices-and-ablations","title":"8. Design Choices and Ablations","text":""},{"location":"DiT/01_dit_foundations/#81-patch-size","title":"8.1 Patch Size","text":"<p>Impact on performance:</p> Patch Size Tokens (256\u00d7256) FID Score Speed 2\u00d72 16,384 Best Slowest 4\u00d74 4,096 Good Moderate 8\u00d78 1,024 Moderate Fast 16\u00d716 256 Worse Fastest <p>Recommendation: </p> <ul> <li>High quality: 2\u00d72 or 4\u00d74</li> <li>Balanced: 8\u00d78</li> <li>Fast: 16\u00d716</li> </ul>"},{"location":"DiT/01_dit_foundations/#82-model-depth-vs-width","title":"8.2 Model Depth vs Width","text":"<p>Depth (number of layers): - More depth = better long-range dependencies - DiT-XL uses 28 layers</p> <p>Width (embedding dimension): - More width = more capacity per layer - DiT-XL uses 1152 dimensions</p> <p>Empirical finding: Depth matters more than width for DiT.</p>"},{"location":"DiT/01_dit_foundations/#83-adaln-vs-other-conditioning","title":"8.3 AdaLN vs Other Conditioning","text":"<p>Alternatives tested: 1. Concatenation: Concat time to input (worse) 2. FiLM: Similar to AdaLN (comparable) 3. Cross-attention: More expensive (slight improvement)</p> <p>Winner: AdaLN (best trade-off of performance and efficiency)</p>"},{"location":"DiT/01_dit_foundations/#84-positional-encoding","title":"8.4 Positional Encoding","text":"<p>Learned vs Sinusoidal:</p> <ul> <li>Learned: Slightly better for fixed resolution</li> <li>Sinusoidal: Better for variable resolution</li> </ul> <p>2D vs 1D:</p> <ul> <li>2D: Respects image structure (slightly better)</li> <li>1D: Simpler (minimal difference)</li> </ul> <p>DiT default: Learned 1D (simplicity wins)</p>"},{"location":"DiT/01_dit_foundations/#9-comparison-with-u-net","title":"9. Comparison with U-Net","text":""},{"location":"DiT/01_dit_foundations/#91-architectural-differences","title":"9.1 Architectural Differences","text":"Aspect U-Net DiT Structure Hierarchical (encoder-decoder) Flat (uniform depth) Operations Convolutions Self-attention Receptive field Local \u2192 Global (via depth) Global from start Skip connections Between encoder-decoder Residual within blocks Time conditioning Concatenation or FiLM AdaLN Inductive bias Spatial locality None (learned)"},{"location":"DiT/01_dit_foundations/#92-when-to-use-which","title":"9.2 When to Use Which","text":"<p>U-Net advantages:</p> <ul> <li>Faster (convolutions are efficient)</li> <li>Lower memory</li> <li>Strong spatial inductive bias</li> <li>Proven for images</li> </ul> <p>DiT advantages:</p> <ul> <li>Better scaling (to larger models)</li> <li>Flexible conditioning</li> <li>Modality-agnostic</li> <li>State-of-the-art results</li> </ul> <p>Recommendation:</p> <ul> <li>Images, fixed size, limited compute: U-Net</li> <li>Large-scale, multi-modal, flexible: DiT</li> </ul>"},{"location":"DiT/01_dit_foundations/#10-implementation-tips","title":"10. Implementation Tips","text":""},{"location":"DiT/01_dit_foundations/#101-memory-optimization","title":"10.1 Memory Optimization","text":"<p>Gradient checkpointing: <pre><code>from torch.utils.checkpoint import checkpoint\n\ndef forward(self, x, t_emb):\n    for block in self.blocks:\n        x = checkpoint(block, x, t_emb, use_reentrant=False)\n    return x\n</code></pre></p> <p>Flash Attention: <pre><code>from flash_attn import flash_attn_func\n\n# Replace standard attention with flash attention\nattn_out = flash_attn_func(q, k, v, causal=False)\n</code></pre></p> <p>Mixed precision: <pre><code>from torch.cuda.amp import autocast\n\nwith autocast():\n    output = model(x, t)\n</code></pre></p>"},{"location":"DiT/01_dit_foundations/#102-training-stability","title":"10.2 Training Stability","text":"<p>Layer scale: <pre><code>class DiTBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads, init_values=1e-4):\n        super().__init__()\n        self.gamma_1 = nn.Parameter(init_values * torch.ones(embed_dim))\n        self.gamma_2 = nn.Parameter(init_values * torch.ones(embed_dim))\n\n    def forward(self, x, t_emb):\n        x = x + self.gamma_1 * self.attn(self.adaLN_attn(x, t_emb))\n        x = x + self.gamma_2 * self.mlp(self.adaLN_mlp(x, t_emb))\n        return x\n</code></pre></p> <p>Gradient clipping: <pre><code>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n</code></pre></p>"},{"location":"DiT/01_dit_foundations/#103-efficient-inference","title":"10.3 Efficient Inference","text":"<p>Compile model (PyTorch 2.0+): <pre><code>model = torch.compile(model)\n</code></pre></p> <p>Batch inference: <pre><code># Generate multiple samples in parallel\nbatch_size = 16\nx = torch.randn(batch_size, 3, 256, 256)\nsamples = sampler.sample(x, num_steps=50)\n</code></pre></p>"},{"location":"DiT/01_dit_foundations/#key-takeaways","title":"Key Takeaways","text":""},{"location":"DiT/01_dit_foundations/#architectural","title":"Architectural","text":"<ol> <li>Tokenization: Images \u2192 patches \u2192 embeddings</li> <li>Positional encoding: Preserve spatial structure</li> <li>AdaLN: Time-dependent normalization</li> <li>Self-attention: Global dependencies</li> <li>Output projection: Tokens \u2192 predictions</li> </ol>"},{"location":"DiT/01_dit_foundations/#design-choices","title":"Design Choices","text":"<ol> <li>Patch size: Trade-off between quality and speed</li> <li>Model size: Depth matters more than width</li> <li>Conditioning: AdaLN is efficient and effective</li> <li>Positional encoding: Learned works well</li> </ol>"},{"location":"DiT/01_dit_foundations/#practical","title":"Practical","text":"<ol> <li>Memory: Use gradient checkpointing and flash attention</li> <li>Stability: Layer scale and gradient clipping</li> <li>Speed: Compile model, batch inference</li> <li>Quality: Smaller patches, larger models</li> </ol>"},{"location":"DiT/01_dit_foundations/#related-documents","title":"Related Documents","text":"<ul> <li>00_dit_overview.md \u2014 High-level introduction</li> <li>02_dit_training.md \u2014 Training pipeline</li> <li>03_dit_sampling.md \u2014 Sampling strategies</li> <li>time_embeddings_explained.md \u2014 Deep dive on time conditioning</li> </ul>"},{"location":"DiT/01_dit_foundations/#references","title":"References","text":"<ul> <li>Peebles &amp; Xie (2023): \"Scalable Diffusion Models with Transformers\"</li> <li>Dosovitskiy et al. (2020): \"An Image is Worth 16x16 Words\" (ViT)</li> <li>Vaswani et al. (2017): \"Attention is All You Need\"</li> <li>Perez et al. (2018): \"FiLM: Visual Reasoning with a General Conditioning Layer\"</li> </ul>"},{"location":"DiT/02_dit_training/","title":"DiT Training: Rectified Flow with Transformers","text":"<p>This document explains how to train Diffusion Transformers (DiT) with rectified flow, covering the complete training pipeline from data preparation to optimization strategies.</p> <p>Prerequisites: Understanding of DiT architecture and rectified flow.</p>"},{"location":"DiT/02_dit_training/#overview","title":"Overview","text":"<p>Training DiT with rectified flow is remarkably simple compared to DDPM:</p> <p>Key advantages:</p> <ul> <li>No noise schedules to tune</li> <li>No variance parameterization</li> <li>Direct regression on velocity</li> <li>Stable training dynamics</li> </ul> <p>Training loop: <pre><code>for batch in dataloader:\n    x_0 = batch  # Real data\n    x_1 = torch.randn_like(x_0)  # Noise\n    t = torch.rand(batch_size)  # Random time\n\n    x_t = t * x_1 + (1 - t) * x_0  # Interpolate\n    v_pred = model(x_t, t)  # Predict velocity\n\n    target = x_1 - x_0  # True velocity\n    loss = F.mse_loss(v_pred, target)\n\n    loss.backward()\n    optimizer.step()\n</code></pre></p>"},{"location":"DiT/02_dit_training/#1-data-preparation","title":"1. Data Preparation","text":""},{"location":"DiT/02_dit_training/#11-image-data","title":"1.1 Image Data","text":"<p>Standard preprocessing:</p> <pre><code>from torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.Resize(256),  # Resize to target resolution\n    transforms.CenterCrop(256),  # Center crop\n    transforms.RandomHorizontalFlip(),  # Data augmentation\n    transforms.ToTensor(),  # Convert to tensor\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n])\n\ndataset = ImageFolder(root='data/images', transform=transform)\ndataloader = DataLoader(dataset, batch_size=256, shuffle=True, num_workers=4)\n</code></pre> <p>Key points:</p> <ul> <li>Normalize to <code>[-1, 1]</code> (not <code>[0, 1]</code>)</li> <li>Use data augmentation (flips, crops)</li> <li>Batch size as large as GPU memory allows</li> </ul>"},{"location":"DiT/02_dit_training/#12-conditional-data","title":"1.2 Conditional Data","text":"<p>Class-conditional (e.g., ImageNet):</p> <pre><code>class ConditionalDataset(Dataset):\n    def __init__(self, root, transform):\n        self.dataset = ImageFolder(root, transform)\n        self.num_classes = len(self.dataset.classes)\n\n    def __getitem__(self, idx):\n        image, label = self.dataset[idx]\n        return image, label\n\n    def __len__(self):\n        return len(self.dataset)\n</code></pre> <p>Text-conditional (e.g., text-to-image):</p> <pre><code>class TextImageDataset(Dataset):\n    def __init__(self, image_dir, caption_file, transform, tokenizer):\n        self.images = load_images(image_dir)\n        self.captions = load_captions(caption_file)\n        self.transform = transform\n        self.tokenizer = tokenizer\n\n    def __getitem__(self, idx):\n        image = self.transform(self.images[idx])\n        caption = self.captions[idx]\n        tokens = self.tokenizer(caption, max_length=77, padding='max_length')\n        return image, tokens\n</code></pre>"},{"location":"DiT/02_dit_training/#13-gene-expression-data","title":"1.3 Gene Expression Data","text":"<p>Preprocessing:</p> <pre><code>import scanpy as sc\n\n# Load data\nadata = sc.read_h5ad('data/expression.h5ad')\n\n# Normalize\nsc.pp.normalize_total(adata, target_sum=1e4)\nsc.pp.log1p(adata)\n\n# Convert to tensor\nexpression = torch.tensor(adata.X.toarray(), dtype=torch.float32)\n\n# Create dataset\nclass GeneExpressionDataset(Dataset):\n    def __init__(self, expression, conditions=None):\n        self.expression = expression\n        self.conditions = conditions\n\n    def __getitem__(self, idx):\n        x = self.expression[idx]\n        if self.conditions is not None:\n            c = self.conditions[idx]\n            return x, c\n        return x\n\n    def __len__(self):\n        return len(self.expression)\n</code></pre>"},{"location":"DiT/02_dit_training/#2-model-architecture","title":"2. Model Architecture","text":""},{"location":"DiT/02_dit_training/#21-instantiate-dit","title":"2.1 Instantiate DiT","text":"<pre><code>from dit import DiT\n\nmodel = DiT(\n    img_size=256,          # Image resolution\n    patch_size=8,          # Patch size (8\u00d78)\n    in_channels=3,         # RGB\n    embed_dim=1152,        # Hidden dimension (DiT-XL)\n    depth=28,              # Number of transformer blocks\n    num_heads=16,          # Attention heads\n    mlp_ratio=4.0,         # MLP expansion ratio\n    num_classes=1000       # For class conditioning (ImageNet)\n)\n\n# Move to GPU\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = model.to(device)\n\n# Count parameters\nnum_params = sum(p.numel() for p in model.parameters())\nprint(f\"Model has {num_params / 1e6:.1f}M parameters\")\n</code></pre>"},{"location":"DiT/02_dit_training/#22-model-sizes","title":"2.2 Model Sizes","text":"<p>Choose based on compute budget:</p> Model Params Patch Size Training Time (ImageNet) DiT-S/8 33M 8\u00d78 ~1 day (8 GPUs) DiT-B/8 130M 8\u00d78 ~2 days (8 GPUs) DiT-L/8 458M 8\u00d78 ~4 days (8 GPUs) DiT-XL/8 675M 8\u00d78 ~7 days (8 GPUs) <p>Recommendation: Start with DiT-B for prototyping, scale to DiT-XL for best results.</p>"},{"location":"DiT/02_dit_training/#3-training-objective","title":"3. Training Objective","text":""},{"location":"DiT/02_dit_training/#31-rectified-flow-loss","title":"3.1 Rectified Flow Loss","text":"<p>Simple regression:</p> \\[ \\mathcal{L} = \\mathbb{E}_{x_0, x_1, t} \\left[ \\left\\| v_\\theta(x_t, t) - (x_1 - x_0) \\right\\|^2 \\right] \\] <p>where:</p> <ul> <li>\\(x_0 \\sim p_{\\text{data}}\\) (real data)</li> <li>\\(x_1 \\sim \\mathcal{N}(0, I)\\) (noise)</li> <li>\\(x_t = t x_1 + (1-t) x_0\\) (linear interpolation)</li> <li>\\(t \\sim \\mathcal{U}(0, 1)\\) (uniform time)</li> </ul>"},{"location":"DiT/02_dit_training/#32-implementation","title":"3.2 Implementation","text":"<pre><code>def compute_loss(model, x_0, t=None, condition=None):\n    \"\"\"\n    Compute rectified flow loss.\n\n    Args:\n        model: DiT model\n        x_0: Real data (B, C, H, W)\n        t: Timesteps (B,) - if None, sample uniformly\n        condition: Optional conditioning (class labels, text, etc.)\n\n    Returns:\n        loss: Scalar loss\n    \"\"\"\n    batch_size = x_0.shape[0]\n    device = x_0.device\n\n    # Sample timesteps\n    if t is None:\n        t = torch.rand(batch_size, device=device)\n\n    # Sample noise\n    x_1 = torch.randn_like(x_0)\n\n    # Linear interpolation\n    t_expanded = t.view(-1, 1, 1, 1)  # (B, 1, 1, 1)\n    x_t = t_expanded * x_1 + (1 - t_expanded) * x_0\n\n    # Predict velocity\n    v_pred = model(x_t, t, condition)\n\n    # Compute target\n    target = x_1 - x_0\n\n    # MSE loss\n    loss = F.mse_loss(v_pred, target)\n\n    return loss\n</code></pre>"},{"location":"DiT/02_dit_training/#33-conditional-training","title":"3.3 Conditional Training","text":"<p>Class-conditional:</p> <pre><code>def compute_loss_conditional(model, x_0, labels):\n    batch_size = x_0.shape[0]\n    device = x_0.device\n\n    # Sample timesteps\n    t = torch.rand(batch_size, device=device)\n\n    # Sample noise\n    x_1 = torch.randn_like(x_0)\n\n    # Interpolate\n    t_expanded = t.view(-1, 1, 1, 1)\n    x_t = t_expanded * x_1 + (1 - t_expanded) * x_0\n\n    # Predict with class conditioning\n    v_pred = model(x_t, t, y=labels)\n\n    # Target\n    target = x_1 - x_0\n\n    # Loss\n    loss = F.mse_loss(v_pred, target)\n\n    return loss\n</code></pre> <p>Classifier-free guidance training:</p> <pre><code>def compute_loss_cfg(model, x_0, labels, p_uncond=0.1):\n    \"\"\"\n    Train with classifier-free guidance.\n\n    Args:\n        p_uncond: Probability of dropping conditioning (typically 0.1)\n    \"\"\"\n    batch_size = x_0.shape[0]\n    device = x_0.device\n\n    # Sample timesteps\n    t = torch.rand(batch_size, device=device)\n\n    # Sample noise\n    x_1 = torch.randn_like(x_0)\n\n    # Interpolate\n    t_expanded = t.view(-1, 1, 1, 1)\n    x_t = t_expanded * x_1 + (1 - t_expanded) * x_0\n\n    # Randomly drop conditioning\n    mask = torch.rand(batch_size, device=device) &lt; p_uncond\n    labels_masked = labels.clone()\n    labels_masked[mask] = model.num_classes  # Use special \"null\" class\n\n    # Predict\n    v_pred = model(x_t, t, y=labels_masked)\n\n    # Target\n    target = x_1 - x_0\n\n    # Loss\n    loss = F.mse_loss(v_pred, target)\n\n    return loss\n</code></pre>"},{"location":"DiT/02_dit_training/#4-training-loop","title":"4. Training Loop","text":""},{"location":"DiT/02_dit_training/#41-basic-training-loop","title":"4.1 Basic Training Loop","text":"<pre><code>import torch\nimport torch.nn.functional as F\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\n# Model\nmodel = DiT(...).to(device)\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.0)\n\n# Scheduler\nscheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0.0\n\n    for batch_idx, (images, labels) in enumerate(dataloader):\n        images = images.to(device)\n        labels = labels.to(device)\n\n        # Compute loss\n        loss = compute_loss_conditional(model, images, labels)\n\n        # Backward\n        optimizer.zero_grad()\n        loss.backward()\n\n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        # Update\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n        # Logging\n        if batch_idx % 100 == 0:\n            print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n\n    # Scheduler step\n    scheduler.step()\n\n    # Epoch logging\n    avg_loss = epoch_loss / len(dataloader)\n    print(f\"Epoch {epoch} completed. Average loss: {avg_loss:.4f}\")\n\n    # Save checkpoint\n    if epoch % 10 == 0:\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': avg_loss,\n        }, f'checkpoints/dit_epoch_{epoch}.pt')\n</code></pre>"},{"location":"DiT/02_dit_training/#42-mixed-precision-training","title":"4.2 Mixed Precision Training","text":"<pre><code>from torch.cuda.amp import autocast, GradScaler\n\n# Scaler for mixed precision\nscaler = GradScaler()\n\nfor epoch in range(num_epochs):\n    for images, labels in dataloader:\n        images = images.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n\n        # Forward with autocast\n        with autocast():\n            loss = compute_loss_conditional(model, images, labels)\n\n        # Backward with scaler\n        scaler.scale(loss).backward()\n\n        # Unscale and clip gradients\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        # Update\n        scaler.step(optimizer)\n        scaler.update()\n</code></pre> <p>Benefits:</p> <ul> <li>2\u00d7 faster training</li> <li>2\u00d7 less memory</li> <li>Minimal quality loss</li> </ul>"},{"location":"DiT/02_dit_training/#43-distributed-training","title":"4.3 Distributed Training","text":"<pre><code>import torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ndef setup(rank, world_size):\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()\n\ndef train_ddp(rank, world_size):\n    setup(rank, world_size)\n\n    # Create model and move to GPU\n    model = DiT(...).to(rank)\n    model = DDP(model, device_ids=[rank])\n\n    # Create distributed sampler\n    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n    dataloader = DataLoader(dataset, batch_size=32, sampler=sampler)\n\n    # Training loop\n    for epoch in range(num_epochs):\n        sampler.set_epoch(epoch)\n\n        for images, labels in dataloader:\n            images = images.to(rank)\n            labels = labels.to(rank)\n\n            loss = compute_loss_conditional(model, images, labels)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n    cleanup()\n\n# Launch\nimport torch.multiprocessing as mp\nworld_size = torch.cuda.device_count()\nmp.spawn(train_ddp, args=(world_size,), nprocs=world_size)\n</code></pre>"},{"location":"DiT/02_dit_training/#5-optimization-strategies","title":"5. Optimization Strategies","text":""},{"location":"DiT/02_dit_training/#51-learning-rate","title":"5.1 Learning Rate","text":"<p>Recommended schedule:</p> <pre><code># Base learning rate\nbase_lr = 1e-4\n\n# Warmup\nwarmup_epochs = 5\nwarmup_lr_schedule = torch.linspace(0, base_lr, warmup_epochs * len(dataloader))\n\n# Cosine decay\nscheduler = CosineAnnealingLR(optimizer, T_max=num_epochs - warmup_epochs)\n\n# Combined\nfor epoch in range(num_epochs):\n    if epoch &lt; warmup_epochs:\n        # Warmup\n        for batch_idx in range(len(dataloader)):\n            step = epoch * len(dataloader) + batch_idx\n            lr = warmup_lr_schedule[step]\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = lr\n    else:\n        # Cosine decay\n        scheduler.step()\n</code></pre> <p>Typical values:</p> <ul> <li>Base LR: <code>1e-4</code> (DiT-B/L/XL)</li> <li>Warmup: 5-10 epochs</li> <li>Decay: Cosine to <code>1e-6</code></li> </ul>"},{"location":"DiT/02_dit_training/#52-batch-size","title":"5.2 Batch Size","text":"<p>Scaling rule: Larger batch = better, but diminishing returns</p> Batch Size GPUs Memory per GPU Training Speed 256 1 40GB Baseline 512 2 40GB 1.8\u00d7 1024 4 40GB 3.2\u00d7 2048 8 40GB 5.5\u00d7 <p>Effective batch size with gradient accumulation:</p> <pre><code>effective_batch_size = 2048\nbatch_size_per_gpu = 256\naccumulation_steps = effective_batch_size // (batch_size_per_gpu * num_gpus)\n\nfor batch_idx, (images, labels) in enumerate(dataloader):\n    loss = compute_loss_conditional(model, images, labels)\n    loss = loss / accumulation_steps\n    loss.backward()\n\n    if (batch_idx + 1) % accumulation_steps == 0:\n        optimizer.step()\n        optimizer.zero_grad()\n</code></pre>"},{"location":"DiT/02_dit_training/#53-weight-decay","title":"5.3 Weight Decay","text":"<p>AdamW with weight decay:</p> <pre><code>optimizer = AdamW(\n    model.parameters(),\n    lr=1e-4,\n    betas=(0.9, 0.999),\n    weight_decay=0.0  # No weight decay for DiT (empirically better)\n)\n</code></pre> <p>Note: DiT works well without weight decay, unlike some other models.</p>"},{"location":"DiT/02_dit_training/#54-gradient-clipping","title":"5.4 Gradient Clipping","text":"<p>Prevent gradient explosion:</p> <pre><code>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n</code></pre> <p>Typical value: <code>max_norm=1.0</code></p>"},{"location":"DiT/02_dit_training/#6-exponential-moving-average-ema","title":"6. Exponential Moving Average (EMA)","text":""},{"location":"DiT/02_dit_training/#61-why-ema","title":"6.1 Why EMA?","text":"<p>Problem: Model weights fluctuate during training</p> <p>Solution: Maintain moving average of weights</p> <p>Benefits:</p> <ul> <li>Smoother convergence</li> <li>Better sample quality</li> <li>Minimal overhead</li> </ul>"},{"location":"DiT/02_dit_training/#62-implementation","title":"6.2 Implementation","text":"<pre><code>class EMA:\n    def __init__(self, model, decay=0.9999):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n\n        # Initialize shadow weights\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n                self.shadow[name] = new_average.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data.clone()\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n        self.backup = {}\n\n# Usage\nema = EMA(model, decay=0.9999)\n\nfor epoch in range(num_epochs):\n    for images, labels in dataloader:\n        # Training step\n        loss = compute_loss_conditional(model, images, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Update EMA\n        ema.update()\n\n# For sampling, use EMA weights\nema.apply_shadow()\nsamples = sample(model, ...)\nema.restore()\n</code></pre> <p>Typical decay: <code>0.9999</code> (slower) or <code>0.999</code> (faster)</p>"},{"location":"DiT/02_dit_training/#7-monitoring-and-debugging","title":"7. Monitoring and Debugging","text":""},{"location":"DiT/02_dit_training/#71-metrics-to-track","title":"7.1 Metrics to Track","text":"<p>During training: 1. Loss: Should decrease steadily 2. Learning rate: Check schedule is correct 3. Gradient norm: Should be stable (not exploding) 4. Sample quality: Generate samples periodically</p> <pre><code>import wandb\n\nwandb.init(project=\"dit-training\")\n\nfor epoch in range(num_epochs):\n    for batch_idx, (images, labels) in enumerate(dataloader):\n        loss = compute_loss_conditional(model, images, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n\n        # Compute gradient norm\n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        optimizer.step()\n\n        # Log\n        wandb.log({\n            'loss': loss.item(),\n            'grad_norm': grad_norm.item(),\n            'lr': optimizer.param_groups[0]['lr'],\n            'epoch': epoch,\n        })\n</code></pre>"},{"location":"DiT/02_dit_training/#72-validation","title":"7.2 Validation","text":"<p>Generate samples periodically:</p> <pre><code>@torch.no_grad()\ndef validate(model, num_samples=16, num_steps=50):\n    model.eval()\n\n    # Generate samples\n    x = torch.randn(num_samples, 3, 256, 256, device=device)\n    dt = 1.0 / num_steps\n\n    for k in range(num_steps):\n        t = torch.full((num_samples,), k * dt, device=device)\n        v = model(x, t)\n        x = x + v * dt\n\n    # Denormalize\n    x = (x + 1) / 2  # [-1, 1] \u2192 [0, 1]\n    x = torch.clamp(x, 0, 1)\n\n    model.train()\n    return x\n\n# During training\nif epoch % 10 == 0:\n    samples = validate(model)\n    wandb.log({'samples': [wandb.Image(img) for img in samples]})\n</code></pre>"},{"location":"DiT/02_dit_training/#73-common-issues","title":"7.3 Common Issues","text":"<p>Loss not decreasing:</p> <ul> <li>Check data normalization (should be [-1, 1])</li> <li>Verify learning rate (try 1e-4)</li> <li>Check model initialization</li> </ul> <p>Gradient explosion:</p> <ul> <li>Use gradient clipping (max_norm=1.0)</li> <li>Reduce learning rate</li> <li>Check for NaN in data</li> </ul> <p>Poor sample quality:</p> <ul> <li>Train longer (DiT needs 400K+ steps)</li> <li>Use EMA</li> <li>Try smaller patch size (better quality, slower)</li> </ul>"},{"location":"DiT/02_dit_training/#8-training-hyperparameters","title":"8. Training Hyperparameters","text":""},{"location":"DiT/02_dit_training/#81-recommended-settings","title":"8.1 Recommended Settings","text":"<p>For ImageNet (256\u00d7256):</p> <pre><code>config = {\n    # Model\n    'model': 'DiT-XL/8',\n    'img_size': 256,\n    'patch_size': 8,\n    'embed_dim': 1152,\n    'depth': 28,\n    'num_heads': 16,\n\n    # Training\n    'batch_size': 256,  # Per GPU\n    'num_gpus': 8,\n    'effective_batch_size': 2048,\n    'num_epochs': 1400,  # ~400K steps\n\n    # Optimization\n    'lr': 1e-4,\n    'weight_decay': 0.0,\n    'warmup_epochs': 5,\n    'grad_clip': 1.0,\n\n    # EMA\n    'ema_decay': 0.9999,\n\n    # Mixed precision\n    'use_amp': True,\n\n    # Logging\n    'log_every': 100,\n    'sample_every': 1000,\n    'save_every': 10000,\n}\n</code></pre>"},{"location":"DiT/02_dit_training/#82-scaling-to-different-resolutions","title":"8.2 Scaling to Different Resolutions","text":"Resolution Patch Size Tokens Batch Size Training Time 64\u00d764 4\u00d74 256 512 1 day 128\u00d7128 8\u00d78 256 256 2 days 256\u00d7256 8\u00d78 1024 256 7 days 512\u00d7512 16\u00d716 1024 128 14 days <p>Rule of thumb: Larger resolution = more tokens = more memory = smaller batch size</p>"},{"location":"DiT/02_dit_training/#9-advanced-techniques","title":"9. Advanced Techniques","text":""},{"location":"DiT/02_dit_training/#91-progressive-growing","title":"9.1 Progressive Growing","text":"<p>Start with low resolution, gradually increase:</p> <pre><code># Stage 1: Train at 64\u00d764\nmodel_64 = DiT(img_size=64, patch_size=4, ...)\ntrain(model_64, resolution=64, epochs=100)\n\n# Stage 2: Upsample to 128\u00d7128\nmodel_128 = DiT(img_size=128, patch_size=8, ...)\nmodel_128.load_state_dict(model_64.state_dict(), strict=False)\ntrain(model_128, resolution=128, epochs=100)\n\n# Stage 3: Upsample to 256\u00d7256\nmodel_256 = DiT(img_size=256, patch_size=8, ...)\nmodel_256.load_state_dict(model_128.state_dict(), strict=False)\ntrain(model_256, resolution=256, epochs=200)\n</code></pre> <p>Benefits: Faster convergence, better quality</p>"},{"location":"DiT/02_dit_training/#92-latent-diffusion","title":"9.2 Latent Diffusion","text":"<p>Train in latent space (like Stable Diffusion):</p> <pre><code># Pretrained VAE\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\")\n\n# Encode images to latent\nwith torch.no_grad():\n    latents = vae.encode(images).latent_dist.sample()\n    latents = latents * 0.18215  # Scaling factor\n\n# Train DiT on latents\nmodel = DiT(in_channels=4, ...)  # VAE latent has 4 channels\nloss = compute_loss(model, latents)\n</code></pre> <p>Benefits:</p> <ul> <li>4-8\u00d7 faster training</li> <li>4-8\u00d7 less memory</li> <li>Similar quality</li> </ul>"},{"location":"DiT/02_dit_training/#93-multi-scale-training","title":"9.3 Multi-Scale Training","text":"<p>Train on multiple resolutions simultaneously:</p> <pre><code>resolutions = [128, 192, 256]\n\nfor images, labels in dataloader:\n    # Random resolution\n    res = random.choice(resolutions)\n    images_resized = F.interpolate(images, size=(res, res))\n\n    # Train\n    loss = compute_loss_conditional(model, images_resized, labels)\n    loss.backward()\n    optimizer.step()\n</code></pre> <p>Benefits: Better generalization, flexible inference</p>"},{"location":"DiT/02_dit_training/#10-comparison-with-ddpm-training","title":"10. Comparison with DDPM Training","text":"Aspect DDPM DiT + Rectified Flow Objective Noise prediction Velocity prediction Noise schedule Required (\u03b2_t) Not needed Variance Parameterized Not needed Loss weighting SNR-based Uniform Training stability Moderate High Hyperparameters Many Few <p>Key advantage: Rectified flow is simpler and more stable.</p>"},{"location":"DiT/02_dit_training/#key-takeaways","title":"Key Takeaways","text":""},{"location":"DiT/02_dit_training/#training-process","title":"Training Process","text":"<ol> <li>Data: Normalize to [-1, 1], augment</li> <li>Model: Choose size based on compute</li> <li>Loss: Simple MSE on velocity</li> <li>Optimization: AdamW, cosine schedule, gradient clipping</li> <li>EMA: Use for better sample quality</li> </ol>"},{"location":"DiT/02_dit_training/#hyperparameters","title":"Hyperparameters","text":"<ol> <li>Learning rate: 1e-4 with warmup</li> <li>Batch size: As large as possible (2048 effective)</li> <li>Training steps: 400K+ for ImageNet</li> <li>EMA decay: 0.9999</li> <li>Gradient clip: 1.0</li> </ol>"},{"location":"DiT/02_dit_training/#best-practices","title":"Best Practices","text":"<ol> <li>Use mixed precision (2\u00d7 speedup)</li> <li>Use EMA (better quality)</li> <li>Monitor gradients (catch instabilities)</li> <li>Generate samples (visual feedback)</li> <li>Save checkpoints (resume training)</li> </ol>"},{"location":"DiT/02_dit_training/#related-documents","title":"Related Documents","text":"<ul> <li>01_dit_foundations.md \u2014 Architecture details</li> <li>03_dit_sampling.md \u2014 Sampling strategies</li> <li>Flow Matching Training \u2014 Rectified flow theory</li> </ul>"},{"location":"DiT/02_dit_training/#references","title":"References","text":"<ul> <li>Peebles &amp; Xie (2023): \"Scalable Diffusion Models with Transformers\"</li> <li>Liu et al. (2022): \"Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow\"</li> <li>Rombach et al. (2022): \"High-Resolution Image Synthesis with Latent Diffusion Models\"</li> </ul>"},{"location":"DiT/03_dit_sampling/","title":"DiT Sampling: Generating with Rectified Flow","text":"<p>This document explains how to generate samples from trained DiT models using rectified flow, covering ODE solvers, conditional generation, and practical strategies.</p> <p>Prerequisites: Understanding of DiT architecture, training, and flow matching sampling.</p>"},{"location":"DiT/03_dit_sampling/#overview","title":"Overview","text":"<p>Sampling from DiT + rectified flow is deterministic ODE integration:</p> <p>Forward ODE (noise \u2192 data):</p> \\[ \\frac{dx}{dt} = v_\\theta(x, t) \\] <p>Key properties:</p> <ul> <li>Deterministic (same noise \u2192 same output)</li> <li>Fast (20-50 steps typical)</li> <li>Straight paths (rectified flow)</li> <li>No stochasticity (unlike DDPM)</li> </ul> <p>Basic sampling: <pre><code># Start from noise\nx = torch.randn(batch_size, 3, 256, 256)\n\n# Integrate ODE\ndt = 1.0 / num_steps\nfor k in range(num_steps):\n    t = k * dt\n    v = model(x, t)\n    x = x + v * dt\n\n# Result: generated image\n</code></pre></p>"},{"location":"DiT/03_dit_sampling/#1-ode-solvers","title":"1. ODE Solvers","text":""},{"location":"DiT/03_dit_sampling/#11-euler-method-simplest","title":"1.1 Euler Method (Simplest)","text":"<p>First-order method:</p> \\[ x_{k+1} = x_k + v_\\theta(x_k, t_k) \\cdot \\Delta t \\] <p>Implementation:</p> <pre><code>@torch.no_grad()\ndef sample_euler(model, shape, num_steps=50, device='cuda'):\n    \"\"\"\n    Sample using Euler method.\n\n    Args:\n        model: Trained DiT model\n        shape: Output shape (B, C, H, W)\n        num_steps: Number of integration steps\n        device: Device to run on\n\n    Returns:\n        samples: Generated images\n    \"\"\"\n    # Start from noise\n    x = torch.randn(shape, device=device)\n\n    # Time step\n    dt = 1.0 / num_steps\n\n    # Integrate\n    for k in range(num_steps):\n        t = torch.full((shape[0],), k * dt, device=device)\n\n        # Predict velocity\n        v = model(x, t)\n\n        # Euler step\n        x = x + v * dt\n\n    # Denormalize from [-1, 1] to [0, 1]\n    x = (x + 1) / 2\n    x = torch.clamp(x, 0, 1)\n\n    return x\n</code></pre> <p>Pros: Simple, fast Cons: Lower accuracy, needs more steps</p>"},{"location":"DiT/03_dit_sampling/#12-heuns-method-2nd-order","title":"1.2 Heun's Method (2<sup>nd</sup> Order)","text":"<p>Predictor-corrector approach:</p> \\[ \\begin{align} \\tilde{x}_{k+1} &amp;= x_k + v_\\theta(x_k, t_k) \\cdot \\Delta t \\quad \\text{(predictor)} \\\\ x_{k+1} &amp;= x_k + \\frac{1}{2}[v_\\theta(x_k, t_k) + v_\\theta(\\tilde{x}_{k+1}, t_{k+1})] \\cdot \\Delta t \\quad \\text{(corrector)} \\end{align} \\] <p>Implementation:</p> <pre><code>@torch.no_grad()\ndef sample_heun(model, shape, num_steps=25, device='cuda'):\n    \"\"\"Sample using Heun's method (2nd order).\"\"\"\n    x = torch.randn(shape, device=device)\n    dt = 1.0 / num_steps\n\n    for k in range(num_steps):\n        t_k = torch.full((shape[0],), k * dt, device=device)\n        t_k1 = torch.full((shape[0],), (k + 1) * dt, device=device)\n\n        # Predictor\n        v_k = model(x, t_k)\n        x_pred = x + v_k * dt\n\n        # Corrector\n        v_k1 = model(x_pred, t_k1)\n        x = x + 0.5 * (v_k + v_k1) * dt\n\n    x = (x + 1) / 2\n    x = torch.clamp(x, 0, 1)\n    return x\n</code></pre> <p>Pros: Better accuracy, fewer steps needed Cons: 2\u00d7 model evaluations per step</p>"},{"location":"DiT/03_dit_sampling/#13-runge-kutta-4th-order-rk4","title":"1.3 Runge-Kutta 4<sup>th</sup> Order (RK4)","text":"<p>Fourth-order method (most accurate):</p> \\[ \\begin{align} k_1 &amp;= v_\\theta(x_k, t_k) \\\\ k_2 &amp;= v_\\theta(x_k + \\frac{\\Delta t}{2} k_1, t_k + \\frac{\\Delta t}{2}) \\\\ k_3 &amp;= v_\\theta(x_k + \\frac{\\Delta t}{2} k_2, t_k + \\frac{\\Delta t}{2}) \\\\ k_4 &amp;= v_\\theta(x_k + \\Delta t \\, k_3, t_k + \\Delta t) \\\\ x_{k+1} &amp;= x_k + \\frac{\\Delta t}{6}(k_1 + 2k_2 + 2k_3 + k_4) \\end{align} \\] <p>Implementation:</p> <pre><code>@torch.no_grad()\ndef sample_rk4(model, shape, num_steps=20, device='cuda'):\n    \"\"\"Sample using RK4 (4th order).\"\"\"\n    x = torch.randn(shape, device=device)\n    dt = 1.0 / num_steps\n\n    for k in range(num_steps):\n        t = k * dt\n        batch_size = shape[0]\n\n        # k1\n        t_tensor = torch.full((batch_size,), t, device=device)\n        k1 = model(x, t_tensor)\n\n        # k2\n        t_tensor = torch.full((batch_size,), t + 0.5 * dt, device=device)\n        k2 = model(x + 0.5 * dt * k1, t_tensor)\n\n        # k3\n        k3 = model(x + 0.5 * dt * k2, t_tensor)\n\n        # k4\n        t_tensor = torch.full((batch_size,), t + dt, device=device)\n        k4 = model(x + dt * k3, t_tensor)\n\n        # Update\n        x = x + (dt / 6.0) * (k1 + 2*k2 + 2*k3 + k4)\n\n    x = (x + 1) / 2\n    x = torch.clamp(x, 0, 1)\n    return x\n</code></pre> <p>Pros: Highest accuracy, fewest steps Cons: 4\u00d7 model evaluations per step</p>"},{"location":"DiT/03_dit_sampling/#14-solver-comparison","title":"1.4 Solver Comparison","text":"Solver Order Steps for Quality Model Evals Speed Accuracy Euler 1<sup>st</sup> 50-100 50-100 Fastest Lowest Heun 2<sup>nd</sup> 25-50 50-100 Moderate Good RK4 4<sup>th</sup> 10-20 40-80 Slowest Best <p>Recommendation: </p> <ul> <li>Fast generation: Euler with 50 steps</li> <li>Balanced: Heun with 25 steps</li> <li>Best quality: RK4 with 20 steps</li> </ul>"},{"location":"DiT/03_dit_sampling/#2-conditional-generation","title":"2. Conditional Generation","text":""},{"location":"DiT/03_dit_sampling/#21-class-conditional-sampling","title":"2.1 Class-Conditional Sampling","text":"<p>For class-conditional models (e.g., ImageNet):</p> <pre><code>@torch.no_grad()\ndef sample_conditional(model, class_labels, num_steps=50, device='cuda'):\n    \"\"\"\n    Generate samples conditioned on class labels.\n\n    Args:\n        model: Trained conditional DiT\n        class_labels: Class indices (B,)\n        num_steps: Number of steps\n        device: Device\n\n    Returns:\n        samples: Generated images\n    \"\"\"\n    batch_size = len(class_labels)\n    shape = (batch_size, 3, 256, 256)\n\n    # Start from noise\n    x = torch.randn(shape, device=device)\n    class_labels = class_labels.to(device)\n\n    # Integrate\n    dt = 1.0 / num_steps\n    for k in range(num_steps):\n        t = torch.full((batch_size,), k * dt, device=device)\n\n        # Predict with class conditioning\n        v = model(x, t, y=class_labels)\n\n        # Update\n        x = x + v * dt\n\n    x = (x + 1) / 2\n    x = torch.clamp(x, 0, 1)\n    return x\n\n# Usage\nclass_labels = torch.tensor([207, 360, 387, 974])  # ImageNet classes\nsamples = sample_conditional(model, class_labels, num_steps=50)\n</code></pre>"},{"location":"DiT/03_dit_sampling/#22-classifier-free-guidance","title":"2.2 Classifier-Free Guidance","text":"<p>Interpolate between conditional and unconditional:</p> \\[ \\tilde{v}_\\theta(x, t, c) = (1 + w) v_\\theta(x, t, c) - w v_\\theta(x, t, \\emptyset) \\] <p>where \\(w\\) is the guidance weight.</p> <p>Implementation:</p> <pre><code>@torch.no_grad()\ndef sample_cfg(model, class_labels, guidance_weight=2.0, num_steps=50, device='cuda'):\n    \"\"\"\n    Sample with classifier-free guidance.\n\n    Args:\n        model: Trained model with CFG\n        class_labels: Class indices (B,)\n        guidance_weight: Guidance strength (typically 1.5-4.0)\n        num_steps: Number of steps\n        device: Device\n\n    Returns:\n        samples: Generated images\n    \"\"\"\n    batch_size = len(class_labels)\n    shape = (batch_size, 3, 256, 256)\n\n    x = torch.randn(shape, device=device)\n    class_labels = class_labels.to(device)\n\n    # Null class for unconditional\n    null_class = torch.full((batch_size,), model.num_classes, device=device)\n\n    dt = 1.0 / num_steps\n    for k in range(num_steps):\n        t = torch.full((batch_size,), k * dt, device=device)\n\n        # Conditional prediction\n        v_cond = model(x, t, y=class_labels)\n\n        # Unconditional prediction\n        v_uncond = model(x, t, y=null_class)\n\n        # Guided prediction\n        v = (1 + guidance_weight) * v_cond - guidance_weight * v_uncond\n\n        # Update\n        x = x + v * dt\n\n    x = (x + 1) / 2\n    x = torch.clamp(x, 0, 1)\n    return x\n\n# Usage\nsamples = sample_cfg(model, class_labels, guidance_weight=2.0, num_steps=50)\n</code></pre> <p>Guidance weight effects:</p> <ul> <li>\\(w = 0\\): Pure conditional (no guidance)</li> <li>\\(w = 1\\): Moderate guidance</li> <li>\\(w = 2-4\\): Strong guidance (better quality, less diversity)</li> <li>\\(w &gt; 5\\): Too strong (artifacts)</li> </ul>"},{"location":"DiT/03_dit_sampling/#23-text-conditional-sampling","title":"2.3 Text-Conditional Sampling","text":"<p>For text-to-image models:</p> <pre><code>@torch.no_grad()\ndef sample_text_conditional(model, text_prompts, tokenizer, guidance_weight=7.5, num_steps=50):\n    \"\"\"\n    Generate images from text prompts.\n\n    Args:\n        model: Text-conditional DiT\n        text_prompts: List of text strings\n        tokenizer: Text tokenizer\n        guidance_weight: CFG weight\n        num_steps: Number of steps\n\n    Returns:\n        samples: Generated images\n    \"\"\"\n    batch_size = len(text_prompts)\n    shape = (batch_size, 3, 256, 256)\n    device = next(model.parameters()).device\n\n    # Tokenize text\n    text_tokens = tokenizer(text_prompts, max_length=77, padding='max_length')\n    text_tokens = text_tokens.to(device)\n\n    # Empty prompt for unconditional\n    empty_tokens = tokenizer([\"\"] * batch_size, max_length=77, padding='max_length')\n    empty_tokens = empty_tokens.to(device)\n\n    x = torch.randn(shape, device=device)\n\n    dt = 1.0 / num_steps\n    for k in range(num_steps):\n        t = torch.full((batch_size,), k * dt, device=device)\n\n        # Conditional\n        v_cond = model(x, t, context=text_tokens)\n\n        # Unconditional\n        v_uncond = model(x, t, context=empty_tokens)\n\n        # Guided\n        v = (1 + guidance_weight) * v_cond - guidance_weight * v_uncond\n\n        x = x + v * dt\n\n    x = (x + 1) / 2\n    x = torch.clamp(x, 0, 1)\n    return x\n</code></pre>"},{"location":"DiT/03_dit_sampling/#3-sampling-strategies","title":"3. Sampling Strategies","text":""},{"location":"DiT/03_dit_sampling/#31-number-of-steps","title":"3.1 Number of Steps","text":"<p>Quality vs speed trade-off:</p> Steps Quality Time (relative) Use Case 10-15 Low 1\u00d7 Quick preview 20-25 Good 2\u00d7 Fast generation 50 High 5\u00d7 Standard 100 Very high 10\u00d7 Best quality <p>Recommendation: 50 steps for most applications.</p>"},{"location":"DiT/03_dit_sampling/#32-non-uniform-time-discretization","title":"3.2 Non-Uniform Time Discretization","text":"<p>Uniform spacing (standard): <pre><code>times = torch.linspace(0, 1, num_steps)\n</code></pre></p> <p>Quadratic spacing (more steps at high noise): <pre><code>times = torch.linspace(0, 1, num_steps) ** 2\n</code></pre></p> <p>Cosine spacing: <pre><code>s = 0.008\ntimes = torch.cos((torch.linspace(0, 1, num_steps) + s) / (1 + s) * math.pi / 2) ** 2\n</code></pre></p> <p>Implementation:</p> <pre><code>@torch.no_grad()\ndef sample_nonuniform(model, shape, num_steps=50, spacing='cosine', device='cuda'):\n    \"\"\"Sample with non-uniform time steps.\"\"\"\n    x = torch.randn(shape, device=device)\n\n    # Generate time steps\n    if spacing == 'uniform':\n        times = torch.linspace(0, 1, num_steps + 1)\n    elif spacing == 'quadratic':\n        times = torch.linspace(0, 1, num_steps + 1) ** 2\n    elif spacing == 'cosine':\n        s = 0.008\n        times = torch.cos((torch.linspace(0, 1, num_steps + 1) + s) / (1 + s) * math.pi / 2) ** 2\n\n    # Integrate\n    for k in range(num_steps):\n        t = times[k]\n        dt = times[k + 1] - times[k]\n\n        t_tensor = torch.full((shape[0],), t, device=device)\n        v = model(x, t_tensor)\n        x = x + v * dt\n\n    x = (x + 1) / 2\n    x = torch.clamp(x, 0, 1)\n    return x\n</code></pre> <p>Empirical finding: Cosine spacing slightly better for DiT.</p>"},{"location":"DiT/03_dit_sampling/#33-adaptive-step-sizes","title":"3.3 Adaptive Step Sizes","text":"<p>Idea: Use larger steps when error is small, smaller when large.</p> <pre><code>@torch.no_grad()\ndef sample_adaptive(model, shape, target_error=1e-3, max_steps=100, device='cuda'):\n    \"\"\"\n    Sample with adaptive step sizes.\n\n    Uses local error estimation to adjust step size.\n    \"\"\"\n    x = torch.randn(shape, device=device)\n    t = 0.0\n    step_count = 0\n    dt = 0.1  # Initial step size\n\n    while t &lt; 1.0 and step_count &lt; max_steps:\n        # Full step\n        t_tensor = torch.full((shape[0],), t, device=device)\n        v = model(x, t_tensor)\n        x_full = x + v * dt\n\n        # Two half steps\n        v1 = model(x, t_tensor)\n        x_half = x + v1 * (dt / 2)\n\n        t_mid_tensor = torch.full((shape[0],), t + dt/2, device=device)\n        v2 = model(x_half, t_mid_tensor)\n        x_double = x_half + v2 * (dt / 2)\n\n        # Estimate error\n        error = torch.abs(x_full - x_double).mean()\n\n        # Adjust step size\n        if error &lt; target_error:\n            # Accept step\n            x = x_double\n            t += dt\n            step_count += 1\n            # Increase step size\n            dt = min(dt * 1.5, 1.0 - t)\n        else:\n            # Reject step, decrease step size\n            dt = dt * 0.5\n\n    x = (x + 1) / 2\n    x = torch.clamp(x, 0, 1)\n    return x\n</code></pre> <p>Note: Adaptive methods can be slower due to error estimation overhead.</p>"},{"location":"DiT/03_dit_sampling/#4-practical-considerations","title":"4. Practical Considerations","text":""},{"location":"DiT/03_dit_sampling/#41-batch-generation","title":"4.1 Batch Generation","text":"<p>Generate multiple samples efficiently:</p> <pre><code>@torch.no_grad()\ndef generate_batch(model, num_samples, batch_size=16, num_steps=50, device='cuda'):\n    \"\"\"\n    Generate many samples in batches.\n\n    Args:\n        model: Trained model\n        num_samples: Total number of samples to generate\n        batch_size: Batch size for generation\n        num_steps: ODE steps\n        device: Device\n\n    Returns:\n        all_samples: All generated samples\n    \"\"\"\n    all_samples = []\n\n    for i in range(0, num_samples, batch_size):\n        current_batch_size = min(batch_size, num_samples - i)\n        shape = (current_batch_size, 3, 256, 256)\n\n        samples = sample_euler(model, shape, num_steps, device)\n        all_samples.append(samples.cpu())\n\n    return torch.cat(all_samples, dim=0)\n\n# Usage\nsamples = generate_batch(model, num_samples=1000, batch_size=16)\n</code></pre>"},{"location":"DiT/03_dit_sampling/#42-memory-optimization","title":"4.2 Memory Optimization","text":"<p>For large models or high resolution:</p> <pre><code>@torch.no_grad()\ndef sample_memory_efficient(model, shape, num_steps=50, device='cuda'):\n    \"\"\"Sample with reduced memory usage.\"\"\"\n    # Use torch.cuda.amp for mixed precision\n    from torch.cuda.amp import autocast\n\n    x = torch.randn(shape, device=device)\n    dt = 1.0 / num_steps\n\n    for k in range(num_steps):\n        t = torch.full((shape[0],), k * dt, device=device)\n\n        # Use autocast for forward pass\n        with autocast():\n            v = model(x, t)\n\n        x = x + v * dt\n\n        # Clear cache periodically\n        if k % 10 == 0:\n            torch.cuda.empty_cache()\n\n    x = (x + 1) / 2\n    x = torch.clamp(x, 0, 1)\n    return x\n</code></pre>"},{"location":"DiT/03_dit_sampling/#43-deterministic-sampling","title":"4.3 Deterministic Sampling","text":"<p>For reproducibility:</p> <pre><code>def set_seed(seed=42):\n    \"\"\"Set random seed for reproducibility.\"\"\"\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\n# Usage\nset_seed(42)\nsamples = sample_euler(model, shape, num_steps=50)\n# Same seed \u2192 same samples\n</code></pre>"},{"location":"DiT/03_dit_sampling/#44-progressive-generation","title":"4.4 Progressive Generation","text":"<p>Generate at low resolution, then upsample:</p> <pre><code>@torch.no_grad()\ndef sample_progressive(model, target_size=256, num_steps=50, device='cuda'):\n    \"\"\"\n    Generate progressively from low to high resolution.\n\n    Faster than direct high-res generation.\n    \"\"\"\n    # Stage 1: Generate at 64\u00d764\n    shape_64 = (1, 3, 64, 64)\n    x_64 = sample_euler(model, shape_64, num_steps=num_steps//2, device=device)\n\n    # Stage 2: Upsample to 128\u00d7128 and refine\n    x_128 = F.interpolate(x_64, size=(128, 128), mode='bilinear')\n    # Optional: Run a few more steps at 128\u00d7128\n\n    # Stage 3: Upsample to 256\u00d7256 and refine\n    x_256 = F.interpolate(x_128, size=(target_size, target_size), mode='bilinear')\n    # Optional: Run final steps at full resolution\n\n    return x_256\n</code></pre>"},{"location":"DiT/03_dit_sampling/#5-advanced-techniques","title":"5. Advanced Techniques","text":""},{"location":"DiT/03_dit_sampling/#51-interpolation-in-latent-space","title":"5.1 Interpolation in Latent Space","text":"<p>Smooth transitions between samples:</p> <pre><code>@torch.no_grad()\ndef interpolate_samples(model, num_frames=10, num_steps=50, device='cuda'):\n    \"\"\"\n    Generate interpolation between two random samples.\n\n    Args:\n        model: Trained model\n        num_frames: Number of interpolation frames\n        num_steps: ODE steps\n        device: Device\n\n    Returns:\n        frames: Interpolated samples\n    \"\"\"\n    shape = (1, 3, 256, 256)\n\n    # Two random starting points\n    z1 = torch.randn(shape, device=device)\n    z2 = torch.randn(shape, device=device)\n\n    frames = []\n    alphas = torch.linspace(0, 1, num_frames)\n\n    for alpha in alphas:\n        # Spherical interpolation (slerp)\n        z = slerp(z1, z2, alpha)\n\n        # Generate from interpolated noise\n        x = z.clone()\n        dt = 1.0 / num_steps\n\n        for k in range(num_steps):\n            t = torch.full((1,), k * dt, device=device)\n            v = model(x, t)\n            x = x + v * dt\n\n        x = (x + 1) / 2\n        x = torch.clamp(x, 0, 1)\n        frames.append(x)\n\n    return torch.cat(frames, dim=0)\n\ndef slerp(z1, z2, alpha):\n    \"\"\"Spherical linear interpolation.\"\"\"\n    z1_norm = z1 / z1.norm(dim=1, keepdim=True)\n    z2_norm = z2 / z2.norm(dim=1, keepdim=True)\n\n    omega = torch.acos((z1_norm * z2_norm).sum(dim=1, keepdim=True))\n    so = torch.sin(omega)\n\n    return (torch.sin((1.0 - alpha) * omega) / so) * z1 + (torch.sin(alpha * omega) / so) * z2\n</code></pre>"},{"location":"DiT/03_dit_sampling/#52-inpainting","title":"5.2 Inpainting","text":"<p>Fill in masked regions:</p> <pre><code>@torch.no_grad()\ndef sample_inpainting(model, image, mask, num_steps=50, device='cuda'):\n    \"\"\"\n    Inpaint masked regions of an image.\n\n    Args:\n        model: Trained model\n        image: Input image with known regions (B, C, H, W)\n        mask: Binary mask (1 = known, 0 = unknown) (B, 1, H, W)\n        num_steps: ODE steps\n        device: Device\n\n    Returns:\n        inpainted: Image with filled regions\n    \"\"\"\n    image = image.to(device)\n    mask = mask.to(device)\n\n    # Start from noise\n    x = torch.randn_like(image)\n\n    # Replace known regions\n    x = mask * image + (1 - mask) * x\n\n    dt = 1.0 / num_steps\n    for k in range(num_steps):\n        t = torch.full((image.shape[0],), k * dt, device=device)\n\n        # Predict velocity\n        v = model(x, t)\n\n        # Update\n        x = x + v * dt\n\n        # Project onto constraint (keep known regions fixed)\n        x = mask * image + (1 - mask) * x\n\n    x = (x + 1) / 2\n    x = torch.clamp(x, 0, 1)\n    return x\n</code></pre>"},{"location":"DiT/03_dit_sampling/#53-image-editing","title":"5.3 Image Editing","text":"<p>Edit images by manipulating latent codes:</p> <pre><code>@torch.no_grad()\ndef edit_image(model, image, direction, strength=1.0, num_steps=50, device='cuda'):\n    \"\"\"\n    Edit image in a semantic direction.\n\n    Args:\n        model: Trained model\n        image: Input image (B, C, H, W)\n        direction: Edit direction in latent space\n        strength: Edit strength\n        num_steps: ODE steps\n        device: Device\n\n    Returns:\n        edited: Edited image\n    \"\"\"\n    image = image.to(device)\n\n    # Encode to noise (reverse ODE)\n    x = image.clone()\n    dt = -1.0 / num_steps  # Negative for reverse\n\n    for k in range(num_steps):\n        t = torch.full((image.shape[0],), 1.0 - k * abs(dt), device=device)\n        v = model(x, t)\n        x = x + v * dt\n\n    # Apply edit in latent space\n    x_edited = x + strength * direction\n\n    # Decode back to image (forward ODE)\n    dt = 1.0 / num_steps\n    for k in range(num_steps):\n        t = torch.full((image.shape[0],), k * dt, device=device)\n        v = model(x_edited, t)\n        x_edited = x_edited + v * dt\n\n    x_edited = (x_edited + 1) / 2\n    x_edited = torch.clamp(x_edited, 0, 1)\n    return x_edited\n</code></pre>"},{"location":"DiT/03_dit_sampling/#6-quality-vs-speed-trade-offs","title":"6. Quality vs Speed Trade-offs","text":""},{"location":"DiT/03_dit_sampling/#61-speed-optimizations","title":"6.1 Speed Optimizations","text":"<p>Techniques to speed up sampling:</p> <ol> <li>Fewer steps: 20-25 instead of 50</li> <li>Better solver: RK4 instead of Euler</li> <li>Compiled model: <code>torch.compile(model)</code></li> <li>Mixed precision: Use FP16</li> <li>Batch generation: Generate multiple samples at once</li> </ol> <pre><code># Fast sampling configuration\nmodel_compiled = torch.compile(model)\n\n@torch.no_grad()\ndef sample_fast(model, shape, device='cuda'):\n    \"\"\"Fast sampling with all optimizations.\"\"\"\n    from torch.cuda.amp import autocast\n\n    with autocast():\n        samples = sample_rk4(model, shape, num_steps=20, device=device)\n\n    return samples\n</code></pre>"},{"location":"DiT/03_dit_sampling/#62-quality-optimizations","title":"6.2 Quality Optimizations","text":"<p>Techniques to improve quality:</p> <ol> <li>More steps: 100 instead of 50</li> <li>Better solver: RK4 with adaptive steps</li> <li>Classifier-free guidance: Increase guidance weight</li> <li>EMA weights: Use EMA model for sampling</li> <li>Non-uniform spacing: Cosine schedule</li> </ol> <pre><code># High-quality sampling configuration\n@torch.no_grad()\ndef sample_high_quality(model_ema, shape, class_labels, device='cuda'):\n    \"\"\"High-quality sampling with all optimizations.\"\"\"\n    samples = sample_cfg(\n        model_ema,\n        class_labels,\n        guidance_weight=3.0,\n        num_steps=100,\n        device=device\n    )\n    return samples\n</code></pre>"},{"location":"DiT/03_dit_sampling/#63-comparison-table","title":"6.3 Comparison Table","text":"Configuration Steps Solver Guidance Time Quality Fast 20 Euler None 1\u00d7 Good Balanced 50 Heun 2.0 3\u00d7 High Best 100 RK4 3.0 10\u00d7 Excellent"},{"location":"DiT/03_dit_sampling/#7-evaluation-metrics","title":"7. Evaluation Metrics","text":""},{"location":"DiT/03_dit_sampling/#71-fid-frechet-inception-distance","title":"7.1 FID (Fr\u00e9chet Inception Distance)","text":"<p>Measure distribution similarity:</p> <pre><code>from pytorch_fid import fid_score\n\n# Generate samples\nsamples = generate_batch(model, num_samples=50000)\n\n# Compute FID\nfid = fid_score.calculate_fid_given_paths(\n    [real_images_path, generated_images_path],\n    batch_size=50,\n    device='cuda',\n    dims=2048\n)\n\nprint(f\"FID: {fid:.2f}\")\n</code></pre> <p>Lower is better: FID &lt; 10 is excellent for ImageNet.</p>"},{"location":"DiT/03_dit_sampling/#72-inception-score-is","title":"7.2 Inception Score (IS)","text":"<p>Measure quality and diversity:</p> <pre><code>from pytorch_fid import inception\n\ndef compute_inception_score(samples, splits=10):\n    \"\"\"Compute Inception Score.\"\"\"\n    # Get Inception predictions\n    preds = inception.get_predictions(samples)\n\n    # Compute IS\n    is_mean, is_std = inception.calculate_inception_score(preds, splits=splits)\n\n    return is_mean, is_std\n</code></pre> <p>Higher is better: IS &gt; 100 is good for ImageNet.</p>"},{"location":"DiT/03_dit_sampling/#73-precision-and-recall","title":"7.3 Precision and Recall","text":"<p>Measure quality vs diversity trade-off:</p> <pre><code>def compute_precision_recall(real_features, fake_features, k=3):\n    \"\"\"\n    Compute precision and recall.\n\n    Precision: Quality (fake samples look real)\n    Recall: Diversity (cover real distribution)\n    \"\"\"\n    from sklearn.neighbors import NearestNeighbors\n\n    # Fit on real features\n    nn_real = NearestNeighbors(n_neighbors=k).fit(real_features)\n    nn_fake = NearestNeighbors(n_neighbors=k).fit(fake_features)\n\n    # Precision: fraction of fake samples close to real\n    distances_fake_to_real, _ = nn_real.kneighbors(fake_features)\n    precision = (distances_fake_to_real[:, 0] &lt; threshold).mean()\n\n    # Recall: fraction of real samples close to fake\n    distances_real_to_fake, _ = nn_fake.kneighbors(real_features)\n    recall = (distances_real_to_fake[:, 0] &lt; threshold).mean()\n\n    return precision, recall\n</code></pre>"},{"location":"DiT/03_dit_sampling/#key-takeaways","title":"Key Takeaways","text":""},{"location":"DiT/03_dit_sampling/#sampling-process","title":"Sampling Process","text":"<ol> <li>ODE integration: Deterministic, fast, straight paths</li> <li>Solvers: Euler (simple), Heun (balanced), RK4 (best)</li> <li>Steps: 20-50 typical, 100 for best quality</li> <li>Conditioning: Class, text, or other modalities</li> </ol>"},{"location":"DiT/03_dit_sampling/#practical-tips","title":"Practical Tips","text":"<ol> <li>Use EMA weights for sampling</li> <li>Classifier-free guidance improves quality</li> <li>RK4 with 20 steps \u2248 Euler with 50 steps</li> <li>Batch generation for efficiency</li> <li>Set seed for reproducibility</li> </ol>"},{"location":"DiT/03_dit_sampling/#quality-vs-speed","title":"Quality vs Speed","text":"<ol> <li>Fast: Euler, 20 steps, no guidance</li> <li>Balanced: Heun, 50 steps, CFG 2.0</li> <li>Best: RK4, 100 steps, CFG 3.0</li> </ol>"},{"location":"DiT/03_dit_sampling/#related-documents","title":"Related Documents","text":"<ul> <li>01_dit_foundations.md \u2014 Architecture details</li> <li>02_dit_training.md \u2014 Training pipeline</li> <li>Flow Matching Sampling \u2014 ODE theory</li> </ul>"},{"location":"DiT/03_dit_sampling/#references","title":"References","text":"<ul> <li>Peebles &amp; Xie (2023): \"Scalable Diffusion Models with Transformers\"</li> <li>Liu et al. (2022): \"Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow\"</li> <li>Ho &amp; Salimans (2022): \"Classifier-Free Diffusion Guidance\"</li> </ul>"},{"location":"DiT/open_research_tokenization/","title":"Open Research: Tokenization in Diffusion Transformers","text":"<p>Status: Active research area (as of January 2026)</p> <p>Core Question: What is the \"right\" way to tokenize complex objects (images, gene expression, molecules) for transformer-based generative models?</p>"},{"location":"DiT/open_research_tokenization/#the-problem","title":"The Problem","text":"<p>Transformers operate on sequences of tokens. For natural language and DNA/RNA, tokenization is natural \u2014 these are inherently sequential. But for other modalities, tokenization feels contrived and arbitrary.</p>"},{"location":"DiT/open_research_tokenization/#the-uncomfortable-truth","title":"The Uncomfortable Truth","text":"<p>Patch-based tokenization (16\u00d716, 8\u00d78, etc.) is a pragmatic hack that works, but lacks principled justification.</p> <pre><code>Engineering Reality: \"Does it work?\" \u2705\nTheoretical Satisfaction: \"Is it right?\" \u274c\n</code></pre> <p>This document explores why current approaches feel unsatisfying and outlines open research directions.</p>"},{"location":"DiT/open_research_tokenization/#1-images-the-patch-problem","title":"1. Images: The Patch Problem","text":""},{"location":"DiT/open_research_tokenization/#current-approach","title":"Current Approach","text":"<p>Standard practice (ViT, DiT, Stable Diffusion 3): <pre><code># Split image into fixed-size patches\npatches = image.unfold(dimension=2, size=16, step=16)  # 16\u00d716 patches\ntokens = embed(patches)\noutput = transformer(tokens)\n</code></pre></p>"},{"location":"DiT/open_research_tokenization/#why-this-feels-wrong","title":"Why This Feels Wrong","text":"<p>Q1: Why 16\u00d716?</p> <ul> <li>Not \"right\" \u2014 just empirically tuned</li> <li>Different models use different sizes (2\u00d72, 4\u00d74, 8\u00d78, 14\u00d714, 16\u00d716)</li> <li>No principled way to choose</li> </ul> <p>Q2: Should patch size depend on content?</p> <ul> <li>Medical images (smooth gradients): Large patches OK</li> <li>Text images (fine details): Small patches needed</li> <li>Satellite images: Depends on scale of features</li> <li>Current approach: One size for all!</li> </ul> <p>Q3: Do patches respect semantic boundaries?</p> <ul> <li>A 16\u00d716 patch might contain:</li> <li>Half a face, half background</li> <li>Part of an object, part of another</li> <li>Arbitrary image regions</li> <li>Our visual cortex doesn't work this way</li> </ul>"},{"location":"DiT/open_research_tokenization/#trade-offs","title":"Trade-offs","text":"Patch Size Pros Cons Small (2\u00d72, 4\u00d74) Fine details, local structure More tokens, O(n\u00b2) attention cost Large (16\u00d716, 32\u00d732) Fewer tokens, faster Loss of detail, coarse representation <p>The problem: This is a hyperparameter, not a principled design choice.</p>"},{"location":"DiT/open_research_tokenization/#2-gene-expression-even-less-obvious","title":"2. Gene Expression: Even Less Obvious","text":"<p>Gene expression vectors: \\(x \\in \\mathbb{R}^{20000}\\) (20K genes)</p> <p>Properties:</p> <ul> <li>Unordered: No natural sequence (unlike DNA)</li> <li>Dense: Most genes have non-zero expression</li> <li>Compositional: Relative values matter</li> <li>High-dimensional: 10K-30K genes typical</li> </ul>"},{"location":"DiT/open_research_tokenization/#current-approaches-2023-2026","title":"Current Approaches (2023-2026)","text":"<p>Approach 1: Rank by Expression (Geneformer) <pre><code># Sort genes by expression level\ngenes_sorted = sort_by_expression(gene_expression)\ntokens = [gene_1, gene_2, ..., gene_20000]\noutput = transformer(tokens)\n</code></pre></p> <p>Problems:</p> <ul> <li>Ranking is arbitrary \u2014 not biological</li> <li>20K tokens = huge sequences (O(n\u00b2) = 400M operations)</li> <li>What about genes with same expression?</li> <li>Loses biological structure</li> </ul> <p>Approach 2: Gene Modules/Pathways <pre><code># Group genes by function\nmodules = {\n    \"glycolysis\": [gene_1, gene_5, ...],\n    \"cell_cycle\": [gene_2, gene_15, ...],\n}\ntokens = [module_embeddings]  # ~500 pathways\n</code></pre></p> <p>Problems:</p> <ul> <li>How to define modules? (Also arbitrary!)</li> <li>Loses individual gene information</li> <li>Ignores within-module correlations</li> </ul> <p>Approach 3: No Explicit Tokenization <pre><code># Direct embedding to latent space\nz = encoder(gene_expression)  # (20000,) \u2192 (512,)\noutput = model(z)  # No tokens!\n</code></pre></p> <p>Problems:</p> <ul> <li>Less interpretable</li> <li>Loses biological structure</li> <li>Black box</li> </ul> <p>Approach 4: Graph-Structured (GRN-aware) <pre><code># Use gene regulatory network\ngrn = load_gene_regulatory_network()\noutput = graph_transformer(gene_expression, grn)\n</code></pre></p> <p>Problems:</p> <ul> <li>GRN knowledge incomplete</li> <li>Still 20K nodes to handle</li> <li>Which GRN to use?</li> </ul>"},{"location":"DiT/open_research_tokenization/#the-core-issue","title":"The Core Issue","text":"<p>There is no natural \"tokenization\" for gene expression.</p> <p>Unlike images (spatial structure) or language (sequential structure), gene expression is: - A set (unordered) - A vector (continuous) - A network (interconnected)</p> <p>Forcing it into a sequence feels wrong because it is wrong.</p>"},{"location":"DiT/open_research_tokenization/#3-why-do-patches-work-despite-being-arbitrary","title":"3. Why Do Patches Work Despite Being Arbitrary?","text":""},{"location":"DiT/open_research_tokenization/#pragmatic-reasons","title":"Pragmatic Reasons","text":"<p>1. Computational Efficiency <pre><code>256\u00d7256 image = 65,536 pixels\nWith 16\u00d716 patches = 256 tokens\nAttention: 65,536\u00b2 \u2192 256\u00b2 (65,000\u00d7 reduction!)\n</code></pre></p> <p>2. Transfer from NLP</p> <ul> <li>Transformers proven for sequences</li> <li>Patches make images \"sequence-like\"</li> <li>Can reuse architectures</li> </ul> <p>3. Good Enough in Practice</p> <ul> <li>ImageNet SOTA achieved</li> <li>Stable Diffusion works</li> <li>Empirical success</li> </ul> <p>4. Implementation Simplicity</p> <ul> <li>Easy to code</li> <li>GPU-efficient</li> <li>Standard operations</li> </ul>"},{"location":"DiT/open_research_tokenization/#but-this-doesnt-make-it-right","title":"But This Doesn't Make It \"Right\"","text":"<p>Engineering success \u2260 Principled design</p> <p>The field has optimized for what works, not what makes sense.</p>"},{"location":"DiT/open_research_tokenization/#4-alternative-approaches-research-frontiers","title":"4. Alternative Approaches (Research Frontiers)","text":""},{"location":"DiT/open_research_tokenization/#41-hierarchical-tokenization","title":"4.1 Hierarchical Tokenization","text":"<p>Idea: Learn local semantics first, then group into \"super tokens\"</p> <p>Swin Transformer (2021): <pre><code>Image \u2192 Small patches \u2192 Local attention \u2192 Merge\n              \u2193\n      \"Super tokens\" (hierarchical)\n              \u2193\n      Global attention\n</code></pre></p> <p>Status: Works well, but still uses fixed patch sizes at each level.</p>"},{"location":"DiT/open_research_tokenization/#42-learned-tokenization","title":"4.2 Learned Tokenization","text":"<p>Idea: Don't fix patch size \u2014 learn how to tokenize!</p> <p>BEiT, VQGAN, MaskGIT: <pre><code># Instead of fixed patches\ntokens = split_into_patches(image, size=16)  # Fixed\n\n# Learn tokenization\ntokens = learned_tokenizer(image)  # Adaptive!\n</code></pre></p> <p>Advantages:</p> <ul> <li>Content-aware</li> <li>Can adapt to different regions</li> <li>Potentially more semantic</li> </ul> <p>Challenges:</p> <ul> <li>How to train the tokenizer?</li> <li>Discrete vs continuous tokens?</li> <li>Computational cost</li> </ul>"},{"location":"DiT/open_research_tokenization/#43-convolutional-stem","title":"4.3 Convolutional Stem","text":"<p>Idea: Use CNNs for local features, Transformers for global</p> <pre><code>class HybridModel(nn.Module):\n    def __init__(self):\n        # CNN extracts local semantics\n        self.conv_stem = ResNet(...)\n        # Transformer on CNN features\n        self.transformer = Transformer(...)\n</code></pre> <p>Status: Used in some models, but not standard for DiT.</p>"},{"location":"DiT/open_research_tokenization/#44-no-tokenization-continuous","title":"4.4 No Tokenization (Continuous)","text":"<p>Idea: Work directly in continuous space</p> <p>For images:</p> <ul> <li>Latent diffusion (VAE \u2192 continuous latent \u2192 diffusion)</li> <li>No explicit tokens</li> </ul> <p>For gene expression:</p> <ul> <li>Direct MLP/attention on expression vector</li> <li>Treat as continuous state, not sequence</li> </ul> <p>Advantage: No arbitrary discretization</p> <p>Disadvantage: May lose interpretability</p>"},{"location":"DiT/open_research_tokenization/#5-biological-inspiration-how-should-we-think-about-this","title":"5. Biological Inspiration: How Should We Think About This?","text":""},{"location":"DiT/open_research_tokenization/#how-visual-cortex-works","title":"How Visual Cortex Works","text":"<pre><code>Retina \u2192 V1 (edges) \u2192 V2 (motion) \u2192 V3 (shape) \u2192 V4 \u2192 IT (objects)\n</code></pre> <p>Key properties: 1. Hierarchical: Simple \u2192 complex features 2. Local receptive fields that grow 3. Specialization: Different areas for different features 4. Sparse coding: Neurons fire selectively 5. Feedback: Top-down and bottom-up</p>"},{"location":"DiT/open_research_tokenization/#current-models-vs-biology","title":"Current Models vs Biology","text":"Aspect Biology Patch-based DiT Hierarchy Yes (V1\u2192V2\u2192V3\u2192V4) Flat (all patches equal) Local first Yes (small receptive fields) No (global attention) Adaptive Yes (attention, feedback) No (fixed patches) Sparse Yes (selective firing) No (dense attention) <p>Conclusion: Current approaches are not biologically inspired.</p>"},{"location":"DiT/open_research_tokenization/#should-we-care","title":"Should We Care?","text":"<p>Two perspectives:</p> <p>Pragmatic: \"Biology is slow, backprop works, patches work \u2014 who cares?\" - Valid for engineering - Gets SOTA results</p> <p>Principled: \"Understanding biology might lead to better architectures\" - Valid for research - May unlock new capabilities</p> <p>Reality: Field is mostly pragmatic (for now).</p>"},{"location":"DiT/open_research_tokenization/#6-open-research-questions","title":"6. Open Research Questions","text":""},{"location":"DiT/open_research_tokenization/#for-images","title":"For Images","text":"<p>Q1: What is the optimal tokenization strategy? - Fixed patches? Learned? Hierarchical? - Content-adaptive? - Task-specific?</p> <p>Q2: Can we learn tokenization end-to-end? - Jointly with the generative model? - Discrete vs continuous?</p> <p>Q3: How important is biological plausibility? - Should we model V1\u2192V2\u2192V3\u2192V4? - Or is attention enough?</p>"},{"location":"DiT/open_research_tokenization/#for-gene-expression","title":"For Gene Expression","text":"<p>Q4: What is the \"right\" representation? - Tokens (if so, what kind)? - Continuous embeddings? - Graph structure?</p> <p>Q5: Should tokenization respect biological structure? - Gene modules/pathways? - Regulatory networks? - Or learn from data?</p> <p>Q6: How to handle high dimensionality? - 20K genes \u2192 how many tokens? - Latent space diffusion? - Hierarchical representation?</p>"},{"location":"DiT/open_research_tokenization/#general-questions","title":"General Questions","text":"<p>Q7: Is tokenization necessary at all? - Can we do generative modeling without tokens? - Continuous-space alternatives?</p> <p>Q8: Should tokenization be modality-specific? - Images: Patches - Audio: Time patches - Gene expression: ??? - Or unified approach?</p> <p>Q9: How to evaluate tokenization quality? - Reconstruction error? - Downstream task performance? - Interpretability?</p>"},{"location":"DiT/open_research_tokenization/#7-current-state-of-the-field-january-2026","title":"7. Current State of the Field (January 2026)","text":""},{"location":"DiT/open_research_tokenization/#whats-working","title":"What's Working","text":"<p>For images:</p> <ul> <li>Fixed patches (8\u00d78, 16\u00d716) are standard</li> <li>Empirically tuned per model</li> <li>Stable Diffusion 3, Sora use patch-based approaches</li> </ul> <p>For gene expression:</p> <ul> <li>Multiple approaches being explored</li> <li>No clear winner yet</li> <li>Geneformer (ranking), scPPDM (tabular), others</li> </ul>"},{"location":"DiT/open_research_tokenization/#whats-being-researched","title":"What's Being Researched","text":"<p>Active areas: 1. Learned tokenization (VQ-VAE, MaskGIT) 2. Hierarchical models (Swin, PVT) 3. Hybrid CNN-Transformer 4. Graph-structured attention 5. Continuous-space alternatives</p>"},{"location":"DiT/open_research_tokenization/#whats-still-unknown","title":"What's Still Unknown","text":"<p>Open problems:</p> <ul> <li>Principled way to choose patch size</li> <li>Optimal tokenization for non-image modalities</li> <li>Whether biological inspiration helps</li> <li>Unified tokenization across modalities</li> </ul>"},{"location":"DiT/open_research_tokenization/#8-recommendations-for-practitioners","title":"8. Recommendations for Practitioners","text":""},{"location":"DiT/open_research_tokenization/#for-image-generation-dit","title":"For Image Generation (DiT)","text":"<p>Current best practice: <pre><code># Use empirically-tuned patch sizes\npatch_size = 8  # For 256\u00d7256 images (DiT-XL/8)\n# or\npatch_size = 4  # For higher quality (more compute)\n</code></pre></p> <p>Experiment with:</p> <ul> <li>Different patch sizes for your data</li> <li>Hierarchical approaches if quality matters</li> <li>Latent diffusion (VAE + diffusion) to avoid tokenization</li> </ul>"},{"location":"DiT/open_research_tokenization/#for-gene-expression_1","title":"For Gene Expression","text":"<p>Recommended approach (as of 2026): <pre><code># Option 1: No explicit tokenization\nz = encoder(gene_expression)  # (20000,) \u2192 (512,)\noutput = diffusion_model(z)\n\n# Option 2: Biologically-structured\ngrn = load_gene_regulatory_network()\noutput = graph_diffusion(gene_expression, grn)\n\n# Option 3: Learned modules\nmodules = learn_gene_modules(data)  # Data-driven\ntokens = embed_by_modules(gene_expression, modules)\noutput = transformer(tokens)\n</code></pre></p> <p>Then:</p> <ul> <li>Compare approaches empirically</li> <li>Publish ablation study</li> <li>Let performance guide you</li> </ul>"},{"location":"DiT/open_research_tokenization/#general-advice","title":"General Advice","text":"<p>Start simple: 1. Use standard approaches (patches for images, embeddings for other) 2. Get baseline working 3. Then experiment with alternatives</p> <p>Don't overthink:</p> <ul> <li>If patches work for your task, use them</li> <li>Principled design is nice, but results matter</li> </ul> <p>But do explore:</p> <ul> <li>This is an open research area</li> <li>Novel tokenization strategies could be publishable</li> <li>Especially for non-image modalities</li> </ul>"},{"location":"DiT/open_research_tokenization/#9-future-directions","title":"9. Future Directions","text":""},{"location":"DiT/open_research_tokenization/#near-term-2026-2027","title":"Near-term (2026-2027)","text":"<p>Likely developments: 1. More learned tokenization methods 2. Better hierarchical models 3. Modality-specific tokenization strategies 4. Improved understanding of why patches work</p>"},{"location":"DiT/open_research_tokenization/#medium-term-2027-2029","title":"Medium-term (2027-2029)","text":"<p>Possible breakthroughs: 1. Unified tokenization framework 2. Biologically-inspired alternatives that match SOTA 3. Continuous-space generative models (no tokens) 4. Neural architecture search for tokenization</p>"},{"location":"DiT/open_research_tokenization/#long-term-2029","title":"Long-term (2029+)","text":"<p>Speculative: 1. Fundamental rethinking of tokenization 2. New architectures that don't need tokens 3. True biological plausibility 4. Modality-agnostic generative models</p>"},{"location":"DiT/open_research_tokenization/#10-the-bigger-picture","title":"10. The Bigger Picture","text":""},{"location":"DiT/open_research_tokenization/#the-tension","title":"The Tension","text":"<pre><code>Engineering Pragmatism     vs     Principled Design\n\"Does it work?\"            vs     \"Is it right?\"\nEmpirical tuning           vs     Theory-driven\nFast iteration             vs     Deep understanding\n</code></pre> <p>Current state: Pragmatism dominates - Patch sizes: Empirically tuned - Architecture choices: What works on benchmarks - Limited theoretical understanding</p> <p>Future direction: More principled approaches - Understanding WHY things work - Biologically-inspired designs - Learned, adaptive strategies</p>"},{"location":"DiT/open_research_tokenization/#why-this-matters","title":"Why This Matters","text":"<p>For science:</p> <ul> <li>Understanding principles leads to better models</li> <li>Biological inspiration may unlock new capabilities</li> <li>Theory guides experimentation</li> </ul> <p>For engineering:</p> <ul> <li>Principled designs generalize better</li> <li>Less hyperparameter tuning</li> <li>More robust to distribution shift</li> </ul> <p>For biology applications:</p> <ul> <li>Gene expression needs better representations</li> <li>Biological structure should inform design</li> <li>Interpretability matters</li> </ul>"},{"location":"DiT/open_research_tokenization/#11-conclusion","title":"11. Conclusion","text":"<p>The honest assessment:</p> <p>Patch-based tokenization is arbitrary and unnatural.</p> <ul> <li>16\u00d716 is not \"right\" \u2014 it's empirically tuned</li> <li>Should vary by resolution, task, data</li> <li>Doesn't respect semantic boundaries</li> <li>Not biologically inspired</li> </ul> <p>But it works.</p> <ul> <li>Achieves SOTA on many tasks</li> <li>Computationally efficient</li> <li>Easy to implement</li> </ul> <p>For gene expression, it's even worse.</p> <ul> <li>No natural tokenization exists</li> <li>Current approaches are hacks</li> <li>Open research problem</li> </ul> <p>The field is still figuring this out.</p> <ul> <li>Active research area</li> <li>No consensus</li> <li>Your skepticism is warranted</li> </ul> <p>Recommendations: 1. Use standard approaches to get started 2. Experiment with alternatives 3. Let performance guide you 4. Contribute to the research!</p>"},{"location":"DiT/open_research_tokenization/#references","title":"References","text":""},{"location":"DiT/open_research_tokenization/#tokenization-approaches","title":"Tokenization Approaches","text":"<p>Patch-based:</p> <ul> <li>Dosovitskiy et al. (2020): \"An Image is Worth 16x16 Words\" (ViT)</li> <li>Peebles &amp; Xie (2023): \"Scalable Diffusion Models with Transformers\" (DiT)</li> </ul> <p>Hierarchical:</p> <ul> <li>Liu et al. (2021): \"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\"</li> <li>Wang et al. (2021): \"Pyramid Vision Transformer\"</li> </ul> <p>Learned Tokenization:</p> <ul> <li>Bao et al. (2021): \"BEiT: BERT Pre-Training of Image Transformers\"</li> <li>Esser et al. (2021): \"Taming Transformers for High-Resolution Image Synthesis\" (VQGAN)</li> <li>Chang et al. (2022): \"MaskGIT: Masked Generative Image Transformer\"</li> </ul> <p>Gene Expression:</p> <ul> <li>Theodoris et al. (2023): \"Transfer learning enables predictions in network biology\" (Geneformer)</li> <li>Cui et al. (2024): \"scGPT: Toward Building a Foundation Model for Single-Cell Multi-omics\"</li> </ul>"},{"location":"DiT/open_research_tokenization/#biological-inspiration","title":"Biological Inspiration","text":"<ul> <li>Hinton et al. (2017): \"Dynamic Routing Between Capsules\" (Capsule Networks)</li> <li>Rao &amp; Ballard (1999): \"Predictive coding in the visual cortex\"</li> </ul>"},{"location":"DiT/open_research_tokenization/#discussion-questions","title":"Discussion Questions","text":"<p>For researchers: 1. Can we develop a principled theory of tokenization? 2. Should tokenization be learned end-to-end with the model? 3. How important is biological plausibility? 4. Can we unify tokenization across modalities?</p> <p>For practitioners: 1. How to choose patch size for my data? 2. When should I use hierarchical models? 3. Is learned tokenization worth the complexity? 4. How to tokenize gene expression data?</p> <p>For the field: 1. Are we over-engineering tokenization? 2. Should we move beyond tokens entirely? 3. What can biology teach us? 4. How to balance pragmatism and principles?</p> <p>Status: Open research area \u2014 contribute your ideas!</p> <p>Last updated: January 13, 2026</p>"},{"location":"EBM/","title":"Energy-Based Models (EBM) Documentation","text":"<p>This folder contains documentation on Energy-Based Models, covering the mathematical foundations and the computational challenges that motivate modern training techniques.</p>"},{"location":"EBM/#reading-order","title":"Reading Order","text":"<p>The documents are designed to be read in sequence, building from foundational concepts to more advanced topics:</p>"},{"location":"EBM/#foundations","title":"Foundations","text":"<ol> <li> <p>Energy Function Normalization    Proves that the energy-based probability formulation \\(p_\\theta(x) = \\exp(-E_\\theta(x))/Z_\\theta\\) is a valid normalized probability density.</p> </li> <li> <p>MLE Gradient Derivation    Derives the gradient of the log-likelihood for EBMs, revealing the intractable expectation \\(\\mathbb{E}_{p_\\theta}[\\nabla_\\theta E_\\theta(x')]\\) that makes MLE computationally challenging. This is the \"villain origin story.\"</p> </li> <li> <p>Stein vs Fisher Score    Clarifies the distinction between the Stein score (\\(\\nabla_x \\log p\\)) and Fisher score (\\(\\nabla_\\theta \\log p\\))\u2014two different \"scores\" used in different contexts.</p> </li> <li> <p>Score Matching Objective Derivation    Proves the integration-by-parts trick that eliminates the unknown \\(p_D\\) from the score matching objective, yielding the tractable trace-of-Jacobian form.</p> </li> <li> <p>Fisher Score Matching Derivation    The parameter-space analogue: proves how integration-by-parts eliminates the intractable \\(\\nabla_\\theta \\log p(x|\\theta)\\) for simulation-based inference.</p> </li> </ol>"},{"location":"EBM/#training-methods","title":"Training Methods","text":"<ol> <li> <p>Score Matching (detailed) \u2014 Full treatment of the score matching objective.</p> </li> <li> <p>Fisher Score Matching \u2014 Parameter-space analogue for simulation-based inference.</p> </li> </ol>"},{"location":"EBM/#coming-soon","title":"Coming Soon","text":"<ul> <li>Contrastive Divergence \u2014 Approximate MCMC for tractable training</li> <li>Noise-Contrastive Estimation \u2014 Reframing EBM training as classification</li> <li>Denoising Score Matching \u2014 Practical variant avoiding the trace term</li> </ul>"},{"location":"EBM/#key-concepts","title":"Key Concepts","text":"Concept Symbol Description Energy function \\(E_\\theta(x)\\) Maps data to scalar \"energy\" (lower = more probable) Partition function \\(Z_\\theta\\) Normalizing constant \\(\\int \\exp(-E_\\theta(x)) dx\\) Score function \\(\\nabla_x \\log p(x)\\) Gradient of log-density w.r.t. data"},{"location":"EBM/#connection-to-other-topics","title":"Connection to Other Topics","text":"<ul> <li>VAE: See <code>../VAE/</code> \u2014 VAEs avoid the partition function problem by using tractable encoder/decoder distributions.</li> <li>Score Matching: See <code>../score_matching/</code> \u2014 Directly estimates the score function without computing \\(Z_\\theta\\).</li> <li>Diffusion Models: Build on score matching to learn \\(\\nabla_x \\log p_t(x)\\) across noise levels.</li> </ul>"},{"location":"EBM/EBM-energy-function-normalization/","title":"Why the Energy Function Formulation Normalizes","text":"<p>In energy-based models (EBMs), we define probability distributions through an energy function \\(E_\\theta(x)\\) rather than directly specifying probabilities. This approach is powerful because it allows us to model complex distributions without worrying about normalization during model design\u2014we simply assign lower energy to more probable configurations. But why does this work? Why does dividing by the partition function \\(Z_\\theta\\) actually produce a valid probability distribution?</p> <p>This document provides a rigorous, step-by-step proof that the energy-based formulation indeed yields a properly normalized probability density. Understanding this foundation is essential before diving into the computational challenges (like intractable partition functions) that motivate techniques such as score matching and contrastive divergence.</p>"},{"location":"EBM/EBM-energy-function-normalization/#goal","title":"Goal","text":"<p>We'll prove\u2014step by step\u2014that defining</p> \\[ Z_\\theta := \\int_{\\mathcal{X}} \\exp\\big(-E_\\theta(x)\\big) \\, dx \\] <p>and then defining</p> \\[ p_\\theta(x) := \\frac{\\exp\\big(-E_\\theta(x)\\big)}{Z_\\theta} \\] <p>indeed makes \\(p_\\theta\\) a normalized probability density on \\(\\mathcal{X} \\subseteq \\mathbb{R}^d\\). Each step includes a brief explanation of what it means.</p>"},{"location":"EBM/EBM-energy-function-normalization/#setup-and-assumptions","title":"Setup and Assumptions","text":"<p>Let:</p> <ul> <li>\\(\\mathcal{X} \\subseteq \\mathbb{R}^d\\) be the data domain (often all of \\(\\mathbb{R}^d\\)).</li> <li>\\(E_\\theta: \\mathcal{X} \\to \\mathbb{R}\\) be the energy function.</li> <li>Define the unnormalized \"density-like\" function:</li> </ul> \\[ \\tilde{p}_\\theta(x) := \\exp(-E_\\theta(x)) \\] <p>Assumption (integrability):</p> \\[ 0 &lt; Z_\\theta := \\int_{\\mathcal{X}} \\exp(-E_\\theta(x)) \\, dx &lt; \\infty \\] <p>This says the integral exists and is finite (otherwise normalization is impossible). In practice, modelers choose \\(E_\\theta\\) so this holds.</p>"},{"location":"EBM/EBM-energy-function-normalization/#claim","title":"Claim","text":"<p>With \\(p_\\theta(x) = \\tilde{p}_\\theta(x) / Z_\\theta\\), we have:</p> <ol> <li>\\(p_\\theta(x) \\ge 0\\) for all \\(x \\in \\mathcal{X}\\)</li> <li>\\(\\int_{\\mathcal{X}} p_\\theta(x) \\, dx = 1\\)</li> </ol> <p>These are exactly the two requirements for a (Lebesgue) probability density.</p>"},{"location":"EBM/EBM-energy-function-normalization/#proof-step-by-step","title":"Proof, Step-by-Step","text":""},{"location":"EBM/EBM-energy-function-normalization/#1-non-negativity","title":"1) Non-negativity","text":"\\[ \\tilde{p}_\\theta(x) = \\exp(-E_\\theta(x)) \\ge 0 \\quad \\text{for all } x \\] <p>Explanation: The exponential of any real number is strictly positive, hence nonnegative.</p> \\[ Z_\\theta = \\int_{\\mathcal{X}} \\tilde{p}_\\theta(x) \\, dx &gt; 0 \\] <p>Explanation: An integral of a nonnegative function is nonnegative; and under the assumption \\(Z_\\theta &gt; 0\\), it's strictly positive (i.e., \\(\\tilde{p}_\\theta\\) isn't zero almost everywhere).</p> \\[ p_\\theta(x) = \\frac{\\tilde{p}_\\theta(x)}{Z_\\theta} \\ge 0 \\] <p>Explanation: A nonnegative numerator divided by a positive constant stays nonnegative. So \\(p_\\theta\\) can't assign negative probability \"density\" anywhere.</p>"},{"location":"EBM/EBM-energy-function-normalization/#2-normalization-the-key-result","title":"2) Normalization (the key result)","text":"<p>Start from the definition:</p> \\[ \\int_{\\mathcal{X}} p_\\theta(x) \\, dx = \\int_{\\mathcal{X}} \\frac{\\exp(-E_\\theta(x))}{Z_\\theta} \\, dx \\] <p>Explanation: We're checking whether the total probability mass under \\(p_\\theta\\) equals 1.</p> <p>Pull the constant \\(1/Z_\\theta\\) outside the integral:</p> \\[ = \\frac{1}{Z_\\theta} \\int_{\\mathcal{X}} \\exp(-E_\\theta(x)) \\, dx \\] <p>Explanation: \\(Z_\\theta\\) depends on \\(\\theta\\), not on \\(x\\), so with respect to the \\(x\\)-integral it's a constant. Constants factor out of integrals.</p> <p>Now substitute the definition of \\(Z_\\theta\\):</p> \\[ = \\frac{1}{Z_\\theta} \\cdot Z_\\theta \\] <p>Explanation: By definition, \\(\\int_{\\mathcal{X}} \\exp(-E_\\theta(x)) \\, dx = Z_\\theta\\). So the integral is literally the partition function.</p> <p>Finally simplify:</p> \\[ = 1 \\] <p>Explanation: The whole point of \\(Z_\\theta\\) is to be exactly the constant that makes this equal to 1.</p> <p>This proves \\(p_\\theta\\) is normalized. \\(\\square\\)</p>"},{"location":"EBM/EBM-energy-function-normalization/#what-integrating-out-x-means-here","title":"What \"Integrating Out \\(x\\)\" Means Here","text":"<p>When people say \"integrate out \\(x\\)\" in this context, they mean:</p> <ul> <li>We start with an unnormalized nonnegative function \\(\\tilde{p}_\\theta(x) = \\exp(-E_\\theta(x))\\).</li> <li>We compute its total mass over \\(x\\):</li> </ul> \\[ Z_\\theta = \\int \\tilde{p}_\\theta(x) \\, dx \\] <ul> <li>Then we divide by that mass so the new function has total mass 1:</li> </ul> \\[ p_\\theta(x) = \\tilde{p}_\\theta(x) / Z_\\theta \\] <p>So \"integrating out \\(x\\)\" is just \"compute the total mass over the data space.\"</p>"},{"location":"EBM/EBM-energy-function-normalization/#two-important-practical-notes","title":"Two Important Practical Notes","text":""},{"location":"EBM/EBM-energy-function-normalization/#a-why-we-need-z_theta-infty","title":"(A) Why we need \\(Z_\\theta &lt; \\infty\\)","text":"<p>If \\(Z_\\theta = \\infty\\), then \\(\\tilde{p}_\\theta\\) has infinite mass and no finite constant can normalize it.</p> <p>A typical sufficient condition (not necessary) is that for large \\(|x|\\),</p> \\[ E_\\theta(x) \\to +\\infty \\] <p>fast enough that \\(\\exp(-E_\\theta(x))\\) decays and is integrable.</p>"},{"location":"EBM/EBM-energy-function-normalization/#b-discrete-vs-continuous","title":"(B) Discrete vs. Continuous","text":"<p>If \\(x\\) is discrete (e.g., categorical), replace integrals with sums:</p> \\[ Z_\\theta = \\sum_x \\exp(-E_\\theta(x)), \\quad p_\\theta(x) = \\frac{\\exp(-E_\\theta(x))}{Z_\\theta}, \\quad \\sum_x p_\\theta(x) = 1 \\] <p>Same proof; sums instead of integrals.</p>"},{"location":"EBM/EBM-energy-function-normalization/#whats-next","title":"What's Next","text":"<p>If you want to connect this directly to why MLE is hard for EBMs, the next proof is: show that \\(\\nabla_\\theta \\log Z_\\theta\\) becomes an expectation under \\(p_\\theta(x)\\), which usually requires MCMC. That's the \"villain origin story\" that score matching is responding to.</p>"},{"location":"EBM/EBM-mle-gradient-derivation/","title":"Why MLE for EBMs is Hard: The Villain Origin Story","text":"<p>This document derives the gradient of the log-likelihood for energy-based models, revealing why maximum likelihood estimation (MLE) is computationally challenging. The derivation shows that the gradient contains an intractable expectation under the model distribution\u2014the fundamental obstacle that motivates alternative training methods like score matching and contrastive divergence.</p>"},{"location":"EBM/EBM-mle-gradient-derivation/#goal","title":"Goal","text":"<p>For an energy-based model</p> \\[ p_\\theta(x) = \\frac{e^{-E_\\theta(x)}}{Z_\\theta}, \\qquad Z_\\theta = \\int e^{-E_\\theta(x)} \\, dx \\] <p>show that the gradient of the log-likelihood has the form</p> \\[ \\nabla_\\theta \\log p_\\theta(x) = -\\nabla_\\theta E_\\theta(x) + \\mathbb{E}_{x' \\sim p_\\theta}\\big[\\nabla_\\theta E_\\theta(x')\\big] \\] <p>That second term is the painful part: it's an expectation under the model distribution \\(p_\\theta\\), which is usually intractable and needs sampling (often MCMC).</p>"},{"location":"EBM/EBM-mle-gradient-derivation/#step-0-notation","title":"Step 0: Notation","text":"<ul> <li>\\(x\\): a data point</li> <li>\\(\\theta\\): model parameters</li> <li>\\(E_\\theta(x)\\): energy function</li> <li>\\(Z_\\theta\\): partition function (normalizer)</li> <li>\\(\\nabla_\\theta\\): gradient w.r.t. parameters \\(\\theta\\)</li> <li>\\(\\mathbb{E}_{p_\\theta}[\\cdot]\\): expectation where \\(x' \\sim p_\\theta(x')\\)</li> </ul> <p>Assumption (standard regularity): we can swap gradient and integral:</p> \\[ \\nabla_\\theta \\int f_\\theta(x) \\, dx = \\int \\nabla_\\theta f_\\theta(x) \\, dx \\] <p>(You can justify this with dominated convergence / Leibniz rule; most ML papers assume it.)</p>"},{"location":"EBM/EBM-mle-gradient-derivation/#step-1-start-with-the-log-density","title":"Step 1: Start with the log density","text":"\\[ \\log p_\\theta(x) = \\log\\left(\\frac{e^{-E_\\theta(x)}}{Z_\\theta}\\right) \\] <p>Explanation: Just take logs of the EBM definition.</p>"},{"location":"EBM/EBM-mle-gradient-derivation/#step-2-split-numerator-and-denominator","title":"Step 2: Split numerator and denominator","text":"\\[ \\log p_\\theta(x) = \\log(e^{-E_\\theta(x)}) - \\log Z_\\theta \\] <p>Explanation: \\(\\log(a/b) = \\log a - \\log b\\).</p>"},{"location":"EBM/EBM-mle-gradient-derivation/#step-3-simplify-the-first-term","title":"Step 3: Simplify the first term","text":"\\[ \\log p_\\theta(x) = -E_\\theta(x) - \\log Z_\\theta \\] <p>Explanation: \\(\\log(e^{u}) = u\\). Here \\(u = -E_\\theta(x)\\).</p>"},{"location":"EBM/EBM-mle-gradient-derivation/#step-4-differentiate-wrt-theta","title":"Step 4: Differentiate w.r.t. \\(\\theta\\)","text":"\\[ \\nabla_\\theta \\log p_\\theta(x) = -\\nabla_\\theta E_\\theta(x) - \\nabla_\\theta \\log Z_\\theta \\] <p>Explanation: Gradient is linear; derivative of \\(-E_\\theta(x)\\) is \\(-\\nabla_\\theta E_\\theta(x)\\).</p> <p>So the only remaining job is to compute \\(\\nabla_\\theta \\log Z_\\theta\\).</p>"},{"location":"EBM/EBM-mle-gradient-derivation/#step-5-differentiate-log-z_theta-using-the-chain-rule","title":"Step 5: Differentiate \\(\\log Z_\\theta\\) using the chain rule","text":"\\[ \\nabla_\\theta \\log Z_\\theta = \\frac{1}{Z_\\theta} \\nabla_\\theta Z_\\theta \\] <p>Explanation: \\(\\nabla_\\theta \\log u = (\\nabla_\\theta u)/u\\).</p>"},{"location":"EBM/EBM-mle-gradient-derivation/#step-6-expand-z_theta-and-move-gradient-inside-the-integral","title":"Step 6: Expand \\(Z_\\theta\\) and move gradient inside the integral","text":"\\[ \\nabla_\\theta Z_\\theta = \\nabla_\\theta \\int e^{-E_\\theta(x')} \\, dx' = \\int \\nabla_\\theta \\left(e^{-E_\\theta(x')}\\right) \\, dx' \\] <p>Explanation: This is the \"swap gradient and integral\" step.</p>"},{"location":"EBM/EBM-mle-gradient-derivation/#step-7-differentiate-the-exponential","title":"Step 7: Differentiate the exponential","text":"\\[ \\nabla_\\theta \\left(e^{-E_\\theta(x')}\\right) = e^{-E_\\theta(x')} \\cdot \\nabla_\\theta(-E_\\theta(x')) = -e^{-E_\\theta(x')} \\cdot \\nabla_\\theta E_\\theta(x') \\] <p>Explanation: Chain rule: derivative of \\(e^{u}\\) is \\(e^{u} \\nabla u\\). Here \\(u = -E_\\theta(x')\\).</p>"},{"location":"EBM/EBM-mle-gradient-derivation/#step-8-substitute-back-into-nabla_theta-z_theta","title":"Step 8: Substitute back into \\(\\nabla_\\theta Z_\\theta\\)","text":"\\[ \\nabla_\\theta Z_\\theta = \\int \\left(-e^{-E_\\theta(x')} \\cdot \\nabla_\\theta E_\\theta(x')\\right) \\, dx' = -\\int e^{-E_\\theta(x')} \\cdot \\nabla_\\theta E_\\theta(x') \\, dx' \\] <p>Explanation: Just plug in the expression from Step 7.</p>"},{"location":"EBM/EBM-mle-gradient-derivation/#step-9-plug-into-nabla_theta-log-z_theta","title":"Step 9: Plug into \\(\\nabla_\\theta \\log Z_\\theta\\)","text":"\\[ \\nabla_\\theta \\log Z_\\theta = \\frac{1}{Z_\\theta}\\left(-\\int e^{-E_\\theta(x')} \\cdot \\nabla_\\theta E_\\theta(x') \\, dx'\\right) = -\\int \\frac{e^{-E_\\theta(x')}}{Z_\\theta} \\cdot \\nabla_\\theta E_\\theta(x') \\, dx' \\] <p>Explanation: Divide the integral by \\(Z_\\theta\\); that's exactly how \\(p_\\theta\\) is defined.</p>"},{"location":"EBM/EBM-mle-gradient-derivation/#step-10-recognize-p_thetax-and-rewrite-as-an-expectation","title":"Step 10: Recognize \\(p_\\theta(x')\\) and rewrite as an expectation","text":"<p>Since</p> \\[ p_\\theta(x') = \\frac{e^{-E_\\theta(x')}}{Z_\\theta} \\] <p>we have</p> \\[ \\nabla_\\theta \\log Z_\\theta = -\\int p_\\theta(x') \\cdot \\nabla_\\theta E_\\theta(x') \\, dx' = -\\mathbb{E}_{x' \\sim p_\\theta}\\left[\\nabla_\\theta E_\\theta(x')\\right] \\] <p>Explanation: An expectation is just an integral weighted by the density.</p>"},{"location":"EBM/EBM-mle-gradient-derivation/#step-11-put-it-all-together-the-classic-ebm-gradient","title":"Step 11: Put it all together (the classic EBM gradient)","text":"<p>Recall Step 4:</p> \\[ \\nabla_\\theta \\log p_\\theta(x) = -\\nabla_\\theta E_\\theta(x) - \\nabla_\\theta \\log Z_\\theta \\] <p>Substitute Step 10:</p> \\[ \\nabla_\\theta \\log p_\\theta(x) = -\\nabla_\\theta E_\\theta(x) - \\left(-\\mathbb{E}_{p_\\theta}[\\nabla_\\theta E_\\theta(x')]\\right) \\] \\[ \\boxed{\\nabla_\\theta \\log p_\\theta(x) = -\\nabla_\\theta E_\\theta(x) + \\mathbb{E}_{x' \\sim p_\\theta}\\big[\\nabla_\\theta E_\\theta(x')\\big]} \\] <p>Explanation (the intuition):</p> <ul> <li>First term (data term): push down energy on observed data \\(x\\) \u2192 \"make data likely.\"</li> <li>Second term (model term): push up energy on typical samples \\(x' \\sim p_\\theta\\) \u2192 \"make non-data less likely.\"</li> </ul>"},{"location":"EBM/EBM-mle-gradient-derivation/#why-this-makes-mle-hard-the-punchline","title":"Why This Makes MLE Hard (The Punchline)","text":"<p>That expectation</p> \\[ \\mathbb{E}_{x' \\sim p_\\theta}[\\nabla_\\theta E_\\theta(x')] \\] <p>requires sampling from \\(p_\\theta\\).</p> <p>But sampling from \\(p_\\theta\\) is hard because:</p> <ul> <li>\\(p_\\theta\\) is only defined via an energy (unnormalized form)</li> <li>You usually need MCMC (Langevin dynamics, HMC, Gibbs, etc.)</li> <li>MCMC can be slow and biased if it doesn't mix well</li> <li>Doing this inside every gradient step is brutal</li> </ul> <p>This is why people use:</p> <ul> <li>Contrastive divergence / persistent CD (approximate MCMC)</li> <li>Score matching / denoising score matching (avoid \\(Z_\\theta\\))</li> <li>Noise-contrastive estimation (reframing as classification)</li> <li>Diffusion/score-based models (learn \\(\\nabla_x \\log p\\) directly)</li> </ul>"},{"location":"EBM/EBM-mle-gradient-derivation/#whats-next","title":"What's Next","text":"<p>If you want to go one level deeper, the natural continuation is: derive the score matching objective's \"trace(Jacobian)\" form via integration by parts, and show exactly where the \\(p_D\\) terms drop out. That's the other half of the magic.</p>"},{"location":"EBM/EBM-score-matching-FSM-analogue/","title":"Fisher Score Matching: The Parameter-Space Integration-by-Parts Trick","text":"<p>This document proves the FSM analogue of the score-matching integration-by-parts trick (Theorem 3.1 from Khoo et al., 2025). It's the same magic as classic score matching, but with a crucial twist:</p> Method Gradient w.r.t. Eliminates Classic score matching Data \\(x\\) Unknown \\(p_D(x)\\) Fisher score matching Parameters \\(\\theta\\) Intractable likelihood score <p>This derivation shows exactly how the intractable likelihood score disappears. </p>"},{"location":"EBM/EBM-score-matching-FSM-analogue/#step-0-notation","title":"Step 0: Notation","text":""},{"location":"EBM/EBM-score-matching-FSM-analogue/#spaces-and-variables","title":"Spaces and variables","text":"<ul> <li>\\(x \\in \\mathbb{R}^k\\) \u2014 data (observation or summary statistic)</li> <li>\\(\\theta \\in \\mathbb{R}^d\\) \u2014 model parameter</li> <li>\\(\\theta_t\\) \u2014 current parameter iterate (a fixed point around which we localize)</li> </ul>"},{"location":"EBM/EBM-score-matching-FSM-analogue/#model-and-proposal","title":"Model and proposal","text":"<ul> <li>\\(p(x|\\theta)\\) \u2014 simulator-defined likelihood (intractable density, but we can sample \\(x \\sim p(\\cdot|\\theta)\\))</li> <li>\\(q(\\theta|\\theta_t)\\) \u2014 a local proposal density around \\(\\theta_t\\) (often Gaussian)</li> </ul>"},{"location":"EBM/EBM-score-matching-FSM-analogue/#joint-distribution-the-key-construction","title":"Joint distribution (the key construction)","text":"<p>Define a joint density over \\((x, \\theta)\\) by:</p> \\[ p(x, \\theta | \\theta_t) := p(x|\\theta) \\cdot q(\\theta|\\theta_t) \\] <p>Interpretation: sample \\(\\theta \\sim q(\\cdot|\\theta_t)\\), then sample \\(x \\sim p(\\cdot|\\theta)\\).</p>"},{"location":"EBM/EBM-score-matching-FSM-analogue/#surrogate-score-model","title":"Surrogate score model","text":"<ul> <li>\\(S_W(x) \\in \\mathbb{R}^d\\) \u2014 a function of data only, meant to approximate the Fisher score</li> <li>\\(W\\) \u2014 its parameters (can be linear weights, NN weights, etc.)</li> </ul>"},{"location":"EBM/EBM-score-matching-FSM-analogue/#fisher-score-the-unknown-target","title":"Fisher score (the unknown target)","text":"\\[ \\nabla_\\theta \\log p(x|\\theta) \\] <p>This is what we cannot compute in SBI.</p>"},{"location":"EBM/EBM-score-matching-FSM-analogue/#boundary-condition","title":"Boundary condition","text":"<p>We need a standard \"boundary term vanishes\" condition:</p> \\[ \\forall x, \\quad \\lim_{|\\theta| \\to \\infty} p(x|\\theta) \\cdot q(\\theta|\\theta_t) = 0 \\] <p>This is the regularity assumption stated in Appendix A.1 of \"Direct Fisher Score Estimation for Likelihood Maximization\" (Khoo et al., 2025). </p>"},{"location":"EBM/EBM-score-matching-FSM-analogue/#step-1-the-intractable-fisher-score-matching-objective","title":"Step 1: The \"Intractable\" Fisher Score Matching Objective","text":"<p>Start from Eq. (1):</p> \\[ J(W; \\theta_t) = \\mathbb{E}_{\\theta \\sim q(\\cdot|\\theta_t), x \\sim p(\\cdot|\\theta)} \\left[ |\\nabla_\\theta \\log p(x|\\theta) - S_W(x)|^2 \\right] \\tag{1} \\] <p>Explanation: We want \\(S_W(x)\\) to predict the Fisher score, but we'll never evaluate the Fisher score directly. So we'll rewrite this objective into something computable.</p>"},{"location":"EBM/EBM-score-matching-FSM-analogue/#step-2-expand-the-square","title":"Step 2: Expand the Square","text":"<p>Expand \\(|a-b|^2 = |a|^2 + |b|^2 - 2a^\\top b\\):</p> \\[ J(W; \\theta_t) = \\mathbb{E}\\left[ |\\nabla_\\theta \\log p(x|\\theta)|^2 + |S_W(x)|^2 - 2 S_W(x)^\\top \\nabla_\\theta \\log p(x|\\theta) \\right] \\] <p>Explanation: Standard algebra. When optimizing over \\(W\\), the first term does not involve \\(W\\), so it's a constant.</p> <p>Up to an additive constant w.r.t. \\(W\\):</p> \\[ J(W; \\theta_t) = \\mathbb{E}\\left[ |S_W(x)|^2 - 2 S_W(x)^\\top \\nabla_\\theta \\log p(x|\\theta) \\right] + \\text{const} \\] <p>Explanation: We keep only terms that influence the optimal \\(S_W\\).</p>"},{"location":"EBM/EBM-score-matching-FSM-analogue/#step-3-rewrite-as-an-integral","title":"Step 3: Rewrite as an Integral","text":"<p>Using the joint density \\(p(x, \\theta | \\theta_t) = p(x|\\theta) q(\\theta|\\theta_t)\\):</p> \\[ \\mathbb{E}[f(x, \\theta)] = \\int_{\\mathbb{R}^d} \\int_{\\mathbb{R}^k} f(x, \\theta) \\cdot p(x|\\theta) \\cdot q(\\theta|\\theta_t) \\, dx \\, d\\theta \\] <p>where the inner integral is over \\(x \\in \\mathbb{R}^k\\) (data space) and the outer integral is over \\(\\theta \\in \\mathbb{R}^d\\) (parameter space).</p> <p>So the problematic cross term becomes:</p> \\[ \\mathbb{E}\\left[ S_W(x)^\\top \\nabla_\\theta \\log p(x|\\theta) \\right] = \\iint S_W(x)^\\top \\nabla_\\theta \\log p(x|\\theta) \\cdot p(x|\\theta) \\cdot q(\\theta|\\theta_t) \\, dx \\, d\\theta \\] <p>Explanation: We're turning expectation into integrals so we can integrate by parts.</p>"},{"location":"EBM/EBM-score-matching-FSM-analogue/#step-4-eliminate-the-log","title":"Step 4: Eliminate the Log","text":"<p>Use the identity:</p> \\[ \\nabla_\\theta \\log p(x|\\theta) \\cdot p(x|\\theta) = \\nabla_\\theta p(x|\\theta) \\] <p>So:</p> \\[ \\iint S_W(x)^\\top \\nabla_\\theta \\log p(x|\\theta) \\cdot p(x|\\theta) \\cdot q(\\theta|\\theta_t) \\, dx \\, d\\theta = \\iint S_W(x)^\\top \\nabla_\\theta p(x|\\theta) \\cdot q(\\theta|\\theta_t) \\, dx \\, d\\theta \\] <p>Explanation: This is the same maneuver as classic score matching where \\(p \\nabla \\log p = \\nabla p\\). We're pushing \"log\" out of the way.</p>"},{"location":"EBM/EBM-score-matching-FSM-analogue/#step-5-integration-by-parts-the-core-trick","title":"Step 5: Integration by Parts (The Core Trick)","text":"<p>Write component-wise. Let \\(S_W(x) = (S_{W,1}(x), \\dots, S_{W,d}(x))^\\top\\). Then:</p> \\[ S_W(x)^\\top \\nabla_\\theta p(x|\\theta) = \\sum_{i=1}^d S_{W,i}(x) \\cdot \\frac{\\partial}{\\partial \\theta_i} p(x|\\theta) \\] <p>So the cross term is:</p> \\[ \\sum_{i=1}^d \\iint S_{W,i}(x) \\cdot \\frac{\\partial}{\\partial \\theta_i} p(x|\\theta) \\cdot q(\\theta|\\theta_t) \\, dx \\, d\\theta \\] <p>Now for each \\(i\\), apply integration by parts in \\(\\theta_i\\):</p> \\[ \\int_{\\mathbb{R}^d} q(\\theta|\\theta_t) \\cdot \\frac{\\partial}{\\partial \\theta_i} p(x|\\theta) \\, d\\theta = \\Big[q(\\theta|\\theta_t) p(x|\\theta)\\Big]_{\\text{boundary}} - \\int_{\\mathbb{R}^d} p(x|\\theta) \\cdot \\frac{\\partial}{\\partial \\theta_i} q(\\theta|\\theta_t) \\, d\\theta \\] <p>Explanation: This is \\(\\int u \\, dv = uv - \\int v \\, du\\) with:</p> <ul> <li>\\(u = q(\\theta|\\theta_t)\\)</li> <li>\\(dv = \\partial_{\\theta_i} p(x|\\theta) \\, d\\theta_i\\)</li> <li>so \\(v = p(x|\\theta)\\), \\(du = \\partial_{\\theta_i} q(\\theta|\\theta_t) \\, d\\theta_i\\)</li> </ul> <p>Under the boundary condition, the boundary term is zero:</p> \\[ \\Big[q(\\theta|\\theta_t) p(x|\\theta)\\Big]_{\\text{boundary}} = 0 \\] <p>So:</p> \\[ \\int q(\\theta|\\theta_t) \\cdot \\frac{\\partial}{\\partial \\theta_i} p(x|\\theta) \\, d\\theta = -\\int p(x|\\theta) \\cdot \\frac{\\partial}{\\partial \\theta_i} q(\\theta|\\theta_t) \\, d\\theta \\] <p>Explanation: The derivative moved from the intractable \\(p(x|\\theta)\\) to the proposal \\(q\\), which we chose and can differentiate.</p> <p>Substitute back into the cross term:</p> \\[ \\iint S_W(x)^\\top \\nabla_\\theta p(x|\\theta) \\cdot q(\\theta|\\theta_t) \\, dx \\, d\\theta = -\\iint S_W(x)^\\top p(x|\\theta) \\cdot \\nabla_\\theta q(\\theta|\\theta_t) \\, dx \\, d\\theta \\]"},{"location":"EBM/EBM-score-matching-FSM-analogue/#step-6-convert-to-log-form","title":"Step 6: Convert to Log Form","text":"<p>Use:</p> \\[ \\nabla_\\theta q(\\theta|\\theta_t) = q(\\theta|\\theta_t) \\cdot \\nabla_\\theta \\log q(\\theta|\\theta_t) \\] <p>So:</p> \\[ -\\iint S_W(x)^\\top p(x|\\theta) \\cdot \\nabla_\\theta q(\\theta|\\theta_t) \\, dx \\, d\\theta = -\\iint S_W(x)^\\top p(x|\\theta) \\cdot q(\\theta|\\theta_t) \\cdot \\nabla_\\theta \\log q(\\theta|\\theta_t) \\, dx \\, d\\theta \\] <p>This is:</p> \\[ = -\\mathbb{E}_{x \\sim p(\\cdot|\\theta), \\theta \\sim q(\\cdot|\\theta_t)} \\left[ S_W(x)^\\top \\nabla_\\theta \\log q(\\theta|\\theta_t) \\right] \\] <p>Explanation: Now the cross term involves only \\(\\nabla_\\theta \\log q\\), which is analytic.</p> <p>The key identity:</p> \\[ \\mathbb{E}\\left[ S_W(x)^\\top \\nabla_\\theta \\log p(x|\\theta) \\right] = -\\mathbb{E}\\left[ S_W(x)^\\top \\nabla_\\theta \\log q(\\theta|\\theta_t) \\right] \\]"},{"location":"EBM/EBM-score-matching-FSM-analogue/#step-7-substitute-back-into-the-objective","title":"Step 7: Substitute Back into the Objective","text":"<p>Recall (up to constants):</p> \\[ J(W; \\theta_t) = \\mathbb{E}\\left[ |S_W(x)|^2 - 2 S_W(x)^\\top \\nabla_\\theta \\log p(x|\\theta) \\right] + \\text{const} \\] <p>Replace the cross term using our identity:</p> \\[ \\mathbb{E}\\left[ S_W^\\top \\nabla_\\theta \\log p \\right] = -\\mathbb{E}\\left[ S_W^\\top \\nabla_\\theta \\log q \\right] \\] <p>Therefore:</p> \\[ -2 \\mathbb{E}\\left[ S_W^\\top \\nabla_\\theta \\log p \\right] = -2 \\left( -\\mathbb{E}[S_W^\\top \\nabla_\\theta \\log q] \\right) = +2 \\mathbb{E}\\left[ S_W^\\top \\nabla_\\theta \\log q \\right] \\] <p>So the rewritten FSM objective is:</p> \\[ \\boxed{J(W; \\theta_t) = \\mathbb{E}_{x \\sim p(\\cdot|\\theta), \\theta \\sim q(\\cdot|\\theta_t)} \\left[ |S_W(x)|^2 + 2 S_W(x)^\\top \\nabla_\\theta \\log q(\\theta|\\theta_t) \\right] + \\text{const}} \\] <p>This is exactly Theorem 3.1 / Eq. (2).</p> <p>Explanation: The entire dependence on the intractable likelihood score is gone. We only need:</p> <ul> <li>Simulated pairs \\((\\theta, x)\\)</li> <li>\\(\\nabla_\\theta \\log q(\\theta|\\theta_t)\\), which is easy to compute</li> </ul>"},{"location":"EBM/EBM-score-matching-FSM-analogue/#the-same-trick-as-classic-score-matching","title":"The Same Trick as Classic Score Matching","text":"<p>Both methods follow the same schema:</p> <ol> <li>Start with a squared error loss against an unknown score</li> <li>Expand the square</li> <li>The cross term has \"unknown score\" inside</li> <li>Use \\(p \\nabla \\log p = \\nabla p\\)</li> <li>Integration by parts moves the derivative onto something we control</li> <li>Classic SM: onto the model score's divergence</li> <li>FSM: onto the proposal score \\(\\nabla_\\theta \\log q\\)</li> </ol> <p>It's score matching\u2026 but in parameter space instead of data space.</p>"},{"location":"EBM/EBM-score-matching-FSM-analogue/#concrete-example-gaussian-proposal","title":"Concrete Example: Gaussian Proposal","text":"<p>If:</p> \\[ q(\\theta|\\theta_t) = \\mathcal{N}(\\theta_t, \\sigma^2 I) \\] <p>then:</p> \\[ \\nabla_\\theta \\log q(\\theta|\\theta_t) = -\\frac{1}{\\sigma^2}(\\theta - \\theta_t) \\] <p>Explanation: This is why the rewritten loss is practical: you can compute this exactly and it behaves like a \"pull back toward \\(\\theta_t\\)\" vector.</p>"},{"location":"EBM/EBM-score-matching-FSM-analogue/#whats-next","title":"What's Next","text":"<p>The minimizer of this objective is a smoothed Fisher score:</p> \\[ S^*(x; \\theta_t) = \\mathbb{E}_{\\theta \\sim p(\\theta|x, \\theta_t)} \\left[ \\nabla_\\theta \\log p(x|\\theta) \\right] \\] <p>This connects to Gaussian smoothing (Theorem 5.1), which explains the method's robustness properties.</p>"},{"location":"EBM/EBM-score-matching-FSM-analogue/#connection-to-data-space-score-matching","title":"Connection to Data-Space Score Matching","text":"<p>See Score Matching Objective Derivation for the data-space analogue that eliminates \\(\\nabla_x \\log p_D(x)\\) instead of \\(\\nabla_\\theta \\log p(x|\\theta)\\).</p>"},{"location":"EBM/EBM-score-matching-objective/","title":"Score Matching Objective: The Integration-by-Parts Derivation","text":"<p>This document proves the classic identity behind (explicit) score matching\u2014the \"integration-by-parts trick\" that makes score matching usable without knowing \\(p_D\\). This derivation shows exactly where the unknown data distribution terms drop out.</p> <p>The key identity:</p> \\[ \\mathbb{E}_{p_D}\\left[\\frac{1}{2}|s_\\theta(x) - \\nabla_x \\log p_D(x)|^2\\right] = \\mathbb{E}_{p_D}\\left[\\frac{1}{2}|s_\\theta(x)|^2 + \\mathrm{tr}(\\nabla_x s_\\theta(x))\\right] + \\text{const} \\] <p>The left side requires the unknown \\(\\nabla_x \\log p_D(x)\\); the right side doesn't. </p>"},{"location":"EBM/EBM-score-matching-objective/#step-0-notation-and-assumptions","title":"Step 0: Notation and Assumptions","text":""},{"location":"EBM/EBM-score-matching-objective/#variables-and-distributions","title":"Variables and distributions","text":"<ul> <li>\\(x = (x_1, \\dots, x_d) \\in \\mathbb{R}^d\\) \u2014 data vector</li> <li>\\(p_D(x)\\) \u2014 true (unknown) data density; we can sample from it</li> <li>\\(p_\\theta(x)\\) \u2014 model density (e.g., an EBM); \\(\\theta\\) are model parameters</li> </ul>"},{"location":"EBM/EBM-score-matching-objective/#score-functions","title":"Score functions","text":"<ul> <li>Model score:</li> </ul> \\[ s_\\theta(x) := \\nabla_x \\log p_\\theta(x) \\in \\mathbb{R}^d \\] <p>Component form: \\(s_{\\theta,i}(x) = \\frac{\\partial}{\\partial x_i} \\log p_\\theta(x)\\)</p> <ul> <li>Data score:</li> </ul> \\[ s_D(x) := \\nabla_x \\log p_D(x) \\in \\mathbb{R}^d \\] <p>which we cannot compute directly because \\(p_D\\) is unknown.</p>"},{"location":"EBM/EBM-score-matching-objective/#differential-operators","title":"Differential operators","text":"<ul> <li>Gradient w.r.t. \\(x\\): \\(\\nabla_x\\)</li> <li>Jacobian of a vector field \\(s_\\theta(x)\\):</li> </ul> \\[ J_x s_\\theta(x) \\in \\mathbb{R}^{d \\times d}, \\quad (J_x s_\\theta)_{ij} = \\frac{\\partial s_{\\theta,i}}{\\partial x_j} \\] <ul> <li>Divergence (a scalar):</li> </ul> \\[ \\nabla_x \\cdot s_\\theta(x) := \\sum_{i=1}^d \\frac{\\partial s_{\\theta,i}(x)}{\\partial x_i} = \\mathrm{tr}(J_x s_\\theta(x)) \\]"},{"location":"EBM/EBM-score-matching-objective/#boundary-condition","title":"Boundary condition","text":"<p>We assume the boundary term vanishes. For \\(\\mathcal{X} = \\mathbb{R}^d\\):</p> \\[ \\lim_{|x| \\to \\infty} p_D(x) \\cdot s_{\\theta,i}(x) = 0 \\quad \\text{for each } i \\] <p>This is the standard regularity condition used in score matching proofs.</p> <p>Why we need it: Integration by parts produces a boundary term; we want it to be zero.</p>"},{"location":"EBM/EBM-score-matching-objective/#step-1-start-from-the-explicit-score-matching-objective","title":"Step 1: Start from the Explicit Score Matching Objective","text":"<p>Define:</p> \\[ \\mathcal{L}_{\\text{ESM}}(\\theta) := \\mathbb{E}_{x \\sim p_D} \\left[ \\frac{1}{2} |s_\\theta(x) - s_D(x)|^2 \\right] \\] <p>Explanation: We want the model score \\(s_\\theta\\) to match the true score \\(s_D\\), because matching scores identifies the density (up to a constant).</p>"},{"location":"EBM/EBM-score-matching-objective/#step-2-expand-the-squared-norm","title":"Step 2: Expand the Squared Norm","text":"\\[ \\frac{1}{2}|s_\\theta - s_D|^2 = \\frac{1}{2}|s_\\theta|^2 - \\langle s_\\theta, s_D \\rangle + \\frac{1}{2}|s_D|^2 \\] <p>Explanation: This is just \\(|a-b|^2 = |a|^2 - 2a^\\top b + |b|^2\\), with the factor \\(\\frac{1}{2}\\) removing the 2.</p> <p>Taking expectation under \\(p_D\\):</p> \\[ \\mathcal{L}_{\\text{ESM}}(\\theta) = \\mathbb{E}_{p_D}\\left[\\frac{1}{2}|s_\\theta(x)|^2\\right] - \\mathbb{E}_{p_D}\\left[\\langle s_\\theta(x), s_D(x) \\rangle\\right] + \\mathbb{E}_{p_D}\\left[\\frac{1}{2}|s_D(x)|^2\\right] \\] <p>Explanation: Linear property of expectation.</p> <p>Now note: the last term does not depend on \\(\\theta\\), because \\(s_D\\) is fixed by the data distribution.</p> <p>So:</p> \\[ \\mathcal{L}_{\\text{ESM}}(\\theta) = \\mathbb{E}_{p_D}\\left[\\frac{1}{2}|s_\\theta(x)|^2\\right] - \\mathbb{E}_{p_D}\\left[\\langle s_\\theta(x), s_D(x) \\rangle\\right] + \\text{const} \\] <p>Explanation: When optimizing over \\(\\theta\\), constants can be ignored.</p> <p>The real enemy is the cross term \\(\\mathbb{E}_{p_D}[\\langle s_\\theta, s_D \\rangle]\\), because it contains \\(s_D = \\nabla \\log p_D\\), which we can't compute.</p>"},{"location":"EBM/EBM-score-matching-objective/#step-3-rewrite-the-cross-term","title":"Step 3: Rewrite the Cross Term","text":"<p>Write expectation as an integral:</p> \\[ \\mathbb{E}_{p_D}\\left[\\langle s_\\theta(x), s_D(x) \\rangle\\right] = \\int p_D(x) \\cdot s_\\theta(x)^\\top \\nabla_x \\log p_D(x) \\, dx \\] <p>Explanation: By definition, \\(\\mathbb{E}_{p_D}[f(x)] = \\int f(x) p_D(x) \\, dx\\).</p> <p>Now use the identity:</p> \\[ \\nabla_x \\log p_D(x) = \\frac{\\nabla_x p_D(x)}{p_D(x)} \\quad \\text{(where } p_D(x) &gt; 0 \\text{)} \\] <p>Substitute:</p> \\[ \\int p_D(x) \\cdot s_\\theta(x)^\\top \\nabla_x \\log p_D(x) \\, dx = \\int p_D(x) \\cdot s_\\theta(x)^\\top \\frac{\\nabla_x p_D(x)}{p_D(x)} \\, dx = \\int s_\\theta(x)^\\top \\nabla_x p_D(x) \\, dx \\] <p>Explanation: The \\(p_D(x)\\) cancels. Now we no longer have \\(\\log p_D\\), only \\(\\nabla p_D\\). Still not computable directly, but now integration by parts can help.</p>"},{"location":"EBM/EBM-score-matching-objective/#step-4-apply-integration-by-parts-component-wise","title":"Step 4: Apply Integration by Parts (Component-wise)","text":"<p>Write the dot product as a sum over components:</p> \\[ \\int s_\\theta(x)^\\top \\nabla_x p_D(x) \\, dx = \\sum_{i=1}^d \\int s_{\\theta,i}(x) \\cdot \\frac{\\partial}{\\partial x_i} p_D(x) \\, dx \\] <p>Explanation: \\(a^\\top b = \\sum_i a_i b_i\\).</p> <p>Now apply 1D integration by parts in the \\(x_i\\) direction while holding other coordinates fixed:</p> \\[ \\int s_{\\theta,i}(x) \\cdot \\frac{\\partial}{\\partial x_i} p_D(x) \\, dx = \\Big[s_{\\theta,i}(x) \\cdot p_D(x)\\Big]_{\\text{boundary}} - \\int p_D(x) \\cdot \\frac{\\partial}{\\partial x_i} s_{\\theta,i}(x) \\, dx \\] <p>Explanation: This is the multivariate version of \\(\\int u \\, dv = uv - \\int v \\, du\\), where \\(u = s_{\\theta,i}\\), \\(dv = \\partial_i p_D \\, dx_i\\), so \\(v = p_D\\), \\(du = \\partial_i s_{\\theta,i} \\, dx_i\\).</p> <p>Under the boundary condition, the boundary term is zero:</p> \\[ \\Big[s_{\\theta,i}(x) \\cdot p_D(x)\\Big]_{\\text{boundary}} = 0 \\] <p>So:</p> \\[ \\int s_{\\theta,i}(x) \\cdot \\frac{\\partial}{\\partial x_i} p_D(x) \\, dx = -\\int p_D(x) \\cdot \\frac{\\partial}{\\partial x_i} s_{\\theta,i}(x) \\, dx \\] <p>Explanation: This is where the \"magic\" happens: the derivative moves from \\(p_D\\) to \\(s_\\theta\\).</p> <p>Summing over \\(i\\):</p> \\[ \\int s_\\theta(x)^\\top \\nabla_x p_D(x) \\, dx = -\\sum_{i=1}^d \\int p_D(x) \\cdot \\frac{\\partial}{\\partial x_i} s_{\\theta,i}(x) \\, dx \\] <p>Recognize divergence:</p> \\[ -\\sum_{i=1}^d \\int p_D(x) \\cdot \\frac{\\partial}{\\partial x_i} s_{\\theta,i}(x) \\, dx = -\\int p_D(x) \\cdot (\\nabla_x \\cdot s_\\theta(x)) \\, dx \\] <p>So the cross term becomes:</p> \\[ \\mathbb{E}_{p_D}\\left[\\langle s_\\theta(x), s_D(x) \\rangle\\right] = -\\mathbb{E}_{p_D}\\left[\\nabla_x \\cdot s_\\theta(x)\\right] \\] <p>Explanation: We have removed the unknown \\(s_D\\). Everything left involves \\(s_\\theta\\) and its derivatives.</p>"},{"location":"EBM/EBM-score-matching-objective/#step-5-substitute-back-into-the-objective","title":"Step 5: Substitute Back into the Objective","text":"<p>Recall:</p> \\[ \\mathcal{L}_{\\text{ESM}}(\\theta) = \\mathbb{E}_{p_D}\\left[\\frac{1}{2}|s_\\theta(x)|^2\\right] - \\mathbb{E}_{p_D}\\left[\\langle s_\\theta(x), s_D(x) \\rangle\\right] + \\text{const} \\] <p>Substitute the identity we just proved:</p> \\[ -\\mathbb{E}_{p_D}\\left[\\langle s_\\theta, s_D \\rangle\\right] = -\\left(-\\mathbb{E}_{p_D}[\\nabla_x \\cdot s_\\theta]\\right) = \\mathbb{E}_{p_D}[\\nabla_x \\cdot s_\\theta] \\] <p>So:</p> \\[ \\boxed{\\mathcal{L}_{\\text{ESM}}(\\theta) = \\mathbb{E}_{p_D}\\left[ \\frac{1}{2}|s_\\theta(x)|^2 + \\nabla_x \\cdot s_\\theta(x) \\right] + \\text{const}} \\] <p>Finally, use \\(\\nabla_x \\cdot s_\\theta(x) = \\mathrm{tr}(J_x s_\\theta(x))\\):</p> \\[ \\boxed{\\mathcal{L}_{\\text{SM}}(\\theta) = \\mathbb{E}_{p_D}\\left[ \\frac{1}{2}|s_\\theta(x)|^2 + \\mathrm{tr}(J_x s_\\theta(x)) \\right] + \\text{const}} \\] <p>Explanation: This is exactly the tractable form stated in the paper's score-matching section. </p>"},{"location":"EBM/EBM-score-matching-objective/#intuition-what-this-proof-achieves","title":"Intuition: What This Proof Achieves","text":"<ul> <li>The explicit objective wanted to match \\(s_\\theta\\) to \\(s_D\\), but \\(s_D\\) is unknown</li> <li>The trick converts the unknown cross term into the divergence of the model score, which is computable (if you can differentiate your model w.r.t. \\(x\\))</li> <li>The remaining constant is \\(\\frac{1}{2}\\mathbb{E}_{p_D}|s_D|^2\\), which doesn't matter for optimization over \\(\\theta\\)</li> </ul>"},{"location":"EBM/EBM-score-matching-objective/#why-this-still-gets-expensive","title":"Why This Still Gets Expensive","text":"<p>That \\(\\mathrm{tr}(J_x s_\\theta(x))\\) term requires computing the trace of a Jacobian (often involving second derivatives of the energy). In high dimensions, this is costly\u2014hence:</p> <ul> <li>Denoising Score Matching (DSM) \u2014 avoids the trace term by adding noise</li> <li>Sliced Score Matching (SSM) \u2014 uses random projections to approximate the trace</li> </ul>"},{"location":"EBM/EBM-score-matching-objective/#connection-to-fisher-score-matching","title":"Connection to Fisher Score Matching","text":"<p>The same integration-by-parts trick applies in parameter space:</p> Aspect Data-Space Score Matching Parameter-Space (Fisher) Score Matching Eliminates \\(\\nabla_x \\log p_D(x)\\) \\(\\nabla_\\theta \\log p(x\\|\\theta)\\) Replaces with \\(\\nabla_x \\cdot s_\\theta(x)\\) \\(\\nabla_\\theta \\log q(\\theta\\|\\theta_t)\\) <p>See Fisher Score Matching for the parameter-space analogue used in simulation-based inference.</p>"},{"location":"EBM/EBM-stein-vs-fisher-score/","title":"Stein Score vs Fisher Score: Two Flavors of \"Score\"","text":"<p>This document clarifies the distinction between the Stein score (gradient w.r.t. data) and the Fisher score (gradient w.r.t. parameters)\u2014two different objects that both go by \"score\" in the literature.</p>"},{"location":"EBM/EBM-stein-vs-fisher-score/#the-two-scores-at-a-glance","title":"The Two Scores at a Glance","text":"Name Symbol Gradient w.r.t. What it measures Stein score \\(s(x) = \\nabla_x \\log p(x)\\) Data \\(x\\) Direction of steepest increase in log-density Fisher score \\(g(\\theta) = \\nabla_\\theta \\log p(x \\| \\theta)\\) Parameters \\(\\theta\\) Sensitivity of log-likelihood to parameters <p>Both are gradients of a log-probability, but they differentiate with respect to different variables.</p>"},{"location":"EBM/EBM-stein-vs-fisher-score/#stein-score-nabla_x-log-px","title":"Stein Score: \\(\\nabla_x \\log p(x)\\)","text":""},{"location":"EBM/EBM-stein-vs-fisher-score/#stein-score-definition","title":"Stein score definition","text":"<p>For a density \\(p(x)\\) over data \\(x \\in \\mathbb{R}^d\\):</p> \\[ s(x) := \\nabla_x \\log p(x) \\] <p>This is a vector field over data space\u2014at each point \\(x\\), it points in the direction where the density increases most rapidly.</p>"},{"location":"EBM/EBM-stein-vs-fisher-score/#stein-score-intuition","title":"Stein score intuition","text":"<ul> <li>High-density regions: The score points \"inward\" toward the mode</li> <li>Low-density regions: The score points toward higher-density areas</li> <li>At the mode: The score is zero (gradient of log-density vanishes at maximum)</li> </ul>"},{"location":"EBM/EBM-stein-vs-fisher-score/#why-the-stein-score-is-useful","title":"Why the Stein score is useful","text":"<p>The Stein score is central to:</p> <ol> <li>Training EBMs: For \\(p_\\theta(x) = \\exp(-E_\\theta(x))/Z_\\theta\\), the score is \\(s_\\theta(x) = -\\nabla_x E_\\theta(x)\\), which doesn't depend on \\(Z_\\theta\\)</li> <li>Diffusion models: Learn \\(\\nabla_x \\log p_t(x)\\) at multiple noise levels</li> <li>Langevin dynamics: Sample from \\(p(x)\\) using \\(x_{t+1} = x_t + \\epsilon \\nabla_x \\log p(x_t) + \\sqrt{2\\epsilon} z\\)</li> </ol>"},{"location":"EBM/EBM-stein-vs-fisher-score/#the-score-matching-objective","title":"The score matching objective","text":"<p>We want to learn \\(s_\\theta(x) \\approx \\nabla_x \\log p_D(x)\\), but \\(p_D\\) is unknown. Score matching solves this via integration by parts:</p> \\[ \\mathcal{L}_{\\text{SM}}(\\theta) = \\mathbb{E}_{p_D}\\left[ \\frac{1}{2}|s_\\theta(x)|^2 + \\mathrm{tr}(\\nabla_x s_\\theta(x)) \\right] \\] <p>See Score Matching Objective Derivation for the full proof. For implementation guidance, see Roadmap Stage 5 and the ESM vs DSM comparison.</p>"},{"location":"EBM/EBM-stein-vs-fisher-score/#fisher-score-nabla_theta-log-pxtheta","title":"Fisher Score: \\(\\nabla_\\theta \\log p(x|\\theta)\\)","text":""},{"location":"EBM/EBM-stein-vs-fisher-score/#fisher-score-definition","title":"Fisher score definition","text":"<p>For a parametric model \\(p(x|\\theta)\\) with parameters \\(\\theta \\in \\mathbb{R}^d\\):</p> \\[ g(x; \\theta) := \\nabla_\\theta \\log p(x|\\theta) \\] <p>This is a vector in parameter space\u2014it tells you how the log-likelihood of observation \\(x\\) changes as you vary \\(\\theta\\).</p>"},{"location":"EBM/EBM-stein-vs-fisher-score/#fisher-score-intuition","title":"Fisher score intuition","text":"<ul> <li>Positive component \\(g_i &gt; 0\\): Increasing \\(\\theta_i\\) would increase the likelihood of \\(x\\)</li> <li>Negative component \\(g_i &lt; 0\\): Increasing \\(\\theta_i\\) would decrease the likelihood of \\(x\\)</li> <li>At the MLE: \\(\\mathbb{E}_{p_D}[g(x; \\hat{\\theta})] = 0\\) (score equations)</li> </ul>"},{"location":"EBM/EBM-stein-vs-fisher-score/#why-the-fisher-score-is-useful","title":"Why the Fisher score is useful","text":"<p>The Fisher score is central to:</p> <ol> <li>Maximum likelihood estimation: The MLE gradient is \\(\\nabla_\\theta \\ell(\\theta) = \\sum_i \\nabla_\\theta \\log p(x_i|\\theta)\\)</li> <li>Fisher information: \\(I(\\theta) = \\mathbb{E}[g(x;\\theta) g(x;\\theta)^\\top]\\) measures parameter uncertainty</li> <li>Simulation-based inference: When \\(p(x|\\theta)\\) is intractable but simulable</li> </ol>"},{"location":"EBM/EBM-stein-vs-fisher-score/#the-fisher-score-matching-objective","title":"The Fisher score matching objective","text":"<p>We want to learn \\(S_W(x) \\approx \\nabla_\\theta \\log p(x|\\theta)\\), but the likelihood is intractable. Fisher score matching solves this via integration by parts in parameter space:</p> \\[ J(W; \\theta_t) = \\mathbb{E}\\left[ |S_W(x)|^2 + 2 S_W(x)^\\top \\nabla_\\theta \\log q(\\theta|\\theta_t) \\right] \\] <p>See Fisher Score Matching Derivation for the full proof.</p>"},{"location":"EBM/EBM-stein-vs-fisher-score/#side-by-side-comparison","title":"Side-by-Side Comparison","text":"Aspect Stein Score Fisher Score Symbol \\(\\nabla_x \\log p(x)\\) \\(\\nabla_\\theta \\log p(x\\|\\theta)\\) Lives in Data space \\(\\mathbb{R}^d\\) Parameter space \\(\\mathbb{R}^p\\) Input Data point \\(x\\) Data \\(x\\) and parameters \\(\\theta\\) Output Vector in \\(\\mathbb{R}^d\\) Vector in \\(\\mathbb{R}^p\\) Measures Where density increases in data space How likelihood changes with parameters Zero at Mode of \\(p(x)\\) MLE (in expectation)"},{"location":"EBM/EBM-stein-vs-fisher-score/#when-to-use-which","title":"When to Use Which","text":""},{"location":"EBM/EBM-stein-vs-fisher-score/#use-stein-score-when","title":"Use Stein score when","text":"<ul> <li>Training generative models (EBMs, diffusion models)</li> <li>Sampling via Langevin dynamics or score-based MCMC</li> <li>Density estimation where you want to model \\(p(x)\\) directly</li> <li>The partition function \\(Z_\\theta\\) is intractable</li> </ul>"},{"location":"EBM/EBM-stein-vs-fisher-score/#use-fisher-score-when","title":"Use Fisher score when","text":"<ul> <li>Parameter estimation via MLE</li> <li>Simulation-based inference where \\(p(x|\\theta)\\) is implicit</li> <li>Sensitivity analysis of model parameters</li> <li>The likelihood \\(p(x|\\theta)\\) is intractable but simulable</li> </ul>"},{"location":"EBM/EBM-stein-vs-fisher-score/#the-same-trick-different-spaces","title":"The Same Trick, Different Spaces","text":"<p>Both score matching methods use the same mathematical trick:</p> <ol> <li>Start with a squared error loss against an unknown score</li> <li>The cross term contains the unknown score</li> <li>Use \\(p \\nabla \\log p = \\nabla p\\) to eliminate the log</li> <li>Integration by parts moves the derivative onto something computable</li> </ol> Method IBP in Eliminates Replaces with Stein score matching Data space Unknown data score Trace of Jacobian Fisher score matching Parameter space Intractable likelihood score Proposal score"},{"location":"EBM/EBM-stein-vs-fisher-score/#historical-note","title":"Historical Note","text":"<p>The terminology can be confusing because:</p> <ul> <li>\"Score function\" in classical statistics usually means the Fisher score</li> <li>\"Score\" in the diffusion/EBM literature usually means the Stein score</li> <li>Both communities use \"score matching\" but for different objects</li> </ul> <p>This document uses explicit names (Stein vs Fisher) to avoid ambiguity.</p>"},{"location":"EBM/EBM-stein-vs-fisher-score/#references","title":"References","text":"<ul> <li>Hyv\u00e4rinen (2005). Estimation of Non-Normalized Statistical Models by Score Matching \u2014 Original Stein score matching</li> <li>Khoo et al. (2025). Direct Fisher Score Estimation for Likelihood Maximization \u2014 Fisher score matching for SBI</li> <li>Song &amp; Ermon (2019). Generative Modeling by Estimating Gradients of the Data Distribution \u2014 Score-based generative models</li> </ul>"},{"location":"GEM-1/gem-1-training-data-analysis/","title":"GEM-1 Training Data Analysis","text":"<p>Based on this post, GEM-1 is not a generative model in the diffusion / VAE / flow sense. It is a large conditional predictive model trained on harmonized expression + metadata, not a stochastic generator of novel transcriptomes.</p> <p>And that distinction matters a lot.</p>"},{"location":"GEM-1/gem-1-training-data-analysis/#what-gem-1-is-based-on-evidence-not-vibes","title":"What GEM-1 is (based on evidence, not vibes)","text":"<p>From everything described in the post by Synthesize Bio, GEM-1 behaves like:</p> <p>A function of the form:</p> \\[ \\hat{x}_{\\text{expr}} = f(\\text{biology}, \\text{perturbation}, \\text{technical}) \\] <p>Where:</p> <ul> <li>the output is gene expression values</li> <li>the inputs are structured metadata</li> <li>the supervision comes from real RNA-seq measurements</li> </ul> <p>This is conditional prediction, not free-form generation.</p> <p>No part of the post suggests:</p> <ul> <li>latent noise variables</li> <li>sampling trajectories</li> <li>likelihood-based generative objectives</li> <li>ELBOs</li> <li>score matching</li> <li>diffusion timesteps</li> <li>stochastic decoders</li> </ul> <p>In other words: no diffusion, no VAE, no GAN, no flow \u2014 at least not explicitly or implicitly.</p>"},{"location":"GEM-1/gem-1-training-data-analysis/#what-they-are-actually-doing-compilation-not-hallucination","title":"What they are actually doing: \u201cCompilation,\u201d not hallucination","text":"<p>What GEM-1 is doing is closer to:</p> <p>Learning a dense, continuous lookup table over the space of experimental conditions.</p> <p>They take:</p> <ul> <li>hundreds of thousands of real experiments</li> <li>aggressively clean and normalize metadata</li> <li>align everything into shared ontologies</li> <li>and train a massive model to interpolate within that space</li> </ul> <p>So instead of generating new biology from noise, they are:</p> <ul> <li>Interpolating biological states</li> <li>Conditioning on metadata</li> <li>Predicting expected expression outcomes</li> </ul> <p>This is much closer to:</p> <ul> <li>supervised regression at scale</li> <li>masked or conditional prediction</li> <li>foundation-model-style representation learning</li> </ul> <p>than to generative modeling in the DDPM sense.</p>"},{"location":"GEM-1/gem-1-training-data-analysis/#why-this-still-feels-like-synthetic-data-to-people","title":"Why this still feels like \u201csynthetic data\u201d to people","text":"<p>Here\u2019s the subtle but important philosophical wrinkle.</p> <p>If you can do this:</p> <p>\u201cGive me gene expression for a 52-year-old female, primary colon tissue, with disease X, treated with compound Y at dose Z for time T\u201d</p> <p>\u2026and get a plausible expression profile without running an experiment, then from the user\u2019s perspective, this behaves like synthetic data.</p> <p>But internally:</p> <ul> <li>There is no stochastic sampling</li> <li>There is no explicit data-generating distribution</li> <li>There is no uncertainty modeling described</li> <li>There is no notion of diversity across samples</li> </ul> <p>It is producing expected expression, not possible worlds.</p> <p>That\u2019s a crucial distinction.</p>"},{"location":"GEM-1/gem-1-training-data-analysis/#why-they-likely-avoided-diffusion-vaes-deliberately","title":"Why they likely avoided diffusion / VAEs (deliberately)","text":"<p>For transcriptomics, classic generative models have hard problems:</p> <ol> <li> <p>What does a \u201csample\u201d mean?    One cell? One bulk experiment? One patient? One replicate?</p> </li> <li> <p>What is the noise model?    Technical noise? Biological variability? Population heterogeneity?</p> </li> <li> <p>How do you validate novelty?    Unlike images, \u201cnew\u201d gene expression is hard to falsify.</p> </li> <li> <p>Metadata conditioning dominates anyway    In biology, context explains most variance. Noise explains less than we wish.</p> </li> </ol> <p>Given that, GEM-1\u2019s designers seem to have chosen a pragmatic route:</p> <p>First, learn the conditional mean structure of biology extremely well. Only later worry about stochasticity.</p> <p>That\u2019s a very sane engineering decision.</p>"},{"location":"GEM-1/gem-1-training-data-analysis/#a-useful-mental-model","title":"A useful mental model","text":"<p>Think of GEM-1 as:</p> <ul> <li>A giant conditional emulator</li> <li>A biological response surface</li> <li>A differentiable experiment simulator</li> <li>A learned assay abstraction</li> </ul> <p>Not as:</p> <ul> <li>a generative sampler</li> <li>a creativity engine</li> <li>a synthetic cohort generator (yet)</li> </ul>"},{"location":"GEM-1/gem-1-training-data-analysis/#where-true-generative-modeling-would-enter-your-genai-lab-sweet-spot","title":"Where true generative modeling would enter (your genai-lab sweet spot)","text":"<p>If you wanted to extend GEM-1 into a real synthetic data generator, you would need to add at least one of:</p> <ul> <li>stochastic latent variables</li> <li>uncertainty-aware outputs</li> <li>conditional diffusion over expression space</li> <li>hierarchical noise models (cell \u2192 sample \u2192 population)</li> <li>multiple plausible outcomes per condition</li> </ul> <p>Conceptually:</p> \\[ x_{\\text{expr}} \\sim p(x \\mid \\text{metadata}) \\quad \\text{not just} \\quad \\mathbb{E}[x \\mid \\text{metadata}] \\] <p>GEM-1, as described, is learning the right-hand side.</p>"},{"location":"GEM-1/gem-1-training-data-analysis/#why-this-matters-for-genai-lab","title":"Why this matters for genai-lab","text":"<p>You\u2019re circling an important architectural fork:</p> <ul> <li>Predictive foundation models (GEM-1 style)</li> <li>Generative biological world models (diffusion / flow / energy-based)</li> </ul> <p>These are not competitors \u2014 they are layers.</p> <p>In fact, the cleanest path is often:</p> <p>Predictive model first \u2192 generative wrapper later</p> <p>GEM-1 looks like a textbook example of stage one.</p>"},{"location":"GEM-1/gem-1-training-data-analysis/#bottom-line-no-hedging","title":"Bottom line (no hedging)","text":"<ul> <li>GEM-1 is not presented as a generative model</li> <li>There is no evidence of diffusion, VAE, or sampling-based synthesis</li> <li>The innovation is in data harmonization + conditional prediction</li> <li>\"Synthetic\" here means computationally predicted, not stochastically generated</li> </ul> <p>The generative fireworks come later \u2014 once the biology is nailed down.</p>"},{"location":"GEM-1/gem-1-training-data-analysis/#next-steps","title":"Next steps","text":"<p>Next we can think about:</p> <ul> <li>how a diffusion model could sit on top of GEM-1</li> <li>how to formalize biological uncertainty vs technical noise</li> <li>or how to design a hybrid predictive\u2013generative pipeline for genai-lab</li> </ul>"},{"location":"GEM-1/inside-gem-1-training-data/","title":"GEM-1: Training Data as the Real Foundation Model","text":"<p>Notes and speculative analysis inspired by Synthesize Bio\u2019s GEM-1</p>"},{"location":"GEM-1/inside-gem-1-training-data/#context-why-gem-1-matters-for-genai-in-computational-biology","title":"Context: Why GEM-1 Matters for GenAI in Computational Biology","text":"<p>One of the hardest problems in computational biology is not modeling\u2014it is data. Not the lack of it, but the lack of structured, harmonized, semantically aligned data.</p> <p>Synthesize Bio's GEM-1 positions itself as a foundation model for gene expression, capable of predicting expression values for 44,592 genes. What makes GEM-1 notable is not just scale, but the deliberate decision to treat training data construction as a first-class research problem.</p> <p>For projects like genai-lab, GEM-1 is best understood less as a single model and more as a case study in how to industrialize transcriptomics for generative modeling.</p>"},{"location":"GEM-1/inside-gem-1-training-data/#the-core-objective","title":"The Core Objective","text":"<p>The stated goal of GEM-1 is deceptively simple:</p> <p>Predict gene expression outcomes as if transcriptomic experiments were computational queries.</p> <p>This reframes RNA-seq from a slow experimental readout into a predictive modality, analogous to how protein structure prediction turned sequences into structures.</p> <p>But transcriptomics lacks a PDB-like resource. Instead, it offers: - heterogeneous experiments - inconsistent metadata - free-text annotations - variable technical effects  </p> <p>GEM-1 tackles this head-on by building the dataset the model needs, rather than bending the model to broken data.</p>"},{"location":"GEM-1/inside-gem-1-training-data/#data-modalities-used-for-training","title":"Data Modalities Used for Training","text":""},{"location":"GEM-1/inside-gem-1-training-data/#bulk-rna-seq-human-short-read","title":"Bulk RNA-seq (Human, Short-Read)","text":"<p>Primary source: - Sequence Read Archive (SRA)</p> <p>Scale: - 500,000+ human bulk RNA-seq samples - Processed directly from FASTQ</p> <p>Key technical achievement: - A custom AWS pipeline processing samples at &lt; $0.05 per sample - End-to-end processing completed in under four months</p> <p>Why this matters Scale here is not marketing. It exposes the model to: - rare tissues - diverse disease states - wide technical variability  </p> <p>This diversity is essential for generalization and synthetic data generation.</p>"},{"location":"GEM-1/inside-gem-1-training-data/#single-cell-rna-seq","title":"Single-Cell RNA-seq","text":"<p>Primary sources: - CellxGene (primary tissue-focused) - scPerturb (perturbation-focused, mostly cell lines)</p> <p>Included data: - ~41.7 million primary cells from CellxGene - A limited subset of scPerturb datasets</p> <p>Notably: - Perturb-seq was not a modeling target in GEM-1 v1 - scPerturb inclusion appears to be for representation exposure, not task dominance</p> <p>This restraint is important: it avoids overfitting the latent space to perturbational effects before the model can disentangle biology from technique.</p>"},{"location":"GEM-1/inside-gem-1-training-data/#the-hard-part-metadata-as-a-learning-problem","title":"The Hard Part: Metadata as a Learning Problem","text":"<p>Processing expression matrices was described as the easy part. The real challenge was metadata.</p>"},{"location":"GEM-1/inside-gem-1-training-data/#the-problem","title":"The Problem","text":"<p>Bulk RNA-seq metadata in SRA is: - free-form - inconsistently named - frequently missing - semantically ambiguous</p> <p>Example: The concept of \u201ctissue\u201d may appear as: - <code>tissue</code> - <code>body site</code> - <code>sampling_site</code> - <code>tisue</code> (typo) - or nowhere at all</p>"},{"location":"GEM-1/inside-gem-1-training-data/#the-solution-metadata-agents-speculative-but-strongly-implied","title":"The Solution: Metadata Agents (Speculative but Strongly Implied)","text":"<p>Synthesize Bio explicitly states they used LLMs + heuristics to extract and standardize metadata.</p> <p>This strongly suggests a pipeline resembling:</p> <ol> <li>Free-text ingestion (titles, abstracts, sample IDs)</li> <li>LLM-based field inference</li> <li>Ontology mapping</li> <li>Confidence scoring</li> <li>Human-in-the-loop validation (at least during early development)</li> </ol> <p>This is a critical idea for genai-lab:</p> <p>AI systems can be used to manufacture their own training supervision\u2014if carefully constrained.</p>"},{"location":"GEM-1/inside-gem-1-training-data/#harmonized-metadata-schema","title":"Harmonized Metadata Schema","text":"<p>Metadata fields were grouped into three semantically meaningful categories, each corresponding to a latent space learned by GEM-1.</p>"},{"location":"GEM-1/inside-gem-1-training-data/#biological-latent-space","title":"Biological Latent Space","text":"<ul> <li>Age</li> <li>Sex</li> <li>Tissue</li> <li>Cell type</li> <li>Cell line</li> <li>Disease</li> <li>Sample type (primary, cell line, xenograft)</li> </ul>"},{"location":"GEM-1/inside-gem-1-training-data/#perturbational-latent-space","title":"Perturbational Latent Space","text":"<ul> <li>Perturbation identity</li> <li>Type (genetic, compound, infection, etc.)</li> <li>Dose</li> <li>Time</li> </ul>"},{"location":"GEM-1/inside-gem-1-training-data/#technical-latent-space","title":"Technical Latent Space","text":"<p>Bulk: - Library selection - Library layout - Sequencing instrument</p> <p>Single-cell: - Assay type</p> <p>This separation is conceptually elegant: - Technical effects are modeled but isolated - Biological signals are preserved - Perturbations become transferable operators</p> <p>For synthetic data generation, this decomposition is crucial.</p>"},{"location":"GEM-1/inside-gem-1-training-data/#learning-from-missing-labels","title":"Learning from Missing Labels","text":"<p>One of GEM-1\u2019s quieter but most powerful ideas is self-completing metadata.</p> <p>Example: - Sex labels missing in ~175,000 bulk samples - GEM-1 predicts sex with &gt;99% agreement where ground truth exists - Validation via XIST expression confirms biological consistency</p> <p>This turns the model into: - a predictor - a label imputer - a dataset amplifier</p> <p>For genai-lab, this hints at a virtuous cycle:</p> <p>better representations \u2192 better labels \u2192 better representations</p>"},{"location":"GEM-1/inside-gem-1-training-data/#dataset-biases-explicitly-acknowledged","title":"Dataset Biases (Explicitly Acknowledged)","text":"<p>Rather than claiming neutrality, GEM-1 documents bias:</p> <ul> <li>Blood dominates bulk RNA-seq (accessibility bias)</li> <li>Brain tissue dominates single-cell datasets</li> <li>Cancer and COVID-19 are overrepresented</li> <li>Sex-linked disease prevalence reflects clinical reality</li> </ul> <p>This transparency matters. Synthetic data generation without bias accounting simply re-encodes historical sampling artifacts.</p>"},{"location":"GEM-1/inside-gem-1-training-data/#perturbation-coverage","title":"Perturbation Coverage","text":"<p>Perturbations are grouped into five types: - Genetic (CRISPR, RNAi, overexpression) - Compounds (drugs, toxins) - Biologics - Infections - Environmental / physiological factors</p> <p>Key insight: - Most perturbations occur in vitro - But every perturbation class appears in primary samples at non-trivial scale</p> <p>This enables controlled generalization:</p> <p>learning perturbation effects in cell lines while grounding them in human tissue data.</p>"},{"location":"GEM-1/inside-gem-1-training-data/#likely-model-level-implications-speculative","title":"Likely Model-Level Implications (Speculative)","text":"<p>While architectural details are not disclosed in the post, the data design strongly implies:</p> <ul> <li>Multi-head or factorized conditioning</li> <li>Separate encoders for metadata classes</li> <li>Latent disentanglement objectives</li> <li>Missing-data robustness by design</li> <li>Potential JEPA-style or masked modeling objectives</li> </ul> <p>In other words:</p> <p>The dataset is structured to force the model to learn causal-like structure\u2014even without explicit causal supervision.</p>"},{"location":"GEM-1/inside-gem-1-training-data/#lessons-for-genai-lab","title":"Lessons for genai-lab","text":"<ol> <li> <p>Training data is the model    Architecture matters less than semantic alignment.</p> </li> <li> <p>Metadata deserves modeling budgets    Treat metadata as signal, not annotation.</p> </li> <li> <p>Ontology mapping is representation learning    It shapes the geometry of latent space.</p> </li> <li> <p>Synthetic biology data requires disentanglement    Technical, biological, and perturbational axes must not collapse.</p> </li> <li> <p>LLMs are infrastructure, not just predictors    They can manufacture supervision at scale.</p> </li> </ol>"},{"location":"GEM-1/inside-gem-1-training-data/#looking-forward","title":"Looking Forward","text":"<p>Synthesize Bio hints at: - richer perturbation schemas - more powerful LLM-assisted curation - increased label density - expanded expression coverage</p> <p>GEM-2, if built on these principles, is likely less about raw parameter count and more about representational fidelity.</p> <p>For genai-lab, GEM-1 is best read not as a competitor\u2014but as a blueprint for how serious synthetic biology datasets are made.</p>"},{"location":"JEPA/","title":"JEPA Documentation","text":"<p>Joint Embedding Predictive Architecture (JEPA) \u2014 A self-supervised learning paradigm that learns by predicting in embedding space rather than reconstructing in data space.</p> <p>This documentation series covers JEPA from first principles through computational biology applications, with a focus on perturbation prediction and trajectory modeling.</p>"},{"location":"JEPA/#core-documentation-series","title":"Core Documentation Series","text":""},{"location":"JEPA/#1-overview","title":"1. Overview","text":"<p>00_jepa_overview.md \u2014 What is JEPA and why it matters - Core concepts: predict embeddings, not pixels - JEPA vs generative models vs contrastive learning - Joint latent spaces (Goku insight) - Why JEPA for biology - When to use JEPA vs generative models</p>"},{"location":"JEPA/#2-foundations","title":"2. Foundations","text":"<p>01_jepa_foundations.md \u2014 Architecture and components - Encoder architecture - Predictor design - VICReg regularization (variance, invariance, covariance) - Masking strategies - Complete PyTorch implementation</p>"},{"location":"JEPA/#3-training","title":"3. Training","text":"<p>02_jepa_training.md \u2014 Training strategies and best practices - Training loop - Loss computation - Hyperparameters - Optimization strategies - Debugging and monitoring - Advanced techniques</p>"},{"location":"JEPA/#4-applications","title":"4. Applications","text":"<p>03_jepa_applications.md \u2014 From vision to biology - I-JEPA (image masking) - V-JEPA (video prediction) - Bio-JEPA (perturbation prediction) - Multi-omics integration - Trajectory inference</p>"},{"location":"JEPA/#5-perturb-seq-application","title":"5. Perturb-seq Application","text":"<p>04_jepa_perturbseq.md \u2014 Detailed Perturb-seq implementation - Dataset preparation - Perturbation conditioning - Model architecture - Training pipeline - Evaluation metrics - Comparison with scGen/CPA</p>"},{"location":"JEPA/#supplementary-documents","title":"Supplementary Documents","text":""},{"location":"JEPA/#open-research","title":"Open Research","text":"<p>open_research_joint_latent.md \u2014 Joint latent spaces - Goku model insights - Static + dynamic data in one latent space - Patch n' Pack for variable-length sequences - Biology applications</p>"},{"location":"JEPA/#quick-navigation","title":"Quick Navigation","text":""},{"location":"JEPA/#for-different-audiences","title":"For Different Audiences","text":"<p>New to JEPA? 1. Start with Overview 2. Read Foundations for architecture 3. Try toy examples from Training</p> <p>Coming from Generative Models? 1. Read Overview comparison section 2. Understand why prediction \u2260 generation 3. Learn when to combine JEPA + diffusion</p> <p>Interested in Biology Applications? 1. Read Overview biology section 2. Jump to Applications 3. Deep dive into Perturb-seq</p> <p>Ready to Implement? 1. Review Foundations architecture 2. Follow Training pipeline 3. Adapt Perturb-seq code</p>"},{"location":"JEPA/#key-concepts","title":"Key Concepts","text":""},{"location":"JEPA/#what-makes-jepa-different","title":"What Makes JEPA Different","text":"<p>Traditional Generative Models (VAE, Diffusion): <pre><code>Input \u2192 Encoder \u2192 Latent \u2192 Decoder \u2192 Reconstruction\nLoss: ||x - x\u0302||\u00b2 (pixel-level)\n</code></pre></p> <p>JEPA: <pre><code>Context \u2192 Encoder \u2192 z_context\n                     \u2193\n                 Predictor \u2192 \u1e91_target\n                     \u2191\nTarget \u2192 Encoder \u2192 z_target\nLoss: ||z_target - \u1e91_target||\u00b2 (embedding-level)\n</code></pre></p> <p>Key advantages:</p> <ul> <li>No decoder (10-100\u00d7 faster)</li> <li>Semantic prediction (robust to noise)</li> <li>No contrastive negatives (simpler than SimCLR)</li> <li>Compositional reasoning (combine perturbations)</li> </ul>"},{"location":"JEPA/#core-components","title":"Core Components","text":"<p>1. Encoder: Maps inputs to embeddings - Shared across all inputs - Vision Transformer (ViT) for images - MLP/Transformer for gene expression</p> <p>2. Predictor: Predicts target embedding from context - Transformer-based - Conditioned on context (time, perturbation, etc.) - Learns relationships in embedding space</p> <p>3. VICReg Loss: Prevents collapse - Variance: Keep embeddings spread out - Invariance: Predictions match targets - Covariance: Decorrelate dimensions</p>"},{"location":"JEPA/#joint-latent-spaces","title":"Joint Latent Spaces","text":"<p>Insight from Goku (ByteDance, 2024):</p> <p>If two data types differ only by dimensionality or observation density, they want the same latent space.</p> <p>For biology:</p> <ul> <li>Bulk RNA-seq (static) + Time-series (dynamic) \u2192 Same latent space</li> <li>Static data teaches spatial priors (cell types, pathways)</li> <li>Dynamic data teaches temporal dynamics</li> <li>Both inform the same representation</li> </ul>"},{"location":"JEPA/#jepa-variants","title":"JEPA Variants","text":""},{"location":"JEPA/#i-jepa-image","title":"I-JEPA (Image)","text":"<p>Task: Predict masked image regions in embedding space</p> <p>Key innovation: Masking in embedding space, not pixel space</p> <p>Papers: Assran et al. (2023)</p>"},{"location":"JEPA/#v-jepa-video","title":"V-JEPA (Video)","text":"<p>Task: Predict future video frames in embedding space</p> <p>Key innovation: Temporal prediction without generation</p> <p>Papers: Bardes et al. (2024), Meta AI (2025)</p>"},{"location":"JEPA/#bio-jepa-proposed","title":"Bio-JEPA (Proposed)","text":"<p>Task: Predict perturbed/future cell states in embedding space</p> <p>Key innovation: Perturbation operators in latent space</p> <p>Applications:</p> <ul> <li>Perturb-seq prediction</li> <li>Trajectory inference</li> <li>Multi-omics translation</li> <li>Drug response prediction</li> </ul>"},{"location":"JEPA/#biology-applications","title":"Biology Applications","text":""},{"location":"JEPA/#1-perturbation-prediction-perturb-seq","title":"1. Perturbation Prediction (Perturb-seq)","text":"<p>Problem: Predict cellular response to genetic/chemical perturbations</p> <p>JEPA approach: <pre><code>z_baseline = encoder(x_baseline)\nz_pert = perturbation_encoder(perturbation_info)\nz_pred = predictor(z_baseline, z_pert)\nloss = ||z_pred - encoder(x_perturbed)||\u00b2\n</code></pre></p> <p>Advantages:</p> <ul> <li>No need to reconstruct all 20K genes</li> <li>Learn perturbation operators</li> <li>Compositional (combine perturbations)</li> <li>Efficient (no decoder)</li> </ul> <p>Datasets: Norman et al. (2019), Replogle et al. (2022)</p>"},{"location":"JEPA/#2-trajectory-inference","title":"2. Trajectory Inference","text":"<p>Problem: Predict developmental or disease trajectories</p> <p>JEPA approach: <pre><code>z_t = encoder(x_t)\nz_t1_pred = predictor(z_t, time_embedding)\nloss = ||z_t1_pred - encoder(x_t1)||\u00b2\n</code></pre></p> <p>Applications:</p> <ul> <li>Developmental biology</li> <li>Disease progression</li> <li>Drug response over time</li> </ul>"},{"location":"JEPA/#3-multi-omics-integration","title":"3. Multi-omics Integration","text":"<p>Problem: Predict one modality from another</p> <p>JEPA approach: <pre><code>z_rna = encoder_rna(x_rna)\nz_protein_pred = predictor(z_rna)\nloss = ||z_protein_pred - encoder_protein(x_protein)||\u00b2\n</code></pre></p> <p>Applications:</p> <ul> <li>RNA \u2192 Protein prediction</li> <li>ATAC \u2192 RNA prediction</li> <li>Cross-species translation</li> </ul>"},{"location":"JEPA/#4-drug-response-prediction","title":"4. Drug Response Prediction","text":"<p>Problem: Predict cellular response to drugs</p> <p>JEPA approach: <pre><code>z_baseline = encoder(x_baseline)\nz_drug = drug_encoder(drug_features)\nz_response = predictor(z_baseline, z_drug)\nloss = ||z_response - encoder(x_treated)||\u00b2\n</code></pre></p> <p>Applications:</p> <ul> <li>Drug screening</li> <li>Combination therapy</li> <li>Patient stratification</li> </ul>"},{"location":"JEPA/#comparison-with-other-methods","title":"Comparison with Other Methods","text":""},{"location":"JEPA/#jepa-vs-vae","title":"JEPA vs VAE","text":"Aspect VAE JEPA Objective Reconstruct input Predict target embedding Loss Pixel-level + KL Embedding-level + VICReg Decoder Required Not needed Speed Slow Fast (10-100\u00d7) Generation Yes No (need wrapper) Robustness Moderate High"},{"location":"JEPA/#jepa-vs-diffusion","title":"JEPA vs Diffusion","text":"Aspect Diffusion JEPA Objective Denoise/predict velocity Predict embedding Loss Pixel-level Embedding-level Sampling ODE/SDE (slow) Direct (fast) Generation Yes No (need wrapper) Uncertainty Via sampling Need wrapper Efficiency Moderate High"},{"location":"JEPA/#jepa-vs-contrastive-simclr","title":"JEPA vs Contrastive (SimCLR)","text":"Aspect SimCLR JEPA Objective Maximize agreement Predict embedding Negatives Required Not needed Loss Contrastive MSE + VICReg Complexity High (negative sampling) Low Prediction No Yes"},{"location":"JEPA/#jepa-vs-scgencpa-perturbation-models","title":"JEPA vs scGen/CPA (Perturbation Models)","text":"Aspect scGen/CPA JEPA Architecture VAE + arithmetic Encoder + Predictor Perturbation Latent arithmetic Learned operators Reconstruction Required Not needed Compositional Limited Natural Efficiency Moderate High"},{"location":"JEPA/#when-to-use-jepa","title":"When to Use JEPA","text":""},{"location":"JEPA/#use-jepa-when","title":"\u2705 Use JEPA When:","text":"<p>Prediction is the goal (not generation) - Perturbation prediction - Trajectory inference - Multi-omics translation</p> <p>Efficiency matters</p> <ul> <li>Large-scale datasets</li> <li>Limited compute</li> <li>Need fast training</li> </ul> <p>Robustness is critical</p> <ul> <li>Noisy data</li> <li>Batch effects</li> <li>Technical variation</li> </ul> <p>Compositional reasoning needed</p> <ul> <li>Combine perturbations</li> <li>Transfer across contexts</li> <li>Causal modeling</li> </ul>"},{"location":"JEPA/#use-generative-models-when","title":"\u274c Use Generative Models When:","text":"<p>Need actual samples</p> <ul> <li>Data augmentation</li> <li>Synthetic data generation</li> <li>Uncertainty quantification</li> </ul> <p>Reconstruction quality matters</p> <ul> <li>Image generation</li> <li>High-fidelity synthesis</li> </ul> <p>Distribution modeling is the goal</p> <ul> <li>Density estimation</li> <li>Anomaly detection</li> </ul>"},{"location":"JEPA/#best-hybrid-jepa-generative","title":"\ud83d\udd04 Best: Hybrid JEPA + Generative","text":"<p>Combine both: 1. JEPA learns dynamics efficiently 2. Generative model handles sampling 3. Get prediction + generation + uncertainty</p> <p>Example: JEPA + Diffusion <pre><code># JEPA predicts perturbed embedding\nz_pred = jepa_predictor(z_baseline, perturbation)\n\n# Diffusion generates samples from embedding\nx_samples = diffusion_decoder(z_pred, num_samples=100)\n\n# Get both prediction and uncertainty\n</code></pre></p>"},{"location":"JEPA/#implementation-roadmap","title":"Implementation Roadmap","text":""},{"location":"JEPA/#phase-1-basic-jepa","title":"Phase 1: Basic JEPA","text":"<ul> <li> Encoder architecture (ViT or MLP)</li> <li> Predictor network (Transformer)</li> <li> VICReg loss implementation</li> <li> Training loop</li> <li> Toy examples (MNIST, synthetic)</li> </ul>"},{"location":"JEPA/#phase-2-bio-jepa","title":"Phase 2: Bio-JEPA","text":"<ul> <li> Gene expression encoder</li> <li> Perturbation conditioning</li> <li> Perturb-seq dataset loader</li> <li> Training on Norman et al. data</li> <li> Evaluation metrics</li> </ul>"},{"location":"JEPA/#phase-3-joint-latent-spaces","title":"Phase 3: Joint Latent Spaces","text":"<ul> <li> Joint encoder for bulk + single-cell</li> <li> Static + dynamic data training</li> <li> Multi-omics integration</li> <li> Cross-dataset transfer</li> </ul>"},{"location":"JEPA/#phase-4-jepa-generative","title":"Phase 4: JEPA + Generative","text":"<ul> <li> Diffusion decoder</li> <li> Uncertainty quantification</li> <li> Sample generation</li> <li> Full predictive-generative system</li> </ul>"},{"location":"JEPA/#learning-path","title":"Learning Path","text":""},{"location":"JEPA/#beginner-path","title":"Beginner Path","text":"<ol> <li>Understand the concept \u2014 Overview</li> <li>Learn the architecture \u2014 Foundations</li> <li>Train on toy data \u2014 Training</li> <li>Explore applications \u2014 Applications</li> </ol>"},{"location":"JEPA/#intermediate-path","title":"Intermediate Path","text":"<ol> <li>Review architecture \u2014 Foundations</li> <li>Implement training \u2014 Training</li> <li>Apply to Perturb-seq \u2014 Perturb-seq</li> <li>Compare with baselines \u2014 Evaluate against scGen/CPA</li> </ol>"},{"location":"JEPA/#advanced-path","title":"Advanced Path","text":"<ol> <li>Joint latent spaces \u2014 Open Research</li> <li>Hybrid JEPA + Diffusion \u2014 Combine prediction and generation</li> <li>Multi-omics integration \u2014 Cross-modality prediction</li> <li>Novel applications \u2014 Extend to new biology problems</li> </ol>"},{"location":"JEPA/#related-documentation","title":"Related Documentation","text":""},{"location":"JEPA/#within-this-project","title":"Within This Project","text":"<p>Generative Models:</p> <ul> <li>DDPM \u2014 Denoising diffusion</li> <li>SDE \u2014 Stochastic differential equations</li> <li>Flow Matching \u2014 Rectified flow</li> <li>DiT \u2014 Diffusion transformers</li> <li>VAE \u2014 Variational autoencoders</li> </ul> <p>Architecture Choices:</p> <ul> <li>Gene Expression Architectures \u2014 Tokenization for biology</li> </ul> <p>Incubation:</p> <ul> <li>Joint Latent Spaces \u2014 Goku insights</li> </ul>"},{"location":"JEPA/#external-resources","title":"External Resources","text":"<p>JEPA Papers:</p> <ul> <li>LeCun (2022): \"A Path Towards Autonomous Machine Intelligence\"</li> <li>Assran et al. (2023): \"Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture\" (I-JEPA)</li> <li>Bardes et al. (2024): \"V-JEPA: Latent Video Prediction\"</li> <li>Meta AI (2025): \"V-JEPA 2: Understanding, Prediction, and Planning\"</li> </ul> <p>Joint Latent Spaces:</p> <ul> <li>ByteDance &amp; HKU (2024): \"Goku: Native Joint Image-Video Generation\"</li> </ul> <p>VICReg:</p> <ul> <li>Bardes et al. (2022): \"VICReg: Variance-Invariance-Covariance Regularization\"</li> </ul> <p>Biology Applications:</p> <ul> <li>Norman et al. (2019): \"Exploring genetic interaction manifolds\" (Perturb-seq)</li> <li>Lotfollahi et al. (2019): \"scGen predicts single-cell perturbation responses\"</li> <li>Roohani et al. (2023): \"Predicting transcriptional outcomes of novel multigene perturbations\" (GEARS)</li> </ul>"},{"location":"JEPA/#key-takeaways","title":"Key Takeaways","text":""},{"location":"JEPA/#conceptual","title":"Conceptual","text":"<ol> <li>Predict embeddings, not pixels \u2014 More efficient, more robust</li> <li>No reconstruction needed \u2014 Focus on semantic content</li> <li>No contrastive negatives \u2014 Simpler than SimCLR/MoCo</li> <li>World models without generation \u2014 Learn dynamics efficiently</li> <li>Joint latent spaces \u2014 Static and dynamic data train each other</li> </ol>"},{"location":"JEPA/#practical","title":"Practical","text":"<ol> <li>JEPA is not generative \u2014 Predicts embeddings, not samples</li> <li>VICReg prevents collapse \u2014 Variance + covariance regularization</li> <li>Powerful predictor needed \u2014 Transformer-based works well</li> <li>Combine with generative \u2014 For sampling and uncertainty</li> <li>Perfect for biology \u2014 Perturbations, trajectories, multi-omics</li> </ol>"},{"location":"JEPA/#for-computational-biology","title":"For Computational Biology","text":"<ol> <li>Perturb-seq is ideal \u2014 Predict perturbed states efficiently</li> <li>Efficiency matters \u2014 20K genes, millions of cells</li> <li>Robustness critical \u2014 Technical noise, batch effects</li> <li>Compositional reasoning \u2014 Combine perturbations naturally</li> <li>Hybrid approach best \u2014 JEPA + diffusion for full system</li> </ol>"},{"location":"JEPA/#getting-started","title":"Getting Started","text":"<p>Quick start: <pre><code># Read overview\ncat docs/JEPA/00_jepa_overview.md\n\n# Understand architecture\ncat docs/JEPA/01_jepa_foundations.md\n\n# See training examples\ncat docs/JEPA/02_jepa_training.md\n</code></pre></p> <p>For biology applications: <pre><code># Jump to applications\ncat docs/JEPA/03_jepa_applications.md\n\n# Deep dive into Perturb-seq\ncat docs/JEPA/04_jepa_perturbseq.md\n</code></pre></p> <p>For implementation: <pre><code># Check source code (when available)\nls src/genailab/jepa/\n\n# Run notebooks (when available)\nls notebooks/jepa/\n</code></pre></p>"},{"location":"JEPA/#status","title":"Status","text":"<p>Documentation: \ud83d\udea7 In Progress - [x] Overview - [ ] Foundations - [ ] Training - [ ] Applications - [ ] Perturb-seq - [ ] Open Research</p> <p>Implementation: \ud83d\udd32 Planned - [ ] Core JEPA modules - [ ] Training infrastructure - [ ] Perturb-seq application - [ ] Evaluation metrics</p> <p>Notebooks: \ud83d\udd32 Planned - [ ] Toy examples - [ ] Gene expression JEPA - [ ] Perturb-seq prediction - [ ] Comparison with baselines</p>"},{"location":"JEPA/00_jepa_overview/","title":"JEPA Overview: Joint Embedding Predictive Architecture","text":"<p>JEPA (Joint Embedding Predictive Architecture) is a self-supervised learning paradigm that learns by predicting in embedding space rather than reconstructing in pixel/data space.</p> <p>Key insight: Instead of generating outputs (like VAE/diffusion), JEPA predicts the latent representation of targets, enabling efficient learning of world models without expensive reconstruction.</p>"},{"location":"JEPA/00_jepa_overview/#what-is-jepa","title":"What is JEPA?","text":""},{"location":"JEPA/00_jepa_overview/#the-core-idea","title":"The Core Idea","text":"<p>Traditional generative models (VAE, diffusion): <pre><code>Input \u2192 Encoder \u2192 Latent \u2192 Decoder \u2192 Reconstruction\n                                    \u2193\n                              Loss: ||x - x\u0302||\u00b2\n</code></pre></p> <p>JEPA: <pre><code>Input x \u2192 Encoder \u2192 z_x\n                     \u2193\n                 Predictor \u2192 \u1e91_y\n                     \u2191\nTarget y \u2192 Encoder \u2192 z_y\n                     \u2193\n              Loss: ||z_y - \u1e91_y||\u00b2\n</code></pre></p> <p>Key difference: Predict embeddings, not pixels/counts.</p>"},{"location":"JEPA/00_jepa_overview/#why-this-matters","title":"Why This Matters","text":"<p>1. Computational efficiency</p> <ul> <li>No expensive decoder</li> <li>Prediction in low-dimensional latent space</li> <li>Faster training, less memory</li> </ul> <p>2. Better representations</p> <ul> <li>Focus on semantic content, not pixel details</li> <li>Invariant to nuisance factors</li> <li>More robust to noise</li> </ul> <p>3. World modeling</p> <ul> <li>Learn dynamics without generation</li> <li>Predict future states efficiently</li> <li>Enable planning and reasoning</li> </ul> <p>4. No contrastive negatives</p> <ul> <li>Unlike SimCLR, MoCo, CLIP</li> <li>No need to sample negative pairs</li> <li>Simpler training objective</li> </ul>"},{"location":"JEPA/00_jepa_overview/#jepa-vs-other-approaches","title":"JEPA vs Other Approaches","text":""},{"location":"JEPA/00_jepa_overview/#comparison-table","title":"Comparison Table","text":"Method What's Predicted Loss Type Negatives? Decoder? VAE Reconstruction Pixel-level No Yes (expensive) Diffusion Noise/velocity Pixel-level No Yes (expensive) Contrastive (SimCLR) Similarity Embedding Yes (required) No JEPA Embedding Embedding No No"},{"location":"JEPA/00_jepa_overview/#visual-comparison","title":"Visual Comparison","text":"<p>Generative (VAE/Diffusion): <pre><code>Learn: p(x|z) \u2014 \"How to generate x from z\"\nGoal: Reconstruct/generate realistic samples\nCost: High (decoder, pixel-level loss)\n</code></pre></p> <p>Contrastive (SimCLR/MoCo): <pre><code>Learn: Similarity in embedding space\nGoal: Pull positives together, push negatives apart\nCost: Moderate (need negative sampling)\n</code></pre></p> <p>JEPA: <pre><code>Learn: Predict z_y from z_x\nGoal: Model relationships in embedding space\nCost: Low (no decoder, no negatives)\n</code></pre></p>"},{"location":"JEPA/00_jepa_overview/#key-components","title":"Key Components","text":""},{"location":"JEPA/00_jepa_overview/#1-encoder","title":"1. Encoder","text":"<p>Maps inputs to embeddings:</p> \\[ z = f_\\theta(x) \\] <p>Shared across all inputs (images, videos, gene expression, etc.)</p>"},{"location":"JEPA/00_jepa_overview/#2-predictor","title":"2. Predictor","text":"<p>Predicts target embedding from context:</p> <p>$$</p> <p>\\hat{z}y = g\\phi(z_x, \\text{context}) $$</p> <p>Context can be: - Time (predict future from past) - Perturbation (predict perturbed state from baseline) - Masked regions (predict masked from visible)</p>"},{"location":"JEPA/00_jepa_overview/#3-vicreg-regularization","title":"3. VICReg Regularization","text":"<p>Prevents collapse via three terms:</p> <p>Variance: Keep embeddings spread out $$</p> <p>\\mathcal{L}_{\\text{var}} = \\sum_d \\max(0, \\gamma - \\sqrt{\\text{Var}(z_d) + \\epsilon}) $$</p> <p>Invariance: Predictions match targets $$</p> <p>\\mathcal{L}_{\\text{inv}} = | z_y - \\hat{z}_y |^2 $$</p> <p>Covariance: Decorrelate dimensions $$</p> <p>\\mathcal{L}{\\text{cov}} = \\sum(z_i, z_j)^2 $$} \\text{Cov</p> <p>Total loss:</p> <p>$$</p> <p>\\mathcal{L} = \\lambda_{\\text{inv}} \\mathcal{L}{\\text{inv}} + \\lambda}} \\mathcal{L{\\text{var}} + \\lambda $$}} \\mathcal{L}_{\\text{cov}</p>"},{"location":"JEPA/00_jepa_overview/#why-jepa-for-biology","title":"Why JEPA for Biology?","text":""},{"location":"JEPA/00_jepa_overview/#problem-with-generative-models-in-biology","title":"Problem with Generative Models in Biology","text":"<p>Reconstruction is often not the goal:</p> <ul> <li>We don't need to generate realistic gene expression</li> <li>We care about predictions (perturbations, trajectories)</li> <li>Pixel-level accuracy is overkill</li> </ul> <p>Example: Perturb-seq - Given: Baseline cell state + perturbation - Want: Predicted perturbed state - Don't need: Perfect reconstruction of all 20K genes</p>"},{"location":"JEPA/00_jepa_overview/#jepa-advantages-for-biology","title":"JEPA Advantages for Biology","text":"<p>1. Efficient perturbation modeling <pre><code>Baseline expression \u2192 Encoder \u2192 z_baseline\n                                    \u2193\nPerturbation info \u2192 Predictor \u2192 \u1e91_perturbed\n                                    \u2191\nActual perturbed \u2192 Encoder \u2192 z_perturbed\n                                    \u2193\n                            Loss: ||z - \u1e91||\u00b2\n</code></pre></p> <p>2. Natural for time-series <pre><code>x_t \u2192 Encoder \u2192 z_t\n                 \u2193\n             Predictor \u2192 \u1e91_{t+1}\n                 \u2191\nx_{t+1} \u2192 Encoder \u2192 z_{t+1}\n</code></pre></p> <p>3. Handles heterogeneity</p> <ul> <li>Cell-level predictions (not population average)</li> <li>Uncertainty in embedding space</li> <li>Robust to technical noise</li> </ul> <p>4. Compositional generalization</p> <ul> <li>Learn perturbation operators</li> <li>Combine multiple perturbations</li> <li>Transfer across cell types</li> </ul>"},{"location":"JEPA/00_jepa_overview/#joint-latent-spaces-the-goku-insight","title":"Joint Latent Spaces: The Goku Insight","text":""},{"location":"JEPA/00_jepa_overview/#the-problem-with-separate-models","title":"The Problem with Separate Models","text":"<p>Traditional approach:</p> <ul> <li>Bulk RNA-seq model (static)</li> <li>Time-series model (dynamic)</li> <li>Perturb-seq model (perturbations)</li> <li>Each has its own latent space</li> </ul> <p>Issues:</p> <ul> <li>No knowledge transfer</li> <li>Can't combine modalities</li> <li>Redundant learning</li> </ul>"},{"location":"JEPA/00_jepa_overview/#joint-latent-space-solution","title":"Joint Latent Space Solution","text":"<p>Key insight from Goku (ByteDance, 2024):</p> <p>If two data types differ only by dimensionality or observation density, they probably want the same latent space.</p> <p>For biology:</p> <ul> <li>Bulk RNA-seq = \"static image\" (baseline state)</li> <li>Time-series = \"video\" (temporal dynamics)</li> <li>Perturb-seq = \"video with interventions\"</li> <li>Single-cell snapshots = \"variable-length clips\"</li> </ul> <p>All map to the same latent manifold: <pre><code>Bulk RNA-seq \u2500\u2500\u2510\n               \u251c\u2500\u2500&gt; Shared Encoder \u2500\u2500&gt; Joint Latent Space\nTime-series \u2500\u2500\u2500\u2524\n               \u2502\nPerturb-seq \u2500\u2500\u2500\u2518\n</code></pre></p> <p>Benefits:</p> <ul> <li>Static data teaches spatial priors (cell types, pathways)</li> <li>Dynamic data teaches temporal dynamics</li> <li>Perturbation data teaches causal relationships</li> <li>All inform the same representation</li> </ul>"},{"location":"JEPA/00_jepa_overview/#jepa-variants","title":"JEPA Variants","text":""},{"location":"JEPA/00_jepa_overview/#i-jepa-image-jepa","title":"I-JEPA (Image JEPA)","text":"<p>Task: Predict masked image regions in embedding space</p> <pre><code>Visible patches \u2192 Encoder \u2192 z_visible\n                              \u2193\n                          Predictor \u2192 \u1e91_masked\n                              \u2191\nMasked patches \u2192 Encoder \u2192 z_masked\n</code></pre> <p>Key innovation: Masking in embedding space, not pixel space</p>"},{"location":"JEPA/00_jepa_overview/#v-jepa-video-jepa","title":"V-JEPA (Video JEPA)","text":"<p>Task: Predict future video frames in embedding space</p> <pre><code>Past frames \u2192 Encoder \u2192 z_past\n                         \u2193\n                     Predictor \u2192 \u1e91_future\n                         \u2191\nFuture frames \u2192 Encoder \u2192 z_future\n</code></pre> <p>V-JEPA 2 (Meta, 2025): Adds planning capabilities</p>"},{"location":"JEPA/00_jepa_overview/#a-jepa-audio-jepa","title":"A-JEPA (Audio JEPA)","text":"<p>Task: Predict masked audio segments in embedding space</p>"},{"location":"JEPA/00_jepa_overview/#bio-jepa-proposed","title":"Bio-JEPA (Proposed)","text":"<p>Task: Predict perturbed/future cell states in embedding space</p> <pre><code>Baseline + perturbation \u2192 Predictor \u2192 \u1e91_perturbed\n                                        \u2191\nActual perturbed state \u2192 Encoder \u2192 z_perturbed\n</code></pre>"},{"location":"JEPA/00_jepa_overview/#jepa-generative-models","title":"JEPA + Generative Models","text":""},{"location":"JEPA/00_jepa_overview/#hybrid-architecture","title":"Hybrid Architecture","text":"<p>JEPA alone doesn't generate samples \u2014 it predicts embeddings.</p> <p>To generate, combine with generative model:</p> <pre><code>JEPA: x \u2192 z \u2192 Predictor \u2192 \u1e91\n                           \u2193\nGenerative: \u1e91 \u2192 Decoder/Diffusion \u2192 x\u0302\n</code></pre> <p>Example: JEPA + Diffusion 1. JEPA predicts perturbed embedding 2. Diffusion generates samples from that embedding 3. Get both prediction AND uncertainty quantification</p> <p>Benefits:</p> <ul> <li>JEPA learns dynamics efficiently</li> <li>Generative model handles distribution</li> <li>Best of both worlds</li> </ul>"},{"location":"JEPA/00_jepa_overview/#applications-in-computational-biology","title":"Applications in Computational Biology","text":""},{"location":"JEPA/00_jepa_overview/#1-perturbation-prediction-perturb-seq","title":"1. Perturbation Prediction (Perturb-seq)","text":"<p>Task: Predict cellular response to genetic/chemical perturbations</p> <p>JEPA approach: <pre><code># Baseline cell\nz_baseline = encoder(x_baseline)\n\n# Perturbation embedding\nz_pert = perturbation_encoder(perturbation_info)\n\n# Predict perturbed state\nz_pred = predictor(z_baseline, z_pert)\n\n# Compare to actual\nz_actual = encoder(x_perturbed)\nloss = ||z_pred - z_actual||\u00b2\n</code></pre></p> <p>Advantages:</p> <ul> <li>No need to reconstruct all 20K genes</li> <li>Learn perturbation operators</li> <li>Compositional (combine perturbations)</li> </ul>"},{"location":"JEPA/00_jepa_overview/#2-trajectory-inference","title":"2. Trajectory Inference","text":"<p>Task: Predict developmental or disease trajectories</p> <p>JEPA approach: <pre><code># Current state\nz_t = encoder(x_t)\n\n# Time embedding\nt_emb = time_encoder(t)\n\n# Predict future\nz_t1 = predictor(z_t, t_emb)\n\n# Compare to actual\nz_actual = encoder(x_t1)\nloss = ||z_t1 - z_actual||\u00b2\n</code></pre></p>"},{"location":"JEPA/00_jepa_overview/#3-multi-omics-integration","title":"3. Multi-omics Integration","text":"<p>Task: Predict one modality from another</p> <p>JEPA approach: <pre><code># RNA-seq\nz_rna = encoder_rna(x_rna)\n\n# Predict protein\nz_protein_pred = predictor(z_rna)\n\n# Compare to actual\nz_protein = encoder_protein(x_protein)\nloss = ||z_protein_pred - z_protein||\u00b2\n</code></pre></p>"},{"location":"JEPA/00_jepa_overview/#4-drug-response-prediction","title":"4. Drug Response Prediction","text":"<p>Task: Predict cellular response to drugs</p> <p>JEPA approach: <pre><code># Baseline + drug\nz_baseline = encoder(x_baseline)\nz_drug = drug_encoder(drug_features)\n\n# Predict response\nz_response = predictor(z_baseline, z_drug)\n\n# Compare to actual\nz_actual = encoder(x_treated)\nloss = ||z_response - z_actual||\u00b2\n</code></pre></p>"},{"location":"JEPA/00_jepa_overview/#key-advantages-for-biology","title":"Key Advantages for Biology","text":""},{"location":"JEPA/00_jepa_overview/#1-efficiency","title":"1. Efficiency","text":"<p>Generative models:</p> <ul> <li>Train decoder on 20K genes</li> <li>Pixel-level reconstruction loss</li> <li>Slow, memory-intensive</li> </ul> <p>JEPA:</p> <ul> <li>No decoder</li> <li>Embedding-level loss (e.g., 256-dim)</li> <li>Fast, memory-efficient</li> </ul> <p>Speedup: 10-100\u00d7 faster training</p>"},{"location":"JEPA/00_jepa_overview/#2-robustness","title":"2. Robustness","text":"<p>Generative models:</p> <ul> <li>Sensitive to technical noise</li> <li>Must model all variation</li> <li>Overfits to batch effects</li> </ul> <p>JEPA:</p> <ul> <li>Focuses on semantic content</li> <li>Invariant to nuisance factors</li> <li>More robust representations</li> </ul>"},{"location":"JEPA/00_jepa_overview/#3-interpretability","title":"3. Interpretability","text":"<p>Generative models:</p> <ul> <li>Black box decoder</li> <li>Hard to interpret latents</li> </ul> <p>JEPA:</p> <ul> <li>Direct prediction in embedding space</li> <li>Can probe what's being predicted</li> <li>Easier to analyze learned representations</li> </ul>"},{"location":"JEPA/00_jepa_overview/#4-compositional-generalization","title":"4. Compositional Generalization","text":"<p>Generative models:</p> <ul> <li>Learn p(x|condition)</li> <li>Hard to combine conditions</li> </ul> <p>JEPA:</p> <ul> <li>Learn perturbation operators</li> <li>Naturally compositional</li> <li>Combine multiple perturbations</li> </ul>"},{"location":"JEPA/00_jepa_overview/#limitations-and-challenges","title":"Limitations and Challenges","text":""},{"location":"JEPA/00_jepa_overview/#1-no-direct-generation","title":"1. No Direct Generation","text":"<p>JEPA predicts embeddings, not samples.</p> <p>Solution: Combine with generative model (VAE, diffusion) for sampling.</p>"},{"location":"JEPA/00_jepa_overview/#2-embedding-collapse","title":"2. Embedding Collapse","text":"<p>Without regularization, embeddings can collapse to constant.</p> <p>Solution: VICReg regularization (variance + covariance terms).</p>"},{"location":"JEPA/00_jepa_overview/#3-predictor-capacity","title":"3. Predictor Capacity","text":"<p>Predictor must be powerful enough to model relationships.</p> <p>Solution: Use transformer-based predictors with sufficient capacity.</p>"},{"location":"JEPA/00_jepa_overview/#4-evaluation","title":"4. Evaluation","text":"<p>How do you evaluate embedding predictions?</p> <p>Solutions:</p> <ul> <li>Downstream task performance</li> <li>Embedding similarity metrics</li> <li>Probe classifiers</li> <li>Generate samples and evaluate</li> </ul>"},{"location":"JEPA/00_jepa_overview/#when-to-use-jepa","title":"When to Use JEPA","text":""},{"location":"JEPA/00_jepa_overview/#use-jepa-when","title":"Use JEPA When:","text":"<p>\u2705 Prediction is the goal (not generation) - Perturbation prediction - Trajectory inference - Multi-omics translation</p> <p>\u2705 Efficiency matters - Large-scale datasets - Limited compute - Need fast training</p> <p>\u2705 Robustness is critical - Noisy data - Batch effects - Technical variation</p> <p>\u2705 Compositional reasoning needed - Combine perturbations - Transfer across contexts - Causal modeling</p>"},{"location":"JEPA/00_jepa_overview/#use-generative-models-when","title":"Use Generative Models When:","text":"<p>\u274c Need actual samples - Data augmentation - Synthetic data generation - Uncertainty quantification</p> <p>\u274c Reconstruction quality matters - Image generation - High-fidelity synthesis</p> <p>\u274c Distribution modeling is the goal - Density estimation - Anomaly detection</p>"},{"location":"JEPA/00_jepa_overview/#best-hybrid-jepa-generative","title":"Best: Hybrid JEPA + Generative","text":"<p>Combine both: 1. JEPA learns dynamics efficiently 2. Generative model handles sampling 3. Get prediction + generation + uncertainty</p>"},{"location":"JEPA/00_jepa_overview/#the-path-forward","title":"The Path Forward","text":""},{"location":"JEPA/00_jepa_overview/#stage-1-basic-jepa-current","title":"Stage 1: Basic JEPA (Current)","text":"<ul> <li>Understand architecture</li> <li>Implement encoder + predictor</li> <li>VICReg regularization</li> <li>Toy examples</li> </ul>"},{"location":"JEPA/00_jepa_overview/#stage-2-bio-jepa","title":"Stage 2: Bio-JEPA","text":"<ul> <li>Apply to Perturb-seq</li> <li>Perturbation conditioning</li> <li>Evaluate on held-out perturbations</li> </ul>"},{"location":"JEPA/00_jepa_overview/#stage-3-joint-latent-spaces","title":"Stage 3: Joint Latent Spaces","text":"<ul> <li>Combine bulk + single-cell</li> <li>Static + dynamic data</li> <li>Multi-omics integration</li> </ul>"},{"location":"JEPA/00_jepa_overview/#stage-4-jepa-generative","title":"Stage 4: JEPA + Generative","text":"<ul> <li>Add diffusion decoder</li> <li>Uncertainty quantification</li> <li>Full predictive-generative system</li> </ul>"},{"location":"JEPA/00_jepa_overview/#key-takeaways","title":"Key Takeaways","text":""},{"location":"JEPA/00_jepa_overview/#conceptual","title":"Conceptual","text":"<ol> <li>Predict embeddings, not pixels \u2014 more efficient, more robust</li> <li>No reconstruction needed \u2014 focus on semantic content</li> <li>No contrastive negatives \u2014 simpler than SimCLR/MoCo</li> <li>World models without generation \u2014 learn dynamics efficiently</li> <li>Joint latent spaces \u2014 static and dynamic data train each other</li> </ol>"},{"location":"JEPA/00_jepa_overview/#practical","title":"Practical","text":"<ol> <li>JEPA is not generative \u2014 predicts embeddings, not samples</li> <li>VICReg prevents collapse \u2014 variance + covariance regularization</li> <li>Powerful predictor needed \u2014 transformer-based works well</li> <li>Combine with generative \u2014 for sampling and uncertainty</li> <li>Perfect for biology \u2014 perturbations, trajectories, multi-omics</li> </ol>"},{"location":"JEPA/00_jepa_overview/#for-computational-biology","title":"For Computational Biology","text":"<ol> <li>Perturb-seq is ideal use case \u2014 predict perturbed states</li> <li>Efficiency matters \u2014 20K genes, millions of cells</li> <li>Robustness critical \u2014 technical noise, batch effects</li> <li>Compositional reasoning \u2014 combine perturbations</li> <li>Hybrid approach best \u2014 JEPA + diffusion for full system</li> </ol>"},{"location":"JEPA/00_jepa_overview/#related-documents","title":"Related Documents","text":"<ul> <li>01_jepa_foundations.md \u2014 Architecture details</li> <li>02_jepa_training.md \u2014 Training strategies</li> <li>03_jepa_applications.md \u2014 Vision to biology mapping</li> <li>04_jepa_perturbseq.md \u2014 Perturb-seq application</li> <li>Joint Latent Spaces \u2014 Goku insights</li> </ul>"},{"location":"JEPA/00_jepa_overview/#references","title":"References","text":"<p>JEPA papers:</p> <ul> <li>LeCun, \"A Path Towards Autonomous Machine Intelligence\" (2022)</li> <li>Assran et al., \"Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture\" (I-JEPA, 2023)</li> <li>Bardes et al., \"V-JEPA: Latent Video Prediction for Visual Representation Learning\" (2024)</li> <li>Meta AI, \"V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction, and Planning\" (2025)</li> </ul> <p>Joint latent spaces:</p> <ul> <li>ByteDance &amp; HKU, \"Goku: Native Joint Image-Video Generation\" (2024)</li> </ul> <p>VICReg:</p> <ul> <li>Bardes et al., \"VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning\" (2022)</li> </ul> <p>Biology applications (to be developed): - Perturb-seq prediction - Trajectory inference - Multi-omics integration</p>"},{"location":"JEPA/01_jepa_foundations/","title":"JEPA Foundations: Architecture and Components","text":"<p>This document covers the detailed architecture of Joint Embedding Predictive Architecture (JEPA), including encoder design, predictor networks, VICReg regularization, and complete PyTorch implementations.</p> <p>Prerequisites: Understanding of JEPA overview, transformers, and self-supervised learning.</p>"},{"location":"JEPA/01_jepa_foundations/#architecture-overview","title":"Architecture Overview","text":""},{"location":"JEPA/01_jepa_foundations/#high-level-structure","title":"High-Level Structure","text":"<p>JEPA consists of three main components:</p> <pre><code>Context x \u2192 Encoder f_\u03b8 \u2192 z_x\n                            \u2193\n                        Predictor g_\u03c6 \u2192 \u1e91_y\n                            \u2191\nTarget y \u2192 Encoder f_\u03b8 \u2192 z_y\n                            \u2193\n                    Loss: ||z_y - \u1e91_y||\u00b2 + VICReg\n</code></pre> <p>Key principles: 1. Shared encoder \u2014 Same encoder for context and target 2. Predictor \u2014 Learns to predict target embedding from context 3. No decoder \u2014 Prediction in embedding space only 4. VICReg regularization \u2014 Prevents collapse</p>"},{"location":"JEPA/01_jepa_foundations/#1-encoder-architecture","title":"1. Encoder Architecture","text":""},{"location":"JEPA/01_jepa_foundations/#11-vision-transformer-vit-encoder","title":"1.1 Vision Transformer (ViT) Encoder","text":"<p>For images/spatial data:</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ViTEncoder(nn.Module):\n    \"\"\"\n    Vision Transformer encoder for JEPA.\n\n    Args:\n        img_size: Input image size (H, W)\n        patch_size: Size of each patch\n        in_channels: Number of input channels\n        embed_dim: Embedding dimension\n        depth: Number of transformer layers\n        num_heads: Number of attention heads\n        mlp_ratio: MLP hidden dim ratio\n    \"\"\"\n    def __init__(\n        self,\n        img_size=(224, 224),\n        patch_size=16,\n        in_channels=3,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4.0,\n    ):\n        super().__init__()\n\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = (img_size[0] // patch_size) * (img_size[1] // patch_size)\n\n        # Patch embedding\n        self.patch_embed = nn.Conv2d(\n            in_channels,\n            embed_dim,\n            kernel_size=patch_size,\n            stride=patch_size\n        )\n\n        # Positional embedding\n        self.pos_embed = nn.Parameter(\n            torch.randn(1, self.num_patches, embed_dim) * 0.02\n        )\n\n        # Transformer blocks\n        self.blocks = nn.ModuleList([\n            TransformerBlock(\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio\n            )\n            for _ in range(depth)\n        ])\n\n        # Layer norm\n        self.norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, x, mask=None):\n        \"\"\"\n        Args:\n            x: Images (B, C, H, W)\n            mask: Optional mask for patches (B, num_patches)\n\n        Returns:\n            z: Embeddings (B, num_patches, embed_dim)\n        \"\"\"\n        # Patch embedding\n        x = self.patch_embed(x)  # (B, embed_dim, H//patch_size, W//patch_size)\n        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\n\n        # Add positional embedding\n        x = x + self.pos_embed\n\n        # Apply mask if provided\n        if mask is not None:\n            x = x * mask.unsqueeze(-1)\n\n        # Transformer blocks\n        for block in self.blocks:\n            x = block(x)\n\n        # Final norm\n        x = self.norm(x)\n\n        return x\n\n\nclass TransformerBlock(nn.Module):\n    \"\"\"Standard transformer block.\"\"\"\n    def __init__(self, dim, num_heads, mlp_ratio=4.0):\n        super().__init__()\n\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n\n        self.norm2 = nn.LayerNorm(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, mlp_hidden_dim),\n            nn.GELU(),\n            nn.Linear(mlp_hidden_dim, dim)\n        )\n\n    def forward(self, x):\n        # Attention\n        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n\n        # MLP\n        x = x + self.mlp(self.norm2(x))\n\n        return x\n</code></pre>"},{"location":"JEPA/01_jepa_foundations/#12-mlp-encoder-for-gene-expression","title":"1.2 MLP Encoder for Gene Expression","text":"<p>For tabular/gene expression data:</p> <pre><code>class GeneExpressionEncoder(nn.Module):\n    \"\"\"\n    MLP encoder for gene expression data.\n\n    Args:\n        num_genes: Number of genes\n        embed_dim: Embedding dimension\n        hidden_dims: List of hidden layer dimensions\n        num_tokens: Number of output tokens\n    \"\"\"\n    def __init__(\n        self,\n        num_genes=20000,\n        embed_dim=256,\n        hidden_dims=[2048, 1024],\n        num_tokens=64,\n    ):\n        super().__init__()\n\n        self.num_genes = num_genes\n        self.embed_dim = embed_dim\n        self.num_tokens = num_tokens\n\n        # MLP layers\n        layers = []\n        in_dim = num_genes\n        for hidden_dim in hidden_dims:\n            layers.extend([\n                nn.Linear(in_dim, hidden_dim),\n                nn.LayerNorm(hidden_dim),\n                nn.GELU(),\n                nn.Dropout(0.1)\n            ])\n            in_dim = hidden_dim\n\n        # Final projection to tokens\n        layers.append(nn.Linear(in_dim, num_tokens * embed_dim))\n\n        self.encoder = nn.Sequential(*layers)\n\n        # Positional embedding for tokens\n        self.pos_embed = nn.Parameter(\n            torch.randn(1, num_tokens, embed_dim) * 0.02\n        )\n\n        # Optional: Transformer layers on tokens\n        self.token_transformer = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads=8)\n            for _ in range(4)\n        ])\n\n        self.norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Gene expression (B, num_genes)\n\n        Returns:\n            z: Token embeddings (B, num_tokens, embed_dim)\n        \"\"\"\n        batch_size = x.shape[0]\n\n        # Encode to flat representation\n        z = self.encoder(x)  # (B, num_tokens * embed_dim)\n\n        # Reshape to tokens\n        z = z.view(batch_size, self.num_tokens, self.embed_dim)\n\n        # Add positional embedding\n        z = z + self.pos_embed\n\n        # Transformer on tokens\n        for block in self.token_transformer:\n            z = block(z)\n\n        # Final norm\n        z = self.norm(z)\n\n        return z\n</code></pre>"},{"location":"JEPA/01_jepa_foundations/#13-encoder-design-choices","title":"1.3 Encoder Design Choices","text":"<p>Key decisions:</p> Choice Options Recommendation Architecture ViT, CNN, MLP ViT for images, MLP for gene expression Depth 6-24 layers 12 for images, 4-8 for gene expression Embed dim 256-1024 768 for images, 256-512 for gene expression Normalization LayerNorm, BatchNorm LayerNorm (more stable) Activation GELU, ReLU GELU (smoother gradients)"},{"location":"JEPA/01_jepa_foundations/#2-predictor-architecture","title":"2. Predictor Architecture","text":""},{"location":"JEPA/01_jepa_foundations/#21-transformer-predictor","title":"2.1 Transformer Predictor","text":"<p>Standard predictor for JEPA:</p> <pre><code>class JEPAPredictor(nn.Module):\n    \"\"\"\n    Predictor network for JEPA.\n\n    Predicts target embeddings from context embeddings.\n\n    Args:\n        embed_dim: Embedding dimension\n        depth: Number of transformer layers\n        num_heads: Number of attention heads\n        mlp_ratio: MLP hidden dim ratio\n    \"\"\"\n    def __init__(\n        self,\n        embed_dim=768,\n        depth=6,\n        num_heads=12,\n        mlp_ratio=4.0,\n    ):\n        super().__init__()\n\n        self.embed_dim = embed_dim\n\n        # Transformer blocks\n        self.blocks = nn.ModuleList([\n            TransformerBlock(\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio\n            )\n            for _ in range(depth)\n        ])\n\n        self.norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, z_context, target_positions=None):\n        \"\"\"\n        Args:\n            z_context: Context embeddings (B, num_context, embed_dim)\n            target_positions: Optional target position embeddings\n\n        Returns:\n            z_pred: Predicted target embeddings (B, num_targets, embed_dim)\n        \"\"\"\n        # Process context\n        z = z_context\n\n        for block in self.blocks:\n            z = block(z)\n\n        z = self.norm(z)\n\n        return z\n</code></pre>"},{"location":"JEPA/01_jepa_foundations/#22-conditional-predictor-for-perturbations","title":"2.2 Conditional Predictor (for Perturbations)","text":"<p>Predictor with perturbation conditioning:</p> <pre><code>class ConditionalPredictor(nn.Module):\n    \"\"\"\n    Predictor with conditioning (e.g., perturbation, time).\n\n    Args:\n        embed_dim: Embedding dimension\n        condition_dim: Dimension of condition embedding\n        depth: Number of transformer layers\n        num_heads: Number of attention heads\n    \"\"\"\n    def __init__(\n        self,\n        embed_dim=256,\n        condition_dim=128,\n        depth=6,\n        num_heads=8,\n    ):\n        super().__init__()\n\n        self.embed_dim = embed_dim\n\n        # Condition embedding\n        self.condition_proj = nn.Sequential(\n            nn.Linear(condition_dim, embed_dim),\n            nn.LayerNorm(embed_dim),\n            nn.GELU()\n        )\n\n        # Cross-attention: context attends to condition\n        self.cross_attn_blocks = nn.ModuleList([\n            CrossAttentionBlock(embed_dim, num_heads)\n            for _ in range(depth)\n        ])\n\n        # Self-attention on context\n        self.self_attn_blocks = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads)\n            for _ in range(depth)\n        ])\n\n        self.norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, z_context, condition):\n        \"\"\"\n        Args:\n            z_context: Context embeddings (B, num_tokens, embed_dim)\n            condition: Condition vector (B, condition_dim)\n\n        Returns:\n            z_pred: Predicted embeddings (B, num_tokens, embed_dim)\n        \"\"\"\n        # Project condition\n        cond_emb = self.condition_proj(condition)  # (B, embed_dim)\n        cond_emb = cond_emb.unsqueeze(1)  # (B, 1, embed_dim)\n\n        z = z_context\n\n        # Alternate cross-attention and self-attention\n        for cross_block, self_block in zip(self.cross_attn_blocks, self.self_attn_blocks):\n            # Cross-attention: context attends to condition\n            z = cross_block(z, cond_emb)\n\n            # Self-attention on context\n            z = self_block(z)\n\n        z = self.norm(z)\n\n        return z\n\n\nclass CrossAttentionBlock(nn.Module):\n    \"\"\"Cross-attention block.\"\"\"\n    def __init__(self, dim, num_heads):\n        super().__init__()\n\n        self.norm_q = nn.LayerNorm(dim)\n        self.norm_kv = nn.LayerNorm(dim)\n        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n\n        self.norm_mlp = nn.LayerNorm(dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, dim * 4),\n            nn.GELU(),\n            nn.Linear(dim * 4, dim)\n        )\n\n    def forward(self, q, kv):\n        \"\"\"\n        Args:\n            q: Query (B, num_q, dim)\n            kv: Key/Value (B, num_kv, dim)\n\n        Returns:\n            out: (B, num_q, dim)\n        \"\"\"\n        # Cross-attention\n        q_norm = self.norm_q(q)\n        kv_norm = self.norm_kv(kv)\n        attn_out = self.attn(q_norm, kv_norm, kv_norm)[0]\n        q = q + attn_out\n\n        # MLP\n        q = q + self.mlp(self.norm_mlp(q))\n\n        return q\n</code></pre>"},{"location":"JEPA/01_jepa_foundations/#23-predictor-design-choices","title":"2.3 Predictor Design Choices","text":"Choice Options Recommendation Depth 4-12 layers 6 for images, 4-6 for gene expression Capacity Same as encoder, smaller Smaller (0.5\u00d7 encoder depth) Conditioning None, cross-attention, FiLM Cross-attention for perturbations Output Direct, with projection Direct (same dim as encoder)"},{"location":"JEPA/01_jepa_foundations/#3-vicreg-regularization","title":"3. VICReg Regularization","text":""},{"location":"JEPA/01_jepa_foundations/#31-the-collapse-problem","title":"3.1 The Collapse Problem","text":"<p>Without regularization, embeddings collapse: - All embeddings \u2192 same vector - Predictor learns trivial solution - No useful representation</p> <p>VICReg prevents collapse via three terms: 1. Variance: Keep embeddings spread out 2. Invariance: Predictions match targets 3. Covariance: Decorrelate dimensions</p>"},{"location":"JEPA/01_jepa_foundations/#32-vicreg-loss-implementation","title":"3.2 VICReg Loss Implementation","text":"<pre><code>class VICRegLoss(nn.Module):\n    \"\"\"\n    VICReg loss: Variance-Invariance-Covariance Regularization.\n\n    Args:\n        lambda_inv: Weight for invariance term\n        lambda_var: Weight for variance term\n        lambda_cov: Weight for covariance term\n        gamma: Target variance\n        epsilon: Small constant for numerical stability\n    \"\"\"\n    def __init__(\n        self,\n        lambda_inv=25.0,\n        lambda_var=25.0,\n        lambda_cov=1.0,\n        gamma=1.0,\n        epsilon=1e-4,\n    ):\n        super().__init__()\n\n        self.lambda_inv = lambda_inv\n        self.lambda_var = lambda_var\n        self.lambda_cov = lambda_cov\n        self.gamma = gamma\n        self.epsilon = epsilon\n\n    def forward(self, z_pred, z_target):\n        \"\"\"\n        Args:\n            z_pred: Predicted embeddings (B, num_tokens, embed_dim)\n            z_target: Target embeddings (B, num_tokens, embed_dim)\n\n        Returns:\n            loss: Total VICReg loss\n            loss_dict: Dictionary with individual losses\n        \"\"\"\n        batch_size, num_tokens, embed_dim = z_pred.shape\n\n        # Flatten tokens\n        z_pred = z_pred.reshape(-1, embed_dim)  # (B*num_tokens, embed_dim)\n        z_target = z_target.reshape(-1, embed_dim)\n\n        # 1. Invariance loss: MSE between predictions and targets\n        loss_inv = F.mse_loss(z_pred, z_target)\n\n        # 2. Variance loss: Ensure embeddings have sufficient variance\n        loss_var = self.variance_loss(z_pred) + self.variance_loss(z_target)\n\n        # 3. Covariance loss: Decorrelate dimensions\n        loss_cov = self.covariance_loss(z_pred) + self.covariance_loss(z_target)\n\n        # Total loss\n        loss = (\n            self.lambda_inv * loss_inv +\n            self.lambda_var * loss_var +\n            self.lambda_cov * loss_cov\n        )\n\n        loss_dict = {\n            'loss': loss.item(),\n            'inv': loss_inv.item(),\n            'var': loss_var.item(),\n            'cov': loss_cov.item(),\n        }\n\n        return loss, loss_dict\n\n    def variance_loss(self, z):\n        \"\"\"\n        Variance loss: Penalize if variance is below gamma.\n\n        Args:\n            z: Embeddings (N, D)\n\n        Returns:\n            loss: Variance loss\n        \"\"\"\n        # Compute variance along batch dimension\n        std = torch.sqrt(z.var(dim=0) + self.epsilon)  # (D,)\n\n        # Penalize if std &lt; gamma\n        loss = torch.mean(F.relu(self.gamma - std))\n\n        return loss\n\n    def covariance_loss(self, z):\n        \"\"\"\n        Covariance loss: Decorrelate dimensions.\n\n        Args:\n            z: Embeddings (N, D)\n\n        Returns:\n            loss: Covariance loss\n        \"\"\"\n        N, D = z.shape\n\n        # Center embeddings\n        z = z - z.mean(dim=0)\n\n        # Covariance matrix\n        cov = (z.T @ z) / (N - 1)  # (D, D)\n\n        # Off-diagonal elements should be zero\n        off_diag = cov.flatten()[:-1].view(D - 1, D + 1)[:, 1:].flatten()\n        loss = off_diag.pow(2).sum() / D\n\n        return loss\n</code></pre>"},{"location":"JEPA/01_jepa_foundations/#33-vicreg-hyperparameters","title":"3.3 VICReg Hyperparameters","text":"<p>Typical values:</p> Parameter Value Notes <code>lambda_inv</code> 25.0 Invariance weight <code>lambda_var</code> 25.0 Variance weight <code>lambda_cov</code> 1.0 Covariance weight <code>gamma</code> 1.0 Target std <code>epsilon</code> 1e-4 Numerical stability <p>Tuning guidelines:</p> <ul> <li>Increase <code>lambda_var</code> if embeddings collapse</li> <li>Increase <code>lambda_cov</code> if dimensions are correlated</li> <li>Decrease <code>lambda_inv</code> if predictions are too rigid</li> </ul>"},{"location":"JEPA/01_jepa_foundations/#4-masking-strategies","title":"4. Masking Strategies","text":""},{"location":"JEPA/01_jepa_foundations/#41-random-block-masking-i-jepa","title":"4.1 Random Block Masking (I-JEPA)","text":"<p>For images: Mask random blocks</p> <pre><code>def random_block_mask(\n    num_patches,\n    num_blocks=4,\n    block_aspect_ratio=(0.75, 1.5),\n    block_scale=(0.15, 0.2),\n):\n    \"\"\"\n    Generate random block mask for I-JEPA.\n\n    Args:\n        num_patches: Total number of patches\n        num_blocks: Number of blocks to mask\n        block_aspect_ratio: Range of aspect ratios\n        block_scale: Range of block scales (fraction of image)\n\n    Returns:\n        mask: Binary mask (num_patches,)\n        target_indices: Indices of masked patches\n    \"\"\"\n    H = W = int(num_patches ** 0.5)\n    mask = torch.ones(H, W)\n    target_indices = []\n\n    for _ in range(num_blocks):\n        # Sample block size\n        scale = torch.rand(1) * (block_scale[1] - block_scale[0]) + block_scale[0]\n        aspect = torch.rand(1) * (block_aspect_ratio[1] - block_aspect_ratio[0]) + block_aspect_ratio[0]\n\n        h = int((scale * H * W / aspect) ** 0.5)\n        w = int(h * aspect)\n\n        # Sample block position\n        top = torch.randint(0, H - h + 1, (1,)).item()\n        left = torch.randint(0, W - w + 1, (1,)).item()\n\n        # Mask block\n        mask[top:top+h, left:left+w] = 0\n\n        # Record target indices\n        for i in range(top, top+h):\n            for j in range(left, left+w):\n                target_indices.append(i * W + j)\n\n    mask = mask.flatten()\n    target_indices = torch.tensor(target_indices)\n\n    return mask, target_indices\n</code></pre>"},{"location":"JEPA/01_jepa_foundations/#42-temporal-masking-v-jepa","title":"4.2 Temporal Masking (V-JEPA)","text":"<p>For videos/time-series: Predict future from past</p> <pre><code>def temporal_mask(num_frames, context_frames=4, target_frames=4):\n    \"\"\"\n    Generate temporal mask for V-JEPA.\n\n    Args:\n        num_frames: Total number of frames\n        context_frames: Number of context frames\n        target_frames: Number of target frames\n\n    Returns:\n        context_indices: Indices of context frames\n        target_indices: Indices of target frames\n    \"\"\"\n    # Context: first context_frames\n    context_indices = torch.arange(context_frames)\n\n    # Target: next target_frames\n    target_indices = torch.arange(context_frames, context_frames + target_frames)\n\n    return context_indices, target_indices\n</code></pre>"},{"location":"JEPA/01_jepa_foundations/#43-perturbation-masking-bio-jepa","title":"4.3 Perturbation Masking (Bio-JEPA)","text":"<p>For perturbations: Baseline is context, perturbed is target</p> <pre><code>def perturbation_mask(baseline, perturbed):\n    \"\"\"\n    No masking needed - baseline is context, perturbed is target.\n\n    Args:\n        baseline: Baseline expression (B, num_genes)\n        perturbed: Perturbed expression (B, num_genes)\n\n    Returns:\n        context: Baseline (context)\n        target: Perturbed (target)\n    \"\"\"\n    return baseline, perturbed\n</code></pre>"},{"location":"JEPA/01_jepa_foundations/#5-complete-jepa-model","title":"5. Complete JEPA Model","text":""},{"location":"JEPA/01_jepa_foundations/#51-full-jepa-implementation","title":"5.1 Full JEPA Implementation","text":"<pre><code>class JEPA(nn.Module):\n    \"\"\"\n    Complete JEPA model.\n\n    Args:\n        encoder: Encoder network\n        predictor: Predictor network\n        embed_dim: Embedding dimension\n    \"\"\"\n    def __init__(self, encoder, predictor, embed_dim=768):\n        super().__init__()\n\n        self.encoder = encoder\n        self.predictor = predictor\n        self.embed_dim = embed_dim\n\n        # VICReg loss\n        self.vicreg = VICRegLoss(\n            lambda_inv=25.0,\n            lambda_var=25.0,\n            lambda_cov=1.0\n        )\n\n    def forward(self, x_context, x_target, context_mask=None, target_mask=None):\n        \"\"\"\n        Args:\n            x_context: Context input (B, ...)\n            x_target: Target input (B, ...)\n            context_mask: Optional context mask\n            target_mask: Optional target mask\n\n        Returns:\n            loss: Total loss\n            loss_dict: Dictionary with individual losses\n        \"\"\"\n        # Encode context and target\n        z_context = self.encoder(x_context, mask=context_mask)\n        z_target = self.encoder(x_target, mask=target_mask)\n\n        # Predict target from context\n        z_pred = self.predictor(z_context)\n\n        # VICReg loss\n        loss, loss_dict = self.vicreg(z_pred, z_target)\n\n        return loss, loss_dict\n\n    @torch.no_grad()\n    def encode(self, x):\n        \"\"\"Encode input to embeddings.\"\"\"\n        return self.encoder(x)\n\n    @torch.no_grad()\n    def predict(self, z_context):\n        \"\"\"Predict target embedding from context.\"\"\"\n        return self.predictor(z_context)\n</code></pre>"},{"location":"JEPA/01_jepa_foundations/#52-jepa-for-gene-expression","title":"5.2 JEPA for Gene Expression","text":"<pre><code>class BioJEPA(nn.Module):\n    \"\"\"\n    JEPA for gene expression / perturbation prediction.\n\n    Args:\n        num_genes: Number of genes\n        embed_dim: Embedding dimension\n        num_tokens: Number of tokens\n        encoder_depth: Encoder depth\n        predictor_depth: Predictor depth\n        condition_dim: Perturbation condition dimension\n    \"\"\"\n    def __init__(\n        self,\n        num_genes=20000,\n        embed_dim=256,\n        num_tokens=64,\n        encoder_depth=6,\n        predictor_depth=4,\n        condition_dim=128,\n    ):\n        super().__init__()\n\n        # Encoder\n        self.encoder = GeneExpressionEncoder(\n            num_genes=num_genes,\n            embed_dim=embed_dim,\n            num_tokens=num_tokens,\n        )\n\n        # Conditional predictor\n        self.predictor = ConditionalPredictor(\n            embed_dim=embed_dim,\n            condition_dim=condition_dim,\n            depth=predictor_depth,\n        )\n\n        # VICReg loss\n        self.vicreg = VICRegLoss()\n\n    def forward(self, x_baseline, x_perturbed, perturbation_emb):\n        \"\"\"\n        Args:\n            x_baseline: Baseline expression (B, num_genes)\n            x_perturbed: Perturbed expression (B, num_genes)\n            perturbation_emb: Perturbation embedding (B, condition_dim)\n\n        Returns:\n            loss: Total loss\n            loss_dict: Loss components\n        \"\"\"\n        # Encode baseline and perturbed\n        z_baseline = self.encoder(x_baseline)\n        z_perturbed = self.encoder(x_perturbed)\n\n        # Predict perturbed from baseline + perturbation\n        z_pred = self.predictor(z_baseline, perturbation_emb)\n\n        # VICReg loss\n        loss, loss_dict = self.vicreg(z_pred, z_perturbed)\n\n        return loss, loss_dict\n\n    @torch.no_grad()\n    def predict_perturbation(self, x_baseline, perturbation_emb):\n        \"\"\"\n        Predict perturbed state from baseline + perturbation.\n\n        Args:\n            x_baseline: Baseline expression (B, num_genes)\n            perturbation_emb: Perturbation embedding (B, condition_dim)\n\n        Returns:\n            z_pred: Predicted perturbed embedding (B, num_tokens, embed_dim)\n        \"\"\"\n        z_baseline = self.encoder(x_baseline)\n        z_pred = self.predictor(z_baseline, perturbation_emb)\n        return z_pred\n</code></pre>"},{"location":"JEPA/01_jepa_foundations/#6-design-principles","title":"6. Design Principles","text":""},{"location":"JEPA/01_jepa_foundations/#61-encoder-predictor-asymmetry","title":"6.1 Encoder-Predictor Asymmetry","text":"<p>Key insight: Predictor should be smaller than encoder</p> <p>Rationale:</p> <ul> <li>Encoder learns rich representations</li> <li>Predictor learns relationships</li> <li>Asymmetry prevents shortcut solutions</li> </ul> <p>Typical ratios:</p> <ul> <li>Predictor depth = 0.5\u00d7 encoder depth</li> <li>Predictor width = 1.0\u00d7 encoder width</li> </ul>"},{"location":"JEPA/01_jepa_foundations/#62-shared-vs-separate-encoders","title":"6.2 Shared vs Separate Encoders","text":"<p>Shared encoder (standard): <pre><code>z_context = encoder(x_context)\nz_target = encoder(x_target)\n</code></pre></p> <p>Pros: Parameter efficient, consistent representations Cons: Must handle both context and target</p> <p>Separate encoders (rare): <pre><code>z_context = encoder_context(x_context)\nz_target = encoder_target(x_target)\n</code></pre></p> <p>Pros: Specialized for each input Cons: More parameters, less parameter sharing</p> <p>Recommendation: Use shared encoder (standard practice)</p>"},{"location":"JEPA/01_jepa_foundations/#63-stop-gradient-on-target","title":"6.3 Stop-Gradient on Target","text":"<p>Important: Stop gradient through target encoder</p> <pre><code># Correct\nz_target = encoder(x_target).detach()\n\n# Or use torch.no_grad()\nwith torch.no_grad():\n    z_target = encoder(x_target)\n</code></pre> <p>Why: Prevents collapse via shortcut through target path</p>"},{"location":"JEPA/01_jepa_foundations/#key-takeaways","title":"Key Takeaways","text":""},{"location":"JEPA/01_jepa_foundations/#architecture","title":"Architecture","text":"<ol> <li>Encoder \u2014 ViT for images, MLP for gene expression</li> <li>Predictor \u2014 Transformer, smaller than encoder</li> <li>VICReg \u2014 Prevents collapse via variance + covariance</li> <li>Masking \u2014 Block for images, temporal for videos, perturbation for biology</li> </ol>"},{"location":"JEPA/01_jepa_foundations/#design-choices","title":"Design Choices","text":"<ol> <li>Shared encoder \u2014 Same for context and target</li> <li>Asymmetric \u2014 Predictor smaller than encoder</li> <li>Stop-gradient \u2014 On target encoder</li> <li>No decoder \u2014 Prediction in embedding space only</li> </ol>"},{"location":"JEPA/01_jepa_foundations/#implementation","title":"Implementation","text":"<ol> <li>Embed dim \u2014 768 for images, 256-512 for gene expression</li> <li>Encoder depth \u2014 12 for images, 4-8 for gene expression</li> <li>Predictor depth \u2014 0.5\u00d7 encoder depth</li> <li>VICReg weights \u2014 \u03bb_inv=25, \u03bb_var=25, \u03bb_cov=1</li> </ol>"},{"location":"JEPA/01_jepa_foundations/#related-documents","title":"Related Documents","text":"<ul> <li>00_jepa_overview.md \u2014 High-level concepts</li> <li>02_jepa_training.md \u2014 Training strategies</li> <li>03_jepa_applications.md \u2014 Applications</li> <li>04_jepa_perturbseq.md \u2014 Perturb-seq implementation</li> </ul>"},{"location":"JEPA/01_jepa_foundations/#references","title":"References","text":"<ul> <li>Assran et al. (2023): \"Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture\" (I-JEPA)</li> <li>Bardes et al. (2022): \"VICReg: Variance-Invariance-Covariance Regularization\"</li> <li>Bardes et al. (2024): \"V-JEPA: Latent Video Prediction\"</li> <li>Dosovitskiy et al. (2020): \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" (ViT)</li> </ul>"},{"location":"JEPA/02_jepa_training/","title":"JEPA Training: Strategies and Best Practices","text":"<p>This document covers training strategies for JEPA models, including optimization, hyperparameters, monitoring, debugging, and advanced techniques.</p> <p>Prerequisites: Understanding of JEPA architecture and overview.</p>"},{"location":"JEPA/02_jepa_training/#training-overview","title":"Training Overview","text":""},{"location":"JEPA/02_jepa_training/#basic-training-loop","title":"Basic Training Loop","text":"<pre><code>import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\ndef train_jepa(\n    model,\n    train_loader,\n    optimizer,\n    num_epochs=100,\n    device='cuda',\n):\n    \"\"\"\n    Basic JEPA training loop.\n\n    Args:\n        model: JEPA model\n        train_loader: Training data loader\n        optimizer: Optimizer\n        num_epochs: Number of epochs\n        device: Device to train on\n    \"\"\"\n    model.to(device)\n    model.train()\n\n    for epoch in range(num_epochs):\n        epoch_loss = 0.0\n        epoch_inv = 0.0\n        epoch_var = 0.0\n        epoch_cov = 0.0\n\n        for batch_idx, (x_context, x_target) in enumerate(train_loader):\n            x_context = x_context.to(device)\n            x_target = x_target.to(device)\n\n            # Forward pass\n            loss, loss_dict = model(x_context, x_target)\n\n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # Accumulate losses\n            epoch_loss += loss_dict['loss']\n            epoch_inv += loss_dict['inv']\n            epoch_var += loss_dict['var']\n            epoch_cov += loss_dict['cov']\n\n        # Average losses\n        num_batches = len(train_loader)\n        epoch_loss /= num_batches\n        epoch_inv /= num_batches\n        epoch_var /= num_batches\n        epoch_cov /= num_batches\n\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(f\"  Loss: {epoch_loss:.4f}\")\n        print(f\"  Inv: {epoch_inv:.4f}, Var: {epoch_var:.4f}, Cov: {epoch_cov:.4f}\")\n</code></pre>"},{"location":"JEPA/02_jepa_training/#1-data-preparation","title":"1. Data Preparation","text":""},{"location":"JEPA/02_jepa_training/#11-image-data-i-jepa","title":"1.1 Image Data (I-JEPA)","text":"<pre><code>import torchvision.transforms as transforms\nfrom torch.utils.data import Dataset\n\nclass IJEPADataset(Dataset):\n    \"\"\"\n    Dataset for I-JEPA training.\n\n    Returns context and target views of the same image.\n    \"\"\"\n    def __init__(\n        self,\n        images,\n        img_size=224,\n        patch_size=16,\n        num_blocks=4,\n    ):\n        self.images = images\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_blocks = num_blocks\n\n        # Image transforms\n        self.transform = transforms.Compose([\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                               std=[0.229, 0.224, 0.225])\n        ])\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        # Load and transform image\n        img = self.images[idx]\n        img = self.transform(img)\n\n        # Context: full image\n        x_context = img\n\n        # Target: same image (masking handled in model)\n        x_target = img\n\n        return x_context, x_target\n</code></pre>"},{"location":"JEPA/02_jepa_training/#12-gene-expression-data-bio-jepa","title":"1.2 Gene Expression Data (Bio-JEPA)","text":"<pre><code>class PerturbSeqDataset(Dataset):\n    \"\"\"\n    Dataset for Perturb-seq JEPA training.\n\n    Returns baseline and perturbed expression pairs.\n    \"\"\"\n    def __init__(\n        self,\n        baseline_expr,\n        perturbed_expr,\n        perturbation_info,\n        perturbation_encoder,\n    ):\n        \"\"\"\n        Args:\n            baseline_expr: Baseline expression (N, num_genes)\n            perturbed_expr: Perturbed expression (N, num_genes)\n            perturbation_info: Perturbation metadata (N, ...)\n            perturbation_encoder: Function to encode perturbation info\n        \"\"\"\n        self.baseline_expr = baseline_expr\n        self.perturbed_expr = perturbed_expr\n        self.perturbation_info = perturbation_info\n        self.perturbation_encoder = perturbation_encoder\n\n    def __len__(self):\n        return len(self.baseline_expr)\n\n    def __getitem__(self, idx):\n        # Baseline (context)\n        x_baseline = torch.tensor(self.baseline_expr[idx], dtype=torch.float32)\n\n        # Perturbed (target)\n        x_perturbed = torch.tensor(self.perturbed_expr[idx], dtype=torch.float32)\n\n        # Perturbation embedding\n        pert_info = self.perturbation_info[idx]\n        pert_emb = self.perturbation_encoder(pert_info)\n\n        return x_baseline, x_perturbed, pert_emb\n\n\ndef encode_perturbation(pert_info, gene_vocab_size=20000):\n    \"\"\"\n    Encode perturbation information.\n\n    Args:\n        pert_info: Dictionary with perturbation details\n            - 'gene_id': ID of perturbed gene\n            - 'type': 'knockout', 'overexpression', etc.\n            - 'dose': Perturbation strength\n\n    Returns:\n        pert_emb: Perturbation embedding\n    \"\"\"\n    # One-hot encode gene\n    gene_onehot = torch.zeros(gene_vocab_size)\n    gene_onehot[pert_info['gene_id']] = 1.0\n\n    # Encode type\n    type_map = {'knockout': 0, 'overexpression': 1, 'knockdown': 2}\n    type_id = type_map.get(pert_info['type'], 0)\n    type_onehot = torch.zeros(len(type_map))\n    type_onehot[type_id] = 1.0\n\n    # Dose\n    dose = torch.tensor([pert_info.get('dose', 1.0)])\n\n    # Concatenate\n    pert_emb = torch.cat([gene_onehot, type_onehot, dose])\n\n    return pert_emb\n</code></pre>"},{"location":"JEPA/02_jepa_training/#13-time-series-data-v-jepa","title":"1.3 Time-Series Data (V-JEPA)","text":"<pre><code>class TimeSeriesDataset(Dataset):\n    \"\"\"\n    Dataset for time-series JEPA training.\n\n    Returns past frames (context) and future frames (target).\n    \"\"\"\n    def __init__(\n        self,\n        time_series,\n        context_length=4,\n        target_length=4,\n    ):\n        \"\"\"\n        Args:\n            time_series: Time-series data (N, T, ...)\n            context_length: Number of past frames\n            target_length: Number of future frames\n        \"\"\"\n        self.time_series = time_series\n        self.context_length = context_length\n        self.target_length = target_length\n\n        # Valid starting indices\n        self.valid_indices = list(range(\n            0,\n            len(time_series) - context_length - target_length + 1\n        ))\n\n    def __len__(self):\n        return len(self.valid_indices)\n\n    def __getitem__(self, idx):\n        start_idx = self.valid_indices[idx]\n\n        # Context: past frames\n        x_context = self.time_series[start_idx:start_idx + self.context_length]\n\n        # Target: future frames\n        x_target = self.time_series[\n            start_idx + self.context_length:\n            start_idx + self.context_length + self.target_length\n        ]\n\n        return torch.tensor(x_context), torch.tensor(x_target)\n</code></pre>"},{"location":"JEPA/02_jepa_training/#2-optimization","title":"2. Optimization","text":""},{"location":"JEPA/02_jepa_training/#21-optimizer-choice","title":"2.1 Optimizer Choice","text":"<p>AdamW (recommended):</p> <pre><code>optimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=1e-4,\n    betas=(0.9, 0.999),\n    eps=1e-8,\n    weight_decay=0.05,\n)\n</code></pre> <p>Why AdamW:</p> <ul> <li>Decoupled weight decay (better than Adam)</li> <li>Stable for transformers</li> <li>Good default choice</li> </ul> <p>Alternatives:</p> <ul> <li>Adam: If weight decay not needed</li> <li>SGD + momentum: For smaller models</li> <li>LAMB: For very large batch sizes</li> </ul>"},{"location":"JEPA/02_jepa_training/#22-learning-rate-schedule","title":"2.2 Learning Rate Schedule","text":"<p>Warmup + Cosine Decay (recommended):</p> <pre><code>from torch.optim.lr_scheduler import CosineAnnealingLR\n\ndef get_cosine_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps,\n    num_training_steps,\n    min_lr=1e-6,\n):\n    \"\"\"\n    Cosine learning rate schedule with warmup.\n\n    Args:\n        optimizer: Optimizer\n        num_warmup_steps: Number of warmup steps\n        num_training_steps: Total training steps\n        min_lr: Minimum learning rate\n\n    Returns:\n        scheduler: Learning rate scheduler\n    \"\"\"\n    def lr_lambda(current_step):\n        if current_step &lt; num_warmup_steps:\n            # Linear warmup\n            return float(current_step) / float(max(1, num_warmup_steps))\n\n        # Cosine decay\n        progress = float(current_step - num_warmup_steps) / float(\n            max(1, num_training_steps - num_warmup_steps)\n        )\n        return max(min_lr, 0.5 * (1.0 + math.cos(math.pi * progress)))\n\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n\n# Usage\nnum_epochs = 100\nsteps_per_epoch = len(train_loader)\nnum_training_steps = num_epochs * steps_per_epoch\nnum_warmup_steps = 10 * steps_per_epoch  # 10 epochs warmup\n\nscheduler = get_cosine_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=num_warmup_steps,\n    num_training_steps=num_training_steps,\n)\n</code></pre> <p>Schedule visualization: <pre><code>LR\n \u2502\n \u2502    \u2571\u2500\u2500\u2500\u2500\u2500\u2572\n \u2502   \u2571       \u2572\n \u2502  \u2571         \u2572___\n \u2502 \u2571               \u2572___\n \u2502\u2571                    \u2572___\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Steps\n   Warmup    Cosine Decay\n</code></pre></p>"},{"location":"JEPA/02_jepa_training/#23-gradient-clipping","title":"2.3 Gradient Clipping","text":"<p>Prevent exploding gradients:</p> <pre><code># In training loop\nloss.backward()\n\n# Clip gradients\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\noptimizer.step()\n</code></pre> <p>When to use:</p> <ul> <li>Always for transformers</li> <li>Especially with large learning rates</li> <li>If seeing NaN losses</li> </ul>"},{"location":"JEPA/02_jepa_training/#3-hyperparameters","title":"3. Hyperparameters","text":""},{"location":"JEPA/02_jepa_training/#31-core-hyperparameters","title":"3.1 Core Hyperparameters","text":"Parameter Image (I-JEPA) Gene Expression (Bio-JEPA) Notes Learning rate 1e-4 1e-3 to 1e-4 Higher for smaller models Batch size 256-1024 64-256 Larger is better (up to memory) Warmup epochs 10 5-10 ~10% of total epochs Weight decay 0.05 0.01-0.05 Regularization Embed dim 768 256-512 Model capacity Encoder depth 12 4-8 Deeper = more capacity Predictor depth 6 2-4 0.5\u00d7 encoder depth"},{"location":"JEPA/02_jepa_training/#32-vicreg-hyperparameters","title":"3.2 VICReg Hyperparameters","text":"Parameter Default Range Notes \u03bb_inv 25.0 10-50 Invariance weight \u03bb_var 25.0 10-50 Variance weight \u03bb_cov 1.0 0.5-2.0 Covariance weight \u03b3 (target std) 1.0 0.5-2.0 Target variance <p>Tuning guidelines:</p> <ul> <li>If embeddings collapse \u2192 increase \u03bb_var</li> <li>If dimensions correlated \u2192 increase \u03bb_cov</li> <li>If predictions too rigid \u2192 decrease \u03bb_inv</li> </ul>"},{"location":"JEPA/02_jepa_training/#33-masking-hyperparameters-i-jepa","title":"3.3 Masking Hyperparameters (I-JEPA)","text":"Parameter Default Range Notes Num blocks 4 2-8 Number of masked blocks Block scale 0.15-0.2 0.1-0.3 Fraction of image Aspect ratio 0.75-1.5 0.5-2.0 Block shape"},{"location":"JEPA/02_jepa_training/#4-training-loop-with-all-features","title":"4. Training Loop with All Features","text":""},{"location":"JEPA/02_jepa_training/#41-complete-training-script","title":"4.1 Complete Training Script","text":"<pre><code>import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nimport os\nimport math\n\ndef train_jepa_complete(\n    model,\n    train_loader,\n    val_loader,\n    num_epochs=100,\n    lr=1e-4,\n    weight_decay=0.05,\n    warmup_epochs=10,\n    device='cuda',\n    save_dir='checkpoints',\n    log_dir='logs',\n):\n    \"\"\"\n    Complete JEPA training with all features.\n\n    Args:\n        model: JEPA model\n        train_loader: Training data loader\n        val_loader: Validation data loader\n        num_epochs: Number of epochs\n        lr: Learning rate\n        weight_decay: Weight decay\n        warmup_epochs: Warmup epochs\n        device: Device\n        save_dir: Checkpoint directory\n        log_dir: Log directory\n    \"\"\"\n    # Setup\n    model.to(device)\n    os.makedirs(save_dir, exist_ok=True)\n    os.makedirs(log_dir, exist_ok=True)\n\n    # Optimizer\n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=lr,\n        weight_decay=weight_decay,\n    )\n\n    # Scheduler\n    steps_per_epoch = len(train_loader)\n    num_training_steps = num_epochs * steps_per_epoch\n    num_warmup_steps = warmup_epochs * steps_per_epoch\n\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=num_warmup_steps,\n        num_training_steps=num_training_steps,\n    )\n\n    # Logging\n    writer = SummaryWriter(log_dir)\n\n    # Training loop\n    global_step = 0\n    best_val_loss = float('inf')\n\n    for epoch in range(num_epochs):\n        # Training\n        model.train()\n        train_metrics = train_one_epoch(\n            model, train_loader, optimizer, scheduler, device, writer, global_step\n        )\n        global_step += steps_per_epoch\n\n        # Validation\n        model.eval()\n        val_metrics = validate(model, val_loader, device)\n\n        # Logging\n        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n        print(f\"  Train Loss: {train_metrics['loss']:.4f}\")\n        print(f\"    Inv: {train_metrics['inv']:.4f}, \"\n              f\"Var: {train_metrics['var']:.4f}, \"\n              f\"Cov: {train_metrics['cov']:.4f}\")\n        print(f\"  Val Loss: {val_metrics['loss']:.4f}\")\n        print(f\"    Inv: {val_metrics['inv']:.4f}, \"\n              f\"Var: {val_metrics['var']:.4f}, \"\n              f\"Cov: {val_metrics['cov']:.4f}\")\n\n        # TensorBoard\n        writer.add_scalar('val/loss', val_metrics['loss'], epoch)\n        writer.add_scalar('val/inv', val_metrics['inv'], epoch)\n        writer.add_scalar('val/var', val_metrics['var'], epoch)\n        writer.add_scalar('val/cov', val_metrics['cov'], epoch)\n\n        # Save checkpoint\n        if val_metrics['loss'] &lt; best_val_loss:\n            best_val_loss = val_metrics['loss']\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n                'best_val_loss': best_val_loss,\n            }, os.path.join(save_dir, 'best_model.pt'))\n            print(f\"  Saved best model (val_loss: {best_val_loss:.4f})\")\n\n        # Save periodic checkpoint\n        if (epoch + 1) % 10 == 0:\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n            }, os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pt'))\n\n    writer.close()\n    print(\"\\nTraining complete!\")\n\n\ndef train_one_epoch(model, train_loader, optimizer, scheduler, device, writer, global_step):\n    \"\"\"Train for one epoch.\"\"\"\n    model.train()\n\n    total_loss = 0.0\n    total_inv = 0.0\n    total_var = 0.0\n    total_cov = 0.0\n\n    for batch_idx, batch in enumerate(train_loader):\n        # Unpack batch\n        if len(batch) == 2:\n            x_context, x_target = batch\n            x_context = x_context.to(device)\n            x_target = x_target.to(device)\n            loss, loss_dict = model(x_context, x_target)\n        elif len(batch) == 3:\n            x_context, x_target, condition = batch\n            x_context = x_context.to(device)\n            x_target = x_target.to(device)\n            condition = condition.to(device)\n            loss, loss_dict = model(x_context, x_target, condition)\n\n        # Backward\n        optimizer.zero_grad()\n        loss.backward()\n\n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        optimizer.step()\n        scheduler.step()\n\n        # Accumulate\n        total_loss += loss_dict['loss']\n        total_inv += loss_dict['inv']\n        total_var += loss_dict['var']\n        total_cov += loss_dict['cov']\n\n        # Log to TensorBoard\n        if batch_idx % 100 == 0:\n            step = global_step + batch_idx\n            writer.add_scalar('train/loss', loss_dict['loss'], step)\n            writer.add_scalar('train/inv', loss_dict['inv'], step)\n            writer.add_scalar('train/var', loss_dict['var'], step)\n            writer.add_scalar('train/cov', loss_dict['cov'], step)\n            writer.add_scalar('train/lr', scheduler.get_last_lr()[0], step)\n\n    # Average\n    num_batches = len(train_loader)\n    return {\n        'loss': total_loss / num_batches,\n        'inv': total_inv / num_batches,\n        'var': total_var / num_batches,\n        'cov': total_cov / num_batches,\n    }\n\n\n@torch.no_grad()\ndef validate(model, val_loader, device):\n    \"\"\"Validate model.\"\"\"\n    model.eval()\n\n    total_loss = 0.0\n    total_inv = 0.0\n    total_var = 0.0\n    total_cov = 0.0\n\n    for batch in val_loader:\n        # Unpack batch\n        if len(batch) == 2:\n            x_context, x_target = batch\n            x_context = x_context.to(device)\n            x_target = x_target.to(device)\n            loss, loss_dict = model(x_context, x_target)\n        elif len(batch) == 3:\n            x_context, x_target, condition = batch\n            x_context = x_context.to(device)\n            x_target = x_target.to(device)\n            condition = condition.to(device)\n            loss, loss_dict = model(x_context, x_target, condition)\n\n        total_loss += loss_dict['loss']\n        total_inv += loss_dict['inv']\n        total_var += loss_dict['var']\n        total_cov += loss_dict['cov']\n\n    num_batches = len(val_loader)\n    return {\n        'loss': total_loss / num_batches,\n        'inv': total_inv / num_batches,\n        'var': total_var / num_batches,\n        'cov': total_cov / num_batches,\n    }\n</code></pre>"},{"location":"JEPA/02_jepa_training/#5-monitoring-and-debugging","title":"5. Monitoring and Debugging","text":""},{"location":"JEPA/02_jepa_training/#51-key-metrics-to-monitor","title":"5.1 Key Metrics to Monitor","text":"<p>1. Loss components:</p> <ul> <li>Total loss</li> <li>Invariance loss (MSE between prediction and target)</li> <li>Variance loss (embedding spread)</li> <li>Covariance loss (dimension decorrelation)</li> </ul> <p>2. Embedding statistics:</p> <ul> <li>Mean embedding norm</li> <li>Embedding variance per dimension</li> <li>Correlation between dimensions</li> </ul> <p>3. Training dynamics:</p> <ul> <li>Learning rate</li> <li>Gradient norms</li> <li>Weight norms</li> </ul>"},{"location":"JEPA/02_jepa_training/#52-monitoring-script","title":"5.2 Monitoring Script","text":"<pre><code>@torch.no_grad()\ndef compute_embedding_stats(model, data_loader, device):\n    \"\"\"\n    Compute embedding statistics for monitoring.\n\n    Args:\n        model: JEPA model\n        data_loader: Data loader\n        device: Device\n\n    Returns:\n        stats: Dictionary of statistics\n    \"\"\"\n    model.eval()\n\n    all_embeddings = []\n\n    for batch in data_loader:\n        x = batch[0].to(device)\n        z = model.encode(x)\n\n        # Flatten tokens\n        z = z.reshape(-1, z.shape[-1])  # (B*num_tokens, embed_dim)\n        all_embeddings.append(z.cpu())\n\n    # Concatenate\n    all_embeddings = torch.cat(all_embeddings, dim=0)  # (N, embed_dim)\n\n    # Compute statistics\n    stats = {\n        'mean_norm': all_embeddings.norm(dim=1).mean().item(),\n        'std_norm': all_embeddings.norm(dim=1).std().item(),\n        'mean_per_dim': all_embeddings.mean(dim=0),\n        'std_per_dim': all_embeddings.std(dim=0),\n        'correlation': torch.corrcoef(all_embeddings.T),\n    }\n\n    return stats\n\n\n# Usage in training loop\nif epoch % 5 == 0:\n    stats = compute_embedding_stats(model, val_loader, device)\n    print(f\"  Embedding norm: {stats['mean_norm']:.4f} \u00b1 {stats['std_norm']:.4f}\")\n    print(f\"  Avg std per dim: {stats['std_per_dim'].mean():.4f}\")\n\n    # Check for collapse\n    if stats['std_per_dim'].min() &lt; 0.1:\n        print(\"  WARNING: Some dimensions have collapsed!\")\n</code></pre>"},{"location":"JEPA/02_jepa_training/#53-common-issues-and-solutions","title":"5.3 Common Issues and Solutions","text":"Issue Symptom Solution Embedding collapse All embeddings \u2192 same vector Increase \u03bb_var, check batch size Dimension collapse Some dims have zero variance Increase \u03bb_var, \u03bb_cov High correlation Dimensions are correlated Increase \u03bb_cov NaN loss Loss becomes NaN Reduce LR, add gradient clipping No learning Loss doesn't decrease Check LR, check data Overfitting Train loss &lt;&lt; val loss Increase weight decay, add dropout"},{"location":"JEPA/02_jepa_training/#6-advanced-techniques","title":"6. Advanced Techniques","text":""},{"location":"JEPA/02_jepa_training/#61-exponential-moving-average-ema","title":"6.1 Exponential Moving Average (EMA)","text":"<p>Maintain EMA of model weights:</p> <pre><code>class EMA:\n    \"\"\"Exponential Moving Average of model parameters.\"\"\"\n    def __init__(self, model, decay=0.999):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n\n        # Initialize shadow parameters\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                self.shadow[name] = param.data.clone()\n\n    def update(self):\n        \"\"\"Update EMA parameters.\"\"\"\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.shadow[name] = (\n                    self.decay * self.shadow[name] +\n                    (1 - self.decay) * param.data\n                )\n\n    def apply_shadow(self):\n        \"\"\"Apply EMA parameters to model.\"\"\"\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        \"\"\"Restore original parameters.\"\"\"\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n        self.backup = {}\n\n\n# Usage\nema = EMA(model, decay=0.999)\n\n# In training loop\nfor batch in train_loader:\n    loss, _ = model(x_context, x_target)\n    loss.backward()\n    optimizer.step()\n\n    # Update EMA\n    ema.update()\n\n# For validation/inference, use EMA weights\nema.apply_shadow()\nval_loss = validate(model, val_loader, device)\nema.restore()\n</code></pre>"},{"location":"JEPA/02_jepa_training/#62-mixed-precision-training","title":"6.2 Mixed Precision Training","text":"<p>Use FP16 for faster training:</p> <pre><code>from torch.cuda.amp import autocast, GradScaler\n\n# Initialize scaler\nscaler = GradScaler()\n\n# Training loop\nfor batch in train_loader:\n    x_context, x_target = batch\n    x_context = x_context.to(device)\n    x_target = x_target.to(device)\n\n    # Forward with autocast\n    with autocast():\n        loss, loss_dict = model(x_context, x_target)\n\n    # Backward with scaler\n    optimizer.zero_grad()\n    scaler.scale(loss).backward()\n    scaler.unscale_(optimizer)\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n    scaler.step(optimizer)\n    scaler.update()\n</code></pre> <p>Benefits:</p> <ul> <li>2-3\u00d7 faster training</li> <li>2\u00d7 less memory</li> <li>Minimal accuracy loss</li> </ul>"},{"location":"JEPA/02_jepa_training/#63-distributed-training","title":"6.3 Distributed Training","text":"<p>Train on multiple GPUs:</p> <pre><code>import torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\n\ndef setup_distributed():\n    \"\"\"Setup distributed training.\"\"\"\n    dist.init_process_group(backend='nccl')\n    local_rank = int(os.environ['LOCAL_RANK'])\n    torch.cuda.set_device(local_rank)\n    return local_rank\n\ndef train_distributed(model, train_dataset, val_dataset, ...):\n    \"\"\"Train with DDP.\"\"\"\n    # Setup\n    local_rank = setup_distributed()\n    device = torch.device(f'cuda:{local_rank}')\n\n    # Wrap model\n    model = model.to(device)\n    model = DDP(model, device_ids=[local_rank])\n\n    # Distributed sampler\n    train_sampler = DistributedSampler(train_dataset)\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        sampler=train_sampler,\n        num_workers=4,\n        pin_memory=True,\n    )\n\n    # Training loop\n    for epoch in range(num_epochs):\n        train_sampler.set_epoch(epoch)\n\n        for batch in train_loader:\n            # Training step\n            ...\n\n    # Cleanup\n    dist.destroy_process_group()\n</code></pre> <p>Launch: <pre><code>torchrun --nproc_per_node=4 train_jepa.py\n</code></pre></p>"},{"location":"JEPA/02_jepa_training/#64-curriculum-learning","title":"6.4 Curriculum Learning","text":"<p>Start with easier tasks, gradually increase difficulty:</p> <pre><code>def get_curriculum_schedule(epoch, total_epochs):\n    \"\"\"\n    Curriculum schedule for masking difficulty.\n\n    Start with small masks, gradually increase.\n    \"\"\"\n    progress = epoch / total_epochs\n\n    # Increase mask size over time\n    min_scale = 0.05\n    max_scale = 0.25\n    scale = min_scale + progress * (max_scale - min_scale)\n\n    # Increase number of blocks\n    min_blocks = 1\n    max_blocks = 8\n    num_blocks = int(min_blocks + progress * (max_blocks - min_blocks))\n\n    return {\n        'block_scale': (scale, scale + 0.05),\n        'num_blocks': num_blocks,\n    }\n\n\n# Usage\nfor epoch in range(num_epochs):\n    curriculum = get_curriculum_schedule(epoch, num_epochs)\n\n    # Update dataset masking parameters\n    train_dataset.set_masking_params(\n        num_blocks=curriculum['num_blocks'],\n        block_scale=curriculum['block_scale'],\n    )\n\n    # Train\n    train_one_epoch(...)\n</code></pre>"},{"location":"JEPA/02_jepa_training/#7-evaluation","title":"7. Evaluation","text":""},{"location":"JEPA/02_jepa_training/#71-downstream-task-evaluation","title":"7.1 Downstream Task Evaluation","text":"<p>Evaluate learned representations on downstream tasks:</p> <pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n@torch.no_grad()\ndef evaluate_downstream(model, train_data, train_labels, test_data, test_labels, device):\n    \"\"\"\n    Evaluate on downstream classification task.\n\n    Args:\n        model: JEPA model\n        train_data: Training data\n        train_labels: Training labels\n        test_data: Test data\n        test_labels: Test labels\n        device: Device\n\n    Returns:\n        accuracy: Classification accuracy\n    \"\"\"\n    model.eval()\n\n    # Extract embeddings\n    train_embeddings = []\n    for x in train_data:\n        x = x.to(device)\n        z = model.encode(x)\n        z = z.mean(dim=1)  # Average over tokens\n        train_embeddings.append(z.cpu())\n    train_embeddings = torch.cat(train_embeddings, dim=0).numpy()\n\n    test_embeddings = []\n    for x in test_data:\n        x = x.to(device)\n        z = model.encode(x)\n        z = z.mean(dim=1)\n        test_embeddings.append(z.cpu())\n    test_embeddings = torch.cat(test_embeddings, dim=0).numpy()\n\n    # Train linear classifier\n    clf = LogisticRegression(max_iter=1000)\n    clf.fit(train_embeddings, train_labels)\n\n    # Evaluate\n    pred_labels = clf.predict(test_embeddings)\n    accuracy = accuracy_score(test_labels, pred_labels)\n\n    return accuracy\n\n\n# Usage\naccuracy = evaluate_downstream(\n    model,\n    train_data, train_labels,\n    test_data, test_labels,\n    device\n)\nprint(f\"Downstream accuracy: {accuracy:.4f}\")\n</code></pre>"},{"location":"JEPA/02_jepa_training/#72-embedding-quality-metrics","title":"7.2 Embedding Quality Metrics","text":"<pre><code>@torch.no_grad()\ndef compute_embedding_quality(model, data_loader, device):\n    \"\"\"\n    Compute embedding quality metrics.\n\n    Returns:\n        metrics: Dictionary of quality metrics\n    \"\"\"\n    model.eval()\n\n    all_embeddings = []\n    for batch in data_loader:\n        x = batch[0].to(device)\n        z = model.encode(x)\n        z = z.reshape(-1, z.shape[-1])\n        all_embeddings.append(z.cpu())\n\n    all_embeddings = torch.cat(all_embeddings, dim=0)\n\n    # 1. Effective rank (measure of dimensionality usage)\n    U, S, V = torch.svd(all_embeddings)\n    S_normalized = S / S.sum()\n    entropy = -(S_normalized * torch.log(S_normalized + 1e-10)).sum()\n    effective_rank = torch.exp(entropy).item()\n\n    # 2. Uniformity (how uniformly distributed on hypersphere)\n    normalized = F.normalize(all_embeddings, dim=1)\n    similarity = normalized @ normalized.T\n    uniformity = torch.log(torch.exp(similarity).mean()).item()\n\n    # 3. Alignment (for paired data)\n    # ... (depends on task)\n\n    metrics = {\n        'effective_rank': effective_rank,\n        'uniformity': uniformity,\n        'max_dim': all_embeddings.shape[1],\n    }\n\n    return metrics\n</code></pre>"},{"location":"JEPA/02_jepa_training/#key-takeaways","title":"Key Takeaways","text":""},{"location":"JEPA/02_jepa_training/#training-strategy","title":"Training Strategy","text":"<ol> <li>Optimizer: AdamW with weight decay</li> <li>Schedule: Warmup + cosine decay</li> <li>Batch size: As large as possible (256-1024)</li> <li>Gradient clipping: Always use (max_norm=1.0)</li> </ol>"},{"location":"JEPA/02_jepa_training/#hyperparameters","title":"Hyperparameters","text":"<ol> <li>Learning rate: 1e-4 for images, 1e-3 for gene expression</li> <li>VICReg weights: \u03bb_inv=25, \u03bb_var=25, \u03bb_cov=1</li> <li>Warmup: ~10% of total epochs</li> <li>Weight decay: 0.05 for images, 0.01-0.05 for gene expression</li> </ol>"},{"location":"JEPA/02_jepa_training/#monitoring","title":"Monitoring","text":"<ol> <li>Watch for collapse: Check embedding variance</li> <li>Monitor all loss components: inv, var, cov</li> <li>Evaluate downstream: Linear probing on tasks</li> <li>Use EMA: For more stable evaluation</li> </ol>"},{"location":"JEPA/02_jepa_training/#advanced","title":"Advanced","text":"<ol> <li>EMA: Decay=0.999 for smoother weights</li> <li>Mixed precision: 2-3\u00d7 speedup</li> <li>Distributed: Scale to multiple GPUs</li> <li>Curriculum: Start easy, increase difficulty</li> </ol>"},{"location":"JEPA/02_jepa_training/#related-documents","title":"Related Documents","text":"<ul> <li>01_jepa_foundations.md \u2014 Architecture details</li> <li>03_jepa_applications.md \u2014 Applications</li> <li>04_jepa_perturbseq.md \u2014 Perturb-seq implementation</li> </ul>"},{"location":"JEPA/02_jepa_training/#references","title":"References","text":"<ul> <li>Assran et al. (2023): \"Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture\"</li> <li>Bardes et al. (2022): \"VICReg: Variance-Invariance-Covariance Regularization\"</li> <li>Loshchilov &amp; Hutter (2019): \"Decoupled Weight Decay Regularization\" (AdamW)</li> </ul>"},{"location":"JEPA/03_jepa_applications/","title":"JEPA Applications: From Vision to Biology","text":"<p>This document maps JEPA concepts from computer vision to computational biology, covering perturbation prediction, trajectory inference, multi-omics integration, and drug response modeling.</p> <p>Prerequisites: Understanding of JEPA overview, foundations, and training.</p>"},{"location":"JEPA/03_jepa_applications/#overview","title":"Overview","text":""},{"location":"JEPA/03_jepa_applications/#vision-biology-mapping","title":"Vision \u2192 Biology Mapping","text":"Vision Domain Biology Domain JEPA Task Image masking Gene expression masking Predict masked genes from visible Video prediction Time-series prediction Predict future states from past Frame interpolation Trajectory interpolation Fill gaps in developmental paths Action conditioning Perturbation conditioning Predict perturbed state from baseline Multi-view learning Multi-omics integration Predict one modality from another <p>Key insight: Replace \"pixels\" with \"genes\", \"frames\" with \"time points\", \"actions\" with \"perturbations\".</p>"},{"location":"JEPA/03_jepa_applications/#1-perturbation-prediction-perturb-seq","title":"1. Perturbation Prediction (Perturb-seq)","text":""},{"location":"JEPA/03_jepa_applications/#11-problem-setup","title":"1.1 Problem Setup","text":"<p>Goal: Predict cellular response to genetic/chemical perturbations</p> <p>Data:</p> <ul> <li>Baseline expression: \\(x_0 \\in \\mathbb{R}^{20000}\\) (unperturbed cells)</li> <li>Perturbed expression: \\(x_p \\in \\mathbb{R}^{20000}\\) (after perturbation)</li> <li>Perturbation info: Gene ID, type (KO/OE), dose</li> </ul> <p>JEPA formulation: <pre><code>Context: Baseline expression + perturbation info\nTarget: Perturbed expression\nTask: Predict z_perturbed from z_baseline and perturbation\n</code></pre></p>"},{"location":"JEPA/03_jepa_applications/#12-architecture","title":"1.2 Architecture","text":"<pre><code>class PerturbationJEPA(nn.Module):\n    \"\"\"\n    JEPA for perturbation prediction.\n\n    Args:\n        num_genes: Number of genes\n        embed_dim: Embedding dimension\n        num_tokens: Number of tokens\n        perturbation_dim: Perturbation embedding dimension\n    \"\"\"\n    def __init__(\n        self,\n        num_genes=20000,\n        embed_dim=256,\n        num_tokens=64,\n        perturbation_dim=128,\n    ):\n        super().__init__()\n\n        # Encoder for gene expression\n        self.encoder = GeneExpressionEncoder(\n            num_genes=num_genes,\n            embed_dim=embed_dim,\n            num_tokens=num_tokens,\n        )\n\n        # Perturbation encoder\n        self.perturbation_encoder = nn.Sequential(\n            nn.Linear(num_genes + 10, perturbation_dim),  # gene_id + metadata\n            nn.LayerNorm(perturbation_dim),\n            nn.GELU(),\n            nn.Linear(perturbation_dim, perturbation_dim),\n        )\n\n        # Conditional predictor\n        self.predictor = ConditionalPredictor(\n            embed_dim=embed_dim,\n            condition_dim=perturbation_dim,\n            depth=4,\n            num_heads=8,\n        )\n\n        # VICReg loss\n        self.vicreg = VICRegLoss()\n\n    def forward(self, x_baseline, x_perturbed, perturbation_info):\n        \"\"\"\n        Args:\n            x_baseline: Baseline expression (B, num_genes)\n            x_perturbed: Perturbed expression (B, num_genes)\n            perturbation_info: Perturbation metadata (B, num_genes + 10)\n\n        Returns:\n            loss: Total loss\n            loss_dict: Loss components\n        \"\"\"\n        # Encode baseline and perturbed\n        z_baseline = self.encoder(x_baseline)\n        z_perturbed = self.encoder(x_perturbed)\n\n        # Encode perturbation\n        pert_emb = self.perturbation_encoder(perturbation_info)\n\n        # Predict perturbed from baseline + perturbation\n        z_pred = self.predictor(z_baseline, pert_emb)\n\n        # VICReg loss\n        loss, loss_dict = self.vicreg(z_pred, z_perturbed)\n\n        return loss, loss_dict\n\n    @torch.no_grad()\n    def predict_perturbation(self, x_baseline, perturbation_info):\n        \"\"\"\n        Predict perturbed state.\n\n        Args:\n            x_baseline: Baseline expression (B, num_genes)\n            perturbation_info: Perturbation metadata (B, num_genes + 10)\n\n        Returns:\n            z_pred: Predicted perturbed embedding (B, num_tokens, embed_dim)\n        \"\"\"\n        z_baseline = self.encoder(x_baseline)\n        pert_emb = self.perturbation_encoder(perturbation_info)\n        z_pred = self.predictor(z_baseline, pert_emb)\n        return z_pred\n</code></pre>"},{"location":"JEPA/03_jepa_applications/#13-training","title":"1.3 Training","text":"<pre><code># Dataset\nfrom datasets import load_perturbseq_data\n\nbaseline_expr, perturbed_expr, pert_info = load_perturbseq_data('norman2019')\n\ndataset = PerturbSeqDataset(\n    baseline_expr=baseline_expr,\n    perturbed_expr=perturbed_expr,\n    perturbation_info=pert_info,\n)\n\ntrain_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n\n# Model\nmodel = PerturbationJEPA(\n    num_genes=20000,\n    embed_dim=256,\n    num_tokens=64,\n)\n\n# Train\ntrain_jepa_complete(\n    model,\n    train_loader,\n    val_loader,\n    num_epochs=100,\n    lr=1e-3,\n)\n</code></pre>"},{"location":"JEPA/03_jepa_applications/#14-evaluation","title":"1.4 Evaluation","text":"<p>Metrics: 1. Embedding similarity: Cosine similarity between predicted and actual embeddings 2. DEG recovery: Fraction of differentially expressed genes correctly predicted 3. Pathway consistency: Predicted perturbations affect correct pathways 4. Held-out perturbations: Generalization to unseen perturbations</p> <pre><code>@torch.no_grad()\ndef evaluate_perturbation_prediction(model, test_loader, device):\n    \"\"\"\n    Evaluate perturbation prediction.\n\n    Returns:\n        metrics: Dictionary of evaluation metrics\n    \"\"\"\n    model.eval()\n\n    all_similarities = []\n    all_deg_recalls = []\n\n    for x_baseline, x_perturbed, pert_info in test_loader:\n        x_baseline = x_baseline.to(device)\n        x_perturbed = x_perturbed.to(device)\n        pert_info = pert_info.to(device)\n\n        # Predict\n        z_pred = model.predict_perturbation(x_baseline, pert_info)\n\n        # Actual\n        z_actual = model.encoder(x_perturbed)\n\n        # Cosine similarity\n        z_pred_flat = z_pred.mean(dim=1)  # Average over tokens\n        z_actual_flat = z_actual.mean(dim=1)\n        similarity = F.cosine_similarity(z_pred_flat, z_actual_flat, dim=1)\n        all_similarities.append(similarity.cpu())\n\n        # DEG recovery (if we have a decoder)\n        # ...\n\n    metrics = {\n        'embedding_similarity': torch.cat(all_similarities).mean().item(),\n        # 'deg_recall': ...,\n        # 'pathway_consistency': ...,\n    }\n\n    return metrics\n</code></pre>"},{"location":"JEPA/03_jepa_applications/#15-advantages-over-existing-methods","title":"1.5 Advantages Over Existing Methods","text":"<p>Comparison with scGen/CPA:</p> Aspect scGen/CPA JEPA Architecture VAE + arithmetic Encoder + Predictor Perturbation Latent arithmetic Learned operators Reconstruction Required Not needed Efficiency Moderate High (no decoder) Compositional Limited Natural Generalization Moderate Better (learned operators) <p>JEPA advantages: 1. No reconstruction \u2014 Focus on prediction, not generation 2. Learned operators \u2014 Perturbations are learned, not hand-crafted 3. Compositional \u2014 Naturally combine multiple perturbations 4. Efficient \u2014 No decoder, faster training</p>"},{"location":"JEPA/03_jepa_applications/#2-trajectory-inference","title":"2. Trajectory Inference","text":""},{"location":"JEPA/03_jepa_applications/#21-problem-setup","title":"2.1 Problem Setup","text":"<p>Goal: Predict developmental or disease trajectories</p> <p>Data:</p> <ul> <li>Time-series expression: \\(\\{x_{t_1}, x_{t_2}, ..., x_{t_n}\\}\\)</li> <li>Time points: \\(\\{t_1, t_2, ..., t_n\\}\\)</li> </ul> <p>JEPA formulation: <pre><code>Context: Expression at time t\nTarget: Expression at time t+\u0394t\nTask: Predict z_{t+\u0394t} from z_t and \u0394t\n</code></pre></p>"},{"location":"JEPA/03_jepa_applications/#22-architecture","title":"2.2 Architecture","text":"<pre><code>class TrajectoryJEPA(nn.Module):\n    \"\"\"\n    JEPA for trajectory inference.\n\n    Predicts future cell states from current state and time.\n    \"\"\"\n    def __init__(\n        self,\n        num_genes=20000,\n        embed_dim=256,\n        num_tokens=64,\n        time_embed_dim=64,\n    ):\n        super().__init__()\n\n        # Encoder\n        self.encoder = GeneExpressionEncoder(\n            num_genes=num_genes,\n            embed_dim=embed_dim,\n            num_tokens=num_tokens,\n        )\n\n        # Time encoder\n        self.time_encoder = nn.Sequential(\n            SinusoidalPositionEmbeddings(time_embed_dim),\n            nn.Linear(time_embed_dim, time_embed_dim),\n            nn.GELU(),\n        )\n\n        # Predictor\n        self.predictor = ConditionalPredictor(\n            embed_dim=embed_dim,\n            condition_dim=time_embed_dim,\n            depth=4,\n        )\n\n        self.vicreg = VICRegLoss()\n\n    def forward(self, x_t, x_t_next, delta_t):\n        \"\"\"\n        Args:\n            x_t: Expression at time t (B, num_genes)\n            x_t_next: Expression at time t+\u0394t (B, num_genes)\n            delta_t: Time difference (B,)\n\n        Returns:\n            loss: Total loss\n            loss_dict: Loss components\n        \"\"\"\n        # Encode current and future\n        z_t = self.encoder(x_t)\n        z_t_next = self.encoder(x_t_next)\n\n        # Encode time\n        time_emb = self.time_encoder(delta_t)\n\n        # Predict future from current + time\n        z_pred = self.predictor(z_t, time_emb)\n\n        # VICReg loss\n        loss, loss_dict = self.vicreg(z_pred, z_t_next)\n\n        return loss, loss_dict\n\n    @torch.no_grad()\n    def predict_trajectory(self, x_start, time_points):\n        \"\"\"\n        Predict trajectory from starting point.\n\n        Args:\n            x_start: Starting expression (B, num_genes)\n            time_points: List of future time points\n\n        Returns:\n            trajectory: Predicted embeddings at each time point\n        \"\"\"\n        trajectory = []\n        z_current = self.encoder(x_start)\n\n        for t in time_points:\n            time_emb = self.time_encoder(torch.tensor([t]))\n            z_next = self.predictor(z_current, time_emb)\n            trajectory.append(z_next)\n            z_current = z_next\n\n        return torch.stack(trajectory, dim=1)  # (B, num_timepoints, num_tokens, embed_dim)\n</code></pre>"},{"location":"JEPA/03_jepa_applications/#23-applications","title":"2.3 Applications","text":"<p>1. Developmental biology:</p> <ul> <li>Predict cell differentiation trajectories</li> <li>Identify branch points</li> <li>Infer lineage relationships</li> </ul> <p>2. Disease progression:</p> <ul> <li>Predict disease state evolution</li> <li>Identify critical transitions</li> <li>Stratify patients by trajectory</li> </ul> <p>3. Drug response over time:</p> <ul> <li>Predict temporal response to drugs</li> <li>Identify optimal treatment timing</li> <li>Detect resistance emergence</li> </ul>"},{"location":"JEPA/03_jepa_applications/#3-multi-omics-integration","title":"3. Multi-Omics Integration","text":""},{"location":"JEPA/03_jepa_applications/#31-problem-setup","title":"3.1 Problem Setup","text":"<p>Goal: Predict one modality from another</p> <p>Data:</p> <ul> <li>RNA-seq: \\(x_{rna} \\in \\mathbb{R}^{20000}\\)</li> <li>Protein: \\(x_{protein} \\in \\mathbb{R}^{5000}\\)</li> <li>ATAC-seq: \\(x_{atac} \\in \\mathbb{R}^{50000}\\)</li> </ul> <p>JEPA formulation: <pre><code>Context: RNA-seq\nTarget: Protein expression\nTask: Predict z_protein from z_rna\n</code></pre></p>"},{"location":"JEPA/03_jepa_applications/#32-architecture","title":"3.2 Architecture","text":"<pre><code>class MultiOmicsJEPA(nn.Module):\n    \"\"\"\n    JEPA for multi-omics integration.\n\n    Predicts one omics modality from another.\n    \"\"\"\n    def __init__(\n        self,\n        rna_dim=20000,\n        protein_dim=5000,\n        embed_dim=256,\n        num_tokens=64,\n    ):\n        super().__init__()\n\n        # Modality-specific encoders\n        self.rna_encoder = GeneExpressionEncoder(\n            num_genes=rna_dim,\n            embed_dim=embed_dim,\n            num_tokens=num_tokens,\n        )\n\n        self.protein_encoder = GeneExpressionEncoder(\n            num_genes=protein_dim,\n            embed_dim=embed_dim,\n            num_tokens=num_tokens,\n        )\n\n        # Cross-modality predictor\n        self.predictor = JEPAPredictor(\n            embed_dim=embed_dim,\n            depth=6,\n        )\n\n        self.vicreg = VICRegLoss()\n\n    def forward(self, x_rna, x_protein):\n        \"\"\"\n        Args:\n            x_rna: RNA-seq (B, rna_dim)\n            x_protein: Protein (B, protein_dim)\n\n        Returns:\n            loss: Total loss\n            loss_dict: Loss components\n        \"\"\"\n        # Encode both modalities\n        z_rna = self.rna_encoder(x_rna)\n        z_protein = self.protein_encoder(x_protein)\n\n        # Predict protein from RNA\n        z_protein_pred = self.predictor(z_rna)\n\n        # VICReg loss\n        loss, loss_dict = self.vicreg(z_protein_pred, z_protein)\n\n        return loss, loss_dict\n\n    @torch.no_grad()\n    def predict_protein_from_rna(self, x_rna):\n        \"\"\"\n        Predict protein expression from RNA-seq.\n\n        Args:\n            x_rna: RNA-seq (B, rna_dim)\n\n        Returns:\n            z_protein_pred: Predicted protein embedding\n        \"\"\"\n        z_rna = self.rna_encoder(x_rna)\n        z_protein_pred = self.predictor(z_rna)\n        return z_protein_pred\n</code></pre>"},{"location":"JEPA/03_jepa_applications/#33-applications","title":"3.3 Applications","text":"<p>1. RNA \u2192 Protein prediction:</p> <ul> <li>Predict protein abundance from transcriptomics</li> <li>Identify post-transcriptional regulation</li> <li>Validate proteomics experiments</li> </ul> <p>2. ATAC \u2192 RNA prediction:</p> <ul> <li>Predict gene expression from chromatin accessibility</li> <li>Identify regulatory relationships</li> <li>Infer transcription factor activity</li> </ul> <p>3. Cross-species translation:</p> <ul> <li>Predict human expression from mouse</li> <li>Transfer knowledge across species</li> <li>Validate evolutionary conservation</li> </ul>"},{"location":"JEPA/03_jepa_applications/#4-drug-response-prediction","title":"4. Drug Response Prediction","text":""},{"location":"JEPA/03_jepa_applications/#41-problem-setup","title":"4.1 Problem Setup","text":"<p>Goal: Predict cellular response to drugs</p> <p>Data:</p> <ul> <li>Baseline expression: \\(x_0\\)</li> <li>Drug features: Chemical structure, target, dose</li> <li>Treated expression: \\(x_{drug}\\)</li> </ul> <p>JEPA formulation: <pre><code>Context: Baseline + drug features\nTarget: Treated expression\nTask: Predict z_treated from z_baseline and drug\n</code></pre></p>"},{"location":"JEPA/03_jepa_applications/#42-architecture","title":"4.2 Architecture","text":"<pre><code>class DrugResponseJEPA(nn.Module):\n    \"\"\"\n    JEPA for drug response prediction.\n\n    Predicts cellular response to drug treatment.\n    \"\"\"\n    def __init__(\n        self,\n        num_genes=20000,\n        drug_feature_dim=512,  # e.g., Morgan fingerprints\n        embed_dim=256,\n        num_tokens=64,\n    ):\n        super().__init__()\n\n        # Expression encoder\n        self.expr_encoder = GeneExpressionEncoder(\n            num_genes=num_genes,\n            embed_dim=embed_dim,\n            num_tokens=num_tokens,\n        )\n\n        # Drug encoder\n        self.drug_encoder = nn.Sequential(\n            nn.Linear(drug_feature_dim, embed_dim),\n            nn.LayerNorm(embed_dim),\n            nn.GELU(),\n            nn.Linear(embed_dim, embed_dim),\n        )\n\n        # Predictor\n        self.predictor = ConditionalPredictor(\n            embed_dim=embed_dim,\n            condition_dim=embed_dim,\n            depth=6,\n        )\n\n        self.vicreg = VICRegLoss()\n\n    def forward(self, x_baseline, x_treated, drug_features):\n        \"\"\"\n        Args:\n            x_baseline: Baseline expression (B, num_genes)\n            x_treated: Treated expression (B, num_genes)\n            drug_features: Drug features (B, drug_feature_dim)\n\n        Returns:\n            loss: Total loss\n            loss_dict: Loss components\n        \"\"\"\n        # Encode baseline and treated\n        z_baseline = self.expr_encoder(x_baseline)\n        z_treated = self.expr_encoder(x_treated)\n\n        # Encode drug\n        drug_emb = self.drug_encoder(drug_features)\n\n        # Predict treated from baseline + drug\n        z_pred = self.predictor(z_baseline, drug_emb)\n\n        # VICReg loss\n        loss, loss_dict = self.vicreg(z_pred, z_treated)\n\n        return loss, loss_dict\n\n    @torch.no_grad()\n    def predict_drug_response(self, x_baseline, drug_features):\n        \"\"\"\n        Predict response to drug.\n\n        Args:\n            x_baseline: Baseline expression (B, num_genes)\n            drug_features: Drug features (B, drug_feature_dim)\n\n        Returns:\n            z_pred: Predicted response embedding\n        \"\"\"\n        z_baseline = self.expr_encoder(x_baseline)\n        drug_emb = self.drug_encoder(drug_features)\n        z_pred = self.predictor(z_baseline, drug_emb)\n        return z_pred\n\n    @torch.no_grad()\n    def screen_drugs(self, x_baseline, drug_library):\n        \"\"\"\n        Screen library of drugs.\n\n        Args:\n            x_baseline: Baseline expression (1, num_genes)\n            drug_library: Library of drug features (N, drug_feature_dim)\n\n        Returns:\n            responses: Predicted responses for each drug (N, num_tokens, embed_dim)\n        \"\"\"\n        # Encode baseline once\n        z_baseline = self.expr_encoder(x_baseline)\n        z_baseline = z_baseline.repeat(len(drug_library), 1, 1)\n\n        # Encode all drugs\n        drug_embs = self.drug_encoder(drug_library)\n\n        # Predict responses\n        responses = self.predictor(z_baseline, drug_embs)\n\n        return responses\n</code></pre>"},{"location":"JEPA/03_jepa_applications/#43-applications","title":"4.3 Applications","text":"<p>1. Drug screening:</p> <ul> <li>Predict response to large drug libraries</li> <li>Identify promising candidates</li> <li>Prioritize experiments</li> </ul> <p>2. Combination therapy:</p> <ul> <li>Predict response to drug combinations</li> <li>Identify synergistic pairs</li> <li>Optimize dosing</li> </ul> <p>3. Patient stratification:</p> <ul> <li>Predict patient-specific responses</li> <li>Personalize treatment</li> <li>Identify biomarkers</li> </ul>"},{"location":"JEPA/03_jepa_applications/#5-combining-jepa-with-generative-models","title":"5. Combining JEPA with Generative Models","text":""},{"location":"JEPA/03_jepa_applications/#51-jepa-diffusion","title":"5.1 JEPA + Diffusion","text":"<p>Motivation: JEPA predicts embeddings, diffusion generates samples</p> <pre><code>class JEPADiffusionHybrid(nn.Module):\n    \"\"\"\n    Hybrid JEPA + Diffusion model.\n\n    JEPA predicts perturbed embedding.\n    Diffusion generates samples from embedding.\n    \"\"\"\n    def __init__(\n        self,\n        jepa_model,\n        diffusion_decoder,\n    ):\n        super().__init__()\n\n        self.jepa = jepa_model\n        self.diffusion = diffusion_decoder\n\n    @torch.no_grad()\n    def predict_and_generate(\n        self,\n        x_baseline,\n        perturbation_info,\n        num_samples=100,\n    ):\n        \"\"\"\n        Predict perturbed state and generate samples.\n\n        Args:\n            x_baseline: Baseline expression (B, num_genes)\n            perturbation_info: Perturbation metadata\n            num_samples: Number of samples to generate\n\n        Returns:\n            samples: Generated perturbed samples (B, num_samples, num_genes)\n            z_pred: Predicted embedding (B, num_tokens, embed_dim)\n        \"\"\"\n        # JEPA: Predict perturbed embedding\n        z_pred = self.jepa.predict_perturbation(x_baseline, perturbation_info)\n\n        # Diffusion: Generate samples from embedding\n        samples = []\n        for _ in range(num_samples):\n            sample = self.diffusion.sample(z_pred)\n            samples.append(sample)\n\n        samples = torch.stack(samples, dim=1)  # (B, num_samples, num_genes)\n\n        return samples, z_pred\n</code></pre> <p>Benefits: 1. Prediction \u2014 JEPA provides point estimate 2. Uncertainty \u2014 Diffusion provides distribution 3. Efficiency \u2014 JEPA is fast, diffusion only for final generation 4. Best of both \u2014 Combine prediction and generation</p>"},{"location":"JEPA/03_jepa_applications/#52-training-strategy","title":"5.2 Training Strategy","text":"<p>Two-stage training:</p> <p>Stage 1: Train JEPA <pre><code># Train JEPA on prediction task\ntrain_jepa(jepa_model, train_loader, num_epochs=100)\n</code></pre></p> <p>Stage 2: Train Diffusion Decoder <pre><code># Freeze JEPA encoder\nfor param in jepa_model.encoder.parameters():\n    param.requires_grad = False\n\n# Train diffusion to decode embeddings\ntrain_diffusion_decoder(\n    diffusion_decoder,\n    jepa_model.encoder,\n    train_loader,\n    num_epochs=50,\n)\n</code></pre></p> <p>Joint fine-tuning (optional): <pre><code># Fine-tune both together\nfor param in jepa_model.parameters():\n    param.requires_grad = True\n\ntrain_hybrid(hybrid_model, train_loader, num_epochs=20)\n</code></pre></p>"},{"location":"JEPA/03_jepa_applications/#6-evaluation-strategies","title":"6. Evaluation Strategies","text":""},{"location":"JEPA/03_jepa_applications/#61-embedding-level-metrics","title":"6.1 Embedding-Level Metrics","text":"<p>1. Cosine similarity: <pre><code>similarity = F.cosine_similarity(z_pred, z_actual, dim=-1).mean()\n</code></pre></p> <p>2. L2 distance: <pre><code>distance = torch.norm(z_pred - z_actual, dim=-1).mean()\n</code></pre></p> <p>3. Rank correlation: <pre><code>from scipy.stats import spearmanr\n\n# Flatten embeddings\nz_pred_flat = z_pred.flatten()\nz_actual_flat = z_actual.flatten()\n\ncorrelation, p_value = spearmanr(z_pred_flat, z_actual_flat)\n</code></pre></p>"},{"location":"JEPA/03_jepa_applications/#62-biological-metrics","title":"6.2 Biological Metrics","text":"<p>1. DEG recovery: <pre><code>def compute_deg_recovery(pred_expr, actual_expr, baseline_expr, threshold=1.5):\n    \"\"\"\n    Compute fraction of DEGs correctly predicted.\n\n    Args:\n        pred_expr: Predicted expression\n        actual_expr: Actual expression\n        baseline_expr: Baseline expression\n        threshold: Fold-change threshold for DEG\n\n    Returns:\n        recall: Fraction of actual DEGs predicted\n        precision: Fraction of predicted DEGs correct\n    \"\"\"\n    # Actual DEGs\n    actual_fc = actual_expr / (baseline_expr + 1e-6)\n    actual_degs = (actual_fc &gt; threshold) | (actual_fc &lt; 1/threshold)\n\n    # Predicted DEGs\n    pred_fc = pred_expr / (baseline_expr + 1e-6)\n    pred_degs = (pred_fc &gt; threshold) | (pred_fc &lt; 1/threshold)\n\n    # Compute metrics\n    true_positives = (actual_degs &amp; pred_degs).sum()\n    recall = true_positives / actual_degs.sum()\n    precision = true_positives / pred_degs.sum()\n\n    return recall.item(), precision.item()\n</code></pre></p> <p>2. Pathway enrichment: <pre><code>from gseapy import enrichr\n\ndef compute_pathway_consistency(pred_expr, actual_expr, baseline_expr):\n    \"\"\"\n    Check if predicted DEGs enrich for same pathways as actual DEGs.\n    \"\"\"\n    # Get actual DEGs\n    actual_fc = actual_expr / (baseline_expr + 1e-6)\n    actual_deg_genes = get_top_genes(actual_fc, top_k=200)\n\n    # Get predicted DEGs\n    pred_fc = pred_expr / (baseline_expr + 1e-6)\n    pred_deg_genes = get_top_genes(pred_fc, top_k=200)\n\n    # Enrichment analysis\n    actual_pathways = enrichr(actual_deg_genes, gene_sets='KEGG_2021')\n    pred_pathways = enrichr(pred_deg_genes, gene_sets='KEGG_2021')\n\n    # Compute overlap\n    actual_top = set(actual_pathways['Term'][:10])\n    pred_top = set(pred_pathways['Term'][:10])\n    overlap = len(actual_top &amp; pred_top) / len(actual_top)\n\n    return overlap\n</code></pre></p>"},{"location":"JEPA/03_jepa_applications/#63-downstream-task-performance","title":"6.3 Downstream Task Performance","text":"<p>Linear probing: <pre><code>@torch.no_grad()\ndef evaluate_linear_probe(model, train_data, train_labels, test_data, test_labels):\n    \"\"\"\n    Train linear classifier on embeddings.\n\n    Measures quality of learned representations.\n    \"\"\"\n    # Extract embeddings\n    train_emb = model.encoder(train_data).mean(dim=1)  # Average over tokens\n    test_emb = model.encoder(test_data).mean(dim=1)\n\n    # Train classifier\n    from sklearn.linear_model import LogisticRegression\n    clf = LogisticRegression(max_iter=1000)\n    clf.fit(train_emb.cpu(), train_labels)\n\n    # Evaluate\n    accuracy = clf.score(test_emb.cpu(), test_labels)\n\n    return accuracy\n</code></pre></p>"},{"location":"JEPA/03_jepa_applications/#key-takeaways","title":"Key Takeaways","text":""},{"location":"JEPA/03_jepa_applications/#vision-biology-mapping_1","title":"Vision \u2192 Biology Mapping","text":"<ol> <li>Images \u2192 Gene expression \u2014 Patches \u2192 Genes/modules</li> <li>Videos \u2192 Time-series \u2014 Frames \u2192 Time points</li> <li>Actions \u2192 Perturbations \u2014 Conditioning \u2192 Interventions</li> <li>Multi-view \u2192 Multi-omics \u2014 Different views \u2192 Different modalities</li> </ol>"},{"location":"JEPA/03_jepa_applications/#applications","title":"Applications","text":"<ol> <li>Perturbation prediction \u2014 Most natural JEPA application</li> <li>Trajectory inference \u2014 Temporal dynamics</li> <li>Multi-omics \u2014 Cross-modality prediction</li> <li>Drug response \u2014 Treatment prediction</li> </ol>"},{"location":"JEPA/03_jepa_applications/#advantages","title":"Advantages","text":"<ol> <li>Efficiency \u2014 No decoder, fast training</li> <li>Robustness \u2014 Focus on semantics, not pixels</li> <li>Compositional \u2014 Combine perturbations naturally</li> <li>Hybrid \u2014 Combine with diffusion for generation</li> </ol>"},{"location":"JEPA/03_jepa_applications/#best-practices","title":"Best Practices","text":"<ol> <li>Start with perturbations \u2014 Most straightforward application</li> <li>Evaluate on biology \u2014 DEGs, pathways, not just embeddings</li> <li>Combine with generative \u2014 For uncertainty quantification</li> <li>Use downstream tasks \u2014 Validate representation quality</li> </ol>"},{"location":"JEPA/03_jepa_applications/#related-documents","title":"Related Documents","text":"<ul> <li>00_jepa_overview.md \u2014 High-level concepts</li> <li>01_jepa_foundations.md \u2014 Architecture details</li> <li>02_jepa_training.md \u2014 Training strategies</li> <li>04_jepa_perturbseq.md \u2014 Detailed Perturb-seq implementation</li> </ul>"},{"location":"JEPA/03_jepa_applications/#references","title":"References","text":"<p>JEPA papers:</p> <ul> <li>Assran et al. (2023): \"Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture\"</li> <li>Bardes et al. (2024): \"V-JEPA: Latent Video Prediction\"</li> </ul> <p>Perturbation modeling:</p> <ul> <li>Lotfollahi et al. (2019): \"scGen predicts single-cell perturbation responses\"</li> <li>Roohani et al. (2023): \"Predicting transcriptional outcomes of novel multigene perturbations with GEARS\"</li> <li>Norman et al. (2019): \"Exploring genetic interaction manifolds constructed from rich single-cell phenotypes\"</li> </ul> <p>Multi-omics:</p> <ul> <li>Ma et al. (2020): \"Integrative Methods and Practical Challenges for Single-Cell Multi-omics\"</li> <li>Argelaguet et al. (2021): \"MOFA+: a statistical framework for comprehensive integration of multi-modal single-cell data\"</li> </ul>"},{"location":"JEPA/04_jepa_perturbseq/","title":"JEPA for Perturb-seq: Complete Implementation","text":"<p>This document provides a complete, end-to-end implementation of JEPA for Perturb-seq data, from data loading through training, evaluation, and comparison with existing methods.</p> <p>Prerequisites: Understanding of JEPA foundations, training, and applications.</p>"},{"location":"JEPA/04_jepa_perturbseq/#1-dataset-norman-et-al-2019","title":"1. Dataset: Norman et al. (2019)","text":""},{"location":"JEPA/04_jepa_perturbseq/#11-dataset-overview","title":"1.1 Dataset Overview","text":"<p>Norman et al. Perturb-seq dataset:</p> <ul> <li>Cells: ~100K K562 cells</li> <li>Perturbations: 101 genes (single and double knockouts)</li> <li>Technology: CRISPR-based genetic perturbations + scRNA-seq</li> <li>Genes: ~20K genes measured</li> </ul> <p>Key features:</p> <ul> <li>Single perturbations: 101 genes</li> <li>Double perturbations: 20 gene pairs</li> <li>Control cells: Non-targeting guides</li> <li>Rich phenotypes: Multiple perturbations per gene</li> </ul>"},{"location":"JEPA/04_jepa_perturbseq/#12-data-loading","title":"1.2 Data Loading","text":"<pre><code>import scanpy as sc\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\ndef load_norman_data(data_path='data/norman2019.h5ad'):\n    \"\"\"\n    Load Norman et al. Perturb-seq data.\n\n    Returns:\n        adata: AnnData object with expression and metadata\n    \"\"\"\n    # Load data\n    adata = sc.read_h5ad(data_path)\n\n    # Basic preprocessing\n    sc.pp.normalize_total(adata, target_sum=1e4)\n    sc.pp.log1p(adata)\n\n    # Select highly variable genes\n    sc.pp.highly_variable_genes(adata, n_top_genes=5000)\n    adata = adata[:, adata.var['highly_variable']]\n\n    return adata\n\n\ndef prepare_perturbseq_pairs(adata):\n    \"\"\"\n    Prepare baseline-perturbed pairs.\n\n    Args:\n        adata: AnnData with perturbation metadata\n\n    Returns:\n        baseline_expr: Baseline expression (control cells)\n        perturbed_expr: Perturbed expression\n        perturbation_info: Perturbation metadata\n    \"\"\"\n    # Get control cells (baseline)\n    control_mask = adata.obs['perturbation'] == 'control'\n    baseline_cells = adata[control_mask]\n\n    # Get perturbed cells\n    perturbed_mask = adata.obs['perturbation'] != 'control'\n    perturbed_cells = adata[perturbed_mask]\n\n    # For each perturbed cell, sample a random control as baseline\n    baseline_expr = []\n    perturbed_expr = []\n    perturbation_info = []\n\n    for i in range(len(perturbed_cells)):\n        # Random baseline\n        baseline_idx = np.random.randint(len(baseline_cells))\n        baseline_expr.append(baseline_cells.X[baseline_idx].toarray().flatten())\n\n        # Perturbed\n        perturbed_expr.append(perturbed_cells.X[i].toarray().flatten())\n\n        # Perturbation info\n        pert_gene = perturbed_cells.obs['perturbation'].iloc[i]\n        perturbation_info.append(pert_gene)\n\n    baseline_expr = np.array(baseline_expr)\n    perturbed_expr = np.array(perturbed_expr)\n\n    return baseline_expr, perturbed_expr, perturbation_info\n</code></pre>"},{"location":"JEPA/04_jepa_perturbseq/#13-perturbation-encoding","title":"1.3 Perturbation Encoding","text":"<pre><code>class PerturbationEncoder:\n    \"\"\"\n    Encode perturbation information.\n\n    Converts gene names to embeddings.\n    \"\"\"\n    def __init__(self, gene_names, embed_dim=128):\n        \"\"\"\n        Args:\n            gene_names: List of all gene names\n            embed_dim: Embedding dimension\n        \"\"\"\n        self.gene_names = gene_names\n        self.gene_to_idx = {gene: idx for idx, gene in enumerate(gene_names)}\n        self.num_genes = len(gene_names)\n        self.embed_dim = embed_dim\n\n        # Learnable gene embeddings\n        self.gene_embeddings = nn.Embedding(self.num_genes, embed_dim)\n\n    def encode(self, perturbation_list):\n        \"\"\"\n        Encode list of perturbations.\n\n        Args:\n            perturbation_list: List of perturbation strings\n                e.g., ['MAPK1', 'MAPK1+BRAF', 'control']\n\n        Returns:\n            embeddings: Perturbation embeddings (B, embed_dim)\n        \"\"\"\n        embeddings = []\n\n        for pert in perturbation_list:\n            if pert == 'control':\n                # Zero embedding for control\n                emb = torch.zeros(self.embed_dim)\n            elif '+' in pert:\n                # Double perturbation: average embeddings\n                genes = pert.split('+')\n                gene_indices = [self.gene_to_idx[g] for g in genes if g in self.gene_to_idx]\n                if gene_indices:\n                    embs = self.gene_embeddings(torch.tensor(gene_indices))\n                    emb = embs.mean(dim=0)\n                else:\n                    emb = torch.zeros(self.embed_dim)\n            else:\n                # Single perturbation\n                if pert in self.gene_to_idx:\n                    gene_idx = self.gene_to_idx[pert]\n                    emb = self.gene_embeddings(torch.tensor(gene_idx))\n                else:\n                    emb = torch.zeros(self.embed_dim)\n\n            embeddings.append(emb)\n\n        return torch.stack(embeddings)\n</code></pre>"},{"location":"JEPA/04_jepa_perturbseq/#14-dataset-class","title":"1.4 Dataset Class","text":"<pre><code>class NormanPerturbSeqDataset(Dataset):\n    \"\"\"\n    Dataset for Norman Perturb-seq with JEPA.\n    \"\"\"\n    def __init__(\n        self,\n        baseline_expr,\n        perturbed_expr,\n        perturbation_info,\n        perturbation_encoder,\n    ):\n        \"\"\"\n        Args:\n            baseline_expr: Baseline expression (N, num_genes)\n            perturbed_expr: Perturbed expression (N, num_genes)\n            perturbation_info: List of perturbation strings\n            perturbation_encoder: PerturbationEncoder instance\n        \"\"\"\n        self.baseline_expr = torch.tensor(baseline_expr, dtype=torch.float32)\n        self.perturbed_expr = torch.tensor(perturbed_expr, dtype=torch.float32)\n        self.perturbation_info = perturbation_info\n        self.perturbation_encoder = perturbation_encoder\n\n    def __len__(self):\n        return len(self.baseline_expr)\n\n    def __getitem__(self, idx):\n        baseline = self.baseline_expr[idx]\n        perturbed = self.perturbed_expr[idx]\n        pert_info = self.perturbation_info[idx]\n\n        # Encode perturbation\n        pert_emb = self.perturbation_encoder.encode([pert_info])[0]\n\n        return baseline, perturbed, pert_emb\n</code></pre>"},{"location":"JEPA/04_jepa_perturbseq/#2-model-architecture","title":"2. Model Architecture","text":""},{"location":"JEPA/04_jepa_perturbseq/#21-complete-jepa-model-for-perturb-seq","title":"2.1 Complete JEPA Model for Perturb-seq","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PerturbSeqJEPA(nn.Module):\n    \"\"\"\n    Complete JEPA model for Perturb-seq.\n\n    Predicts perturbed cell state from baseline + perturbation.\n    \"\"\"\n    def __init__(\n        self,\n        num_genes=5000,\n        embed_dim=256,\n        num_tokens=64,\n        encoder_depth=6,\n        predictor_depth=4,\n        num_heads=8,\n        perturbation_dim=128,\n    ):\n        super().__init__()\n\n        self.num_genes = num_genes\n        self.embed_dim = embed_dim\n        self.num_tokens = num_tokens\n\n        # Gene expression encoder\n        self.encoder = GeneExpressionEncoder(\n            num_genes=num_genes,\n            embed_dim=embed_dim,\n            hidden_dims=[2048, 1024],\n            num_tokens=num_tokens,\n        )\n\n        # Perturbation encoder (learnable)\n        self.perturbation_encoder = PerturbationEncoder(\n            gene_names=None,  # Will be set later\n            embed_dim=perturbation_dim,\n        )\n\n        # Conditional predictor\n        self.predictor = ConditionalPredictor(\n            embed_dim=embed_dim,\n            condition_dim=perturbation_dim,\n            depth=predictor_depth,\n            num_heads=num_heads,\n        )\n\n        # VICReg loss\n        self.vicreg = VICRegLoss(\n            lambda_inv=25.0,\n            lambda_var=25.0,\n            lambda_cov=1.0,\n        )\n\n    def forward(self, x_baseline, x_perturbed, pert_emb):\n        \"\"\"\n        Forward pass.\n\n        Args:\n            x_baseline: Baseline expression (B, num_genes)\n            x_perturbed: Perturbed expression (B, num_genes)\n            pert_emb: Perturbation embedding (B, perturbation_dim)\n\n        Returns:\n            loss: Total loss\n            loss_dict: Loss components\n        \"\"\"\n        # Encode baseline and perturbed\n        z_baseline = self.encoder(x_baseline)\n\n        with torch.no_grad():\n            z_perturbed = self.encoder(x_perturbed)\n\n        # Predict perturbed from baseline + perturbation\n        z_pred = self.predictor(z_baseline, pert_emb)\n\n        # VICReg loss\n        loss, loss_dict = self.vicreg(z_pred, z_perturbed)\n\n        return loss, loss_dict\n\n    @torch.no_grad()\n    def predict(self, x_baseline, pert_emb):\n        \"\"\"\n        Predict perturbed state.\n\n        Args:\n            x_baseline: Baseline expression (B, num_genes)\n            pert_emb: Perturbation embedding (B, perturbation_dim)\n\n        Returns:\n            z_pred: Predicted perturbed embedding (B, num_tokens, embed_dim)\n        \"\"\"\n        z_baseline = self.encoder(x_baseline)\n        z_pred = self.predictor(z_baseline, pert_emb)\n        return z_pred\n</code></pre>"},{"location":"JEPA/04_jepa_perturbseq/#3-training","title":"3. Training","text":""},{"location":"JEPA/04_jepa_perturbseq/#31-training-script","title":"3.1 Training Script","text":"<pre><code>def train_perturbseq_jepa(\n    data_path='data/norman2019.h5ad',\n    save_dir='checkpoints/perturbseq_jepa',\n    num_epochs=100,\n    batch_size=64,\n    lr=1e-3,\n    device='cuda',\n):\n    \"\"\"\n    Train JEPA on Perturb-seq data.\n\n    Args:\n        data_path: Path to Norman data\n        save_dir: Checkpoint directory\n        num_epochs: Number of epochs\n        batch_size: Batch size\n        lr: Learning rate\n        device: Device\n    \"\"\"\n    # Load data\n    print(\"Loading data...\")\n    adata = load_norman_data(data_path)\n    baseline_expr, perturbed_expr, pert_info = prepare_perturbseq_pairs(adata)\n\n    # Split train/val/test\n    n_samples = len(baseline_expr)\n    n_train = int(0.7 * n_samples)\n    n_val = int(0.15 * n_samples)\n\n    indices = np.random.permutation(n_samples)\n    train_idx = indices[:n_train]\n    val_idx = indices[n_train:n_train+n_val]\n    test_idx = indices[n_train+n_val:]\n\n    # Create perturbation encoder\n    gene_names = adata.var_names.tolist()\n    pert_encoder = PerturbationEncoder(gene_names, embed_dim=128)\n\n    # Create datasets\n    train_dataset = NormanPerturbSeqDataset(\n        baseline_expr[train_idx],\n        perturbed_expr[train_idx],\n        [pert_info[i] for i in train_idx],\n        pert_encoder,\n    )\n\n    val_dataset = NormanPerturbSeqDataset(\n        baseline_expr[val_idx],\n        perturbed_expr[val_idx],\n        [pert_info[i] for i in val_idx],\n        pert_encoder,\n    )\n\n    test_dataset = NormanPerturbSeqDataset(\n        baseline_expr[test_idx],\n        perturbed_expr[test_idx],\n        [pert_info[i] for i in test_idx],\n        pert_encoder,\n    )\n\n    # Data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n\n    # Model\n    print(\"Creating model...\")\n    model = PerturbSeqJEPA(\n        num_genes=adata.n_vars,\n        embed_dim=256,\n        num_tokens=64,\n        encoder_depth=6,\n        predictor_depth=4,\n    )\n    model.perturbation_encoder = pert_encoder\n    model.to(device)\n\n    # Train\n    print(\"Training...\")\n    train_jepa_complete(\n        model,\n        train_loader,\n        val_loader,\n        num_epochs=num_epochs,\n        lr=lr,\n        weight_decay=0.01,\n        warmup_epochs=10,\n        device=device,\n        save_dir=save_dir,\n    )\n\n    # Evaluate\n    print(\"\\nEvaluating on test set...\")\n    test_metrics = evaluate_perturbseq(model, test_loader, device)\n    print(f\"Test embedding similarity: {test_metrics['embedding_similarity']:.4f}\")\n\n    return model, test_metrics\n\n\n# Run training\nif __name__ == '__main__':\n    model, metrics = train_perturbseq_jepa(\n        data_path='data/norman2019.h5ad',\n        num_epochs=100,\n        batch_size=64,\n        lr=1e-3,\n    )\n</code></pre>"},{"location":"JEPA/04_jepa_perturbseq/#32-hyperparameters","title":"3.2 Hyperparameters","text":"<p>Recommended settings for Norman data:</p> Parameter Value Notes Batch size 64 Adjust based on GPU memory Learning rate 1e-3 Higher than images Embed dim 256 Balance capacity and speed Num tokens 64 Compress 5K genes to 64 tokens Encoder depth 6 Moderate depth Predictor depth 4 0.67\u00d7 encoder depth Warmup epochs 10 ~10% of total Weight decay 0.01 Regularization"},{"location":"JEPA/04_jepa_perturbseq/#4-evaluation","title":"4. Evaluation","text":""},{"location":"JEPA/04_jepa_perturbseq/#41-embedding-level-metrics","title":"4.1 Embedding-Level Metrics","text":"<pre><code>@torch.no_grad()\ndef evaluate_perturbseq(model, test_loader, device):\n    \"\"\"\n    Evaluate JEPA on Perturb-seq test set.\n\n    Returns:\n        metrics: Dictionary of evaluation metrics\n    \"\"\"\n    model.eval()\n\n    all_similarities = []\n    all_distances = []\n\n    for x_baseline, x_perturbed, pert_emb in test_loader:\n        x_baseline = x_baseline.to(device)\n        x_perturbed = x_perturbed.to(device)\n        pert_emb = pert_emb.to(device)\n\n        # Predict\n        z_pred = model.predict(x_baseline, pert_emb)\n\n        # Actual\n        z_actual = model.encoder(x_perturbed)\n\n        # Average over tokens\n        z_pred_mean = z_pred.mean(dim=1)  # (B, embed_dim)\n        z_actual_mean = z_actual.mean(dim=1)\n\n        # Cosine similarity\n        similarity = F.cosine_similarity(z_pred_mean, z_actual_mean, dim=1)\n        all_similarities.append(similarity.cpu())\n\n        # L2 distance\n        distance = torch.norm(z_pred_mean - z_actual_mean, dim=1)\n        all_distances.append(distance.cpu())\n\n    # Aggregate\n    all_similarities = torch.cat(all_similarities)\n    all_distances = torch.cat(all_distances)\n\n    metrics = {\n        'embedding_similarity': all_similarities.mean().item(),\n        'embedding_similarity_std': all_similarities.std().item(),\n        'embedding_distance': all_distances.mean().item(),\n        'embedding_distance_std': all_distances.std().item(),\n    }\n\n    return metrics\n</code></pre>"},{"location":"JEPA/04_jepa_perturbseq/#42-held-out-perturbation-evaluation","title":"4.2 Held-Out Perturbation Evaluation","text":"<pre><code>def evaluate_held_out_perturbations(\n    model,\n    adata,\n    held_out_genes,\n    device='cuda',\n):\n    \"\"\"\n    Evaluate on held-out perturbations.\n\n    Tests generalization to unseen perturbations.\n\n    Args:\n        model: Trained JEPA model\n        adata: Full AnnData\n        held_out_genes: List of genes to hold out\n        device: Device\n\n    Returns:\n        metrics: Evaluation metrics on held-out perturbations\n    \"\"\"\n    model.eval()\n\n    # Get cells with held-out perturbations\n    held_out_mask = adata.obs['perturbation'].isin(held_out_genes)\n    held_out_cells = adata[held_out_mask]\n\n    # Get control cells\n    control_mask = adata.obs['perturbation'] == 'control'\n    control_cells = adata[control_mask]\n\n    # Prepare pairs\n    baseline_expr = []\n    perturbed_expr = []\n    pert_info = []\n\n    for i in range(len(held_out_cells)):\n        baseline_idx = np.random.randint(len(control_cells))\n        baseline_expr.append(control_cells.X[baseline_idx].toarray().flatten())\n        perturbed_expr.append(held_out_cells.X[i].toarray().flatten())\n        pert_info.append(held_out_cells.obs['perturbation'].iloc[i])\n\n    baseline_expr = torch.tensor(np.array(baseline_expr), dtype=torch.float32).to(device)\n    perturbed_expr = torch.tensor(np.array(perturbed_expr), dtype=torch.float32).to(device)\n\n    # Encode perturbations\n    pert_embs = model.perturbation_encoder.encode(pert_info).to(device)\n\n    # Predict\n    z_pred = model.predict(baseline_expr, pert_embs)\n    z_actual = model.encoder(perturbed_expr)\n\n    # Metrics\n    z_pred_mean = z_pred.mean(dim=1)\n    z_actual_mean = z_actual.mean(dim=1)\n\n    similarity = F.cosine_similarity(z_pred_mean, z_actual_mean, dim=1).mean().item()\n\n    print(f\"Held-out perturbations ({len(held_out_genes)} genes):\")\n    print(f\"  Embedding similarity: {similarity:.4f}\")\n\n    return {'held_out_similarity': similarity}\n</code></pre>"},{"location":"JEPA/04_jepa_perturbseq/#43-comparison-with-baselines","title":"4.3 Comparison with Baselines","text":"<pre><code>def compare_with_baselines(\n    jepa_model,\n    test_loader,\n    device='cuda',\n):\n    \"\"\"\n    Compare JEPA with baseline methods.\n\n    Baselines:\n    1. Mean prediction (predict mean of perturbed cells)\n    2. Baseline copy (no change from baseline)\n    3. scGen (if available)\n\n    Args:\n        jepa_model: Trained JEPA model\n        test_loader: Test data loader\n        device: Device\n\n    Returns:\n        comparison: Dictionary with results for each method\n    \"\"\"\n    jepa_model.eval()\n\n    # Collect all data\n    all_baseline = []\n    all_perturbed = []\n    all_pert_emb = []\n\n    for x_baseline, x_perturbed, pert_emb in test_loader:\n        all_baseline.append(x_baseline)\n        all_perturbed.append(x_perturbed)\n        all_pert_emb.append(pert_emb)\n\n    all_baseline = torch.cat(all_baseline, dim=0).to(device)\n    all_perturbed = torch.cat(all_perturbed, dim=0).to(device)\n    all_pert_emb = torch.cat(all_pert_emb, dim=0).to(device)\n\n    # 1. JEPA\n    z_pred_jepa = jepa_model.predict(all_baseline, all_pert_emb)\n    z_actual = jepa_model.encoder(all_perturbed)\n\n    z_pred_jepa_mean = z_pred_jepa.mean(dim=1)\n    z_actual_mean = z_actual.mean(dim=1)\n\n    jepa_similarity = F.cosine_similarity(z_pred_jepa_mean, z_actual_mean, dim=1).mean().item()\n\n    # 2. Mean prediction (predict mean of all perturbed)\n    z_mean_pred = z_actual_mean.mean(dim=0, keepdim=True).repeat(len(z_actual_mean), 1)\n    mean_similarity = F.cosine_similarity(z_mean_pred, z_actual_mean, dim=1).mean().item()\n\n    # 3. Baseline copy (no change)\n    z_baseline = jepa_model.encoder(all_baseline).mean(dim=1)\n    baseline_similarity = F.cosine_similarity(z_baseline, z_actual_mean, dim=1).mean().item()\n\n    # Results\n    comparison = {\n        'JEPA': jepa_similarity,\n        'Mean prediction': mean_similarity,\n        'Baseline copy': baseline_similarity,\n    }\n\n    print(\"\\nComparison with baselines:\")\n    for method, sim in comparison.items():\n        print(f\"  {method}: {sim:.4f}\")\n\n    return comparison\n</code></pre>"},{"location":"JEPA/04_jepa_perturbseq/#5-analysis-and-visualization","title":"5. Analysis and Visualization","text":""},{"location":"JEPA/04_jepa_perturbseq/#51-embedding-space-visualization","title":"5.1 Embedding Space Visualization","text":"<pre><code>import matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\n\n@torch.no_grad()\ndef visualize_embeddings(model, test_loader, device='cuda'):\n    \"\"\"\n    Visualize predicted vs actual embeddings.\n\n    Args:\n        model: Trained JEPA model\n        test_loader: Test data loader\n        device: Device\n    \"\"\"\n    model.eval()\n\n    # Collect embeddings\n    z_pred_list = []\n    z_actual_list = []\n    pert_list = []\n\n    for x_baseline, x_perturbed, pert_emb in test_loader:\n        x_baseline = x_baseline.to(device)\n        x_perturbed = x_perturbed.to(device)\n        pert_emb = pert_emb.to(device)\n\n        z_pred = model.predict(x_baseline, pert_emb).mean(dim=1)\n        z_actual = model.encoder(x_perturbed).mean(dim=1)\n\n        z_pred_list.append(z_pred.cpu())\n        z_actual_list.append(z_actual.cpu())\n\n    z_pred_all = torch.cat(z_pred_list, dim=0).numpy()\n    z_actual_all = torch.cat(z_actual_list, dim=0).numpy()\n\n    # PCA\n    pca = PCA(n_components=2)\n    z_combined = np.vstack([z_pred_all, z_actual_all])\n    z_pca = pca.fit_transform(z_combined)\n\n    z_pred_pca = z_pca[:len(z_pred_all)]\n    z_actual_pca = z_pca[len(z_pred_all):]\n\n    # Plot\n    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n\n    ax.scatter(z_pred_pca[:, 0], z_pred_pca[:, 1], \n               alpha=0.5, label='Predicted', s=10)\n    ax.scatter(z_actual_pca[:, 0], z_actual_pca[:, 1], \n               alpha=0.5, label='Actual', s=10)\n\n    # Draw lines connecting pairs\n    for i in range(min(100, len(z_pred_pca))):\n        ax.plot([z_pred_pca[i, 0], z_actual_pca[i, 0]],\n                [z_pred_pca[i, 1], z_actual_pca[i, 1]],\n                'k-', alpha=0.1, linewidth=0.5)\n\n    ax.set_xlabel('PC1')\n    ax.set_ylabel('PC2')\n    ax.legend()\n    ax.set_title('Predicted vs Actual Embeddings')\n\n    plt.tight_layout()\n    plt.savefig('embeddings_visualization.png', dpi=300)\n    plt.show()\n</code></pre>"},{"location":"JEPA/04_jepa_perturbseq/#52-per-perturbation-analysis","title":"5.2 Per-Perturbation Analysis","text":"<pre><code>@torch.no_grad()\ndef analyze_per_perturbation(model, adata, device='cuda'):\n    \"\"\"\n    Analyze prediction quality per perturbation.\n\n    Args:\n        model: Trained JEPA model\n        adata: AnnData with all data\n        device: Device\n\n    Returns:\n        results: DataFrame with per-perturbation metrics\n    \"\"\"\n    model.eval()\n\n    # Get unique perturbations\n    perturbations = adata.obs['perturbation'].unique()\n    perturbations = [p for p in perturbations if p != 'control']\n\n    results = []\n\n    for pert in perturbations:\n        # Get cells with this perturbation\n        pert_mask = adata.obs['perturbation'] == pert\n        pert_cells = adata[pert_mask]\n\n        if len(pert_cells) &lt; 10:\n            continue\n\n        # Get control cells\n        control_mask = adata.obs['perturbation'] == 'control'\n        control_cells = adata[control_mask]\n\n        # Prepare data\n        baseline_expr = []\n        perturbed_expr = []\n\n        for i in range(len(pert_cells)):\n            baseline_idx = np.random.randint(len(control_cells))\n            baseline_expr.append(control_cells.X[baseline_idx].toarray().flatten())\n            perturbed_expr.append(pert_cells.X[i].toarray().flatten())\n\n        baseline_expr = torch.tensor(np.array(baseline_expr), dtype=torch.float32).to(device)\n        perturbed_expr = torch.tensor(np.array(perturbed_expr), dtype=torch.float32).to(device)\n\n        # Encode perturbation\n        pert_emb = model.perturbation_encoder.encode([pert] * len(baseline_expr)).to(device)\n\n        # Predict\n        z_pred = model.predict(baseline_expr, pert_emb).mean(dim=1)\n        z_actual = model.encoder(perturbed_expr).mean(dim=1)\n\n        # Metrics\n        similarity = F.cosine_similarity(z_pred, z_actual, dim=1).mean().item()\n        distance = torch.norm(z_pred - z_actual, dim=1).mean().item()\n\n        results.append({\n            'perturbation': pert,\n            'n_cells': len(pert_cells),\n            'similarity': similarity,\n            'distance': distance,\n        })\n\n    results_df = pd.DataFrame(results)\n    results_df = results_df.sort_values('similarity', ascending=False)\n\n    # Plot\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n    # Similarity\n    axes[0].barh(range(len(results_df)), results_df['similarity'])\n    axes[0].set_yticks(range(len(results_df)))\n    axes[0].set_yticklabels(results_df['perturbation'], fontsize=6)\n    axes[0].set_xlabel('Embedding Similarity')\n    axes[0].set_title('Prediction Quality per Perturbation')\n\n    # Distance\n    axes[1].barh(range(len(results_df)), results_df['distance'])\n    axes[1].set_yticks(range(len(results_df)))\n    axes[1].set_yticklabels(results_df['perturbation'], fontsize=6)\n    axes[1].set_xlabel('Embedding Distance')\n    axes[1].set_title('Prediction Error per Perturbation')\n\n    plt.tight_layout()\n    plt.savefig('per_perturbation_analysis.png', dpi=300)\n    plt.show()\n\n    return results_df\n</code></pre>"},{"location":"JEPA/04_jepa_perturbseq/#6-downstream-applications","title":"6. Downstream Applications","text":""},{"location":"JEPA/04_jepa_perturbseq/#61-virtual-screening","title":"6.1 Virtual Screening","text":"<pre><code>@torch.no_grad()\ndef virtual_screen_perturbations(\n    model,\n    baseline_cells,\n    candidate_perturbations,\n    target_phenotype,\n    device='cuda',\n):\n    \"\"\"\n    Screen candidate perturbations for desired phenotype.\n\n    Args:\n        model: Trained JEPA model\n        baseline_cells: Baseline expression (N, num_genes)\n        candidate_perturbations: List of perturbation names\n        target_phenotype: Target embedding (embed_dim,)\n        device: Device\n\n    Returns:\n        rankings: Perturbations ranked by similarity to target\n    \"\"\"\n    model.eval()\n\n    baseline_cells = torch.tensor(baseline_cells, dtype=torch.float32).to(device)\n    target_phenotype = target_phenotype.to(device)\n\n    results = []\n\n    for pert in candidate_perturbations:\n        # Encode perturbation\n        pert_emb = model.perturbation_encoder.encode([pert] * len(baseline_cells)).to(device)\n\n        # Predict\n        z_pred = model.predict(baseline_cells, pert_emb).mean(dim=1)  # (N, embed_dim)\n\n        # Average over cells\n        z_pred_mean = z_pred.mean(dim=0)  # (embed_dim,)\n\n        # Similarity to target\n        similarity = F.cosine_similarity(z_pred_mean.unsqueeze(0), target_phenotype.unsqueeze(0)).item()\n\n        results.append({\n            'perturbation': pert,\n            'similarity_to_target': similarity,\n        })\n\n    # Rank by similarity\n    results_df = pd.DataFrame(results)\n    results_df = results_df.sort_values('similarity_to_target', ascending=False)\n\n    print(\"Top 10 perturbations for target phenotype:\")\n    print(results_df.head(10))\n\n    return results_df\n</code></pre>"},{"location":"JEPA/04_jepa_perturbseq/#62-combination-prediction","title":"6.2 Combination Prediction","text":"<pre><code>@torch.no_grad()\ndef predict_combination(\n    model,\n    baseline_cells,\n    gene1,\n    gene2,\n    device='cuda',\n):\n    \"\"\"\n    Predict effect of double perturbation.\n\n    Args:\n        model: Trained JEPA model\n        baseline_cells: Baseline expression (N, num_genes)\n        gene1: First gene to perturb\n        gene2: Second gene to perturb\n        device: Device\n\n    Returns:\n        z_pred: Predicted embedding for combination\n    \"\"\"\n    model.eval()\n\n    baseline_cells = torch.tensor(baseline_cells, dtype=torch.float32).to(device)\n\n    # Encode combination\n    combination_name = f\"{gene1}+{gene2}\"\n    pert_emb = model.perturbation_encoder.encode([combination_name] * len(baseline_cells)).to(device)\n\n    # Predict\n    z_pred = model.predict(baseline_cells, pert_emb)\n\n    return z_pred\n</code></pre>"},{"location":"JEPA/04_jepa_perturbseq/#key-takeaways","title":"Key Takeaways","text":""},{"location":"JEPA/04_jepa_perturbseq/#dataset","title":"Dataset","text":"<ol> <li>Norman et al. \u2014 Standard Perturb-seq benchmark</li> <li>101 genes \u2014 Single and double perturbations</li> <li>~100K cells \u2014 Rich dataset for training</li> <li>5K genes \u2014 Use highly variable genes</li> </ol>"},{"location":"JEPA/04_jepa_perturbseq/#model","title":"Model","text":"<ol> <li>Gene expression encoder \u2014 MLP + transformer on tokens</li> <li>Perturbation encoder \u2014 Learnable gene embeddings</li> <li>Conditional predictor \u2014 Cross-attention with perturbation</li> <li>VICReg loss \u2014 Prevents collapse</li> </ol>"},{"location":"JEPA/04_jepa_perturbseq/#training","title":"Training","text":"<ol> <li>Batch size 64 \u2014 Balance speed and memory</li> <li>LR 1e-3 \u2014 Higher than images</li> <li>100 epochs \u2014 Sufficient for convergence</li> <li>Warmup 10 epochs \u2014 Stabilize early training</li> </ol>"},{"location":"JEPA/04_jepa_perturbseq/#evaluation","title":"Evaluation","text":"<ol> <li>Embedding similarity \u2014 Primary metric</li> <li>Held-out perturbations \u2014 Test generalization</li> <li>Per-perturbation analysis \u2014 Identify strengths/weaknesses</li> <li>Comparison with baselines \u2014 Validate improvements</li> </ol>"},{"location":"JEPA/04_jepa_perturbseq/#applications","title":"Applications","text":"<ol> <li>Virtual screening \u2014 Predict effects of new perturbations</li> <li>Combination prediction \u2014 Double perturbations</li> <li>Phenotype search \u2014 Find perturbations for target state</li> <li>Mechanism discovery \u2014 Analyze learned representations</li> </ol>"},{"location":"JEPA/04_jepa_perturbseq/#related-documents","title":"Related Documents","text":"<ul> <li>00_jepa_overview.md \u2014 High-level concepts</li> <li>01_jepa_foundations.md \u2014 Architecture details</li> <li>02_jepa_training.md \u2014 Training strategies</li> <li>03_jepa_applications.md \u2014 General applications</li> </ul>"},{"location":"JEPA/04_jepa_perturbseq/#references","title":"References","text":"<p>Perturb-seq data:</p> <ul> <li>Norman et al. (2019): \"Exploring genetic interaction manifolds constructed from rich single-cell phenotypes\"</li> <li>Replogle et al. (2022): \"Mapping information-rich genotype-phenotype landscapes with genome-scale Perturb-seq\"</li> </ul> <p>Baseline methods:</p> <ul> <li>Lotfollahi et al. (2019): \"scGen predicts single-cell perturbation responses\"</li> <li>Roohani et al. (2023): \"Predicting transcriptional outcomes of novel multigene perturbations with GEARS\"</li> </ul> <p>JEPA:</p> <ul> <li>Assran et al. (2023): \"Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture\"</li> </ul>"},{"location":"JEPA/open_research_joint_latent/","title":"Open Research: Joint Latent Spaces for Biology","text":"<p>This document explores the concept of joint latent spaces for computational biology, drawing insights from the Goku model and discussing how static and dynamic biological data can share the same latent manifold.</p> <p>Key insight: If two data types differ only by dimensionality or observation density, they probably want the same latent space.</p>"},{"location":"JEPA/open_research_joint_latent/#the-goku-insight","title":"The Goku Insight","text":""},{"location":"JEPA/open_research_joint_latent/#the-problem-with-separate-models","title":"The Problem with Separate Models","text":"<p>Historical accident in computer vision:</p> <ul> <li>Images and videos treated as fundamentally different</li> <li>Separate models, separate latent spaces</li> <li>No knowledge transfer</li> </ul> <p>Mathematical reality:</p> \\[ \\text{Video} \\in \\mathbb{R}^{T \\times H \\times W \\times C} \\] <p>An image is just \\(T = 1\\).</p> <p>The split was artificial \u2014 driven by engineering constraints, not fundamental differences.</p>"},{"location":"JEPA/open_research_joint_latent/#the-solution-one-latent-space","title":"The Solution: One Latent Space","text":"<p>Goku's approach (ByteDance &amp; HKU, 2024): - Single encoder-decoder for both images and videos - Same latent manifold for static and dynamic data - Mutual training \u2014 images teach spatial priors, videos teach dynamics</p> <p>Why this works: 1. Shared structure \u2014 Both have spatial organization 2. Complementary information \u2014 Static (appearance) + dynamic (motion) 3. Prior sharing \u2014 Learn better representations together</p>"},{"location":"JEPA/open_research_joint_latent/#biology-parallel","title":"Biology Parallel","text":""},{"location":"JEPA/open_research_joint_latent/#static-vs-dynamic-data","title":"Static vs Dynamic Data","text":"<p>Static data (like images): - Bulk RNA-seq \u2014 Population-level expression - Baseline scRNA-seq \u2014 Single-cell snapshots - Spatial transcriptomics \u2014 Tissue organization - Proteomics \u2014 Protein abundance</p> <p>Dynamic data (like videos): - Time-series \u2014 Developmental trajectories - Perturb-seq \u2014 Perturbation responses - Lineage tracing \u2014 Cell fate decisions - Drug response \u2014 Temporal treatment effects</p> <p>Key observation: These differ in observation density and temporal resolution, not fundamental biology.</p>"},{"location":"JEPA/open_research_joint_latent/#the-biological-manifold","title":"The Biological Manifold","text":"<p>Hypothesis: All cellular states live on the same biological manifold.</p> <pre><code>Bulk RNA-seq \u2500\u2500\u2510\n               \u251c\u2500\u2500&gt; Shared Encoder \u2500\u2500&gt; Joint Latent Space \u2500\u2500&gt; Shared Decoder\nTime-series \u2500\u2500\u2500\u2524\n               \u2502\nPerturb-seq \u2500\u2500\u2500\u2518\n</code></pre> <p>Benefits: 1. Static data teaches spatial priors \u2014 Cell types, pathways, gene modules 2. Dynamic data teaches temporal dynamics \u2014 Transitions, trajectories, causality 3. Both inform the same representation \u2014 Better than either alone</p>"},{"location":"JEPA/open_research_joint_latent/#joint-vae-architecture","title":"Joint VAE Architecture","text":""},{"location":"JEPA/open_research_joint_latent/#concept","title":"Concept","text":"<p>Traditional approach: <pre><code># Separate models\nbulk_vae = VAE(input_dim=20000, latent_dim=256)\ntimeseries_vae = VAE(input_dim=20000, latent_dim=256)\n\n# Different latent spaces\nz_bulk = bulk_vae.encode(x_bulk)\nz_time = timeseries_vae.encode(x_time)\n# z_bulk and z_time are incompatible\n</code></pre></p> <p>Joint approach: <pre><code># Single model\njoint_vae = JointVAE(input_dim=20000, latent_dim=256)\n\n# Same latent space\nz_bulk = joint_vae.encode(x_bulk)\nz_time = joint_vae.encode(x_time)\n# z_bulk and z_time are comparable\n</code></pre></p>"},{"location":"JEPA/open_research_joint_latent/#implementation","title":"Implementation","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass JointBioVAE(nn.Module):\n    \"\"\"\n    Joint VAE for static and dynamic biological data.\n\n    Encodes both bulk RNA-seq and time-series into same latent space.\n    \"\"\"\n    def __init__(\n        self,\n        num_genes=20000,\n        latent_dim=256,\n        hidden_dims=[2048, 1024, 512],\n    ):\n        super().__init__()\n\n        # Shared encoder\n        encoder_layers = []\n        in_dim = num_genes\n        for hidden_dim in hidden_dims:\n            encoder_layers.extend([\n                nn.Linear(in_dim, hidden_dim),\n                nn.LayerNorm(hidden_dim),\n                nn.GELU(),\n                nn.Dropout(0.1),\n            ])\n            in_dim = hidden_dim\n\n        self.encoder_backbone = nn.Sequential(*encoder_layers)\n\n        # Latent projection\n        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)\n        self.fc_logvar = nn.Linear(hidden_dims[-1], latent_dim)\n\n        # Shared decoder\n        decoder_layers = []\n        in_dim = latent_dim\n        for hidden_dim in reversed(hidden_dims):\n            decoder_layers.extend([\n                nn.Linear(in_dim, hidden_dim),\n                nn.LayerNorm(hidden_dim),\n                nn.GELU(),\n                nn.Dropout(0.1),\n            ])\n            in_dim = hidden_dim\n\n        decoder_layers.append(nn.Linear(in_dim, num_genes))\n        self.decoder = nn.Sequential(*decoder_layers)\n\n    def encode(self, x):\n        \"\"\"\n        Encode to latent space.\n\n        Works for both static and dynamic data.\n\n        Args:\n            x: Expression (B, num_genes) or (B, T, num_genes)\n\n        Returns:\n            mu: Mean (B, latent_dim) or (B, T, latent_dim)\n            logvar: Log variance\n        \"\"\"\n        original_shape = x.shape\n\n        # Flatten time dimension if present\n        if len(x.shape) == 3:\n            B, T, G = x.shape\n            x = x.view(B * T, G)\n\n        # Encode\n        h = self.encoder_backbone(x)\n        mu = self.fc_mu(h)\n        logvar = self.fc_logvar(h)\n\n        # Reshape if time dimension\n        if len(original_shape) == 3:\n            mu = mu.view(B, T, -1)\n            logvar = logvar.view(B, T, -1)\n\n        return mu, logvar\n\n    def reparameterize(self, mu, logvar):\n        \"\"\"Reparameterization trick.\"\"\"\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def decode(self, z):\n        \"\"\"\n        Decode from latent space.\n\n        Args:\n            z: Latent (B, latent_dim) or (B, T, latent_dim)\n\n        Returns:\n            x_recon: Reconstructed expression\n        \"\"\"\n        original_shape = z.shape\n\n        # Flatten time dimension if present\n        if len(z.shape) == 3:\n            B, T, L = z.shape\n            z = z.view(B * T, L)\n\n        # Decode\n        x_recon = self.decoder(z)\n\n        # Reshape if time dimension\n        if len(original_shape) == 3:\n            x_recon = x_recon.view(B, T, -1)\n\n        return x_recon\n\n    def forward(self, x):\n        \"\"\"Forward pass.\"\"\"\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        x_recon = self.decode(z)\n        return x_recon, mu, logvar\n</code></pre>"},{"location":"JEPA/open_research_joint_latent/#training-strategy","title":"Training Strategy","text":"<p>Joint training on mixed batches:</p> <pre><code>def train_joint_vae(\n    model,\n    bulk_data,\n    timeseries_data,\n    num_epochs=100,\n    batch_size=64,\n    device='cuda',\n):\n    \"\"\"\n    Train joint VAE on mixed batches.\n\n    Args:\n        model: JointBioVAE\n        bulk_data: Static bulk RNA-seq (N, num_genes)\n        timeseries_data: Time-series data (M, T, num_genes)\n        num_epochs: Number of epochs\n        batch_size: Batch size\n        device: Device\n    \"\"\"\n    model.to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n\n    for epoch in range(num_epochs):\n        # Sample mixed batch\n        # Half bulk, half time-series\n        bulk_idx = np.random.choice(len(bulk_data), batch_size // 2)\n        time_idx = np.random.choice(len(timeseries_data), batch_size // 2)\n\n        x_bulk = torch.tensor(bulk_data[bulk_idx], dtype=torch.float32).to(device)\n        x_time = torch.tensor(timeseries_data[time_idx], dtype=torch.float32).to(device)\n\n        # Forward pass on bulk\n        x_bulk_recon, mu_bulk, logvar_bulk = model(x_bulk)\n        loss_bulk = vae_loss(x_bulk_recon, x_bulk, mu_bulk, logvar_bulk)\n\n        # Forward pass on time-series\n        x_time_recon, mu_time, logvar_time = model(x_time)\n        loss_time = vae_loss(x_time_recon, x_time, mu_time, logvar_time)\n\n        # Total loss\n        loss = loss_bulk + loss_time\n\n        # Backward\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if epoch % 10 == 0:\n            print(f\"Epoch {epoch}: Bulk loss = {loss_bulk:.4f}, Time loss = {loss_time:.4f}\")\n\n\ndef vae_loss(x_recon, x, mu, logvar):\n    \"\"\"VAE loss (ELBO).\"\"\"\n    # Reconstruction loss\n    recon_loss = F.mse_loss(x_recon, x, reduction='sum')\n\n    # KL divergence\n    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n\n    return recon_loss + kl_loss\n</code></pre>"},{"location":"JEPA/open_research_joint_latent/#joint-jepa-architecture","title":"Joint JEPA Architecture","text":""},{"location":"JEPA/open_research_joint_latent/#concept_1","title":"Concept","text":"<p>Extend JEPA to joint latent spaces:</p> <pre><code>class JointBioJEPA(nn.Module):\n    \"\"\"\n    Joint JEPA for static and dynamic biological data.\n\n    Predicts in shared embedding space.\n    \"\"\"\n    def __init__(\n        self,\n        num_genes=20000,\n        embed_dim=256,\n        num_tokens=64,\n    ):\n        super().__init__()\n\n        # Shared encoder (works for both static and dynamic)\n        self.encoder = GeneExpressionEncoder(\n            num_genes=num_genes,\n            embed_dim=embed_dim,\n            num_tokens=num_tokens,\n        )\n\n        # Predictor for temporal dynamics\n        self.temporal_predictor = TemporalPredictor(\n            embed_dim=embed_dim,\n            depth=6,\n        )\n\n        # Predictor for perturbations\n        self.perturbation_predictor = ConditionalPredictor(\n            embed_dim=embed_dim,\n            condition_dim=128,\n            depth=4,\n        )\n\n        # VICReg loss\n        self.vicreg = VICRegLoss()\n\n    def forward_temporal(self, x_past, x_future):\n        \"\"\"\n        Temporal prediction task.\n\n        Args:\n            x_past: Past frames (B, T_past, num_genes)\n            x_future: Future frames (B, T_future, num_genes)\n\n        Returns:\n            loss: Temporal prediction loss\n        \"\"\"\n        # Encode past and future\n        B, T_past, G = x_past.shape\n        x_past_flat = x_past.view(B * T_past, G)\n        z_past = self.encoder(x_past_flat)\n        z_past = z_past.view(B, T_past, -1, self.encoder.embed_dim)\n\n        _, T_future, _ = x_future.shape\n        x_future_flat = x_future.view(B * T_future, G)\n        with torch.no_grad():\n            z_future = self.encoder(x_future_flat)\n        z_future = z_future.view(B, T_future, -1, self.encoder.embed_dim)\n\n        # Predict future from past\n        z_future_pred = self.temporal_predictor(z_past)\n\n        # VICReg loss\n        loss, loss_dict = self.vicreg(z_future_pred, z_future)\n\n        return loss, loss_dict\n\n    def forward_perturbation(self, x_baseline, x_perturbed, pert_emb):\n        \"\"\"\n        Perturbation prediction task.\n\n        Args:\n            x_baseline: Baseline (B, num_genes)\n            x_perturbed: Perturbed (B, num_genes)\n            pert_emb: Perturbation embedding (B, condition_dim)\n\n        Returns:\n            loss: Perturbation prediction loss\n        \"\"\"\n        # Encode\n        z_baseline = self.encoder(x_baseline)\n        with torch.no_grad():\n            z_perturbed = self.encoder(x_perturbed)\n\n        # Predict\n        z_perturbed_pred = self.perturbation_predictor(z_baseline, pert_emb)\n\n        # VICReg loss\n        loss, loss_dict = self.vicreg(z_perturbed_pred, z_perturbed)\n\n        return loss, loss_dict\n</code></pre>"},{"location":"JEPA/open_research_joint_latent/#multi-task-training","title":"Multi-Task Training","text":"<p>Train on both tasks simultaneously:</p> <pre><code>def train_joint_jepa(\n    model,\n    timeseries_loader,\n    perturbseq_loader,\n    num_epochs=100,\n    device='cuda',\n):\n    \"\"\"\n    Train joint JEPA on multiple tasks.\n\n    Args:\n        model: JointBioJEPA\n        timeseries_loader: Time-series data loader\n        perturbseq_loader: Perturb-seq data loader\n        num_epochs: Number of epochs\n        device: Device\n    \"\"\"\n    model.to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n\n    for epoch in range(num_epochs):\n        # Iterate over both datasets\n        for (x_past, x_future), (x_baseline, x_perturbed, pert_emb) in zip(\n            timeseries_loader, perturbseq_loader\n        ):\n            # Temporal task\n            x_past = x_past.to(device)\n            x_future = x_future.to(device)\n            loss_temporal, _ = model.forward_temporal(x_past, x_future)\n\n            # Perturbation task\n            x_baseline = x_baseline.to(device)\n            x_perturbed = x_perturbed.to(device)\n            pert_emb = pert_emb.to(device)\n            loss_pert, _ = model.forward_perturbation(x_baseline, x_perturbed, pert_emb)\n\n            # Total loss\n            loss = loss_temporal + loss_pert\n\n            # Backward\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        print(f\"Epoch {epoch}: Temporal = {loss_temporal:.4f}, Pert = {loss_pert:.4f}\")\n</code></pre>"},{"location":"JEPA/open_research_joint_latent/#patch-n-pack-variable-length-sequences","title":"Patch n' Pack: Variable-Length Sequences","text":""},{"location":"JEPA/open_research_joint_latent/#the-problem","title":"The Problem","text":"<p>Traditional approach:</p> <ul> <li>Pad all sequences to same length</li> <li>Waste computation on padding tokens</li> <li>Inefficient for variable-length data</li> </ul> <p>Biological example:</p> <ul> <li>Time-series with different lengths (3 time points vs 10 time points)</li> <li>Single-cell samples with different cell counts</li> <li>Spatial data with different tissue sizes</li> </ul>"},{"location":"JEPA/open_research_joint_latent/#the-solution-patch-n-pack","title":"The Solution: Patch n' Pack","text":"<p>Key idea: Concatenate all tokens into one long sequence, use block attention masks.</p> <pre><code>class PatchAndPack:\n    \"\"\"\n    Patch n' Pack for variable-length biological sequences.\n\n    Allows mixing different data types in same batch.\n    \"\"\"\n    def __init__(self, max_tokens=2048):\n        self.max_tokens = max_tokens\n\n    def pack_batch(self, samples):\n        \"\"\"\n        Pack variable-length samples into single batch.\n\n        Args:\n            samples: List of samples, each (num_tokens, embed_dim)\n\n        Returns:\n            packed: Packed tokens (1, total_tokens, embed_dim)\n            attention_mask: Block diagonal mask (total_tokens, total_tokens)\n            sample_boundaries: List of (start, end) indices\n        \"\"\"\n        # Concatenate all tokens\n        all_tokens = []\n        sample_boundaries = []\n        current_pos = 0\n\n        for sample in samples:\n            num_tokens = sample.shape[0]\n            all_tokens.append(sample)\n            sample_boundaries.append((current_pos, current_pos + num_tokens))\n            current_pos += num_tokens\n\n            if current_pos &gt; self.max_tokens:\n                break\n\n        # Stack\n        packed = torch.cat(all_tokens, dim=0).unsqueeze(0)  # (1, total_tokens, embed_dim)\n\n        # Create block diagonal attention mask\n        total_tokens = packed.shape[1]\n        attention_mask = torch.zeros(total_tokens, total_tokens)\n\n        for start, end in sample_boundaries:\n            attention_mask[start:end, start:end] = 1\n\n        return packed, attention_mask, sample_boundaries\n\n    def unpack_batch(self, packed_output, sample_boundaries):\n        \"\"\"\n        Unpack batch back to individual samples.\n\n        Args:\n            packed_output: Packed output (1, total_tokens, embed_dim)\n            sample_boundaries: List of (start, end) indices\n\n        Returns:\n            samples: List of unpacked samples\n        \"\"\"\n        samples = []\n        for start, end in sample_boundaries:\n            sample = packed_output[0, start:end, :]\n            samples.append(sample)\n\n        return samples\n</code></pre>"},{"location":"JEPA/open_research_joint_latent/#usage","title":"Usage","text":"<pre><code># Create samples of different lengths\nsample1 = torch.randn(32, 256)  # 32 tokens\nsample2 = torch.randn(64, 256)  # 64 tokens\nsample3 = torch.randn(48, 256)  # 48 tokens\n\n# Pack\npacker = PatchAndPack(max_tokens=2048)\npacked, mask, boundaries = packer.pack_batch([sample1, sample2, sample3])\n\nprint(f\"Packed shape: {packed.shape}\")  # (1, 144, 256)\nprint(f\"Mask shape: {mask.shape}\")      # (144, 144)\n\n# Process with transformer\noutput = transformer(packed, attention_mask=mask)\n\n# Unpack\nsamples_out = packer.unpack_batch(output, boundaries)\nprint(f\"Sample 1 out: {samples_out[0].shape}\")  # (32, 256)\nprint(f\"Sample 2 out: {samples_out[1].shape}\")  # (64, 256)\nprint(f\"Sample 3 out: {samples_out[2].shape}\")  # (48, 256)\n</code></pre>"},{"location":"JEPA/open_research_joint_latent/#open-research-questions","title":"Open Research Questions","text":""},{"location":"JEPA/open_research_joint_latent/#1-optimal-latent-dimensionality","title":"1. Optimal Latent Dimensionality","text":"<p>Question: What is the right dimensionality for joint biological latent space?</p> <p>Current practice:</p> <ul> <li>Images: 256-1024 dim</li> <li>Gene expression: 128-512 dim</li> </ul> <p>Open issues:</p> <ul> <li>Does biology need more or less than vision?</li> <li>How does dimensionality affect transfer?</li> <li>Can we learn optimal dimensionality?</li> </ul> <p>Proposed experiments:</p> <ul> <li>Train joint models with varying latent dims</li> <li>Measure downstream task performance</li> <li>Analyze information content (effective rank)</li> </ul>"},{"location":"JEPA/open_research_joint_latent/#2-cross-modality-transfer","title":"2. Cross-Modality Transfer","text":"<p>Question: How much does static data help dynamic modeling, and vice versa?</p> <p>Hypothesis: Joint training improves both tasks.</p> <p>Proposed experiments: 1. Train separate models (static only, dynamic only) 2. Train joint model 3. Compare performance on both tasks 4. Measure transfer via ablation</p> <p>Metrics:</p> <ul> <li>Downstream task accuracy</li> <li>Sample efficiency (performance with less data)</li> <li>Generalization to held-out conditions</li> </ul>"},{"location":"JEPA/open_research_joint_latent/#3-biological-priors","title":"3. Biological Priors","text":"<p>Question: Should we encode biological structure (pathways, GRNs) into latent space?</p> <p>Options: 1. Fully learned \u2014 Let model discover structure 2. Structured latent \u2014 Enforce pathway/module structure 3. Hybrid \u2014 Soft biological priors</p> <p>Trade-offs:</p> <ul> <li>Learned: Flexible but data-hungry</li> <li>Structured: Sample-efficient but rigid</li> <li>Hybrid: Best of both?</li> </ul> <p>Proposed approach: <pre><code>class StructuredLatentJEPA(nn.Module):\n    \"\"\"\n    JEPA with structured latent space.\n\n    Latent dimensions correspond to biological pathways.\n    \"\"\"\n    def __init__(self, num_genes, pathways):\n        super().__init__()\n\n        # Encoder projects to pathway space\n        self.encoder = PathwayEncoder(num_genes, pathways)\n\n        # Predictor operates on pathway embeddings\n        self.predictor = PathwayPredictor(len(pathways))\n</code></pre></p>"},{"location":"JEPA/open_research_joint_latent/#4-temporal-consistency","title":"4. Temporal Consistency","text":"<p>Question: How to enforce temporal consistency in joint models?</p> <p>Challenge: Time-series should have smooth trajectories, but static data has no temporal structure.</p> <p>Proposed solutions: 1. Separate losses \u2014 Temporal smoothness only for time-series 2. Pseudo-time \u2014 Infer temporal ordering for static data 3. Consistency regularization \u2014 Encourage smooth latent paths</p> <p>Implementation: <pre><code>def temporal_consistency_loss(z_sequence):\n    \"\"\"\n    Penalize large jumps in latent space.\n\n    Args:\n        z_sequence: Latent trajectory (B, T, latent_dim)\n\n    Returns:\n        loss: Temporal consistency loss\n    \"\"\"\n    # Compute differences between consecutive time points\n    dz = z_sequence[:, 1:, :] - z_sequence[:, :-1, :]\n\n    # Penalize large jumps\n    loss = torch.norm(dz, dim=-1).mean()\n\n    return loss\n</code></pre></p>"},{"location":"JEPA/open_research_joint_latent/#5-multi-species-transfer","title":"5. Multi-Species Transfer","text":"<p>Question: Can joint latent spaces transfer across species?</p> <p>Motivation: Conserved biology should have conserved latent structure.</p> <p>Proposed experiments: 1. Train joint model on human + mouse data 2. Test cross-species prediction 3. Identify conserved vs species-specific dimensions</p> <p>Applications:</p> <ul> <li>Translate mouse perturbations to human</li> <li>Validate drug effects across species</li> <li>Identify evolutionary conservation</li> </ul>"},{"location":"JEPA/open_research_joint_latent/#6-scalability","title":"6. Scalability","text":"<p>Question: How to scale joint models to millions of cells and thousands of conditions?</p> <p>Challenges:</p> <ul> <li>Memory constraints</li> <li>Training time</li> <li>Data heterogeneity</li> </ul> <p>Proposed solutions: 1. Hierarchical latents \u2014 Coarse-to-fine structure 2. Distributed training \u2014 Multi-GPU/node 3. Efficient attention \u2014 Sparse, linear, or kernel-based 4. Patch n' Pack \u2014 Variable-length batching</p>"},{"location":"JEPA/open_research_joint_latent/#practical-recommendations","title":"Practical Recommendations","text":""},{"location":"JEPA/open_research_joint_latent/#when-to-use-joint-latent-spaces","title":"When to Use Joint Latent Spaces","text":"<p>Use joint latent spaces when: 1. You have both static and dynamic data 2. Data types are related (same genes, different conditions) 3. You want to transfer knowledge across modalities 4. Sample efficiency matters (limited data per modality)</p> <p>Don't use joint latent spaces when: 1. Data types are fundamentally different (e.g., images + text) 2. You have abundant data for each modality separately 3. Modalities have different downstream tasks with no overlap</p>"},{"location":"JEPA/open_research_joint_latent/#implementation-checklist","title":"Implementation Checklist","text":"<p>1. Data preparation:</p> <ul> <li> Normalize static and dynamic data consistently</li> <li> Align feature spaces (same genes)</li> <li> Create mixed batches</li> </ul> <p>2. Model architecture:</p> <ul> <li> Shared encoder for both modalities</li> <li> Modality-specific predictors (optional)</li> <li> VICReg or similar regularization</li> </ul> <p>3. Training:</p> <ul> <li> Mixed batch sampling</li> <li> Balanced loss weighting</li> <li> Monitor both tasks</li> </ul> <p>4. Evaluation:</p> <ul> <li> Test on both modalities</li> <li> Measure cross-modality transfer</li> <li> Validate on downstream tasks</li> </ul>"},{"location":"JEPA/open_research_joint_latent/#key-takeaways","title":"Key Takeaways","text":""},{"location":"JEPA/open_research_joint_latent/#conceptual","title":"Conceptual","text":"<ol> <li>Joint latent spaces \u2014 Static and dynamic data can share representations</li> <li>Mutual benefit \u2014 Each modality improves the other</li> <li>Biological manifold \u2014 All cellular states on same manifold</li> <li>Patch n' Pack \u2014 Efficient variable-length batching</li> </ol>"},{"location":"JEPA/open_research_joint_latent/#practical","title":"Practical","text":"<ol> <li>Shared encoder \u2014 Same architecture for all modalities</li> <li>Mixed training \u2014 Alternate between tasks</li> <li>VICReg regularization \u2014 Prevent collapse</li> <li>Evaluate transfer \u2014 Measure cross-modality benefits</li> </ol>"},{"location":"JEPA/open_research_joint_latent/#open-questions","title":"Open Questions","text":"<ol> <li>Optimal dimensionality \u2014 How large should latent space be?</li> <li>Biological priors \u2014 Should we encode known structure?</li> <li>Temporal consistency \u2014 How to enforce smooth trajectories?</li> <li>Multi-species \u2014 Can latents transfer across species?</li> <li>Scalability \u2014 How to handle millions of cells?</li> </ol>"},{"location":"JEPA/open_research_joint_latent/#related-documents","title":"Related Documents","text":"<ul> <li>00_jepa_overview.md \u2014 JEPA concepts</li> <li>01_jepa_foundations.md \u2014 Architecture details</li> <li>03_jepa_applications.md \u2014 Applications</li> <li>04_jepa_perturbseq.md \u2014 Perturb-seq implementation</li> </ul>"},{"location":"JEPA/open_research_joint_latent/#references","title":"References","text":"<p>Joint latent spaces:</p> <ul> <li>ByteDance &amp; HKU (2024): \"Goku: Native Joint Image-Video Generation\"</li> <li>Meta AI (2025): \"V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction, and Planning\"</li> </ul> <p>JEPA:</p> <ul> <li>LeCun (2022): \"A Path Towards Autonomous Machine Intelligence\"</li> <li>Assran et al. (2023): \"Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture\"</li> </ul> <p>Biological applications:</p> <ul> <li>Lotfollahi et al. (2023): \"Predicting cellular responses to novel drug combinations\"</li> <li>Bunne et al. (2023): \"Learning Single-Cell Perturbation Responses using Neural Optimal Transport\"</li> </ul> <p>Variable-length sequences:</p> <ul> <li>Vaswani et al. (2017): \"Attention is All You Need\"</li> <li>Press et al. (2021): \"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation\"</li> </ul>"},{"location":"SDE/","title":"SDE Formulation of Diffusion Models","text":"<p>This directory contains comprehensive documentation on the Stochastic Differential Equation (SDE) perspective on diffusion models, providing a continuous-time view that unifies DDPM, DDIM, and other variants.</p> <p>The SDE formulation reveals diffusion models as continuous stochastic processes, enabling flexible sampling, theoretical analysis, and connections to classical stochastic calculus.</p>"},{"location":"SDE/#core-documentation-series","title":"Core Documentation Series","text":"<p>This series mirrors the structure of DDPM and flow matching documentation, separating theory, training, and sampling.</p> Document Description 00_sde_overview.md Overview: High-level introduction, why SDE view matters, key concepts 01_diffusion_sde_view.md Foundations: Detailed SDE formulation, forward/reverse processes 02_sde_training.md Training: How training works \u2014 NO SDE solvers needed! 03_sde_sampling.md Sampling: How to generate samples \u2014 SDE/ODE solvers used here"},{"location":"SDE/#supplementary-documents","title":"Supplementary Documents","text":"Document Description 01a_diffusion_sde_view_QA.md Design principles Q&amp;A 02a_taylor_expansion.md Taylor expansions in diffusion 02b_fokker_plank_eq.md Fokker-Planck equation derivation 02c_ddpm_to_vpsde.md From DDPM to VP-SDE (reverse direction) 03b_ddim_update_coeff.md DDIM coefficients from theory"},{"location":"SDE/#quick-navigation","title":"Quick Navigation","text":""},{"location":"SDE/#for-beginners","title":"For Beginners","text":"<ol> <li>Start with SDE Overview for high-level understanding</li> <li>Read SDE Foundations for detailed formulation</li> <li>See DDPM from SDE for discrete-continuous connection</li> <li>Understand Solving VP-SDE for exact solutions</li> </ol>"},{"location":"SDE/#for-implementation","title":"For Implementation","text":"<ol> <li>DDPM Connection \u2014 How DDPM emerges from VP-SDE</li> <li>DDIM Coefficients \u2014 Exact formulas for code</li> <li>Reverse SDE &amp; ODE \u2014 Sampling methods</li> </ol>"},{"location":"SDE/#for-theory-deep-dive","title":"For Theory Deep Dive","text":"<ol> <li>Fokker-Planck Equation \u2014 Probability evolution PDE</li> <li>Taylor Expansions \u2014 Mathematical foundations</li> <li>VP-SDE from DDPM \u2014 Continuous limit derivation</li> </ol>"},{"location":"SDE/#key-concepts","title":"Key Concepts","text":""},{"location":"SDE/#the-sde-formulation","title":"The SDE Formulation","text":"<p>Forward SDE (data \u2192 noise):</p> \\[ dx = f(x, t)\\,dt + g(t)\\,dw \\] <p>Reverse SDE (noise \u2192 data):</p> <p>$$</p> <p>dx = \\left[f(x, t) - g(t)^2 \\nabla_x \\log p_t(x)\\right]dt + g(t)\\,d\\bar{w} $$</p> <p>Key insight: Only the score function \\(\\nabla_x \\log p_t(x)\\) needs to be learned.</p>"},{"location":"SDE/#variance-preserving-sde-vp-sde","title":"Variance-Preserving SDE (VP-SDE)","text":"<p>The most common formulation, corresponding to DDPM:</p> \\[ dx = -\\frac{1}{2}\\beta(t) x\\,dt + \\sqrt{\\beta(t)}\\,dw \\] <p>Properties:</p> <ul> <li>Preserves variance over time</li> <li>Discretizes to DDPM forward process</li> <li>Closed-form marginals: \\(q(x_t | x_0) = \\mathcal{N}(\\sqrt{\\bar{\\alpha}_t} x_0, (1-\\bar{\\alpha}_t)I)\\)</li> </ul>"},{"location":"SDE/#probability-flow-ode","title":"Probability Flow ODE","text":"<p>Deterministic alternative to reverse SDE with same marginals:</p> \\[ dx = \\left[f(x, t) - \\frac{1}{2}g(t)^2 \\nabla_x \\log p_t(x)\\right]dt \\] <p>Key property: DDIM is the discretization of this ODE.</p>"},{"location":"SDE/#comparison-discrete-vs-continuous","title":"Comparison: Discrete vs Continuous","text":"Aspect DDPM (Discrete) SDE (Continuous) Time Steps \\(t = 0, 1, \\ldots, T\\) Continuous \\(t \\in [0, T]\\) Forward Markov chain Stochastic process Notation \\(\\alpha_t\\), \\(\\bar{\\alpha}_t\\) (products) \\(\\beta(t)\\), integrals Sampling Fixed schedule Flexible steps Theory Discrete probability Stochastic calculus Flexibility Limited High <p>Key connection: DDPM is the Euler-Maruyama discretization of VP-SDE.</p>"},{"location":"SDE/#learning-path","title":"Learning Path","text":""},{"location":"SDE/#conceptual-understanding","title":"Conceptual Understanding","text":"<ol> <li>SDE Overview \u2014 Why SDE view matters</li> <li>Unified framework for diffusion variants</li> <li>Flexible sampling strategies</li> <li> <p>Theoretical foundations</p> </li> <li> <p>SDE Foundations \u2014 Detailed formulation</p> </li> <li>Forward and reverse SDEs</li> <li>Score functions</li> <li>Brownian motion</li> </ol>"},{"location":"SDE/#practical-connection","title":"Practical Connection","text":"<ol> <li>DDPM from SDE \u2014 Discretization</li> <li>Euler-Maruyama method</li> <li>Why DDPM predicts noise</li> <li> <p>Forward and reverse processes</p> </li> <li> <p>Solving VP-SDE \u2014 Exact solutions</p> </li> <li>Closed-form marginals</li> <li>Connection to \\(\\bar{\\alpha}_t\\)</li> <li>Products \u2192 integrals</li> </ol>"},{"location":"SDE/#advanced-topics","title":"Advanced Topics","text":"<ol> <li>Reverse SDE &amp; Probability Flow ODE \u2014 Sampling</li> <li>Stochastic vs deterministic</li> <li>DDPM vs DDIM</li> <li> <p>The \\(\\eta\\) parameter</p> </li> <li> <p>DDIM Coefficients \u2014 Theory to code</p> </li> <li>Exact update formulas</li> <li>Fast sampling</li> <li> <p>Implementation details</p> </li> <li> <p>Fokker-Planck Equation \u2014 Advanced theory</p> </li> <li>Probability density evolution</li> <li>PDE perspective</li> <li>Weak vs strong solutions</li> </ol>"},{"location":"SDE/#interactive-resources","title":"Interactive Resources","text":"<p>For hands-on learning with code:</p> <ul> <li>Location: <code>notebooks/diffusion/02_sde_formulation/</code></li> <li>Contents:</li> <li><code>README.md</code> \u2014 Comprehensive theory document</li> <li><code>02_sde_formulation.ipynb</code> \u2014 Interactive notebook with visualizations</li> <li><code>sde_QA.md</code> \u2014 Common questions answered</li> </ul> <p>8 Focused Supplements: 1. Forward SDE Design Choices (VP/VE/sub-VP) 2. Brownian Motion Dimensionality 3. Equivalent Parameterizations (score \u2194 noise \u2194 \\(x_0\\)) 4. Training Loss and Denoising 5. Reverse SDE and Probability Flow ODE 6. Fokker-Planck and Effective Drift 7. Fokker-Planck Equation (detailed derivation) 8. Dimensional Analysis</p>"},{"location":"SDE/#whats-here","title":"What's Here","text":"<p>This <code>docs/SDE/</code> directory contains tutorial-style mathematical derivations that complement the interactive notebooks. These documents provide:</p> <ul> <li>Detailed line-by-line derivations</li> <li>Mathematical foundations and proofs</li> <li>Historical context and development</li> <li>Advanced theoretical connections</li> </ul>"},{"location":"SDE/#documents-in-this-directory","title":"Documents in This Directory","text":""},{"location":"SDE/#core-sde-theory","title":"Core SDE Theory","text":"<ol> <li>01_diffusion_sde_view.md \u2014 SDE Formulation Overview</li> <li>Forward and reverse SDEs</li> <li>Score functions and their role</li> <li>Brownian motion fundamentals</li> <li> <p>Connection to diffusion models</p> </li> <li> <p>01a_diffusion_sde_view_QA.md \u2014 Design Principles Q&amp;A</p> </li> <li>Why specific drift functions?</li> <li>Score vs. noise prediction</li> <li>High-dimensional intuition</li> <li>Practical design choices</li> </ol>"},{"location":"SDE/#ddpm-sde-connections","title":"DDPM \u2194 SDE Connections","text":"<ol> <li>02_sde_and_ddpm.md \u2014 Deriving DDPM from VP-SDE</li> <li>Euler\u2013Maruyama discretization</li> <li>Variance-preserving structure</li> <li>Why DDPM predicts noise</li> <li> <p>Forward and reverse processes</p> </li> <li> <p>02c_ddpm_to_vpsde.md \u2014 From DDPM to VP-SDE (Continuous Limit)</p> </li> <li>Moment matching approach</li> <li>Taylor expansion of discrete steps</li> <li>Recovering the continuous SDE</li> <li>Identity check derivation</li> </ol>"},{"location":"SDE/#mathematical-foundations","title":"Mathematical Foundations","text":"<ol> <li>02a_taylor_expansion.md \u2014 Taylor Expansions in Diffusion</li> <li>Role in Euler\u2013Maruyama</li> <li>Square root approximation in DDPM</li> <li>Fokker\u2013Planck equation derivation</li> <li> <p>Continuous vs. discrete connections</p> </li> <li> <p>02b_fokker_plank_eq.md \u2014 Fokker\u2013Planck Equation Derivation \u2b50</p> </li> <li>Line-by-line derivation from SDEs</li> <li>Test function approach</li> <li>Integration by parts</li> <li>Weak vs. strong solutions</li> <li>Examples and intuition</li> </ol>"},{"location":"SDE/#solving-and-sampling","title":"Solving and Sampling","text":"<ol> <li>03_solving_vpsde.md \u2014 Solving the VP-SDE \u2b50 NEW</li> <li>Exact solution via integrating factor</li> <li>Closed-form marginal distribution</li> <li>Connection: \\(\\bar{\\alpha}(t) = \\exp(-\\int_0^t \\beta(s)\\,ds)\\)</li> <li> <p>Discrete products \u2192 continuous integrals</p> </li> <li> <p>03a_reverse_time_sde_and_proba_flow_ode.md \u2014 Reverse SDE and Probability Flow ODE \u2b50 NEW</p> </li> <li>Reverse-time SDE (Anderson, 1982)</li> <li>Probability flow ODE (Song et al., 2021)</li> <li>DDPM as SDE discretization</li> <li>DDIM as ODE discretization</li> <li> <p>The \\(\\eta\\) parameter</p> </li> <li> <p>03b_ddim_update_coeff.md \u2014 DDIM Update Coefficients \u2b50 NEW</p> </li> <li>Exact DDIM update formula derivation</li> <li>From \\(\\bar{\\alpha}(t)\\) to <code>alphas_cumprod</code> array</li> <li>Why \\(\\sqrt{\\bar{\\alpha}_t}\\) coefficients appear</li> <li>Fast sampling (skipping steps)</li> <li>Complete theory-to-code connection</li> </ol> <p>Current status: These documents provide rigorous mathematical foundations that complement the code-focused notebooks.</p>"},{"location":"SDE/#quick-links","title":"Quick Links","text":""},{"location":"SDE/#notebooks-interactive-code","title":"Notebooks (Interactive + Code)","text":"<ul> <li>SDE Tutorial (Theory)</li> <li>SDE Tutorial (Code)</li> <li>DDPM Basics</li> </ul>"},{"location":"SDE/#docs-mathematical-derivations","title":"Docs (Mathematical Derivations)","text":"<ul> <li>SDE View Overview</li> <li>DDPM from VP-SDE</li> <li>VP-SDE from DDPM</li> <li>Solving VP-SDE \u2b50 NEW</li> <li>Reverse SDE &amp; Probability Flow ODE \u2b50 NEW</li> <li>DDIM Update Coefficients \u2b50 NEW</li> <li>Fokker\u2013Planck Equation</li> <li>Taylor Expansions</li> </ul>"},{"location":"SDE/#learning-path_1","title":"Learning Path","text":""},{"location":"SDE/#for-beginners_1","title":"For Beginners","text":"<ol> <li>Start with SDE View Overview for conceptual understanding</li> <li>Read DDPM from VP-SDE to see discrete-continuous connection</li> <li>Understand Solving VP-SDE for the forward solution</li> <li>See DDIM Update Coefficients for exact code formulas</li> <li>Work through the interactive notebook</li> </ol>"},{"location":"SDE/#for-deep-dive","title":"For Deep Dive","text":"<ol> <li>Study Taylor Expansions for mathematical foundations</li> <li>Work through Fokker\u2013Planck derivation for PDE theory</li> <li>Complete the identity check with VP-SDE from DDPM</li> <li>Master Solving VP-SDE for closed-form solutions</li> <li>Understand sampling with Reverse SDE &amp; Probability Flow ODE</li> <li>Connect theory to code with DDIM Update Coefficients</li> <li>Explore Design Principles Q&amp;A for practical insights</li> </ol>"},{"location":"SDE/#topics-covered","title":"Topics Covered","text":"<ol> <li>Brownian Motion: Properties, scaling, and dimensionality</li> <li>Forward SDE: Data corruption as a stochastic process</li> <li>Score Functions: Gradient of log density and its role</li> <li>Reverse SDE: Time-reversal and sampling</li> <li>VP-SDE: Variance-preserving formulation (DDPM connection)</li> <li>Fokker\u2013Planck Equation: Probability density evolution</li> <li>Discretization: Euler\u2013Maruyama and variance preservation</li> <li>Moment Matching: Connecting discrete and continuous views</li> </ol>"},{"location":"SDE/#archive","title":"Archive","text":"<p>Note: Original draft files have been moved to <code>dev/diffusion/sde/</code> (private development area).</p> <p>These drafts have been superseded by comprehensive tutorials in <code>notebooks/diffusion/02_sde_formulation/supplements/</code>: - Supplement 07: Fokker-Planck Equation (replaces div_and_laplace content) - Supplement 08: Dimensional Analysis (replaces unit_analysis content) - Main sde_QA.md: Canonical Q&amp;A version</p> <p>For historical reference: See <code>dev/diffusion/sde/README.md</code> (not tracked in git)</p>"},{"location":"SDE/00_sde_overview/","title":"SDE Formulation of Diffusion Models: Overview","text":"<p>This document provides a high-level overview of the Stochastic Differential Equation (SDE) perspective on diffusion models, explaining why this view is valuable and how it connects to discrete formulations like DDPM.</p>"},{"location":"SDE/00_sde_overview/#what-is-the-sde-view","title":"What is the SDE View?","text":""},{"location":"SDE/00_sde_overview/#the-core-idea","title":"The Core Idea","text":"<p>Diffusion models can be understood as continuous-time stochastic processes that gradually transform data into noise (forward) and noise back into data (reverse).</p> <p>Key insight: Instead of thinking about discrete timesteps (DDPM), we can view diffusion as a continuous flow governed by stochastic differential equations.</p>"},{"location":"SDE/00_sde_overview/#why-the-sde-view-matters","title":"Why the SDE View Matters","text":"<p>1. Unified Framework</p> <ul> <li>DDPM, NCSN, and other variants are all discretizations of the same continuous process</li> <li>Different sampling methods (ancestral, DDIM) correspond to different ODE/SDE solvers</li> <li>Provides theoretical foundation for understanding what diffusion models do</li> </ul> <p>2. Flexible Sampling</p> <ul> <li>Can use any number of steps (not fixed to training schedule)</li> <li>Adaptive step sizes based on error estimates</li> <li>Trade-off between quality and speed</li> </ul> <p>3. Theoretical Clarity</p> <ul> <li>Connects to classical stochastic calculus</li> <li>Enables rigorous analysis (Fokker-Planck equation, probability flow ODE)</li> <li>Explains why certain design choices work</li> </ul> <p>4. Generalization</p> <ul> <li>Extends to manifolds and non-Euclidean spaces</li> <li>Connects to optimal transport and flow matching</li> <li>Provides path to new model designs</li> </ul>"},{"location":"SDE/00_sde_overview/#the-forward-sde","title":"The Forward SDE","text":""},{"location":"SDE/00_sde_overview/#general-form","title":"General Form","text":"<p>A diffusion process is defined by a forward SDE:</p> \\[ dx = f(x, t)\\,dt + g(t)\\,dw \\] <p>where:</p> <ul> <li>\\(x(t) \\in \\mathbb{R}^d\\) is the data state at time \\(t\\)</li> <li>\\(f(x, t)\\) is the drift (deterministic component)</li> <li>\\(g(t)\\) is the diffusion coefficient (noise strength)</li> <li>\\(w(t)\\) is Brownian motion (random component)</li> </ul>"},{"location":"SDE/00_sde_overview/#interpretation","title":"Interpretation","text":"<p>Drift term \\(f(x, t)\\,dt\\): - Average direction of movement - Designed by model creator (not learned) - Controls how data is corrupted</p> <p>Diffusion term \\(g(t)\\,dw\\): - Random fluctuations - Noise strength varies with time - Brownian motion provides randomness</p>"},{"location":"SDE/00_sde_overview/#key-property","title":"Key Property","text":"<p>The forward SDE is completely specified \u2014 nothing is learned. It's a design choice that determines how data is gradually destroyed.</p>"},{"location":"SDE/00_sde_overview/#the-reverse-sde","title":"The Reverse SDE","text":""},{"location":"SDE/00_sde_overview/#the-fundamental-result","title":"The Fundamental Result","text":"<p>Given a forward SDE, there exists a reverse-time SDE that reconstructs the data:</p> \\[ dx = \\left[f(x, t) - g(t)^2 \\nabla_x \\log p_t(x)\\right]dt + g(t)\\,d\\bar{w} \\] <p>where:</p> <ul> <li>\\(\\nabla_x \\log p_t(x)\\) is the score function (gradient of log density)</li> <li>\\(\\bar{w}\\) is reverse-time Brownian motion</li> </ul> <p>Critical observation: The only unknown is the score function \\(\\nabla_x \\log p_t(x)\\).</p>"},{"location":"SDE/00_sde_overview/#what-diffusion-models-learn","title":"What Diffusion Models Learn","text":"<p>Diffusion models train a neural network to approximate the score:</p> \\[ s_\\theta(x, t) \\approx \\nabla_x \\log p_t(x) \\] <p>Once we have the score, we can: 1. Sample via reverse SDE: Stochastic sampling (like DDPM) 2. Sample via probability flow ODE: Deterministic sampling (like DDIM)</p>"},{"location":"SDE/00_sde_overview/#three-main-sde-formulations","title":"Three Main SDE Formulations","text":"<p>Different choices of \\(f(x, t)\\) and \\(g(t)\\) lead to different diffusion models.</p>"},{"location":"SDE/00_sde_overview/#1-variance-preserving-vp-sde","title":"1. Variance Preserving (VP-SDE)","text":"<p>SDE:</p> <p>$$</p> <p>dx = -\\frac{1}{2}\\beta(t) x\\,dt + \\sqrt{\\beta(t)}\\,dw $$</p> <p>Properties:</p> <ul> <li>Preserves variance: \\(\\mathbb{E}[\\|x(t)\\|^2] \\approx \\text{const}\\)</li> <li>Corresponds to DDPM when discretized</li> <li>Most common in practice</li> </ul> <p>Noise schedule: \\(\\beta(t)\\) controls corruption rate</p>"},{"location":"SDE/00_sde_overview/#2-variance-exploding-ve-sde","title":"2. Variance Exploding (VE-SDE)","text":"<p>SDE:</p> <p>$$</p> <p>dx = \\sqrt{\\frac{d[\\sigma^2(t)]}{dt}}\\,dw $$</p> <p>Properties:</p> <ul> <li>No drift term (\\(f = 0\\))</li> <li>Variance grows: \\(\\mathbb{E}[\\|x(t)\\|^2]\\) increases</li> <li>Corresponds to NCSN (Noise Conditional Score Networks)</li> </ul> <p>Noise schedule: \\(\\sigma(t)\\) controls noise level</p>"},{"location":"SDE/00_sde_overview/#3-sub-vp-sde","title":"3. Sub-VP SDE","text":"<p>SDE:</p> <p>$$</p> <p>dx = -\\frac{1}{2}\\beta(t) x\\,dt + \\sqrt{\\beta(t)(1 - e<sup>{-2\\int_0</sup>t \\beta(s)ds})}\\,dw $$</p> <p>Properties:</p> <ul> <li>Interpolates between VP and VE</li> <li>Better numerical stability</li> <li>Less common in practice</li> </ul>"},{"location":"SDE/00_sde_overview/#connection-to-ddpm","title":"Connection to DDPM","text":""},{"location":"SDE/00_sde_overview/#ddpm-as-discretization","title":"DDPM as Discretization","text":"<p>DDPM is the Euler-Maruyama discretization of the VP-SDE:</p> <p>Continuous (VP-SDE):</p> <p>$$</p> <p>dx = -\\frac{1}{2}\\beta(t) x\\,dt + \\sqrt{\\beta(t)}\\,dw $$</p> <p>Discrete (DDPM):</p> <p>$$</p> <p>x_{t+1} = \\sqrt{1 - \\beta_t} \\, x_t + \\sqrt{\\beta_t} \\, \\epsilon_t $$</p> <p>where \\(\\beta_t = \\beta(t) \\Delta t\\) for small \\(\\Delta t\\).</p>"},{"location":"SDE/00_sde_overview/#key-notation-mapping","title":"Key Notation Mapping","text":"DDPM (Discrete) SDE (Continuous) \\(\\beta_t\\) \\(\\beta(t) \\Delta t\\) \\(\\alpha_t = 1 - \\beta_t\\) \\(1 - \\frac{1}{2}\\beta(t)\\Delta t\\) \\(\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s\\) \\(\\exp(-\\int_0^t \\beta(s)ds)\\) Noise prediction \\(\\epsilon_\\theta\\) Score \\(s_\\theta = -\\epsilon_\\theta / \\sigma_t\\) <p>Key insight: Products in DDPM become integrals in continuous time.</p>"},{"location":"SDE/00_sde_overview/#sampling-methods","title":"Sampling Methods","text":""},{"location":"SDE/00_sde_overview/#ancestral-sampling-sde","title":"Ancestral Sampling (SDE)","text":"<p>Use the reverse SDE:</p> \\[ dx = \\left[f(x, t) - g(t)^2 s_\\theta(x, t)\\right]dt + g(t)\\,d\\bar{w} \\] <p>Properties:</p> <ul> <li>Stochastic (injects noise at each step)</li> <li>Corresponds to DDPM sampling</li> <li>Multiple samples from same noise give different outputs</li> </ul>"},{"location":"SDE/00_sde_overview/#probability-flow-ode","title":"Probability Flow ODE","text":"<p>Use the deterministic ODE:</p> <p>$$</p> <p>dx = \\left[f(x, t) - \\frac{1}{2}g(t)^2 s_\\theta(x, t)\\right]dt $$</p> <p>Properties:</p> <ul> <li>Deterministic (no noise injection)</li> <li>Corresponds to DDIM sampling</li> <li>Same noise always gives same output</li> <li>Faster (fewer steps needed)</li> </ul> <p>Key result: The ODE has the same marginals as the SDE but follows deterministic paths.</p>"},{"location":"SDE/00_sde_overview/#why-score-functions","title":"Why Score Functions?","text":""},{"location":"SDE/00_sde_overview/#what-is-a-score","title":"What is a Score?","text":"<p>The score function is the gradient of the log probability density:</p> \\[ s(x, t) = \\nabla_x \\log p_t(x) \\] <p>Interpretation: Points in the direction of increasing probability density.</p>"},{"location":"SDE/00_sde_overview/#why-learn-scores-instead-of-densities","title":"Why Learn Scores Instead of Densities?","text":"<p>1. Tractability</p> <ul> <li>Computing \\(p_t(x)\\) requires normalization (intractable)</li> <li>Computing \\(\\nabla_x \\log p_t(x)\\) avoids normalization</li> </ul> <p>2. Denoising Connection</p> <ul> <li>Score matching \u2248 denoising</li> <li>Training objective is simple MSE on noise prediction</li> </ul> <p>3. Sampling Efficiency</p> <ul> <li>Only need score to run reverse SDE</li> <li>Don't need to evaluate likelihood</li> </ul>"},{"location":"SDE/00_sde_overview/#score-vs-noise-prediction","title":"Score vs Noise Prediction","text":"<p>In DDPM, we predict noise \\(\\epsilon_\\theta\\) instead of score directly:</p> \\[ s_\\theta(x_t, t) = -\\frac{\\epsilon_\\theta(x_t, t)}{\\sigma_t} \\] <p>Why predict noise?</p> <ul> <li>Numerically more stable</li> <li>Simpler training objective</li> <li>Better empirical performance</li> </ul>"},{"location":"SDE/00_sde_overview/#mathematical-foundations","title":"Mathematical Foundations","text":""},{"location":"SDE/00_sde_overview/#brownian-motion","title":"Brownian Motion","text":"<p>Definition: \\(w(t)\\) is Brownian motion if: 1. \\(w(0) = 0\\) 2. \\(w(t) - w(s) \\sim \\mathcal{N}(0, t-s)\\) for \\(t &gt; s\\) 3. Independent increments 4. Continuous paths</p> <p>Key property: Infinitesimal increment has variance \\(dt\\):</p> \\[ dw \\sim \\mathcal{N}(0, dt) \\quad \\Rightarrow \\quad dw = \\sqrt{dt}\\,\\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, 1) \\]"},{"location":"SDE/00_sde_overview/#itos-lemma","title":"It\u00f4's Lemma","text":"<p>For a function \\(f(x, t)\\) where \\(x\\) follows an SDE:</p> \\[ df = \\left(\\frac{\\partial f}{\\partial t} + \\frac{\\partial f}{\\partial x} \\mu + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}\\sigma^2\\right)dt + \\frac{\\partial f}{\\partial x}\\sigma\\,dw \\] <p>Key difference from ordinary calculus: The \\(\\frac{1}{2}\\sigma^2\\) term appears due to stochastic nature.</p>"},{"location":"SDE/00_sde_overview/#fokker-planck-equation","title":"Fokker-Planck Equation","text":"<p>The probability density \\(p_t(x)\\) evolves according to:</p> \\[ \\frac{\\partial p_t}{\\partial t} = -\\nabla \\cdot (f p_t) + \\frac{1}{2}g^2 \\nabla^2 p_t \\] <p>Interpretation: Describes how probability mass flows and diffuses over time.</p>"},{"location":"SDE/00_sde_overview/#advantages-of-sde-view","title":"Advantages of SDE View","text":""},{"location":"SDE/00_sde_overview/#theoretical-benefits","title":"Theoretical Benefits","text":"<p>\u2705 Unified framework: All diffusion variants in one formulation \u2705 Rigorous analysis: Connects to classical stochastic calculus \u2705 Probability flow ODE: Deterministic sampling with same marginals \u2705 Flexible discretization: Any number of steps, adaptive solvers</p>"},{"location":"SDE/00_sde_overview/#practical-benefits","title":"Practical Benefits","text":"<p>\u2705 Fast sampling: DDIM uses 10-50 steps vs 1000 for DDPM \u2705 Exact likelihood: Can compute via probability flow ODE \u2705 Interpolation: Smooth paths in latent space \u2705 Controllability: Easier to design new corruption processes</p>"},{"location":"SDE/00_sde_overview/#limitations","title":"Limitations","text":"<p>\u274c Complexity: Requires stochastic calculus background \u274c Abstraction: Less intuitive than discrete view \u274c Implementation: Need to choose discretization carefully</p>"},{"location":"SDE/00_sde_overview/#comparison-discrete-vs-continuous","title":"Comparison: Discrete vs Continuous","text":"Aspect Discrete (DDPM) Continuous (SDE) Time Fixed steps \\(t = 0, 1, \\ldots, T\\) Continuous \\(t \\in [0, T]\\) Notation \\(\\alpha_t\\), \\(\\bar{\\alpha}_t\\), products \\(\\beta(t)\\), integrals Forward Markov chain Stochastic process Reverse Learned transitions Reverse SDE/ODE Sampling Fixed schedule Flexible steps Theory Discrete probability Stochastic calculus Intuition More concrete More abstract Flexibility Less More <p>Key insight: They describe the same underlying process, just in different mathematical languages.</p>"},{"location":"SDE/00_sde_overview/#learning-path","title":"Learning Path","text":""},{"location":"SDE/00_sde_overview/#for-beginners","title":"For Beginners","text":"<ol> <li>Start here: Understand the basic idea (this document)</li> <li>DDPM connection: See how discrete and continuous relate</li> <li>VP-SDE solution: Closed-form marginals</li> <li>Sampling: Reverse SDE vs probability flow ODE</li> </ol>"},{"location":"SDE/00_sde_overview/#for-deep-dive","title":"For Deep Dive","text":"<ol> <li>Mathematical foundations: Brownian motion, It\u00f4 calculus</li> <li>Fokker-Planck equation: How probability densities evolve</li> <li>Score matching: Why and how we learn scores</li> <li>Advanced topics: Manifolds, optimal transport, flow matching</li> </ol>"},{"location":"SDE/00_sde_overview/#document-organization","title":"Document Organization","text":"<p>This SDE documentation is organized into focused topics:</p>"},{"location":"SDE/00_sde_overview/#core-theory","title":"Core Theory","text":"<ul> <li>01_diffusion_sde_view.md \u2014 Detailed SDE formulation</li> <li>01a_diffusion_sde_view_QA.md \u2014 Design principles Q&amp;A</li> </ul>"},{"location":"SDE/00_sde_overview/#ddpm-connection","title":"DDPM Connection","text":"<ul> <li>02_sde_and_ddpm.md \u2014 Deriving DDPM from VP-SDE</li> <li>02c_ddpm_to_vpsde.md \u2014 From DDPM to VP-SDE (reverse direction)</li> </ul>"},{"location":"SDE/00_sde_overview/#mathematical-foundations_1","title":"Mathematical Foundations","text":"<ul> <li>02a_taylor_expansion.md \u2014 Taylor expansions in diffusion</li> <li>02b_fokker_plank_eq.md \u2014 Fokker-Planck equation derivation</li> </ul>"},{"location":"SDE/00_sde_overview/#solutions-and-sampling","title":"Solutions and Sampling","text":"<ul> <li>03_solving_vpsde.md \u2014 Exact VP-SDE solution</li> <li>03a_reverse_time_sde_and_proba_flow_ode.md \u2014 Reverse SDE and probability flow ODE</li> <li>03b_ddim_update_coeff.md \u2014 DDIM coefficients from theory</li> </ul>"},{"location":"SDE/00_sde_overview/#key-takeaways","title":"Key Takeaways","text":""},{"location":"SDE/00_sde_overview/#conceptual","title":"Conceptual","text":"<ol> <li>Diffusion models are continuous-time processes discretized for computation</li> <li>The forward process is designed, not learned (key simplification)</li> <li>Only the score function is learned (via denoising/noise prediction)</li> <li>Reverse SDE and probability flow ODE give same marginals (stochastic vs deterministic)</li> </ol>"},{"location":"SDE/00_sde_overview/#practical","title":"Practical","text":"<ol> <li>DDPM is Euler-Maruyama discretization of VP-SDE</li> <li>DDIM is ODE solver for probability flow ODE</li> <li>Fewer steps possible with better solvers (adaptive, higher-order)</li> <li>Score = negative noise / noise level (connection to DDPM)</li> </ol>"},{"location":"SDE/00_sde_overview/#mathematical","title":"Mathematical","text":"<ol> <li>Brownian motion provides randomness with \\(\\sqrt{dt}\\) scaling</li> <li>Fokker-Planck describes probability evolution (forward equation)</li> <li>Reverse SDE requires score function (Anderson's theorem)</li> <li>Probability flow ODE is deterministic (same marginals as SDE)</li> </ol>"},{"location":"SDE/00_sde_overview/#related-documentation","title":"Related Documentation","text":""},{"location":"SDE/00_sde_overview/#within-genai-lab","title":"Within GenAI Lab","text":"<ul> <li>DDPM Documentation \u2014 Discrete diffusion formulation</li> <li>Flow Matching \u2014 Modern alternative to diffusion</li> <li>Evaluation Metrics \u2014 How to evaluate generated samples</li> </ul>"},{"location":"SDE/00_sde_overview/#external-resources","title":"External Resources","text":"<ul> <li>Song et al. (2021): Score-Based Generative Modeling through SDEs</li> <li>Anderson (1982): Reverse-time diffusion equation models</li> <li>S\u00e4rkk\u00e4 &amp; Solin (2019): Applied Stochastic Differential Equations</li> </ul>"},{"location":"SDE/00_sde_overview/#summary","title":"Summary","text":"<p>The SDE view provides a unified, continuous-time perspective on diffusion models:</p> <ul> <li>Forward SDE: Gradually destroys data structure (designed, not learned)</li> <li>Reverse SDE: Reconstructs data by reversing the process (requires score)</li> <li>Score function: Gradient of log density (learned via denoising)</li> <li>Probability flow ODE: Deterministic alternative with same marginals</li> </ul> <p>Key advantage: Flexibility in sampling (any number of steps, adaptive solvers) while maintaining theoretical rigor.</p> <p>Connection to practice: DDPM and DDIM are specific discretizations of the continuous SDE/ODE framework.</p>"},{"location":"SDE/01_diffusion_sde_view/","title":"The SDE View of Diffusion Models","text":"<p>This document provides a unified mathematical perspective on diffusion models through the lens of stochastic differential equations (SDEs). Understanding this view clarifies what diffusion models fundamentally do and why different variants (DDPM, NCSN, flow matching) are all solving the same core problem.</p>"},{"location":"SDE/01_diffusion_sde_view/#1-what-diffusion-models-really-are","title":"1. What Diffusion Models Really Are","text":"<p>At their core, diffusion models are continuous-time stochastic processes that transform a simple distribution (Gaussian noise) into a complex data distribution by reversing a corruption process.</p> <p>Key insight: The corruption process is not learned. It is deliberately chosen to be:</p> <ul> <li>Simple: Analytically tractable</li> <li>Stable: No explosions or pathologies</li> <li>Analyzable: Closed-form marginals</li> </ul> <p>This design choice is what makes diffusion models practical.</p>"},{"location":"SDE/01_diffusion_sde_view/#2-the-sde-formulation","title":"2. The SDE Formulation","text":""},{"location":"SDE/01_diffusion_sde_view/#the-forward-sde","title":"The Forward SDE","text":"<p>A diffusion model can be written as a stochastic differential equation (SDE):</p> \\[ dx(t) = f(x(t), t)\\,dt + g(t)\\,dw(t) \\] <p>This equation has two components:</p> <p>Deterministic drift: \\(f(x, t)\\,dt\\)</p> <ul> <li>The average direction the system moves</li> <li>Chosen by the model designer</li> <li>Examples: linear drift, zero drift</li> </ul> <p>Stochastic diffusion: \\(g(t)\\,dw(t)\\)</p> <ul> <li>Random fluctuations</li> <li>Noise strength controlled by \\(g(t)\\)</li> <li>Randomness from Brownian motion \\(w(t)\\)</li> </ul>"},{"location":"SDE/01_diffusion_sde_view/#brownian-motion","title":"Brownian Motion","text":"<p>The randomness comes from Brownian motion \\(w(t)\\), whose defining property is:</p> \\[ w(t+dt) - w(t) \\sim \\mathcal{N}(0, dt) \\] <p>This implies the differential form:</p> \\[ dw(t) = \\sqrt{dt}\\,\\varepsilon \\quad \\text{where } \\varepsilon \\sim \\mathcal{N}(0, 1) \\] <p>The \\(\\sqrt{dt}\\) scaling is not arbitrary \u2014 it is the only scaling that yields finite, non-trivial randomness in continuous time. This is a fundamental result from stochastic calculus.</p>"},{"location":"SDE/01_diffusion_sde_view/#3-notation-and-terminology","title":"3. Notation and Terminology","text":"<p>Let's be precise about what each symbol means:</p> Symbol Meaning Type \\(x(t)\\) State at time \\(t\\) Random variable \\(p_t(x)\\) Probability density at time \\(t\\) Distribution \\(f(x, t)\\) Drift function Deterministic, chosen \\(g(t)\\) Diffusion coefficient Scalar function, chosen \\(w(t)\\) Wiener process (Brownian motion) Stochastic process <p>Critical point: Nothing in the forward SDE is learned. The entire forward process is designed, not trained.</p>"},{"location":"SDE/01_diffusion_sde_view/#4-forward-vs-reverse-processes","title":"4. Forward vs Reverse Processes","text":""},{"location":"SDE/01_diffusion_sde_view/#the-forward-process","title":"The Forward Process","text":"<p>The forward SDE gradually destroys structure:</p> \\[ \\text{clean data} \\rightarrow \\text{noisy data} \\rightarrow \\text{pure noise} \\] <p>This defines a family of distributions \\(\\{p_t(x)\\}_{t \\in [0, T]}\\). While we can sample from this process, we typically do not know \\(p_t(x)\\) analytically.</p>"},{"location":"SDE/01_diffusion_sde_view/#the-reverse-time-sde","title":"The Reverse-Time SDE","text":"<p>The reverse-time SDE reconstructs structure by running the process backward:</p> \\[ dx = \\left[f(x, t) - g(t)^2 \\nabla_x \\log p_t(x)\\right]dt + g(t)\\,d\\bar{w} \\] <p>where \\(\\bar{w}\\) is a reverse-time Brownian motion.</p> <p>The correction term involves:</p> \\[ \\nabla_x \\log p_t(x) \\] <p>This is the score \u2014 the gradient of the log probability density.</p> <p>Key insight: The score is the only unknown object in the entire framework. Everything else is specified by the forward process design.</p>"},{"location":"SDE/01_diffusion_sde_view/#5-what-is-learned-and-what-is-not","title":"5. What Is Learned (and What Is Not)","text":""},{"location":"SDE/01_diffusion_sde_view/#not-learned","title":"Not Learned","text":"<ul> <li>The SDE form</li> <li>The drift function \\(f(x, t)\\)</li> <li>The noise schedule \\(g(t)\\)</li> <li>Brownian motion \\(w(t)\\)</li> </ul>"},{"location":"SDE/01_diffusion_sde_view/#learned","title":"Learned","text":"<p>A neural network \\(s_\\theta(x, t)\\) approximates the score:</p> \\[ s_\\theta(x, t) \\approx \\nabla_x \\log p_t(x) \\] <p>Interpretation: The network learns a time-dependent vector field that points toward regions of higher data density.</p>"},{"location":"SDE/01_diffusion_sde_view/#6-training-score-matching","title":"6. Training: Score Matching","text":"<p>Training never solves an SDE. Instead, it uses a simple supervised learning approach:</p>"},{"location":"SDE/01_diffusion_sde_view/#training-algorithm","title":"Training Algorithm","text":"<ol> <li>Sample clean data \\(x_0 \\sim p_{\\text{data}}\\)</li> <li>Sample time \\(t \\sim \\text{Uniform}(0, T)\\)</li> <li>Corrupt \\(x_0\\) using the known forward process to get \\(x_t\\)</li> <li>Compute the analytic score of the corruption (closed-form)</li> <li>Train neural network to match that score: \\(\\min_\\theta \\|s_\\theta(x_t, t) - \\nabla_x \\log p(x_t \\mid x_0)\\|^2\\)</li> </ol>"},{"location":"SDE/01_diffusion_sde_view/#equivalent-parameterizations","title":"Equivalent Parameterizations","text":"<p>These are all mathematically equivalent:</p> <ul> <li>Predicting noise: \\(\\epsilon_\\theta(x_t, t) \\approx \\epsilon\\)</li> <li>Predicting denoised data: \\(\\hat{x}_\\theta(x_t, t) \\approx x_0\\)</li> <li>Predicting score: \\(s_\\theta(x_t, t) \\approx \\nabla_x \\log p_t(x)\\)</li> </ul> <p>Different papers use different parameterizations, but they're learning the same object.</p>"},{"location":"SDE/01_diffusion_sde_view/#7-sampling-numerical-integration","title":"7. Sampling: Numerical Integration","text":"<p>Generation does solve an equation. We have two options:</p>"},{"location":"SDE/01_diffusion_sde_view/#option-1-reverse-sde-stochastic","title":"Option 1: Reverse SDE (Stochastic)","text":"\\[ dx = \\left[f(x, t) - g(t)^2 s_\\theta(x, t)\\right]dt + g(t)\\,d\\bar{w} \\] <ul> <li>Stochastic sampling</li> <li>Produces diverse samples</li> <li>Requires more steps</li> </ul>"},{"location":"SDE/01_diffusion_sde_view/#option-2-probability-flow-ode-deterministic","title":"Option 2: Probability-Flow ODE (Deterministic)","text":"\\[ \\frac{dx}{dt} = f(x, t) - \\frac{1}{2}g(t)^2 s_\\theta(x, t) \\] <ul> <li>Deterministic sampling</li> <li>Same marginals as SDE</li> <li>Faster, exact likelihoods</li> </ul>"},{"location":"SDE/01_diffusion_sde_view/#numerical-solvers","title":"Numerical Solvers","text":"<p>Both require discretization:</p> Solver Type Examples Euler\u2013Maruyama SDE DDPM Predictor\u2013Corrector SDE PC sampler Runge\u2013Kutta ODE DDIM, DPM-Solver <p>Key insight: DDPM, DDIM, and modern samplers are all discretizations of these continuous equations.</p>"},{"location":"SDE/01_diffusion_sde_view/#8-why-brownian-motion","title":"8. Why Brownian Motion?","text":"<p>Brownian motion is not the only possible noise model in SDEs (finance uses jumps, stochastic volatility, etc.), but it is used in diffusion models because it guarantees:</p> <ul> <li>Gaussian increments: Tractable distributions</li> <li>Markov structure: Memoryless dynamics</li> <li>Tractable reverse-time dynamics: Closed-form score targets</li> <li>Clean score matching objectives: Simple training</li> </ul> <p>This mathematical control is what makes diffusion models practical for high-dimensional data.</p>"},{"location":"SDE/01_diffusion_sde_view/#9-the-unifying-principle","title":"9. The Unifying Principle","text":"<p>Diffusion models learn a time-dependent score field that tells you how to move probability mass uphill from noise back to data.</p> <p>Everything else \u2014 DDPM, NCSN, SDEs, ODEs, samplers \u2014 is just a different coordinate system for expressing this core idea.</p>"},{"location":"SDE/01_diffusion_sde_view/#three-views-one-object","title":"Three Views, One Object","text":"View Focus Natural Formulation Variational ELBO, likelihood DDPM (discrete) Score-based \\(\\nabla_x \\log p_t(x)\\) NCSN, reverse SDE Flow-based Deterministic transport Probability-flow ODE SDE Continuous-time umbrella Unifies all three"},{"location":"SDE/01_diffusion_sde_view/#next-steps","title":"Next Steps","text":"<p>To deepen understanding:</p> <ol> <li>Derive DDPM from VP-SDE: See how discrete diffusion emerges from continuous time</li> <li>Compare VP-SDE vs VE-SDE: Understand different noise schedules</li> <li>Implement a simple sampler: Connect theory to code</li> </ol>"},{"location":"SDE/01_diffusion_sde_view/#references","title":"References","text":"<ul> <li>Song et al. (2021) - \"Score-Based Generative Modeling through Stochastic Differential Equations\"</li> <li>Ho et al. (2020) - \"Denoising Diffusion Probabilistic Models\" (DDPM)</li> <li>Song &amp; Ermon (2019) - \"Generative Modeling by Estimating Gradients of the Data Distribution\" (NCSN)</li> </ul>"},{"location":"SDE/01_diffusion_sde_view/#related-documents","title":"Related Documents","text":""},{"location":"SDE/01_diffusion_sde_view/#core-theory","title":"Core Theory","text":"<ul> <li>DDPM Foundations \u2014 Discrete-time perspective</li> <li>DDPM to VP-SDE \u2014 How DDPM becomes an SDE</li> <li>Fokker-Planck Equation \u2014 How probability densities evolve</li> </ul>"},{"location":"SDE/01_diffusion_sde_view/#extensions","title":"Extensions","text":"<ul> <li>Classifier-Free Guidance \u2014 Conditional generation (works in both DDPM and SDE views)</li> </ul>"},{"location":"SDE/01_diffusion_sde_view/#background","title":"Background","text":"<ul> <li>Historical Development \u2014 How DDPM, score-based, and SDE views unified</li> <li>SDE Q&amp;A \u2014 Common questions</li> </ul>"},{"location":"SDE/01a_diffusion_sde_view_QA/","title":"SDE View: Design Principles and Intuitions","text":"<p>This document addresses deeper questions about the SDE formulation of diffusion models: Why are specific drifts chosen? How do we think about the score-noise relationship in high dimensions? What are the design principles?</p>"},{"location":"SDE/01a_diffusion_sde_view_QA/#1-choosing-the-drift-design-principles","title":"1. Choosing the Drift: Design Principles","text":""},{"location":"SDE/01a_diffusion_sde_view_QA/#the-question","title":"The Question","text":"<p>When we write the forward SDE:</p> \\[ dx(t) = f(x(t), t)\\,dt + g(t)\\,dw(t) \\] <p>How do we choose the drift \\(f(x, t)\\)? For example, the VP-SDE uses:</p> \\[ f(x, t) = -\\frac{1}{2}\\beta(t) x \\] <p>Should we think of this as an \"external force\" moving probability mass?</p>"},{"location":"SDE/01a_diffusion_sde_view_QA/#the-answer","title":"The Answer","text":"<p>Yes \u2014 it is very helpful to think of the drift as an external force field acting on probability mass.</p> <p>The main guideline is:</p> <p>Choose a drift + noise schedule such that the forward process is simple, stable, and ends in a known distribution.</p>"},{"location":"SDE/01a_diffusion_sde_view_QA/#2-what-the-drift-actually-does","title":"2. What the Drift Actually Does","text":""},{"location":"SDE/01a_diffusion_sde_view_QA/#geometric-view","title":"Geometric View","text":"<p>In the Fokker\u2013Planck equation, which governs how densities evolve under an SDE:</p> \\[ \\frac{\\partial p_t(x)}{\\partial t} = -\\nabla \\cdot \\left(f(x, t) p_t(x)\\right) + \\frac{1}{2}\\nabla^2 \\left(g(t)^2 p_t(x)\\right) \\] <p>You can literally read off the roles:</p> <ul> <li>\\(f(x, t)\\): transports probability mass (advection term)</li> <li>\\(g(t)\\): diffuses (spreads) probability mass (diffusion term)</li> </ul> <p>Key insight: Drift is an external force field moving probability mass. Noise spreads mass; drift organizes where it flows.</p>"},{"location":"SDE/01a_diffusion_sde_view_QA/#3-why-the-vp-drift-is-linear","title":"3. Why the VP Drift Is Linear","text":"<p>The VP-SDE uses:</p> \\[ f(x, t) = -\\frac{1}{2}\\beta(t) x \\] <p>This choice is not arbitrary. It satisfies three extremely strong constraints simultaneously:</p>"},{"location":"SDE/01a_diffusion_sde_view_QA/#a-globally-stabilizing","title":"(a) Globally Stabilizing","text":"<p>The drift points toward the origin everywhere:</p> <ul> <li>No explosion</li> <li>No runaway trajectories</li> <li>Well-defined reverse dynamics</li> </ul> <p>This is crucial in very high dimensions (e.g., \\(d = 3072\\) for a 32\u00d732 RGB image).</p>"},{"location":"SDE/01a_diffusion_sde_view_QA/#b-preserves-gaussian-structure","title":"(b) Preserves Gaussian Structure","text":"<p>If the forward process starts Gaussian, it stays Gaussian.</p> <p>With this drift:</p> \\[ x(t) \\mid x_0 \\sim \\mathcal{N}\\left(\\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I\\right) \\] <p>This closed-form marginal is gold \u2014 it makes training trivial.</p>"},{"location":"SDE/01a_diffusion_sde_view_QA/#lets-noise-dominate-gradually","title":"\u00a9 Lets Noise Dominate Gradually","text":"<p>The linear drift slowly shrinks signal while noise grows:</p> <ul> <li>Early times: signal-dominated</li> <li>Late times: noise-dominated</li> </ul> <p>This creates a smooth continuum of difficulty levels for learning.</p>"},{"location":"SDE/01a_diffusion_sde_view_QA/#4-what-other-drifts-would-mean","title":"4. What Other Drifts Would Mean","text":"<p>You could choose:</p> <ul> <li>Nonlinear drift: \\(f(x, t) = -\\nabla U(x)\\) (energy-based)</li> <li>Data-dependent drift: \\(f(x, t) = h(x)\\)</li> <li>Anisotropic drift: Different behavior in different directions</li> </ul> <p>But then:</p> <ul> <li>Marginals are no longer analytic</li> <li>Score targets become implicit</li> <li>Training becomes unstable or intractable</li> </ul> <p>Guiding principle:</p> <p>Choose the simplest drift that gives you a tractable forward process and a learnable reverse process.</p> <p>VP-SDE hits that sweet spot.</p>"},{"location":"SDE/01a_diffusion_sde_view_QA/#5-mental-model-wind-fog-system","title":"5. Mental Model: Wind + Fog System","text":"<p>Think of the forward diffusion as:</p> <p>A carefully engineered wind + fog system that:</p> <ol> <li>Blows all probability mass toward a simple attractor (the origin)</li> <li>Blurs everything along the way (Gaussian noise)</li> <li>Leaves behind a smooth gradient field that can be learned (the score)</li> </ol>"},{"location":"SDE/01a_diffusion_sde_view_QA/#6-the-score-noise-relationship-in-high-dimensions","title":"6. The Score-Noise Relationship in High Dimensions","text":""},{"location":"SDE/01a_diffusion_sde_view_QA/#the-question_1","title":"The Question","text":"<p>When input \\(x\\) is an image (flattened to \\(\\mathbb{R}^d\\)), we have:</p> \\[ \\nabla_x \\log p(x_t \\mid x_0) = -\\frac{1}{\\sigma_t} \\varepsilon \\] <p>Should I think of this score-to-noise relation as being prescribed to each dimension of \\(x\\) independently? Do we have \\(d\\) different score-to-noise relationships of the same form?</p>"},{"location":"SDE/01a_diffusion_sde_view_QA/#the-answer_1","title":"The Answer","text":"<p>Yes mathematically, but no conceptually.</p>"},{"location":"SDE/01a_diffusion_sde_view_QA/#7-the-score-is-a-gradient-in-mathbbrd","title":"7. The Score Is a Gradient in \\(\\mathbb{R}^d\\)","text":"<p>When an image is flattened:</p> \\[ x \\in \\mathbb{R}^d \\] <p>The score is:</p> \\[ \\nabla_x \\log p_t(x) = \\left(\\frac{\\partial}{\\partial x_1}, \\ldots, \\frac{\\partial}{\\partial x_d}\\right) \\log p_t(x) \\] <p>Formally:</p> <ul> <li>Yes, there are \\(d\\) partial derivatives</li> <li>One per coordinate</li> </ul> <p>For Gaussian corruption:</p> \\[ \\nabla_x \\log p(x_t \\mid x_0) = -\\frac{1}{\\sigma_t^2}(x_t - \\mu_t) = -\\frac{1}{\\sigma_t} \\varepsilon \\] <p>This relation does hold coordinate-wise.</p>"},{"location":"SDE/01a_diffusion_sde_view_QA/#8-why-this-does-not-mean-independent-dimensions","title":"8. Why This Does NOT Mean \"Independent Dimensions\"","text":"<p>Key conceptual correction:</p> <p>Even though the formula is coordinate-wise, and:</p> \\[ \\varepsilon \\sim \\mathcal{N}(0, I) \\] <p>the marginal distribution \\(p_t(x)\\) does not factorize.</p> <p>That means:</p> <ul> <li>Pixels are not independent</li> <li>Genes are not independent</li> <li>Features are not independent</li> </ul> <p>The dependencies live in:</p> \\[ p_t(x) = \\int p(x_t \\mid x_0) p_{\\text{data}}(x_0)\\,dx_0 \\] <p>The score reflects global structure.</p>"},{"location":"SDE/01a_diffusion_sde_view_QA/#9-why-the-network-output-must-be-global","title":"9. Why the Network Output Must Be Global","text":"<p>Although the score is a \\(d\\)-vector, each component:</p> \\[ \\frac{\\partial}{\\partial x_i} \\log p_t(x) \\] <p>depends on all coordinates of \\(x\\).</p> <p>That's why:</p> <ul> <li>CNNs use receptive fields</li> <li>Transformers use attention</li> <li>U-Nets use multi-scale context</li> </ul> <p>The model is not learning \"\\(d\\) independent scores\". It is learning one high-dimensional vector field.</p>"},{"location":"SDE/01a_diffusion_sde_view_QA/#10-a-helpful-analogy","title":"10. A Helpful Analogy","text":"<p>Think of a potential energy surface \\(U(x)\\) in physics:</p> <ul> <li>Force = \\(-\\nabla U(x)\\)</li> <li>Each force component is a partial derivative</li> <li>But the potential is global</li> </ul> <p>Same here:</p> \\[ \\text{score}(x) = \\nabla \\log p(x) \\] <p>Local components, global meaning.</p>"},{"location":"SDE/01a_diffusion_sde_view_QA/#11-summary-the-conceptual-core","title":"11. Summary: The Conceptual Core","text":""},{"location":"SDE/01a_diffusion_sde_view_QA/#on-drift","title":"On Drift","text":"<ul> <li>The drift is an externally designed force field that shapes how probability mass flows during corruption</li> <li>VP-SDE uses a linear, stabilizing drift because it preserves Gaussian structure and yields tractable training</li> <li>Think of it as a \"wind system\" guiding probability mass toward a simple attractor</li> </ul>"},{"location":"SDE/01a_diffusion_sde_view_QA/#on-score-noise-relationship","title":"On Score-Noise Relationship","text":"<ul> <li>The score-noise relation is coordinate-wise, but the score itself is globally coupled</li> <li>High-dimensional diffusion does not mean independent dimensions</li> <li>It means a vector field whose components depend on the entire object</li> </ul>"},{"location":"SDE/01a_diffusion_sde_view_QA/#12-next-deep-steps","title":"12. Next Deep Steps","text":"<p>To go deeper:</p> <ol> <li>Contrast VP-SDE with VE-SDE in this same force-field language</li> <li>Discuss nonlinear drifts for structured data like graphs or gene networks</li> <li>Derive the Fokker-Planck equation to see how drift and diffusion shape \\(p_t(x)\\)</li> </ol>"},{"location":"SDE/01a_diffusion_sde_view_QA/#related-documents","title":"Related Documents","text":"<ul> <li>SDE View Overview</li> <li>Historical Development</li> <li>DDPM Foundations</li> </ul>"},{"location":"SDE/02_sde_and_ddpm/","title":"Deriving DDPM from the VP-SDE","text":"<p>This document shows how the discrete DDPM algorithm emerges naturally from the continuous VP-SDE through Euler\u2013Maruyama discretization. We'll derive both the forward noising process and the reverse denoising process, making explicit why DDPM predicts noise rather than the score directly.</p>"},{"location":"SDE/02_sde_and_ddpm/#overview","title":"Overview","text":"<p>We'll proceed in two parts:</p> <ol> <li>Forward VP-SDE \u2192 DDPM forward Markov chain (Euler\u2013Maruyama discretization)</li> <li>Reverse VP-SDE \u2192 DDPM reverse step (why the network predicts noise/score)</li> </ol>"},{"location":"SDE/02_sde_and_ddpm/#notation","title":"Notation","text":"<p>Let's establish precise notation to avoid confusion:</p> Symbol Meaning \\(x(t) \\in \\mathbb{R}^d\\) Data state at continuous time \\(t \\in [0, T]\\) \\(0 = t_0 &lt; t_1 &lt; \\cdots &lt; t_N = T\\) Discrete time grid \\(\\Delta t_k := t_{k+1} - t_k\\) Time step size \\(w(t)\\) Brownian motion (Wiener process) \\(\\Delta w_k := w(t_{k+1}) - w(t_k)\\) Brownian increment, \\(\\sim \\mathcal{N}(0, \\Delta t_k I)\\) \\(dw(t)\\) Infinitesimal increment: \\(dw \\approx \\sqrt{dt}\\,\\varepsilon\\), \\(\\varepsilon \\sim \\mathcal{N}(0, I)\\) \\(\\beta(t) \\geq 0\\) Noise rate schedule (chosen by designer)"},{"location":"SDE/02_sde_and_ddpm/#part-a-forward-vp-sde-ddpm-forward-noising","title":"Part A: Forward VP-SDE \u2192 DDPM Forward Noising","text":""},{"location":"SDE/02_sde_and_ddpm/#step-1-the-variance-preserving-sde","title":"Step 1: The Variance-Preserving SDE","text":"<p>The variance-preserving SDE (VP-SDE) is:</p> \\[ dx(t) = -\\frac{1}{2}\\beta(t) x(t)\\,dt + \\sqrt{\\beta(t)}\\,dw(t) \\] <p>This SDE has:</p> <ul> <li>Drift: \\(f(x, t) = -\\frac{1}{2}\\beta(t) x\\)</li> <li>Diffusion coefficient: \\(g(t) = \\sqrt{\\beta(t)}\\)</li> </ul>"},{"location":"SDE/02_sde_and_ddpm/#step-2-apply-eulermaruyama-discretization","title":"Step 2: Apply Euler\u2013Maruyama Discretization","text":"<p>Euler\u2013Maruyama is a numerical method for discretizing SDEs. For a general SDE:</p> \\[ dx = f(x, t)\\,dt + g(t)\\,dw \\] <p>The discrete update from \\(t_k \\to t_{k+1}\\) is:</p> \\[ x_{k+1} = x_k + f(x_k, t_k)\\,\\Delta t_k + g(t_k)\\,\\Delta w_k \\] <p>where \\(\\Delta w_k := w(t_{k+1}) - w(t_k) \\sim \\mathcal{N}(0, \\Delta t_k I)\\).</p> <p>Rewrite the Brownian increment:</p> \\[ \\Delta w_k = \\sqrt{\\Delta t_k}\\,\\varepsilon_k, \\quad \\varepsilon_k \\sim \\mathcal{N}(0, I) \\] <p>Then:</p> \\[ x_{k+1} = x_k + f(x_k, t_k)\\,\\Delta t_k + g(t_k)\\sqrt{\\Delta t_k}\\,\\varepsilon_k \\]"},{"location":"SDE/02_sde_and_ddpm/#step-3-plug-in-vp-sde-components","title":"Step 3: Plug in VP-SDE Components","text":"<p>Substitute \\(f(x, t) = -\\frac{1}{2}\\beta(t) x\\) and \\(g(t) = \\sqrt{\\beta(t)}\\):</p> \\[ x_{k+1} = x_k - \\frac{1}{2}\\beta(t_k) x_k\\,\\Delta t_k + \\sqrt{\\beta(t_k)}\\sqrt{\\Delta t_k}\\,\\varepsilon_k \\] <p>Factor out \\(x_k\\):</p> \\[ x_{k+1} = \\left(1 - \\frac{1}{2}\\beta(t_k)\\Delta t_k\\right) x_k + \\sqrt{\\beta(t_k)\\Delta t_k}\\,\\varepsilon_k \\] <p>Define discrete noise parameter:</p> \\[ \\beta_k := \\beta(t_k)\\,\\Delta t_k \\] <p>Then:</p> \\[ \\boxed{x_{k+1} = \\left(1 - \\frac{1}{2}\\beta_k\\right) x_k + \\sqrt{\\beta_k}\\,\\varepsilon_k} \\quad \\text{(Euler\u2013Maruyama form)} \\] <p>This is already a \"diffusion-like\" forward step!</p>"},{"location":"SDE/02_sde_and_ddpm/#step-4-why-ddpm-uses-sqrt1-beta_k-instead","title":"Step 4: Why DDPM Uses \\(\\sqrt{1-\\beta_k}\\) Instead","text":"<p>The actual DDPM forward step is written as:</p> \\[ \\boxed{x_{k+1} = \\sqrt{1-\\beta_k}\\,x_k + \\sqrt{\\beta_k}\\,\\varepsilon_k} \\] <p>Where did \\(\\sqrt{1-\\beta_k}\\) come from instead of \\(1 - \\frac{1}{2}\\beta_k\\)?</p> <p>Answer: It's a variance-preserving tweak that matches the first-order Taylor expansion:</p> \\[ \\sqrt{1-\\beta_k} = 1 - \\frac{1}{2}\\beta_k + O(\\beta_k^2) \\] <p>Comparison:</p> Form Accuracy Variance Control \\(1 - \\frac{1}{2}\\beta_k\\) First-order accurate Approximate \\(\\sqrt{1-\\beta_k}\\) First-order accurate Exact <p>DDPM uses \\(\\sqrt{1-\\beta_k}\\) because:</p> <ul> <li>It agrees with Euler\u2013Maruyama to first order</li> <li>It exactly controls variance in discrete time</li> <li>It keeps the process well-behaved when \\(\\beta_k\\) isn't infinitesimal</li> </ul> <p>DDPM notation: Define \\(\\alpha_k := 1 - \\beta_k\\). Then:</p> \\[ q(x_{k+1} \\mid x_k) = \\mathcal{N}\\left(\\sqrt{\\alpha_k} x_k, (1-\\alpha_k) I\\right) \\] <p>Sampling form:</p> \\[ x_{k+1} = \\sqrt{\\alpha_k}\\,x_k + \\sqrt{1-\\alpha_k}\\,\\varepsilon_k \\] <p>Key insight: DDPM's forward chain is a renormalized Euler step that preserves variance exactly.</p>"},{"location":"SDE/02_sde_and_ddpm/#part-b-reverse-vp-sde-ddpm-reverse-denoising","title":"Part B: Reverse VP-SDE \u2192 DDPM Reverse Denoising","text":""},{"location":"SDE/02_sde_and_ddpm/#step-1-the-reverse-time-sde-formula","title":"Step 1: The Reverse-Time SDE Formula","text":"<p>For a forward SDE:</p> \\[ dx = f(x, t)\\,dt + g(t)\\,dw \\] <p>The reverse-time SDE (running from \\(T \\to 0\\)) is:</p> \\[ dx = \\left[f(x, t) - g(t)^2 \\nabla_x \\log p_t(x)\\right]dt + g(t)\\,d\\bar{w} \\] <p>where:</p> <ul> <li>\\(p_t(x)\\) is the marginal density of \\(x(t)\\)</li> <li>\\(\\nabla_x \\log p_t(x)\\) is the score (gradient of log density)</li> <li>\\(d\\bar{w}\\) is Brownian noise in reverse time</li> </ul>"},{"location":"SDE/02_sde_and_ddpm/#step-2-apply-to-vp-sde","title":"Step 2: Apply to VP-SDE","text":"<p>Substitute \\(f = -\\frac{1}{2}\\beta(t) x\\) and \\(g^2 = \\beta(t)\\):</p> \\[ \\boxed{dx = \\left[-\\frac{1}{2}\\beta(t) x - \\beta(t)\\nabla_x \\log p_t(x)\\right]dt + \\sqrt{\\beta(t)}\\,d\\bar{w}} \\] <p>This is the reverse diffusion equation.</p> <p>Key observation: The only unknown term is the score \\(\\nabla_x \\log p_t(x)\\).</p> <p>Solution: Learn a neural network:</p> \\[ s_\\theta(x, t) \\approx \\nabla_x \\log p_t(x) \\]"},{"location":"SDE/02_sde_and_ddpm/#step-3-discretize-the-reverse-sde","title":"Step 3: Discretize the Reverse SDE","text":"<p>Apply Euler\u2013Maruyama again, but stepping backward in time:</p> \\[ x_{k-1} \\approx x_k + \\left[-\\frac{1}{2}\\beta_k x_k - \\beta_k s_\\theta(x_k, t_k)\\right] + \\sqrt{\\beta_k}\\,z_k \\] <p>where \\(z_k \\sim \\mathcal{N}(0, I)\\).</p> <p>(Note: We've absorbed \\(\\Delta t\\) factors into \\(\\beta_k\\) to match DDPM convention.)</p> <p>This is the SDE-solver view of reverse sampling.</p>"},{"location":"SDE/02_sde_and_ddpm/#step-4-connection-to-ddpms-learned-gaussian","title":"Step 4: Connection to DDPM's Learned Gaussian","text":"<p>DDPM doesn't present sampling as \"Euler\u2013Maruyama on the reverse SDE.\" Instead, it presents it as a learned Gaussian transition:</p> \\[ p_\\theta(x_{k-1} \\mid x_k) = \\mathcal{N}\\left(\\mu_\\theta(x_k, k), \\Sigma_k\\right) \\] <p>These are consistent: An Euler step of an SDE is a Gaussian update where:</p> <ul> <li>Mean: current state + drift term</li> <li>Variance: diffusion strength</li> </ul>"},{"location":"SDE/02_sde_and_ddpm/#step-5-why-predict-noise-instead-of-score","title":"Step 5: Why Predict Noise Instead of Score?","text":"<p>The remaining question: Why parameterize via noise prediction \\(\\varepsilon_\\theta\\) instead of score \\(s_\\theta\\)?</p> <p>Answer: Under the forward marginal:</p> \\[ x_k = \\sqrt{\\bar{\\alpha}_k} x_0 + \\sqrt{1 - \\bar{\\alpha}_k}\\,\\varepsilon \\] <p>The conditional score has a clean identity:</p> \\[ \\nabla_{x_k} \\log q(x_k \\mid x_0) = -\\frac{1}{\\sqrt{1 - \\bar{\\alpha}_k}}\\,\\varepsilon \\] <p>Therefore: If a network predicts \\(\\varepsilon\\), it is (up to scaling) predicting the score!</p>"},{"location":"SDE/02_sde_and_ddpm/#the-bridge-between-views","title":"The Bridge Between Views","text":"View What to Learn Relationship SDE view Score \\(s_\\theta(x, t)\\) Direct DDPM view Noise \\(\\varepsilon_\\theta(x_t, t)\\) \\(s_\\theta = -\\frac{1}{\\sqrt{1-\\bar{\\alpha}_t}} \\varepsilon_\\theta\\) <p>These are equivalent up to a known scale factor.</p>"},{"location":"SDE/02_sde_and_ddpm/#summary","title":"Summary","text":""},{"location":"SDE/02_sde_and_ddpm/#forward-ddpm","title":"Forward DDPM","text":"<p>The forward process is the VP-SDE discretized via Euler\u2013Maruyama, with a variance-preserving square-root coefficient:</p> \\[ x_{k+1} = \\sqrt{1-\\beta_k}\\,x_k + \\sqrt{\\beta_k}\\,\\varepsilon_k \\] <p>Key points:</p> <ul> <li>Emerges from continuous-time SDE</li> <li>\\(\\sqrt{1-\\beta_k}\\) preserves variance exactly</li> <li>Agrees with Euler\u2013Maruyama to first order</li> </ul>"},{"location":"SDE/02_sde_and_ddpm/#reverse-ddpm","title":"Reverse DDPM","text":"<p>The reverse process discretizes the reverse-time SDE:</p> \\[ x_{k-1} = x_k + \\left[-\\frac{1}{2}\\beta_k x_k - \\beta_k s_\\theta(x_k, t_k)\\right] + \\sqrt{\\beta_k}\\,z_k \\] <p>Key points:</p> <ul> <li>Only unknown: the score \\(\\nabla_x \\log p_t(x)\\)</li> <li>Noise prediction \\(\\varepsilon_\\theta\\) is a reparameterization of score</li> <li>Equivalent to learning the score up to scaling</li> </ul>"},{"location":"SDE/02_sde_and_ddpm/#next-steps","title":"Next Steps","text":"<p>To deepen understanding:</p> <ol> <li>Continuous limit: Start from DDPM and recover VP-SDE as \\(\\Delta t \\to 0\\)</li> <li>Variance analysis: Verify variance preservation in the forward chain</li> <li>Sampling algorithms: Derive DDPM sampler, DDIM, and other variants</li> </ol>"},{"location":"SDE/02_sde_and_ddpm/#related-documents","title":"Related Documents","text":"<ul> <li>SDE View Overview</li> <li>Taylor Expansions in Diffusion</li> <li>Historical Development</li> </ul>"},{"location":"SDE/02_sde_training/","title":"Training Diffusion Models: The SDE Perspective","text":"<p>This document explains how diffusion models are trained from the SDE perspective, clarifying a common source of confusion: SDE solvers are NOT used during training.</p> <p>Training uses closed-form solutions of the forward SDE, making it computationally simple and efficient.</p>"},{"location":"SDE/02_sde_training/#overview","title":"Overview","text":""},{"location":"SDE/02_sde_training/#the-key-insight","title":"The Key Insight","text":"<p>Common misconception: Since diffusion models are based on SDEs, training must involve solving SDEs.</p> <p>Reality: Training uses closed-form marginals from the forward SDE solution. No numerical SDE solver is needed.</p>"},{"location":"SDE/02_sde_training/#training-vs-sampling","title":"Training vs Sampling","text":"Phase SDE Solver? What's Used Training \u274c No Closed-form \\(q(x_t \\mid x_0)\\) Sampling \u2705 Yes Numerical discretization of reverse SDE/ODE <p>This document focuses on training \u2014 see 03_sde_sampling.md for sampling.</p>"},{"location":"SDE/02_sde_training/#the-forward-sde-solution","title":"The Forward SDE Solution","text":""},{"location":"SDE/02_sde_training/#vp-sde-variance-preserving","title":"VP-SDE (Variance-Preserving)","text":"<p>The forward SDE is:</p> \\[ dx = -\\frac{1}{2}\\beta(t) x\\,dt + \\sqrt{\\beta(t)}\\,dw \\] <p>Closed-form solution (see 03_solving_vpsde.md for derivation):</p> \\[ x_t = \\sqrt{\\bar{\\alpha}_t} \\, x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon \\] <p>where:</p> <ul> <li>\\(\\bar{\\alpha}_t = \\exp\\left(-\\int_0^t \\beta(s)\\,ds\\right)\\)</li> <li>\\(\\epsilon \\sim \\mathcal{N}(0, I)\\)</li> </ul> <p>Key property: We can sample \\(x_t\\) directly from \\(x_0\\) without simulating the SDE!</p>"},{"location":"SDE/02_sde_training/#marginal-distribution","title":"Marginal Distribution","text":"<p>The marginal distribution at time \\(t\\) is:</p> \\[ q(x_t \\mid x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1-\\bar{\\alpha}_t)I) \\] <p>This is all we need for training \u2014 no SDE solver required.</p>"},{"location":"SDE/02_sde_training/#training-objective","title":"Training Objective","text":""},{"location":"SDE/02_sde_training/#score-matching","title":"Score Matching","text":"<p>The goal is to learn the score function \\(\\nabla_x \\log p_t(x)\\).</p> <p>Theoretical objective (intractable):</p> \\[ \\mathbb{E}_{t, x_t} \\left[ \\left\\| s_\\theta(x_t, t) - \\nabla_x \\log p_t(x_t) \\right\\|^2 \\right] \\] <p>Practical objective (tractable via denoising score matching):</p> \\[ \\mathbb{E}_{t, x_0, \\epsilon} \\left[ \\lambda(t) \\left\\| s_\\theta(x_t, t) - \\nabla_x \\log q(x_t \\mid x_0) \\right\\|^2 \\right] \\] <p>where \\(x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1-\\bar{\\alpha}_t} \\epsilon\\).</p>"},{"location":"SDE/02_sde_training/#connection-to-noise-prediction","title":"Connection to Noise Prediction","text":"<p>The conditional score has a simple form:</p> \\[ \\nabla_x \\log q(x_t \\mid x_0) = -\\frac{x_t - \\sqrt{\\bar{\\alpha}_t} x_0}{1 - \\bar{\\alpha}_t} = -\\frac{\\epsilon}{\\sqrt{1-\\bar{\\alpha}_t}} \\] <p>In practice, we predict noise instead of score:</p> \\[ \\epsilon_\\theta(x_t, t) \\approx \\epsilon \\] <p>Relationship:</p> \\[ s_\\theta(x_t, t) = -\\frac{\\epsilon_\\theta(x_t, t)}{\\sqrt{1-\\bar{\\alpha}_t}} \\]"},{"location":"SDE/02_sde_training/#ddpm-training-loss","title":"DDPM Training Loss","text":"<p>The standard DDPM loss is:</p> \\[ \\mathcal{L}_{\\text{simple}} = \\mathbb{E}_{t, x_0, \\epsilon} \\left[ \\left\\| \\epsilon_\\theta(x_t, t) - \\epsilon \\right\\|^2 \\right] \\] <p>Why this works: 1. Equivalent to score matching with specific weighting \\(\\lambda(t) = 1\\) 2. Simpler to implement (predict noise, not score) 3. Better empirical performance</p>"},{"location":"SDE/02_sde_training/#training-algorithm","title":"Training Algorithm","text":""},{"location":"SDE/02_sde_training/#pseudocode","title":"Pseudocode","text":"<pre><code># Training loop for VP-SDE diffusion model\n\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        x_0 = batch  # Clean data samples\n\n        # 1. Sample random timesteps\n        t = torch.rand(batch_size) * T  # t \u2208 [0, T]\n\n        # 2. Sample noise\n        epsilon = torch.randn_like(x_0)\n\n        # 3. Compute \u03b1\u0305_t (cumulative product)\n        alpha_bar_t = compute_alpha_bar(t)  # exp(-\u222b\u2080\u1d57 \u03b2(s)ds)\n\n        # 4. Create noisy samples (closed-form!)\n        x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * epsilon\n\n        # 5. Predict noise\n        epsilon_pred = model(x_t, t)\n\n        # 6. Compute loss\n        loss = F.mse_loss(epsilon_pred, epsilon)\n\n        # 7. Update model\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n</code></pre> <p>Key observation: Step 4 uses the closed-form solution \u2014 no SDE solver!</p>"},{"location":"SDE/02_sde_training/#pytorch-implementation","title":"PyTorch Implementation","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DiffusionTrainer:\n    def __init__(self, model, beta_schedule, T=1.0):\n        self.model = model\n        self.T = T\n\n        # Precompute \u03b1\u0305_t for efficiency\n        # In continuous time: \u03b1\u0305(t) = exp(-\u222b\u2080\u1d57 \u03b2(s)ds)\n        # For linear schedule: \u03b2(t) = \u03b2_min + t(\u03b2_max - \u03b2_min)\n        self.beta_min = beta_schedule['beta_min']\n        self.beta_max = beta_schedule['beta_max']\n\n    def compute_alpha_bar(self, t):\n        \"\"\"Compute \u03b1\u0305_t = exp(-\u222b\u2080\u1d57 \u03b2(s)ds) for linear schedule.\"\"\"\n        # For \u03b2(s) = \u03b2_min + s(\u03b2_max - \u03b2_min):\n        # \u222b\u2080\u1d57 \u03b2(s)ds = \u03b2_min*t + (\u03b2_max - \u03b2_min)*t\u00b2/2\n        integral = self.beta_min * t + 0.5 * (self.beta_max - self.beta_min) * t**2\n        return torch.exp(-integral)\n\n    def training_step(self, x_0):\n        \"\"\"Single training step.\"\"\"\n        batch_size = x_0.shape[0]\n\n        # Sample timesteps uniformly\n        t = torch.rand(batch_size, device=x_0.device) * self.T\n\n        # Sample noise\n        epsilon = torch.randn_like(x_0)\n\n        # Compute \u03b1\u0305_t\n        alpha_bar_t = self.compute_alpha_bar(t)\n\n        # Reshape for broadcasting\n        alpha_bar_t = alpha_bar_t.view(-1, 1, 1, 1)  # For images\n\n        # Create noisy samples (closed-form marginal)\n        sqrt_alpha_bar_t = torch.sqrt(alpha_bar_t)\n        sqrt_one_minus_alpha_bar_t = torch.sqrt(1 - alpha_bar_t)\n        x_t = sqrt_alpha_bar_t * x_0 + sqrt_one_minus_alpha_bar_t * epsilon\n\n        # Predict noise\n        epsilon_pred = self.model(x_t, t)\n\n        # Compute loss\n        loss = F.mse_loss(epsilon_pred, epsilon)\n\n        return loss\n\n# Usage\nmodel = UNet(...)  # Your noise prediction network\ntrainer = DiffusionTrainer(\n    model=model,\n    beta_schedule={'beta_min': 0.1, 'beta_max': 20.0},\n    T=1.0\n)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        loss = trainer.training_step(batch)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n</code></pre>"},{"location":"SDE/02_sde_training/#why-no-sde-solver-in-training","title":"Why No SDE Solver in Training?","text":""},{"location":"SDE/02_sde_training/#mathematical-reason","title":"Mathematical Reason","text":"<p>The forward SDE has a closed-form solution:</p> \\[ x_t = \\sqrt{\\bar{\\alpha}_t} \\, x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon \\] <p>This means:</p> <ul> <li>We can sample \\(x_t\\) directly from \\(x_0\\)</li> <li>No need to simulate the SDE step-by-step</li> <li>Training is fast and exact</li> </ul>"},{"location":"SDE/02_sde_training/#computational-advantage","title":"Computational Advantage","text":"<p>With SDE solver (hypothetical, not used): - Start with \\(x_0\\) - Simulate many small steps: \\(x_0 \\to x_{\\Delta t} \\to x_{2\\Delta t} \\to \\cdots \\to x_t\\) - Slow and introduces discretization error</p> <p>With closed-form (actual approach): - Directly compute \\(x_t\\) from \\(x_0\\) in one step - Fast and exact - This is what makes training practical!</p>"},{"location":"SDE/02_sde_training/#connection-to-ddpm","title":"Connection to DDPM","text":""},{"location":"SDE/02_sde_training/#discrete-ddpm-training","title":"Discrete DDPM Training","text":"<p>DDPM uses discrete timesteps \\(k = 1, 2, \\ldots, N\\):</p> <pre><code># DDPM training\nt = torch.randint(0, N, (batch_size,))\nalpha_bar_t = alpha_bar[t]  # Precomputed cumulative product\nx_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * epsilon\n</code></pre>"},{"location":"SDE/02_sde_training/#continuous-sde-training","title":"Continuous SDE Training","text":"<p>SDE uses continuous time \\(t \\in [0, T]\\):</p> <pre><code># SDE training\nt = torch.rand(batch_size) * T\nalpha_bar_t = compute_alpha_bar(t)  # exp(-\u222b\u2080\u1d57 \u03b2(s)ds)\nx_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * epsilon\n</code></pre> <p>Key difference: How \\(\\bar{\\alpha}_t\\) is computed - DDPM: Discrete products \\(\\prod_{i=1}^t \\alpha_i\\) - SDE: Continuous integral \\(\\exp(-\\int_0^t \\beta(s)ds)\\)</p> <p>Training loop: Identical structure!</p>"},{"location":"SDE/02_sde_training/#noise-schedules","title":"Noise Schedules","text":""},{"location":"SDE/02_sde_training/#linear-schedule-most-common","title":"Linear Schedule (Most Common)","text":"\\[ \\beta(t) = \\beta_{\\min} + t(\\beta_{\\max} - \\beta_{\\min}), \\quad t \\in [0, 1] \\] <p>Cumulative:</p> \\[ \\bar{\\alpha}_t = \\exp\\left(-\\beta_{\\min} t - \\frac{1}{2}(\\beta_{\\max} - \\beta_{\\min}) t^2\\right) \\] <p>Typical values: \\(\\beta_{\\min} = 0.1\\), \\(\\beta_{\\max} = 20\\)</p>"},{"location":"SDE/02_sde_training/#cosine-schedule","title":"Cosine Schedule","text":"\\[ \\bar{\\alpha}_t = \\frac{f(t)}{f(0)}, \\quad f(t) = \\cos\\left(\\frac{t/T + s}{1+s} \\cdot \\frac{\\pi}{2}\\right)^2 \\] <p>where \\(s = 0.008\\) is a small offset.</p> <p>Advantage: More uniform SNR across timesteps.</p>"},{"location":"SDE/02_sde_training/#implementation","title":"Implementation","text":"<pre><code>def linear_beta_schedule(t, beta_min=0.1, beta_max=20.0):\n    \"\"\"Linear noise schedule.\"\"\"\n    return beta_min + t * (beta_max - beta_min)\n\ndef compute_alpha_bar_linear(t, beta_min=0.1, beta_max=20.0):\n    \"\"\"Compute \u03b1\u0305_t for linear schedule.\"\"\"\n    integral = beta_min * t + 0.5 * (beta_max - beta_min) * t**2\n    return torch.exp(-integral)\n\ndef compute_alpha_bar_cosine(t, s=0.008):\n    \"\"\"Compute \u03b1\u0305_t for cosine schedule.\"\"\"\n    def f(t):\n        return torch.cos((t + s) / (1 + s) * torch.pi / 2) ** 2\n    return f(t) / f(torch.zeros_like(t))\n</code></pre>"},{"location":"SDE/02_sde_training/#training-strategies","title":"Training Strategies","text":""},{"location":"SDE/02_sde_training/#time-sampling","title":"Time Sampling","text":"<p>Uniform sampling (standard): <pre><code>t = torch.rand(batch_size) * T\n</code></pre></p> <p>Importance sampling (advanced): - Sample more frequently at difficult timesteps - Weight loss by inverse sampling probability</p>"},{"location":"SDE/02_sde_training/#weighting","title":"Weighting","text":"<p>Simple weighting (DDPM):</p> \\[ \\mathcal{L} = \\mathbb{E}_{t, x_0, \\epsilon} \\left[ \\left\\| \\epsilon_\\theta(x_t, t) - \\epsilon \\right\\|^2 \\right] \\] <p>SNR weighting (improved):</p> <p>$$</p> <p>\\mathcal{L} = \\mathbb{E}{t, x_0, \\epsilon} \\left[ \\frac{1}{\\text{SNR}(t)} \\left| \\epsilon\\theta(x_t, t) - \\epsilon \\right|^2 \\right] $$</p> <p>where \\(\\text{SNR}(t) = \\bar{\\alpha}_t / (1 - \\bar{\\alpha}_t)\\).</p>"},{"location":"SDE/02_sde_training/#exponential-moving-average-ema","title":"Exponential Moving Average (EMA)","text":"<p>Maintain EMA of model parameters for better sample quality:</p> <pre><code>ema_model = copy.deepcopy(model)\nema_decay = 0.9999\n\n# After each training step\nfor param_ema, param in zip(ema_model.parameters(), model.parameters()):\n    param_ema.data.mul_(ema_decay).add_(param.data, alpha=1 - ema_decay)\n\n# Use ema_model for sampling\n</code></pre>"},{"location":"SDE/02_sde_training/#network-architecture","title":"Network Architecture","text":""},{"location":"SDE/02_sde_training/#u-net-standard-for-images","title":"U-Net (Standard for Images)","text":"<pre><code>class UNet(nn.Module):\n    def __init__(self, in_channels=3, out_channels=3, \n                 base_channels=128, time_emb_dim=256):\n        super().__init__()\n\n        # Time embedding\n        self.time_mlp = nn.Sequential(\n            SinusoidalPositionEmbeddings(time_emb_dim),\n            nn.Linear(time_emb_dim, time_emb_dim),\n            nn.GELU(),\n            nn.Linear(time_emb_dim, time_emb_dim),\n        )\n\n        # Encoder\n        self.encoder = nn.ModuleList([\n            ResBlock(in_channels, base_channels, time_emb_dim),\n            ResBlock(base_channels, base_channels * 2, time_emb_dim),\n            ResBlock(base_channels * 2, base_channels * 4, time_emb_dim),\n        ])\n\n        # Bottleneck\n        self.bottleneck = ResBlock(base_channels * 4, base_channels * 4, time_emb_dim)\n\n        # Decoder\n        self.decoder = nn.ModuleList([\n            ResBlock(base_channels * 8, base_channels * 2, time_emb_dim),\n            ResBlock(base_channels * 4, base_channels, time_emb_dim),\n            ResBlock(base_channels * 2, base_channels, time_emb_dim),\n        ])\n\n        # Output\n        self.out = nn.Conv2d(base_channels, out_channels, 1)\n\n    def forward(self, x, t):\n        # Embed time\n        t_emb = self.time_mlp(t)\n\n        # Encoder with skip connections\n        skips = []\n        for block in self.encoder:\n            x = block(x, t_emb)\n            skips.append(x)\n            x = F.avg_pool2d(x, 2)\n\n        # Bottleneck\n        x = self.bottleneck(x, t_emb)\n\n        # Decoder with skip connections\n        for block in self.decoder:\n            x = F.interpolate(x, scale_factor=2, mode='nearest')\n            x = torch.cat([x, skips.pop()], dim=1)\n            x = block(x, t_emb)\n\n        return self.out(x)\n</code></pre>"},{"location":"SDE/02_sde_training/#time-embedding","title":"Time Embedding","text":"<p>Sinusoidal embeddings (like Transformer positional encoding):</p> <pre><code>class SinusoidalPositionEmbeddings(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, time):\n        device = time.device\n        half_dim = self.dim // 2\n        embeddings = math.log(10000) / (half_dim - 1)\n        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n        embeddings = time[:, None] * embeddings[None, :]\n        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n        return embeddings\n</code></pre>"},{"location":"SDE/02_sde_training/#training-tips","title":"Training Tips","text":""},{"location":"SDE/02_sde_training/#hyperparameters","title":"Hyperparameters","text":"<p>Learning rate: </p> <ul> <li>Start: \\(1 \\times 10^{-4}\\)</li> <li>Use cosine annealing or constant</li> </ul> <p>Batch size:</p> <ul> <li>Larger is better (256-2048)</li> <li>Use gradient accumulation if memory limited</li> </ul> <p>Training steps:</p> <ul> <li>Images: 500K - 1M steps</li> <li>High resolution: 1M+ steps</li> </ul>"},{"location":"SDE/02_sde_training/#monitoring","title":"Monitoring","text":"<p>Track during training: 1. Loss: Should decrease steadily 2. Sample quality: Generate samples every N steps 3. EMA decay: Use EMA model for evaluation</p> <p>Early stopping:</p> <ul> <li>Monitor FID or other metrics on validation set</li> <li>Diffusion models benefit from long training</li> </ul>"},{"location":"SDE/02_sde_training/#debugging","title":"Debugging","text":"<p>Common issues:</p> <ol> <li>Loss not decreasing:</li> <li>Check data normalization (typically [-1, 1])</li> <li>Verify \\(\\bar{\\alpha}_t\\) computation</li> <li> <p>Check time embedding</p> </li> <li> <p>Poor sample quality:</p> </li> <li>Train longer</li> <li>Use EMA</li> <li> <p>Try different noise schedule</p> </li> <li> <p>Mode collapse:</p> </li> <li>Rare in diffusion models</li> <li>Check data diversity</li> </ol>"},{"location":"SDE/02_sde_training/#comparison-training-vs-sampling","title":"Comparison: Training vs Sampling","text":""},{"location":"SDE/02_sde_training/#training-this-document","title":"Training (This Document)","text":"<p>What: Learn to predict noise \\(\\epsilon_\\theta(x_t, t)\\)</p> <p>How: 1. Sample \\(x_0\\), \\(t\\), \\(\\epsilon\\) 2. Compute \\(x_t\\) using closed-form 3. Predict \\(\\epsilon_\\theta(x_t, t)\\) 4. Minimize \\(\\|\\epsilon_\\theta - \\epsilon\\|^2\\)</p> <p>SDE solver: \u274c Not used</p> <p>Speed: Fast (single forward pass per sample)</p>"},{"location":"SDE/02_sde_training/#sampling-see-03_sde_samplingmd","title":"Sampling (See 03_sde_sampling.md)","text":"<p>What: Generate samples from learned model</p> <p>How: 1. Start with \\(x_T \\sim \\mathcal{N}(0, I)\\) 2. Iteratively denoise using reverse SDE/ODE 3. End with \\(x_0 \\approx\\) data sample</p> <p>SDE solver: \u2705 Used (Euler, RK4, adaptive)</p> <p>Speed: Slow (many steps required)</p>"},{"location":"SDE/02_sde_training/#key-takeaways","title":"Key Takeaways","text":""},{"location":"SDE/02_sde_training/#conceptual","title":"Conceptual","text":"<ol> <li>Training uses closed-form marginals \u2014 no SDE solver needed</li> <li>The forward SDE solution is exact \u2014 we can sample \\(x_t\\) directly from \\(x_0\\)</li> <li>Score matching = denoising \u2014 predicting noise is equivalent to learning scores</li> <li>Training is the same as DDPM \u2014 just continuous time instead of discrete</li> </ol>"},{"location":"SDE/02_sde_training/#practical","title":"Practical","text":"<ol> <li>Implementation is simple: Sample \\(t\\), add noise, predict, compute loss</li> <li>No numerical integration during training</li> <li>Fast and stable \u2014 closed-form solution avoids discretization errors</li> <li>SDE solvers only used for sampling (generation)</li> </ol>"},{"location":"SDE/02_sde_training/#mathematical","title":"Mathematical","text":"<ol> <li>Forward SDE: \\(dx = -\\frac{1}{2}\\beta(t) x\\,dt + \\sqrt{\\beta(t)}\\,dw\\)</li> <li>Closed-form: \\(x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1-\\bar{\\alpha}_t} \\epsilon\\)</li> <li>Training loss: \\(\\mathbb{E}[\\|\\epsilon_\\theta(x_t, t) - \\epsilon\\|^2]\\)</li> <li>No solver needed: Direct sampling from \\(q(x_t | x_0)\\)</li> </ol>"},{"location":"SDE/02_sde_training/#related-documents","title":"Related Documents","text":""},{"location":"SDE/02_sde_training/#sde-documentation","title":"SDE Documentation","text":"<ul> <li>00_sde_overview.md \u2014 High-level SDE introduction</li> <li>01_diffusion_sde_view.md \u2014 Detailed SDE formulation</li> <li>03_solving_vpsde.md \u2014 Derivation of closed-form solution</li> <li>03_sde_sampling.md \u2014 How to sample (where SDE solvers are used)</li> </ul>"},{"location":"SDE/02_sde_training/#related-topics","title":"Related Topics","text":"<ul> <li>DDPM Training \u2014 Discrete version</li> <li>Flow Matching Training \u2014 Alternative approach</li> </ul>"},{"location":"SDE/02_sde_training/#summary","title":"Summary","text":"<p>Training diffusion models from the SDE perspective is simple:</p> <ol> <li>Use closed-form marginals from the forward SDE solution</li> <li>No SDE solver required \u2014 direct sampling of \\(x_t\\) from \\(x_0\\)</li> <li>Same as DDPM training \u2014 just continuous time</li> <li>Fast and exact \u2014 no discretization errors</li> </ol> <p>The confusion arises because:</p> <ul> <li>The SDE formulation seems complex</li> <li>But training only uses the closed-form solution</li> <li>SDE solvers are only needed for sampling (generation)</li> </ul> <p>Bottom line: Training is straightforward regression on noise prediction, using the exact solution of the forward SDE.</p>"},{"location":"SDE/02a_taylor_expansion/","title":"Taylor Expansions in Diffusion Models","text":"<p>Taylor expansions are the quiet workhorse behind diffusion models, SDE discretization, and the Fokker\u2013Planck equation. They're rarely foregrounded, but almost every \"natural-looking\" formula in diffusion comes from a first- or second-order expansion where higher-order terms are deliberately thrown away.</p> <p>This document explains why Taylor expansions appear everywhere in diffusion models and how they enable the bridge between continuous theory and discrete algorithms.</p>"},{"location":"SDE/02a_taylor_expansion/#1-the-continuous-discrete-boundary","title":"1. The Continuous-Discrete Boundary","text":"<p>Diffusion models live at an awkward boundary:</p> <ul> <li>The true theory is continuous in time (SDEs, PDEs)</li> <li>The algorithms are discrete (finite steps, finite networks)</li> </ul> <p>Taylor expansions bridge these worlds.</p>"},{"location":"SDE/02a_taylor_expansion/#signs-youre-seeing-taylor-expansion","title":"Signs You're Seeing Taylor Expansion","text":"<p>Any time you encounter:</p> <ul> <li>\"small time step\"</li> <li>\"as \\(dt \\to 0\\)\"</li> <li>\"ignore higher-order terms\"</li> <li>\"first-order accurate\"</li> </ul> <p>you are seeing Taylor expansion at work.</p>"},{"location":"SDE/02a_taylor_expansion/#2-taylor-expansion-for-dynamics","title":"2. Taylor Expansion for Dynamics","text":"<p>For a smooth function \\(f(t)\\), the Taylor expansion is:</p> \\[ f(t + \\Delta t) = f(t) + f'(t)\\,\\Delta t + \\frac{1}{2}f''(t)\\,\\Delta t^2 + \\cdots \\]"},{"location":"SDE/02a_taylor_expansion/#the-core-philosophy","title":"The Core Philosophy","text":"<p>Over a small enough time step, the future is approximately linear in the present.</p> <p>Diffusion models lean heavily on this idea, but applied to:</p> <ul> <li>Random processes (SDEs)</li> <li>Probability densities (Fokker\u2013Planck equation)</li> </ul>"},{"location":"SDE/02a_taylor_expansion/#3-warm-up-deterministic-dynamics","title":"3. Warm-Up: Deterministic Dynamics","text":"<p>Consider an ordinary differential equation (ODE):</p> \\[ \\frac{dx(t)}{dt} = a(x(t), t) \\] <p>By Taylor expansion:</p> \\[ x(t + \\Delta t) = x(t) + a(x(t), t)\\,\\Delta t + O(\\Delta t^2) \\] <p>This is Euler's method for numerical integration.</p> <p>Nothing fancy yet\u2014just Taylor applied to deterministic dynamics.</p>"},{"location":"SDE/02a_taylor_expansion/#4-enter-randomness-why-sdes-change-the-rules","title":"4. Enter Randomness: Why SDEs Change the Rules","text":"<p>Now consider a stochastic differential equation (SDE):</p> \\[ dx(t) = f(x, t)\\,dt + g(t)\\,dw(t) \\]"},{"location":"SDE/02a_taylor_expansion/#the-key-twist","title":"The Key Twist","text":"<ul> <li>\\(dt\\) is \"small\"</li> <li>But \\(dw(t)\\) is not order \\(dt\\)</li> <li>It is order \\(\\sqrt{dt}\\)</li> </ul>"},{"location":"SDE/02a_taylor_expansion/#two-scales","title":"Two Scales","text":"<p>We have two different scales:</p> Term Type Scaling Deterministic \\(\\sim dt\\) Stochastic \\(\\sim \\sqrt{dt}\\) <p>This single fact reshapes Taylor expansion for stochastic processes.</p>"},{"location":"SDE/02a_taylor_expansion/#5-taylor-logic-behind-eulermaruyama","title":"5. Taylor Logic Behind Euler\u2013Maruyama","text":"<p>When discretizing the SDE, we write:</p> \\[ x(t + \\Delta t) - x(t) = f(x, t)\\,\\Delta t + g(t)\\,\\Delta w \\] <p>with:</p> \\[ \\Delta w \\sim \\mathcal{N}(0, \\Delta t) \\]"},{"location":"SDE/02a_taylor_expansion/#hierarchy-of-terms","title":"Hierarchy of Terms","text":"<p>Observe the order of magnitude:</p> Term Order \\(f\\,\\Delta t\\) \\(\\Delta t\\) \\(g\\,\\Delta w\\) \\(\\sqrt{\\Delta t}\\) \\((\\Delta w)^2\\) \\(\\Delta t\\) \\((\\Delta w)^3\\) \\(\\Delta t^{3/2}\\)"},{"location":"SDE/02a_taylor_expansion/#truncation-rule","title":"Truncation Rule","text":"<ul> <li>Keep terms up to order \\(\\Delta t\\)</li> <li>Drop terms like \\(\\Delta t^{3/2}\\)</li> </ul> <p>This truncation is a Taylor expansion, adapted to stochastic scaling.</p>"},{"location":"SDE/02a_taylor_expansion/#6-example-why-sqrt1-beta-becomes-1-frac12beta","title":"6. Example: Why \\(\\sqrt{1-\\beta}\\) Becomes \\(1 - \\frac{1}{2}\\beta\\)","text":"<p>In DDPM, the forward step is:</p> \\[ x_{k+1} = \\sqrt{1-\\beta_k}\\,x_k + \\sqrt{\\beta_k}\\,\\varepsilon \\] <p>Why not just \\(1 - \\frac{1}{2}\\beta_k\\)?</p>"},{"location":"SDE/02a_taylor_expansion/#taylor-expansion-of-the-square-root","title":"Taylor Expansion of the Square Root","text":"\\[ \\sqrt{1-\\beta_k} = 1 - \\frac{1}{2}\\beta_k - \\frac{1}{8}\\beta_k^2 + \\cdots \\] <p>To first order in \\(\\beta_k\\):</p> \\[ \\sqrt{1-\\beta_k} \\approx 1 - \\frac{1}{2}\\beta_k \\] <p>Higher-order terms are deliberately ignored.</p>"},{"location":"SDE/02a_taylor_expansion/#connection-to-vp-sde","title":"Connection to VP-SDE","text":"<p>This is exactly the same approximation that appears when discretizing:</p> \\[ dx = -\\frac{1}{2}\\beta(t) x\\,dt + \\sqrt{\\beta(t)}\\,dw \\]"},{"location":"SDE/02a_taylor_expansion/#why-keep-the-square-root","title":"Why Keep the Square Root?","text":"<p>DDPM keeps \\(\\sqrt{1-\\beta_k}\\) because:</p> <ul> <li>It matches the first-order Taylor expansion</li> <li>It behaves better at finite step sizes</li> <li>It exactly preserves variance</li> </ul> <p>This is a numerical stabilization choice, not an accident.</p>"},{"location":"SDE/02a_taylor_expansion/#7-taylor-expansion-behind-the-fokkerplanck-equation","title":"7. Taylor Expansion Behind the Fokker\u2013Planck Equation","text":"<p>Now we reach the deeper place where Taylor expansions do heavy conceptual lifting.</p>"},{"location":"SDE/02a_taylor_expansion/#setup","title":"Setup","text":"<p>Let \\(p(x, t)\\) be the probability density of \\(x(t)\\). We want an equation for how \\(p\\) evolves over time.</p> <p>Start from:</p> \\[ p(x, t + \\Delta t) = \\mathbb{E}\\left[p(x - \\Delta x, t)\\right] \\]"},{"location":"SDE/02a_taylor_expansion/#taylor-expand-the-density","title":"Taylor Expand the Density","text":"<p>Expand the density itself in space:</p> \\[ p(x - \\Delta x, t) \\approx p(x, t) - \\Delta x \\cdot \\nabla p(x, t) + \\frac{1}{2}(\\Delta x \\Delta x^\\top) : \\nabla^2 p(x, t) \\]"},{"location":"SDE/02a_taylor_expansion/#substitute-sde-increment","title":"Substitute SDE Increment","text":"<p>From the SDE:</p> \\[ \\Delta x = f\\,\\Delta t + g\\,\\Delta w \\]"},{"location":"SDE/02a_taylor_expansion/#take-expectations","title":"Take Expectations","text":"<p>Key facts:</p> <ul> <li>\\(\\mathbb{E}[\\Delta w] = 0\\)</li> <li>\\(\\mathbb{E}[\\Delta w \\Delta w^\\top] = \\Delta t\\,I\\)</li> </ul> <p>All higher-order terms vanish or are \\(o(\\Delta t)\\).</p>"},{"location":"SDE/02a_taylor_expansion/#the-result-fokkerplanck-equation","title":"The Result: Fokker\u2013Planck Equation","text":"\\[ \\frac{\\partial p}{\\partial t} = -\\nabla \\cdot (f p) + \\frac{1}{2}\\nabla^2 (g^2 p) \\] <p>Key insight: This entire PDE is literally a second-order Taylor expansion of the density under random motion.</p>"},{"location":"SDE/02a_taylor_expansion/#8-why-stop-at-second-order","title":"8. Why Stop at Second Order?","text":"<p>You might ask: why stop at second order in the Fokker\u2013Planck derivation?</p>"},{"location":"SDE/02a_taylor_expansion/#the-scaling-argument","title":"The Scaling Argument","text":"Order Physical Meaning Scaling First Drift (deterministic flow) \\(\\Delta t\\) Second Diffusion (spreading) \\(\\Delta t\\) Third+ Higher moments \\(\\Delta t^{3/2}\\) or higher <p>Key fact: \\((\\Delta w)^n \\sim (\\Delta t)^{n/2}\\)</p> <p>For \\(n \\geq 3\\), these terms vanish in the \\(\\Delta t \\to 0\\) limit.</p>"},{"location":"SDE/02a_taylor_expansion/#this-is-not-hand-waving","title":"This Is Not Hand-Waving","text":"<p>It's a rigorous scaling argument. The Fokker\u2013Planck equation is the exact continuous-time limit.</p>"},{"location":"SDE/02a_taylor_expansion/#9-the-hidden-pattern-across-diffusion-models","title":"9. The Hidden Pattern Across Diffusion Models","text":"<p>You can now recognize a repeating structure:</p> Component What Taylor Expansion Does DDPM forward step Linearize SDE over small time Noise schedule Match first-order decay Reverse SDE Drop higher-order stochastic terms Score matching Linearize log-density gradients Fokker\u2013Planck Second-order expansion of density Probability-flow ODE Remove stochastic second-order term <p>Philosophy: Diffusion models are Taylor expansions with taste\u2014you keep just enough terms to stay correct, stable, and learnable.</p>"},{"location":"SDE/02a_taylor_expansion/#10-the-big-picture","title":"10. The Big Picture","text":"<p>Diffusion models work because, over infinitesimal time, random dynamics are simple\u2014and Taylor expansions let us exploit that simplicity repeatedly.</p>"},{"location":"SDE/02a_taylor_expansion/#everything-magical-comes-from-taylor","title":"Everything \"Magical\" Comes from Taylor","text":"<p>Everything that feels magical in diffusion models:</p> <ul> <li>Gaussian noise</li> <li>Linear drift</li> <li>Quadratic variance</li> <li>Score as gradient</li> <li>Clean discretizations</li> </ul> <p>comes from discarding higher-order terms in a controlled way.</p>"},{"location":"SDE/02a_taylor_expansion/#summary","title":"Summary","text":"<p>Taylor expansions are the mathematical glue that turns continuous stochastic dynamics into tractable learning rules in diffusion models, governing:</p> <ol> <li>SDE discretization (Euler\u2013Maruyama)</li> <li>Fokker\u2013Planck equation (density evolution)</li> <li>Variance-preserving structure (DDPM forward chain)</li> <li>Score matching (gradient approximations)</li> </ol>"},{"location":"SDE/02a_taylor_expansion/#next-steps","title":"Next Steps","text":"<p>To deepen understanding:</p> <ol> <li>Derive Fokker\u2013Planck line by line: Spell out every expectation, no black boxes</li> <li>Verify variance preservation: Show \\(\\sqrt{1-\\beta_k}\\) exactly preserves variance</li> <li>Study It\u00f4 calculus: Understand \\((dw)^2 = dt\\) rigorously</li> </ol>"},{"location":"SDE/02a_taylor_expansion/#related-documents","title":"Related Documents","text":"<ul> <li>Deriving DDPM from VP-SDE</li> <li>SDE View Overview</li> <li>Historical Development</li> </ul>"},{"location":"SDE/02b_fokker_plank_eq/","title":"Deriving the Fokker\u2013Planck Equation","text":"<p>=====================================</p> <p>This document derives the Fokker\u2013Planck equation (also known as the Kolmogorov forward equation) line by line from an SDE using Taylor expansion. This is one of the cleanest places to see Taylor expansion doing real mathematical work in diffusion theory.</p> <p>We'll derive it for the general It\u00f4 SDE with isotropic noise, then note the vector/matrix generalization.</p>"},{"location":"SDE/02b_fokker_plank_eq/#overview","title":"Overview","text":"<p>We'll derive the PDE that governs how probability densities evolve under stochastic dynamics. This is the continuous-time analog of the Chapman-Kolmogorov equation for Markov chains.</p>"},{"location":"SDE/02b_fokker_plank_eq/#notation-and-goal","title":"Notation and Goal","text":""},{"location":"SDE/02b_fokker_plank_eq/#variables","title":"Variables","text":"Symbol Meaning \\(x(t) \\in \\mathbb{R}^d\\) Random state at time \\(t\\) \\(p(x, t)\\) Probability density of \\(x(t)\\) \\(f(x, t) \\in \\mathbb{R}^d\\) Drift function \\(g(t) \\in \\mathbb{R}\\) Diffusion strength (scalar for simplicity) \\(w(t) \\in \\mathbb{R}^d\\) Brownian motion (independent components) \\(\\Delta w := w(t+\\Delta t) - w(t)\\) Brownian increment, \\(\\sim \\mathcal{N}(0, \\Delta t I)\\)"},{"location":"SDE/02b_fokker_plank_eq/#goal","title":"Goal","text":"<p>Show that \\(p(x, t)\\) satisfies:</p> \\[ \\boxed{\\frac{\\partial p}{\\partial t}(x, t) = -\\nabla \\cdot \\left(f(x, t) p(x, t)\\right) + \\frac{1}{2}g(t)^2 \\Delta p(x, t)} \\] <p>where:</p> <ul> <li>\\(\\nabla \\cdot\\) is the divergence operator</li> <li>\\(\\Delta = \\nabla^2 = \\sum_{i=1}^d \\frac{\\partial^2}{\\partial x_i^2}\\) is the Laplacian</li> </ul>"},{"location":"SDE/02b_fokker_plank_eq/#step-1-use-a-test-function","title":"Step 1: Use a Test Function","text":"<p>Instead of manipulating densities directly (which gets messy), we use a smooth test function \\(\\varphi: \\mathbb{R}^d \\to \\mathbb{R}\\) (think of it as a smooth probe).</p>"},{"location":"SDE/02b_fokker_plank_eq/#what-is-a-test-function","title":"What is a Test Function?","text":"<p>A test function is a smooth function used to \"probe\" or \"measure\" properties of distributions and generalized functions. Think of it as a mathematical instrument that lets us study objects that are too rough to handle directly.</p> <p>Formal definition: \\(\\varphi \\in C_c^\\infty(\\mathbb{R}^d)\\) means: - \\(\\varphi\\) is infinitely differentiable (smooth) - \\(\\varphi\\) has compact support (zero outside some bounded region)</p> <p>Practical requirements (we can relax compact support): - \\(\\varphi\\) is at least twice continuously differentiable - \\(\\varphi\\) and its derivatives decay rapidly at infinity - Think: \\(\\varphi(x) = e^{-\\|x\\|^2}\\) or any smooth bump function</p>"},{"location":"SDE/02b_fokker_plank_eq/#why-use-test-functions","title":"Why Use Test Functions?","text":"<p>1. Avoid Direct Density Manipulation</p> <p>Probability densities \\(p(x, t)\\) can be: - Discontinuous (e.g., uniform distributions) - Non-differentiable (e.g., at boundaries) - Singular (e.g., delta functions)</p> <p>Working with them directly in differential equations is mathematically treacherous.</p> <p>2. Weak Formulation</p> <p>Test functions allow us to work with weak solutions\u2014solutions that satisfy the PDE \"on average\" rather than pointwise. This is the standard approach in modern PDE theory.</p> <p>3. Integration by Parts</p> <p>Test functions have nice decay properties, so boundary terms vanish when integrating by parts. This lets us transfer derivatives freely.</p> <p>4. Distribution Theory Connection</p> <p>This is how Laurent Schwartz formalized distributions (generalized functions). A distribution is defined by how it acts on test functions.</p>"},{"location":"SDE/02b_fokker_plank_eq/#the-test-function-strategy","title":"The Test Function Strategy","text":"<p>Expected Value:</p> \\[ \\mathbb{E}[\\varphi(x(t))] = \\int_{\\mathbb{R}^d} \\varphi(x) p(x, t)\\,dx \\] <p>Strategy:  1. Compute how \\(\\mathbb{E}[\\varphi(x(t))]\\) changes with time 2. Express this in terms of \\(p(x, t)\\) 3. Use integration by parts to transfer derivatives from \\(\\varphi\\) to \\(p\\) 4. Conclude that \\(p\\) satisfies the Fokker-Planck equation</p> <p>Key insight: If we know how \\(\\mathbb{E}[\\varphi(x(t))]\\) evolves for all test functions \\(\\varphi\\), we know everything about \\(p(x, t)\\).</p>"},{"location":"SDE/02b_fokker_plank_eq/#step-2-sde-increment","title":"Step 2: SDE Increment","text":"<p>For the SDE:</p> \\[ dx(t) = f(x(t), t)\\,dt + g(t)\\,dw(t) \\] <p>Over a small interval \\([t, t+\\Delta t]\\), the increment is:</p> \\[ \\Delta x := x(t+\\Delta t) - x(t) = f(x(t), t)\\,\\Delta t + g(t)\\,\\Delta w \\]"},{"location":"SDE/02b_fokker_plank_eq/#key-scaling-facts","title":"Key Scaling Facts","text":"<ul> <li>\\(\\Delta t\\) is small</li> <li>\\(\\Delta w \\sim \\mathcal{N}(0, \\Delta t I)\\), so typical size \\(|\\Delta w| \\sim \\sqrt{\\Delta t}\\)</li> </ul>"},{"location":"SDE/02b_fokker_plank_eq/#step-3-taylor-expand-the-test-function","title":"Step 3: Taylor Expand the Test Function","text":"<p>We write:</p> \\[ \\varphi(x(t+\\Delta t)) = \\varphi(x(t) + \\Delta x) \\] <p>Now perform a second-order Taylor expansion around \\(x(t)\\):</p> \\[ \\varphi(x + \\Delta x) = \\varphi(x) + \\nabla\\varphi(x)^\\top \\Delta x + \\frac{1}{2}\\Delta x^\\top H_\\varphi(x) \\Delta x + \\text{higher-order terms} \\] <p>where:</p> <ul> <li>\\(\\nabla\\varphi\\) is the gradient (a \\(d\\)-vector)</li> <li>\\(H_\\varphi\\) is the Hessian matrix of second derivatives</li> </ul>"},{"location":"SDE/02b_fokker_plank_eq/#why-second-order","title":"Why Second Order?","text":"<p>Because \\(\\Delta x\\) has a \\(\\sqrt{\\Delta t}\\) piece (from the noise), squaring it produces order-\\(\\Delta t\\) terms that survive in the limit.</p>"},{"location":"SDE/02b_fokker_plank_eq/#step-4-take-conditional-expectation","title":"Step 4: Take Conditional Expectation","text":"<p>Condition on the current state \\(x(t) = x\\):</p> \\[ \\mathbb{E}[\\varphi(x(t+\\Delta t)) \\mid x(t)=x] \\approx \\varphi(x) + \\nabla\\varphi(x)^\\top \\mathbb{E}[\\Delta x \\mid x] + \\frac{1}{2}\\mathbb{E}[\\Delta x^\\top H_\\varphi(x) \\Delta x \\mid x] \\] <p>Now we compute the needed moments.</p>"},{"location":"SDE/02b_fokker_plank_eq/#first-moment-of-delta-x","title":"First Moment of \\(\\Delta x\\)","text":"\\[ \\mathbb{E}[\\Delta x \\mid x] = f(x, t)\\,\\Delta t + g(t)\\,\\mathbb{E}[\\Delta w] = f(x, t)\\,\\Delta t \\] <p>since \\(\\mathbb{E}[\\Delta w] = 0\\).</p> <p>First-order contribution:</p> \\[ \\nabla\\varphi(x)^\\top f(x, t)\\,\\Delta t \\]"},{"location":"SDE/02b_fokker_plank_eq/#second-moment-term","title":"Second Moment Term","text":"<p>Substitute \\(\\Delta x = f\\,\\Delta t + g\\,\\Delta w\\):</p> \\[ \\Delta x^\\top H \\Delta x = (f\\,\\Delta t + g\\,\\Delta w)^\\top H (f\\,\\Delta t + g\\,\\Delta w) \\] <p>Expanding:</p> \\[ = (f\\,\\Delta t)^\\top H (f\\,\\Delta t) + 2(f\\,\\Delta t)^\\top H (g\\,\\Delta w) + (g\\,\\Delta w)^\\top H (g\\,\\Delta w) \\] <p>Take conditional expectation:</p> Term Order Survives? \\((f\\,\\Delta t)^\\top H (f\\,\\Delta t)\\) \\(O(\\Delta t^2)\\) No \\(2(f\\,\\Delta t)^\\top H (g\\,\\Delta w)\\) \\(\\mathbb{E}[\\Delta w] = 0\\) No \\((g\\,\\Delta w)^\\top H (g\\,\\Delta w)\\) \\(O(\\Delta t)\\) Yes <p>For the surviving term:</p> \\[ \\mathbb{E}[(g\\,\\Delta w)^\\top H (g\\,\\Delta w) \\mid x] = g(t)^2\\,\\mathbb{E}[\\Delta w^\\top H \\Delta w] \\] <p>Key identity for Gaussian increments:</p> \\[ \\mathbb{E}[\\Delta w \\Delta w^\\top] = \\Delta t\\,I \\] <p>For any symmetric matrix \\(H\\):</p> \\[ \\mathbb{E}[\\Delta w^\\top H \\Delta w] = \\text{tr}(H\\,\\mathbb{E}[\\Delta w \\Delta w^\\top]) = \\text{tr}(H\\,\\Delta t I) = \\Delta t\\,\\text{tr}(H) \\] <p>Therefore:</p> \\[ \\mathbb{E}[\\Delta x^\\top H \\Delta x \\mid x] = g(t)^2\\,\\Delta t\\,\\text{tr}(H_\\varphi(x)) \\] <p>But \\(\\text{tr}(H_\\varphi) = \\sum_i \\frac{\\partial^2 \\varphi}{\\partial x_i^2} = \\Delta \\varphi\\) (the Laplacian).</p> <p>Second-order contribution:</p> \\[ \\frac{1}{2}g(t)^2\\,\\Delta t\\,\\Delta\\varphi(x) \\]"},{"location":"SDE/02b_fokker_plank_eq/#step-5-the-infinitesimal-generator","title":"Step 5: The Infinitesimal Generator","text":"<p>Combining both contributions:</p> \\[ \\mathbb{E}[\\varphi(x(t+\\Delta t)) \\mid x(t)=x] = \\varphi(x) + \\Delta t\\left(f(x, t) \\cdot \\nabla\\varphi(x) + \\frac{1}{2}g(t)^2 \\Delta\\varphi(x)\\right) + o(\\Delta t) \\] <p>Subtract \\(\\varphi(x)\\), divide by \\(\\Delta t\\), and take \\(\\Delta t \\to 0\\):</p> \\[ \\frac{d}{dt}\\mathbb{E}[\\varphi(x(t)) \\mid x(t)=x] = f(x, t) \\cdot \\nabla\\varphi(x) + \\frac{1}{2}g(t)^2 \\Delta\\varphi(x) \\]"},{"location":"SDE/02b_fokker_plank_eq/#define-the-generator","title":"Define the Generator","text":"<p>The infinitesimal generator \\(\\mathcal{L}\\) is:</p> \\[ (\\mathcal{L}\\varphi)(x, t) := f(x, t) \\cdot \\nabla\\varphi(x) + \\frac{1}{2}g(t)^2 \\Delta\\varphi(x) \\] <p>Then:</p> \\[ \\frac{d}{dt}\\mathbb{E}[\\varphi(x(t))] = \\mathbb{E}[(\\mathcal{L}\\varphi)(x(t), t)] \\]"},{"location":"SDE/02b_fokker_plank_eq/#understanding-the-infinitesimal-generator","title":"Understanding the Infinitesimal Generator","text":"<p>What is it?</p> <p>The generator \\(\\mathcal{L}\\) is an operator that acts on functions, telling us how their expected values change when evaluated along the stochastic process.</p> <p>Interpretation:</p> \\[ \\mathbb{E}[\\varphi(x(t+dt))] \\approx \\mathbb{E}[\\varphi(x(t))] + dt\\,\\mathbb{E}[(\\mathcal{L}\\varphi)(x(t))] \\] <p>So \\(\\mathcal{L}\\varphi\\) is the \"instantaneous rate of change\" of \\(\\mathbb{E}[\\varphi(x(t))]\\).</p> <p>Components:</p> <ol> <li>Drift term \\(f \\cdot \\nabla\\varphi\\): How \\(\\varphi\\) changes along the flow</li> <li>If \\(f\\) points toward regions where \\(\\varphi\\) is larger, this is positive</li> <li> <p>Pure directional derivative along the drift</p> </li> <li> <p>Diffusion term \\(\\frac{1}{2}g^2 \\Delta\\varphi\\): How \\(\\varphi\\) changes due to spreading</p> </li> <li>Laplacian measures local curvature</li> <li>Positive \\(\\Delta\\varphi\\) means \\(\\varphi\\) is locally concave (bowl-shaped)</li> <li>Diffusion tends to move probability toward regions of positive curvature</li> </ol> <p>Example: Consider \\(\\varphi(x) = x^2\\) (measuring second moment) with drift \\(f = -\\beta x\\) (OU process) and constant diffusion \\(g\\):</p> \\[ \\mathcal{L}(x^2) = -\\beta x \\cdot 2x + \\frac{1}{2}g^2 \\cdot 2 = -2\\beta x^2 + g^2 \\] <p>So:</p> \\[ \\frac{d}{dt}\\mathbb{E}[x(t)^2] = -2\\beta\\mathbb{E}[x^2] + g^2 \\] <p>This shows variance decays toward equilibrium \\(\\mathbb{E}[x^2] = g^2/(2\\beta)\\).</p>"},{"location":"SDE/02b_fokker_plank_eq/#step-6-transfer-to-the-density","title":"Step 6: Transfer to the Density","text":"<p>Recall:</p> \\[ \\mathbb{E}[\\varphi(x(t))] = \\int \\varphi(x) p(x, t)\\,dx \\] <p>Differentiate in time:</p> \\[ \\frac{d}{dt}\\mathbb{E}[\\varphi(x(t))] = \\int \\varphi(x)\\,\\frac{\\partial p}{\\partial t}(x, t)\\,dx \\] <p>But we also have:</p> \\[ \\frac{d}{dt}\\mathbb{E}[\\varphi(x(t))] = \\int (\\mathcal{L}\\varphi)(x, t)\\,p(x, t)\\,dx \\] <p>Therefore, for all smooth \\(\\varphi\\):</p> \\[ \\int \\varphi(x)\\,\\partial_t p(x, t)\\,dx = \\int \\left(f \\cdot \\nabla\\varphi + \\frac{1}{2}g^2 \\Delta\\varphi\\right) p\\,dx \\]"},{"location":"SDE/02b_fokker_plank_eq/#integration-by-parts","title":"Integration by Parts","text":"<p>Now we move derivatives from \\(\\varphi\\) to \\(p\\). This is the crucial step where test functions earn their keep.</p>"},{"location":"SDE/02b_fokker_plank_eq/#drift-term","title":"Drift Term","text":"<p>Start with:</p> \\[ \\int (f \\cdot \\nabla\\varphi)\\,p\\,dx = \\int \\sum_i f_i(x, t)\\,\\frac{\\partial \\varphi}{\\partial x_i}\\,p(x, t)\\,dx \\] <p>Integrate by parts (product rule for derivatives):</p> \\[ \\int f_i\\,\\frac{\\partial \\varphi}{\\partial x_i}\\,p\\,dx = \\underbrace{\\left[f_i\\,\\varphi\\,p\\right]_{-\\infty}^{+\\infty}}_{\\text{boundary term} = 0} - \\int \\varphi\\,\\frac{\\partial}{\\partial x_i}(f_i\\,p)\\,dx \\] <p>Why boundary term vanishes:</p> <ul> <li>Test function \\(\\varphi\\) has compact support (or decays rapidly)</li> <li>Probability density \\(p\\) decays at infinity</li> <li>Their product \\(\\to 0\\) as \\(|x| \\to \\infty\\)</li> </ul> <p>Summing over all components:</p> \\[ \\int (f \\cdot \\nabla\\varphi)\\,p\\,dx = -\\int \\varphi\\,\\nabla \\cdot (fp)\\,dx \\] <p>where \\(\\nabla \\cdot (fp) = \\sum_i \\frac{\\partial}{\\partial x_i}(f_i p)\\) is the divergence.</p>"},{"location":"SDE/02b_fokker_plank_eq/#diffusion-term","title":"Diffusion Term","text":"<p>The Laplacian \\(\\Delta\\varphi = \\sum_i \\frac{\\partial^2 \\varphi}{\\partial x_i^2}\\).</p> <p>Integrate by parts twice (once for each derivative):</p> <p>First integration:</p> \\[ \\int \\frac{\\partial^2 \\varphi}{\\partial x_i^2}\\,p\\,dx = \\underbrace{\\left[\\frac{\\partial \\varphi}{\\partial x_i}\\,p\\right]_{-\\infty}^{+\\infty}}_{=0} - \\int \\frac{\\partial \\varphi}{\\partial x_i}\\,\\frac{\\partial p}{\\partial x_i}\\,dx \\] <p>Second integration:</p> \\[ -\\int \\frac{\\partial \\varphi}{\\partial x_i}\\,\\frac{\\partial p}{\\partial x_i}\\,dx = -\\underbrace{\\left[\\varphi\\,\\frac{\\partial p}{\\partial x_i}\\right]_{-\\infty}^{+\\infty}}_{=0} + \\int \\varphi\\,\\frac{\\partial^2 p}{\\partial x_i^2}\\,dx \\] <p>Summing over all components:</p> \\[ \\int (\\Delta\\varphi)\\,p\\,dx = \\int \\varphi\\,(\\Delta p)\\,dx \\] <p>Key point: We've transferred two derivatives from \\(\\varphi\\) to \\(p\\).</p>"},{"location":"SDE/02b_fokker_plank_eq/#the-result","title":"The Result","text":"<p>Combining both terms:</p> \\[ \\int \\varphi\\,\\partial_t p\\,dx = \\int \\varphi\\left(-\\nabla \\cdot (fp) + \\frac{1}{2}g^2 \\Delta p\\right)dx \\] <p>Rearranging:</p> \\[ \\int \\varphi(x)\\left[\\partial_t p(x, t) + \\nabla \\cdot (fp) - \\frac{1}{2}g^2 \\Delta p\\right]dx = 0 \\]"},{"location":"SDE/02b_fokker_plank_eq/#the-fundamental-lemma-du-bois-reymond","title":"The Fundamental Lemma (du Bois-Reymond)","text":"<p>Statement: If \\(\\int \\varphi(x)\\,h(x)\\,dx = 0\\) for all smooth test functions \\(\\varphi\\), then \\(h(x) = 0\\) almost everywhere.</p> <p>Proof sketch: </p> <ul> <li>Suppose \\(h(x_0) &gt; 0\\) at some point \\(x_0\\)</li> <li>By continuity, \\(h &gt; 0\\) in some neighborhood \\(U\\) of \\(x_0\\)</li> <li>Choose \\(\\varphi\\) to be a smooth bump function: \\(\\varphi &gt; 0\\) on \\(U\\), \\(\\varphi = 0\\) outside \\(U\\)</li> <li>Then \\(\\int \\varphi\\,h\\,dx &gt; 0\\), contradiction!</li> </ul> <p>Application: Since our equation holds for all \\(\\varphi\\), we conclude:</p> \\[ \\boxed{\\frac{\\partial p}{\\partial t}(x, t) = -\\nabla \\cdot \\left(f(x, t) p(x, t)\\right) + \\frac{1}{2}g(t)^2 \\Delta p(x, t)} \\] <p>This is the Fokker\u2013Planck equation.</p>"},{"location":"SDE/02b_fokker_plank_eq/#what-does-this-mean","title":"What Does This Mean?","text":"<p>The Fokker-Planck equation says that the probability density evolves according to:</p> <ol> <li>Transport term \\(-\\nabla \\cdot (fp)\\): Probability flows along the drift \\(f\\)</li> <li>Diffusion term \\(\\frac{1}{2}g^2 \\Delta p\\): Probability spreads out due to randomness</li> </ol> <p>Physical interpretation:</p> <ul> <li>If you have a swarm of particles following the SDE</li> <li>The density \\(p(x, t)\\) describes their distribution</li> <li>The drift \\(f\\) creates a flow (like a river current)</li> <li>The diffusion \\(g^2 \\Delta p\\) creates spreading (like heat diffusion)</li> </ul>"},{"location":"SDE/02b_fokker_plank_eq/#generalization-matrix-valued-diffusion","title":"Generalization: Matrix-Valued Diffusion","text":"<p>For a more general SDE:</p> \\[ dx = f(x, t)\\,dt + G(x, t)\\,dw \\] <p>with \\(G \\in \\mathbb{R}^{d \\times d}\\), define the diffusion matrix:</p> \\[ D(x, t) = G(x, t) G(x, t)^\\top \\] <p>Then the Fokker\u2013Planck equation becomes:</p> \\[ \\frac{\\partial p}{\\partial t} = -\\nabla \\cdot (fp) + \\frac{1}{2}\\sum_{i,j} \\frac{\\partial^2}{\\partial x_i \\partial x_j}\\left(D_{ij}(x, t)\\,p\\right) \\] <p>In diffusion models: We often choose isotropic noise so \\(D \\propto I\\), which collapses back to the Laplacian form.</p>"},{"location":"SDE/02b_fokker_plank_eq/#weak-vs-strong-solutions-advanced-note","title":"Weak vs. Strong Solutions (Advanced Note)","text":""},{"location":"SDE/02b_fokker_plank_eq/#strong-solutions","title":"Strong Solutions","text":"<p>A strong solution to the Fokker-Planck equation is a function \\(p(x, t)\\) that: - Is differentiable in both \\(x\\) and \\(t\\) - Satisfies the equation pointwise everywhere</p> <p>Problem: Many important probability densities are not this smooth (e.g., boundaries, discontinuities).</p>"},{"location":"SDE/02b_fokker_plank_eq/#weak-solutions","title":"Weak Solutions","text":"<p>A weak solution satisfies:</p> \\[ \\int \\varphi\\,\\partial_t p\\,dx = \\int \\left(-\\varphi\\,\\nabla \\cdot (fp) + \\varphi\\,\\frac{1}{2}g^2 \\Delta p\\right)dx \\] <p>for all test functions \\(\\varphi\\).</p> <p>Advantages:</p> <ul> <li>Exists even when \\(p\\) is not differentiable</li> <li>Includes generalized functions (distributions)</li> <li>More physically relevant (measurements are always averaged)</li> </ul> <p>Our derivation: We worked in the weak formulation from the start, which is why we never needed \\(p\\) to be differentiable!</p>"},{"location":"SDE/02b_fokker_plank_eq/#physical-interpretation","title":"Physical Interpretation","text":"<p>Weak solutions capture the idea that: - We never measure a probability density at a point - We always measure averages over regions: \\(\\int_A p\\,dx\\) - Test functions represent these measurements</p> <p>This is how nature actually works\u2014you can't measure an infinitesimal probability, only finite probabilities over finite regions.</p>"},{"location":"SDE/02b_fokker_plank_eq/#connection-to-diffusion-models","title":"Connection to Diffusion Models","text":"<p>In diffusion models:</p> <ul> <li>The forward SDE defines how \\(p_t(x)\\) evolves toward noise via the Fokker\u2013Planck equation</li> <li>The reverse-time SDE introduces the score \\(\\nabla_x \\log p_t(x)\\)</li> <li>The score is the gradient of the log of the very density governed by Fokker\u2013Planck</li> <li>The score network learns the \"missing information\" needed to run the probability flow backward</li> </ul> <p>This creates a beautiful closed loop:</p> <pre><code>Forward SDE \u2192 Fokker\u2013Planck \u2192 Density p_t(x) \u2192 Score \u2207log p_t \u2192 Reverse SDE\n</code></pre>"},{"location":"SDE/02b_fokker_plank_eq/#why-this-matters-for-score-based-models","title":"Why This Matters for Score-Based Models","text":"<p>The Fokker-Planck equation guarantees that: 1. The forward diffusion process has a well-defined probability flow 2. The density \\(p_t(x)\\) evolves smoothly (in the weak sense) 3. The score \\(\\nabla_x \\log p_t(x)\\) exists (under mild conditions) 4. The reverse process can be constructed using this score</p> <p>Key insight: Score-based diffusion models work because the Fokker-Planck equation ensures the forward process creates a smooth probability path that can be reversed.</p>"},{"location":"SDE/02b_fokker_plank_eq/#examples-and-intuition","title":"Examples and Intuition","text":""},{"location":"SDE/02b_fokker_plank_eq/#example-1-pure-diffusion-brownian-motion","title":"Example 1: Pure Diffusion (Brownian Motion)","text":"<p>For \\(dx = dw\\) (no drift, \\(g=1\\)):</p> \\[ \\frac{\\partial p}{\\partial t} = \\frac{1}{2}\\Delta p \\] <p>This is the heat equation! Probability diffuses like heat, spreading out over time.</p> <p>Solution: If \\(p(x, 0) = \\delta(x)\\) (point mass at origin), then:</p> \\[ p(x, t) = \\frac{1}{\\sqrt{2\\pi t}}e^{-x^2/(2t)} \\] <p>The variance grows linearly: \\(\\text{Var}(x(t)) = t\\).</p>"},{"location":"SDE/02b_fokker_plank_eq/#example-2-ornstein-uhlenbeck-process","title":"Example 2: Ornstein-Uhlenbeck Process","text":"<p>For \\(dx = -\\beta x\\,dt + \\sigma\\,dw\\) (mean-reverting):</p> \\[ \\frac{\\partial p}{\\partial t} = \\beta\\,\\nabla \\cdot (xp) + \\frac{1}{2}\\sigma^2 \\Delta p \\] <p>Expanding the divergence:</p> \\[ \\frac{\\partial p}{\\partial t} = \\beta\\,\\nabla \\cdot (xp) + \\frac{1}{2}\\sigma^2 \\Delta p = \\beta(p + x \\cdot \\nabla p) + \\frac{1}{2}\\sigma^2 \\Delta p \\] <p>Equilibrium: Set \\(\\partial_t p = 0\\), solve to get:</p> \\[ p_{\\infty}(x) = \\mathcal{N}\\left(0, \\frac{\\sigma^2}{2\\beta}\\right) \\] <p>The system equilibrates to a Gaussian!</p>"},{"location":"SDE/02b_fokker_plank_eq/#example-3-forward-diffusion-in-generative-models","title":"Example 3: Forward Diffusion in Generative Models","text":"<p>For the VP-SDE: \\(dx = -\\frac{1}{2}\\beta(t)x\\,dt + \\sqrt{\\beta(t)}\\,dw\\):</p> \\[ \\frac{\\partial p_t}{\\partial t} = \\frac{1}{2}\\beta(t)\\,\\nabla \\cdot (xp_t) + \\frac{1}{2}\\beta(t)\\,\\Delta p_t \\] <p>This drives any initial \\(p_0(x)\\) toward a Gaussian as \\(t \\to T\\).</p>"},{"location":"SDE/02b_fokker_plank_eq/#intuition-conservation-of-probability","title":"Intuition: Conservation of Probability","text":"<p>The Fokker-Planck equation is a conservation law. Rewrite it as:</p> \\[ \\frac{\\partial p}{\\partial t} + \\nabla \\cdot J = 0 \\] <p>where the probability current is:</p> \\[ J = fp - \\frac{1}{2}g^2 \\nabla p \\] <p>Interpretation:</p> <ul> <li>\\(fp\\): Probability flows along the drift</li> <li>\\(-\\frac{1}{2}g^2 \\nabla p\\): Probability flows down gradients (from high to low density)</li> </ul> <p>Total probability is conserved: \\(\\frac{d}{dt}\\int p\\,dx = 0\\).</p>"},{"location":"SDE/02b_fokker_plank_eq/#summary","title":"Summary","text":"<p>We derived the Fokker\u2013Planck equation through:</p> <ol> <li>Test function approach (cleaner than direct density manipulation)</li> <li>Avoids issues with non-smooth densities</li> <li>Leads to weak solutions naturally</li> <li> <p>Based on fundamental distribution theory</p> </li> <li> <p>Second-order Taylor expansion (captures both drift and diffusion)</p> </li> <li>First order \u2192 drift term</li> <li>Second order \u2192 diffusion term</li> <li> <p>Higher orders vanish in the limit \\(\\Delta t \\to 0\\)</p> </li> <li> <p>Moment matching (first moment \u2192 drift, second moment \u2192 diffusion)</p> </li> <li>\\(\\mathbb{E}[\\Delta x] \\sim O(\\Delta t)\\) \u2192 drift</li> <li>\\(\\mathbb{E}[\\Delta x^2] \\sim O(\\Delta t)\\) \u2192 diffusion</li> <li> <p>Cross terms vanish</p> </li> <li> <p>Integration by parts (transfers derivatives to the density)</p> </li> <li>Boundary terms vanish due to test function properties</li> <li>Fundamental lemma ensures pointwise equality</li> </ol> <p>Key insights:</p> <ul> <li>Stochastic dynamics at infinitesimal scales are governed by just two terms\u2014drift and diffusion\u2014both emerging from Taylor expansion</li> <li>Test functions provide a rigorous framework for working with probability densities</li> <li>The Fokker-Planck equation is a conservation law for probability</li> <li>This equation is the foundation for understanding diffusion models</li> </ul>"},{"location":"SDE/02b_fokker_plank_eq/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"SDE/02b_fokker_plank_eq/#q1-why-do-we-need-second-order-taylor-expansion","title":"Q1: Why do we need second-order Taylor expansion?","text":"<p>A: Because the stochastic term \\(g\\,dw\\) has size \\(O(\\sqrt{dt})\\). When squared, it produces \\(O(dt)\\) terms that survive in the limit. First-order expansion would miss the diffusion term entirely.</p>"},{"location":"SDE/02b_fokker_plank_eq/#q2-what-if-the-test-function-doesnt-have-compact-support","title":"Q2: What if the test function doesn't have compact support?","text":"<p>A: We can relax this requirement. We only need \\(\\varphi\\) and \\(p\\) to decay fast enough at infinity that boundary terms vanish. In practice, \\(p\\) is a probability density (decays at infinity) and we choose \\(\\varphi\\) to decay at least as fast.</p>"},{"location":"SDE/02b_fokker_plank_eq/#q3-how-does-this-relate-to-the-chapman-kolmogorov-equation","title":"Q3: How does this relate to the Chapman-Kolmogorov equation?","text":"<p>A: The Fokker-Planck equation is the continuous-time limit of Chapman-Kolmogorov. In discrete time, Chapman-Kolmogorov describes how transition probabilities compose. Taking the limit gives Fokker-Planck.</p>"},{"location":"SDE/02b_fokker_plank_eq/#q4-what-about-non-markovian-processes","title":"Q4: What about non-Markovian processes?","text":"<p>A: The Fokker-Planck equation assumes the process is Markovian (memoryless). For non-Markovian processes, you need generalized forms like the generalized Langevin equation or path integral methods.</p>"},{"location":"SDE/02b_fokker_plank_eq/#q5-can-i-solve-the-fokker-planck-equation-analytically","title":"Q5: Can I solve the Fokker-Planck equation analytically?","text":"<p>A: Rarely. Analytical solutions exist for: - Constant coefficients (Gaussian processes) - Ornstein-Uhlenbeck process - Some special 1D cases</p> <p>Generally, you need numerical methods or approximations.</p>"},{"location":"SDE/02b_fokker_plank_eq/#q6-why-is-it-called-fokker-planck","title":"Q6: Why is it called \"Fokker-Planck\"?","text":"<p>A: Named after Adriaan Fokker and Max Planck who derived it (independently) in the 1910s for studying Brownian motion. It's also called the Kolmogorov forward equation.</p>"},{"location":"SDE/02b_fokker_plank_eq/#further-reading","title":"Further Reading","text":""},{"location":"SDE/02b_fokker_plank_eq/#classical-references","title":"Classical References","text":"<ul> <li>Risken, H. (1996). The Fokker-Planck Equation. Springer. (The definitive reference)</li> <li>Gardiner, C. W. (2009). Stochastic Methods. Springer. (More accessible)</li> </ul>"},{"location":"SDE/02b_fokker_plank_eq/#modern-perspectives","title":"Modern Perspectives","text":"<ul> <li>Pavliotis, G. A. (2014). Stochastic Processes and Applications. Springer. (Rigorous treatment)</li> <li>\u00d8ksendal, B. (2013). Stochastic Differential Equations. Springer. (Standard textbook)</li> </ul>"},{"location":"SDE/02b_fokker_plank_eq/#for-diffusion-models","title":"For Diffusion Models","text":"<ul> <li>Song et al. (2021). \"Score-Based Generative Modeling through SDEs\"</li> <li>See: SDE View Overview in this repository</li> </ul>"},{"location":"SDE/02b_fokker_plank_eq/#related-documents","title":"Related Documents","text":"<ul> <li>Taylor Expansions in Diffusion - Mathematical background for the derivation</li> <li>Deriving DDPM from VP-SDE - Connecting discrete and continuous views</li> <li>DDPM to VP-SDE (Continuous Limit) - How DDPM becomes an SDE</li> <li>SDE View Overview - Big picture of SDEs in diffusion models</li> </ul>"},{"location":"SDE/02c_ddpm_to_vpsde/","title":"From DDPM to VP-SDE: The Continuous Limit","text":"<p>This \"identity check\" is one of the most satisfying derivations in diffusion theory. We'll start from the DDPM forward discrete step, take the small-step limit, and recover the VP-SDE:</p> \\[ dx(t) = -\\frac{1}{2}\\beta(t) x(t)\\,dt + \\sqrt{\\beta(t)}\\,dw(t) \\] <p>We'll derive this by matching conditional mean and conditional variance of increments\u2014the cleanest way to pass from discrete Markov chains to continuous SDEs.</p>"},{"location":"SDE/02c_ddpm_to_vpsde/#overview","title":"Overview","text":"<p>This derivation shows that DDPM is not an arbitrary discrete algorithm\u2014it's a discretization of a continuous stochastic process. Understanding this connection:</p> <ul> <li>Unifies DDPM with the SDE framework</li> <li>Explains the variance-preserving structure</li> <li>Justifies the \\(\\sqrt{1-\\beta_k}\\) coefficient</li> <li>Enables continuous-time analysis and samplers</li> </ul>"},{"location":"SDE/02c_ddpm_to_vpsde/#notation","title":"Notation","text":""},{"location":"SDE/02c_ddpm_to_vpsde/#time-discretization","title":"Time Discretization","text":"Symbol Meaning \\(k = 0, 1, \\ldots, N\\) Discrete DDPM time index \\(t \\in [0, T]\\) Continuous time \\(t_k = k\\,\\Delta t\\) Time grid (uniform steps) \\(\\Delta t = T/N\\) Step size"},{"location":"SDE/02c_ddpm_to_vpsde/#ddpm-forward-step","title":"DDPM Forward Step","text":"<p>The standard DDPM forward step is:</p> \\[ x_{k+1} = \\sqrt{\\alpha_k}\\,x_k + \\sqrt{1-\\alpha_k}\\,\\varepsilon_k, \\quad \\varepsilon_k \\sim \\mathcal{N}(0, I) \\] <p>with \\(\\alpha_k = 1 - \\beta_k\\). Equivalently:</p> \\[ x_{k+1} = \\sqrt{1-\\beta_k}\\,x_k + \\sqrt{\\beta_k}\\,\\varepsilon_k \\] <p>where:</p> <ul> <li>\\(\\beta_k\\) is the discrete \"noise amount\" at step \\(k\\)</li> <li>In the continuous limit, we'll set \\(\\beta_k \\propto \\Delta t\\)</li> </ul>"},{"location":"SDE/02c_ddpm_to_vpsde/#step-1-continuous-time-scaling","title":"Step 1: Continuous-Time Scaling","text":"<p>To obtain an SDE limit, the per-step noise must shrink as the step size shrinks. The standard scaling is:</p> \\[ \\boxed{\\beta_k = \\beta(t_k)\\,\\Delta t} \\] <p>where \\(\\beta(t)\\) is a smooth nonnegative function called the noise rate per unit time.</p> <p>Intuition: As we take finer time steps (\\(\\Delta t \\to 0\\)), the noise added per step must also shrink proportionally.</p> <p>Substituting into the DDPM step:</p> \\[ x_{k+1} = \\sqrt{1 - \\beta(t_k)\\,\\Delta t}\\,x_k + \\sqrt{\\beta(t_k)\\,\\Delta t}\\,\\varepsilon_k \\]"},{"location":"SDE/02c_ddpm_to_vpsde/#step-2-write-the-increment","title":"Step 2: Write the Increment","text":"<p>Define the increment:</p> \\[ \\Delta x_k := x_{k+1} - x_k \\] <p>Substituting:</p> \\[ \\Delta x_k = \\left(\\sqrt{1 - \\beta(t_k)\\,\\Delta t} - 1\\right) x_k + \\sqrt{\\beta(t_k)\\,\\Delta t}\\,\\varepsilon_k \\] <p>Now we'll Taylor-expand the square root term.</p>"},{"location":"SDE/02c_ddpm_to_vpsde/#step-3-taylor-expand-the-square-root","title":"Step 3: Taylor Expand the Square Root","text":"<p>For small \\(u\\), the Taylor expansion of \\(\\sqrt{1-u}\\) is:</p> \\[ \\sqrt{1-u} = 1 - \\frac{1}{2}u - \\frac{1}{8}u^2 + O(u^3) \\] <p>With \\(u = \\beta(t_k)\\,\\Delta t\\):</p> \\[ \\sqrt{1 - \\beta(t_k)\\,\\Delta t} - 1 = -\\frac{1}{2}\\beta(t_k)\\,\\Delta t + O(\\Delta t^2) \\] <p>Substituting back:</p> \\[ \\Delta x_k = \\left(-\\frac{1}{2}\\beta(t_k)\\,\\Delta t + O(\\Delta t^2)\\right) x_k + \\sqrt{\\beta(t_k)\\,\\Delta t}\\,\\varepsilon_k \\] <p>Observation: This already looks like an SDE increment!</p>"},{"location":"SDE/02c_ddpm_to_vpsde/#step-4-match-moments-the-key-move","title":"Step 4: Match Moments (The Key Move)","text":"<p>For an It\u00f4 SDE:</p> \\[ dx = f(x, t)\\,dt + g(t)\\,dw \\] <p>the increment over \\(\\Delta t\\) satisfies:</p> <p>Conditional mean:</p> \\[ \\mathbb{E}[\\Delta x \\mid x(t)=x] \\approx f(x, t)\\,\\Delta t \\] <p>Conditional covariance (for isotropic noise):</p> \\[ \\text{Cov}[\\Delta x \\mid x(t)=x] \\approx g(t)^2\\,\\Delta t\\,I \\] <p>We'll compute these for DDPM and match them to identify \\(f\\) and \\(g\\).</p>"},{"location":"SDE/02c_ddpm_to_vpsde/#conditional-mean","title":"Conditional Mean","text":"<p>Since \\(\\mathbb{E}[\\varepsilon_k] = 0\\):</p> \\[ \\mathbb{E}[\\Delta x_k \\mid x_k] = \\left(-\\frac{1}{2}\\beta(t_k)\\,\\Delta t + O(\\Delta t^2)\\right) x_k \\] <p>Divide by \\(\\Delta t\\) and take \\(\\Delta t \\to 0\\):</p> \\[ \\lim_{\\Delta t \\to 0} \\frac{1}{\\Delta t}\\mathbb{E}[\\Delta x_k \\mid x_k] = -\\frac{1}{2}\\beta(t)\\,x(t) \\] <p>Therefore, the drift is:</p> \\[ \\boxed{f(x, t) = -\\frac{1}{2}\\beta(t)\\,x} \\]"},{"location":"SDE/02c_ddpm_to_vpsde/#conditional-covariance","title":"Conditional Covariance","text":"<p>The only random part is \\(\\sqrt{\\beta(t_k)\\,\\Delta t}\\,\\varepsilon_k\\). Since \\(\\varepsilon_k \\sim \\mathcal{N}(0, I)\\):</p> \\[ \\text{Cov}[\\Delta x_k \\mid x_k] = \\text{Cov}\\left[\\sqrt{\\beta(t_k)\\,\\Delta t}\\,\\varepsilon_k\\right] = \\beta(t_k)\\,\\Delta t\\,I \\] <p>Comparing with \\(g(t)^2\\,\\Delta t\\,I\\):</p> \\[ g(t)^2 = \\beta(t) \\quad \\Rightarrow \\quad \\boxed{g(t) = \\sqrt{\\beta(t)}} \\]"},{"location":"SDE/02c_ddpm_to_vpsde/#step-5-recognize-brownian-scaling","title":"Step 5: Recognize Brownian Scaling","text":"<p>We can rewrite the noise term:</p> \\[ \\sqrt{\\beta(t_k)\\,\\Delta t}\\,\\varepsilon_k = \\sqrt{\\beta(t_k)} \\cdot \\underbrace{\\left(\\sqrt{\\Delta t}\\,\\varepsilon_k\\right)}_{\\Delta w_k} \\] <p>But \\(\\sqrt{\\Delta t}\\,\\varepsilon_k\\) is exactly a discretized Brownian increment:</p> \\[ \\Delta w_k \\sim \\mathcal{N}(0, \\Delta t\\,I) \\]"},{"location":"SDE/02c_ddpm_to_vpsde/#the-ddpm-increment","title":"The DDPM Increment","text":"<p>Combining everything:</p> \\[ \\Delta x_k = -\\frac{1}{2}\\beta(t_k)\\,x_k\\,\\Delta t + \\sqrt{\\beta(t_k)}\\,\\Delta w_k + O(\\Delta t^2)\\,x_k \\]"},{"location":"SDE/02c_ddpm_to_vpsde/#the-continuous-limit","title":"The Continuous Limit","text":"<p>In the limit \\(\\Delta t \\to 0\\), this converges to the It\u00f4 SDE:</p> \\[ \\boxed{dx(t) = -\\frac{1}{2}\\beta(t)\\,x(t)\\,dt + \\sqrt{\\beta(t)}\\,dw(t)} \\] <p>This is exactly the variance-preserving SDE (VP-SDE).</p>"},{"location":"SDE/02c_ddpm_to_vpsde/#what-we-proved","title":"What We Proved","text":"<p>We can now state precisely:</p> <p>DDPM's forward Markov chain is a discrete-time process whose small-step continuous-time limit is the VP-SDE, with drift \\(-\\frac{1}{2}\\beta(t) x\\) and diffusion \\(\\sqrt{\\beta(t)}\\).</p>"},{"location":"SDE/02c_ddpm_to_vpsde/#key-insights","title":"Key Insights","text":"<ol> <li> <p>The \\(\\sqrt{1-\\beta_k}\\) coefficient is a variance-preserving discretization whose Taylor expansion agrees with the SDE drift to first order</p> </li> <li> <p>DDPM is not arbitrary\u2014it's a principled discretization of a continuous stochastic process</p> </li> <li> <p>The connection is exact\u2014matching moments uniquely determines both drift and diffusion</p> </li> </ol>"},{"location":"SDE/02c_ddpm_to_vpsde/#why-variance-preserving","title":"Why \"Variance-Preserving\"?","text":"<p>The VP-SDE has a special property:</p> <ul> <li>The linear drift \\(-\\frac{1}{2}\\beta(t) x\\) shrinks \\(x\\) toward zero</li> <li>The noise \\(\\sqrt{\\beta(t)}\\,dw\\) injects variance</li> <li>The coefficients are tuned so the overall variance stays controlled</li> </ul> <p>Under typical schedules, the process smoothly approaches a standard Gaussian at \\(t = T\\), without variance explosion.</p>"},{"location":"SDE/02c_ddpm_to_vpsde/#connection-to-closed-form-marginals","title":"Connection to Closed-Form Marginals","text":"<p>The VP-SDE has a closed-form solution:</p> \\[ x(t) = \\exp\\left(-\\frac{1}{2}\\int_0^t \\beta(s)\\,ds\\right) x(0) + \\text{Gaussian noise} \\] <p>In DDPM notation, \\(\\bar{\\alpha}_t\\) corresponds to:</p> \\[ \\bar{\\alpha}_t = \\exp\\left(-\\int_0^t \\beta(s)\\,ds\\right) \\] <p>This is the last piece that makes discrete and continuous notations line up perfectly.</p>"},{"location":"SDE/02c_ddpm_to_vpsde/#summary","title":"Summary","text":"<p>We derived the VP-SDE from DDPM through:</p> <ol> <li>Continuous-time scaling: \\(\\beta_k = \\beta(t_k)\\,\\Delta t\\)</li> <li>Taylor expansion: \\(\\sqrt{1-\\beta_k} \\approx 1 - \\frac{1}{2}\\beta_k\\)</li> <li>Moment matching: Identify drift and diffusion from mean and covariance</li> <li>Brownian scaling: Recognize \\(\\sqrt{\\Delta t}\\,\\varepsilon\\) as Brownian increment</li> </ol> <p>The result: DDPM is the Euler\u2013Maruyama discretization of the VP-SDE, with variance-preserving modifications.</p>"},{"location":"SDE/02c_ddpm_to_vpsde/#related-documents","title":"Related Documents","text":"<ul> <li>Deriving DDPM from VP-SDE (the reverse direction)</li> <li>Taylor Expansions in Diffusion</li> <li>Fokker\u2013Planck Equation</li> <li>SDE View Overview</li> </ul>"},{"location":"SDE/03_sde_sampling/","title":"Sampling from Diffusion Models: The SDE Perspective","text":"<p>This document explains how to generate samples from trained diffusion models using the SDE perspective. Unlike training (which uses closed-form solutions), sampling requires numerical SDE/ODE solvers.</p>"},{"location":"SDE/03_sde_sampling/#overview","title":"Overview","text":""},{"location":"SDE/03_sde_sampling/#the-sampling-problem","title":"The Sampling Problem","text":"<p>Given: A trained score network \\(s_\\theta(x, t) \\approx \\nabla_x \\log p_t(x)\\)</p> <p>Goal: Generate samples from the data distribution \\(p_0(x)\\)</p> <p>Approach: Start with noise \\(x_T \\sim \\mathcal{N}(0, I)\\) and run the reverse process to get \\(x_0\\)</p>"},{"location":"SDE/03_sde_sampling/#two-sampling-strategies","title":"Two Sampling Strategies","text":"<p>1. Reverse SDE (Stochastic)</p> <ul> <li>Uses the reverse-time SDE</li> <li>Injects noise at each step (like DDPM)</li> <li>Multiple samples from same initial noise give different outputs</li> </ul> <p>2. Probability Flow ODE (Deterministic)</p> <ul> <li>Uses a deterministic ODE with same marginals</li> <li>No noise injection (like DDIM)</li> <li>Same initial noise always gives same output</li> </ul> <p>Key difference: SDE is stochastic, ODE is deterministic, but both have the same marginal distributions.</p>"},{"location":"SDE/03_sde_sampling/#the-reverse-time-sde","title":"The Reverse-Time SDE","text":""},{"location":"SDE/03_sde_sampling/#mathematical-form","title":"Mathematical Form","text":"<p>Given the forward SDE:</p> \\[ dx = f(x, t)\\,dt + g(t)\\,dw \\] <p>The reverse-time SDE is:</p> \\[ dx = \\left[f(x, t) - g(t)^2 \\nabla_x \\log p_t(x)\\right]dt + g(t)\\,d\\bar{w} \\] <p>where \\(\\bar{w}\\) is reverse-time Brownian motion.</p>"},{"location":"SDE/03_sde_sampling/#for-vp-sde","title":"For VP-SDE","text":"<p>Forward VP-SDE:</p> \\[ dx = -\\frac{1}{2}\\beta(t) x\\,dt + \\sqrt{\\beta(t)}\\,dw \\] <p>Reverse VP-SDE:</p> \\[ dx = \\left[-\\frac{1}{2}\\beta(t) x - \\beta(t) \\nabla_x \\log p_t(x)\\right]dt + \\sqrt{\\beta(t)}\\,d\\bar{w} \\] <p>Using noise prediction \\(\\epsilon_\\theta(x, t)\\):</p> \\[ \\nabla_x \\log p_t(x) \\approx -\\frac{\\epsilon_\\theta(x, t)}{\\sqrt{1-\\bar{\\alpha}_t}} \\] <p>Reverse SDE becomes:</p> \\[ dx = \\left[-\\frac{1}{2}\\beta(t) x + \\frac{\\beta(t)}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_\\theta(x, t)\\right]dt + \\sqrt{\\beta(t)}\\,d\\bar{w} \\]"},{"location":"SDE/03_sde_sampling/#the-probability-flow-ode","title":"The Probability Flow ODE","text":""},{"location":"SDE/03_sde_sampling/#mathematical-form_1","title":"Mathematical Form","text":"<p>For any SDE with forward process:</p> \\[ dx = f(x, t)\\,dt + g(t)\\,dw \\] <p>There exists a probability flow ODE:</p> \\[ dx = \\left[f(x, t) - \\frac{1}{2}g(t)^2 \\nabla_x \\log p_t(x)\\right]dt \\] <p>Key property: This ODE has the same marginal distributions \\(p_t(x)\\) as the SDE, but follows deterministic paths.</p>"},{"location":"SDE/03_sde_sampling/#for-vp-sde_1","title":"For VP-SDE","text":"<p>Probability flow ODE:</p> \\[ dx = \\left[-\\frac{1}{2}\\beta(t) x - \\frac{1}{2}\\beta(t) \\nabla_x \\log p_t(x)\\right]dt \\] <p>Using noise prediction:</p> \\[ dx = \\left[-\\frac{1}{2}\\beta(t) x + \\frac{\\beta(t)}{2\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_\\theta(x, t)\\right]dt \\] <p>Note: Factor of \\(\\frac{1}{2}\\) compared to reverse SDE (no noise term).</p>"},{"location":"SDE/03_sde_sampling/#numerical-discretization","title":"Numerical Discretization","text":""},{"location":"SDE/03_sde_sampling/#why-we-need-solvers","title":"Why We Need Solvers","text":"<p>Unlike training, we cannot use closed-form solutions for sampling because: 1. We don't know the true score \\(\\nabla_x \\log p_t(x)\\) \u2014 only an approximation \\(s_\\theta(x, t)\\) 2. The reverse process depends on the learned network at each step 3. We must simulate the process step-by-step</p>"},{"location":"SDE/03_sde_sampling/#euler-maruyama-em-method","title":"Euler-Maruyama (EM) Method","text":"<p>For SDEs: The simplest discretization scheme.</p> <p>General form:</p> \\[ x_{k-1} = x_k + f(x_k, t_k)\\Delta t + g(t_k)\\sqrt{|\\Delta t|}\\,z_k \\] <p>where \\(z_k \\sim \\mathcal{N}(0, I)\\) and \\(\\Delta t &lt; 0\\) (going backward in time).</p> <p>For VP-SDE reverse process:</p> \\[ x_{k-1} = x_k + \\left[-\\frac{1}{2}\\beta(t_k) x_k + \\frac{\\beta(t_k)}{\\sqrt{1-\\bar{\\alpha}_{t_k}}} \\epsilon_\\theta(x_k, t_k)\\right]\\Delta t + \\sqrt{\\beta(t_k)|\\Delta t|}\\,z_k \\] <p>This is ancestral sampling (DDPM-style).</p>"},{"location":"SDE/03_sde_sampling/#euler-method-for-ode","title":"Euler Method for ODE","text":"<p>For ODEs: No noise term.</p> <p>General form:</p> \\[ x_{k-1} = x_k + f(x_k, t_k)\\Delta t \\] <p>For probability flow ODE:</p> \\[ x_{k-1} = x_k + \\left[-\\frac{1}{2}\\beta(t_k) x_k + \\frac{\\beta(t_k)}{2\\sqrt{1-\\bar{\\alpha}_{t_k}}} \\epsilon_\\theta(x_k, t_k)\\right]\\Delta t \\] <p>This is DDIM-style sampling.</p>"},{"location":"SDE/03_sde_sampling/#sampling-algorithms","title":"Sampling Algorithms","text":""},{"location":"SDE/03_sde_sampling/#ancestral-sampling-reverse-sde","title":"Ancestral Sampling (Reverse SDE)","text":"<p>Pseudocode:</p> <pre><code>def ancestral_sampling(model, shape, num_steps=1000, T=1.0):\n    \"\"\"\n    Sample using reverse SDE (stochastic).\n    Corresponds to DDPM sampling.\n    \"\"\"\n    # Start from pure noise\n    x = torch.randn(shape)\n\n    # Time discretization\n    dt = -T / num_steps  # Negative (going backward)\n\n    for k in range(num_steps):\n        t = T - k * abs(dt)  # Current time\n\n        # Compute \u03b2(t) and \u03b1\u0305_t\n        beta_t = compute_beta(t)\n        alpha_bar_t = compute_alpha_bar(t)\n\n        # Predict noise\n        epsilon_pred = model(x, t)\n\n        # Compute drift\n        drift = -0.5 * beta_t * x + beta_t / sqrt(1 - alpha_bar_t) * epsilon_pred\n\n        # Compute diffusion (noise term)\n        if k &lt; num_steps - 1:  # No noise at final step\n            noise = torch.randn_like(x)\n            diffusion = sqrt(beta_t * abs(dt)) * noise\n        else:\n            diffusion = 0\n\n        # Update\n        x = x + drift * dt + diffusion\n\n    return x\n</code></pre>"},{"location":"SDE/03_sde_sampling/#ddim-sampling-probability-flow-ode","title":"DDIM Sampling (Probability Flow ODE)","text":"<p>Pseudocode:</p> <pre><code>def ddim_sampling(model, shape, num_steps=50, T=1.0):\n    \"\"\"\n    Sample using probability flow ODE (deterministic).\n    Corresponds to DDIM sampling.\n    \"\"\"\n    # Start from pure noise\n    x = torch.randn(shape)\n\n    # Time discretization\n    dt = -T / num_steps  # Negative (going backward)\n\n    for k in range(num_steps):\n        t = T - k * abs(dt)  # Current time\n\n        # Compute \u03b2(t) and \u03b1\u0305_t\n        beta_t = compute_beta(t)\n        alpha_bar_t = compute_alpha_bar(t)\n\n        # Predict noise\n        epsilon_pred = model(x, t)\n\n        # Compute ODE drift (factor of 1/2 compared to SDE)\n        drift = -0.5 * beta_t * x + 0.5 * beta_t / sqrt(1 - alpha_bar_t) * epsilon_pred\n\n        # Update (no noise term)\n        x = x + drift * dt\n\n    return x\n</code></pre>"},{"location":"SDE/03_sde_sampling/#pytorch-implementation","title":"PyTorch Implementation","text":"<pre><code>import torch\nimport math\n\nclass SDESampler:\n    def __init__(self, model, beta_schedule, T=1.0):\n        self.model = model\n        self.T = T\n        self.beta_min = beta_schedule['beta_min']\n        self.beta_max = beta_schedule['beta_max']\n\n    def compute_beta(self, t):\n        \"\"\"Linear schedule: \u03b2(t) = \u03b2_min + t(\u03b2_max - \u03b2_min)\"\"\"\n        return self.beta_min + t * (self.beta_max - self.beta_min)\n\n    def compute_alpha_bar(self, t):\n        \"\"\"\u03b1\u0305(t) = exp(-\u222b\u2080\u1d57 \u03b2(s)ds)\"\"\"\n        integral = self.beta_min * t + 0.5 * (self.beta_max - self.beta_min) * t**2\n        return torch.exp(-integral)\n\n    @torch.no_grad()\n    def sample_sde(self, shape, num_steps=1000, device='cuda'):\n        \"\"\"Ancestral sampling (stochastic).\"\"\"\n        x = torch.randn(shape, device=device)\n        dt = -self.T / num_steps\n\n        for k in range(num_steps):\n            t = self.T - k * abs(dt)\n            t_tensor = torch.full((shape[0],), t, device=device)\n\n            # Compute schedule values\n            beta_t = self.compute_beta(t_tensor)\n            alpha_bar_t = self.compute_alpha_bar(t_tensor)\n\n            # Reshape for broadcasting\n            beta_t = beta_t.view(-1, 1, 1, 1)\n            alpha_bar_t = alpha_bar_t.view(-1, 1, 1, 1)\n\n            # Predict noise\n            epsilon_pred = self.model(x, t_tensor)\n\n            # Drift term\n            drift = -0.5 * beta_t * x + beta_t / torch.sqrt(1 - alpha_bar_t) * epsilon_pred\n\n            # Diffusion term\n            if k &lt; num_steps - 1:\n                noise = torch.randn_like(x)\n                diffusion = torch.sqrt(beta_t * abs(dt)) * noise\n            else:\n                diffusion = 0\n\n            # Update\n            x = x + drift * dt + diffusion\n\n        return x\n\n    @torch.no_grad()\n    def sample_ode(self, shape, num_steps=50, device='cuda'):\n        \"\"\"DDIM sampling (deterministic).\"\"\"\n        x = torch.randn(shape, device=device)\n        dt = -self.T / num_steps\n\n        for k in range(num_steps):\n            t = self.T - k * abs(dt)\n            t_tensor = torch.full((shape[0],), t, device=device)\n\n            # Compute schedule values\n            beta_t = self.compute_beta(t_tensor)\n            alpha_bar_t = self.compute_alpha_bar(t_tensor)\n\n            # Reshape for broadcasting\n            beta_t = beta_t.view(-1, 1, 1, 1)\n            alpha_bar_t = alpha_bar_t.view(-1, 1, 1, 1)\n\n            # Predict noise\n            epsilon_pred = self.model(x, t_tensor)\n\n            # ODE drift (factor of 1/2)\n            drift = -0.5 * beta_t * x + 0.5 * beta_t / torch.sqrt(1 - alpha_bar_t) * epsilon_pred\n\n            # Update (no noise)\n            x = x + drift * dt\n\n        return x\n\n# Usage\nsampler = SDESampler(\n    model=trained_model,\n    beta_schedule={'beta_min': 0.1, 'beta_max': 20.0},\n    T=1.0\n)\n\n# Stochastic sampling (1000 steps)\nsamples_sde = sampler.sample_sde(shape=(16, 3, 32, 32), num_steps=1000)\n\n# Deterministic sampling (50 steps)\nsamples_ode = sampler.sample_ode(shape=(16, 3, 32, 32), num_steps=50)\n</code></pre>"},{"location":"SDE/03_sde_sampling/#higher-order-solvers","title":"Higher-Order Solvers","text":""},{"location":"SDE/03_sde_sampling/#runge-kutta-4th-order-rk4","title":"Runge-Kutta 4<sup>th</sup> Order (RK4)","text":"<p>For ODEs: More accurate than Euler method.</p> <p>Algorithm:</p> <pre><code>@torch.no_grad()\ndef sample_ode_rk4(self, shape, num_steps=50, device='cuda'):\n    \"\"\"DDIM sampling with RK4 solver.\"\"\"\n    x = torch.randn(shape, device=device)\n    dt = -self.T / num_steps\n\n    for k in range(num_steps):\n        t = self.T - k * abs(dt)\n\n        # RK4 stages\n        k1 = self.ode_drift(x, t)\n        k2 = self.ode_drift(x + 0.5 * dt * k1, t + 0.5 * dt)\n        k3 = self.ode_drift(x + 0.5 * dt * k2, t + 0.5 * dt)\n        k4 = self.ode_drift(x + dt * k3, t + dt)\n\n        # Update\n        x = x + dt / 6 * (k1 + 2*k2 + 2*k3 + k4)\n\n    return x\n\ndef ode_drift(self, x, t):\n    \"\"\"Compute ODE drift at (x, t).\"\"\"\n    t_tensor = torch.full((x.shape[0],), t, device=x.device)\n\n    beta_t = self.compute_beta(t_tensor).view(-1, 1, 1, 1)\n    alpha_bar_t = self.compute_alpha_bar(t_tensor).view(-1, 1, 1, 1)\n\n    epsilon_pred = self.model(x, t_tensor)\n\n    return -0.5 * beta_t * x + 0.5 * beta_t / torch.sqrt(1 - alpha_bar_t) * epsilon_pred\n</code></pre> <p>Advantage: Fewer steps needed for same quality (20-30 steps vs 50-100 for Euler).</p>"},{"location":"SDE/03_sde_sampling/#heuns-method-2nd-order","title":"Heun's Method (2<sup>nd</sup> Order)","text":"<p>For SDEs: Improved accuracy over Euler-Maruyama.</p> <p>Algorithm: Predictor-corrector approach 1. Predictor: Euler step 2. Corrector: Average drift at current and predicted points</p> <pre><code>@torch.no_grad()\ndef sample_sde_heun(self, shape, num_steps=500, device='cuda'):\n    \"\"\"Ancestral sampling with Heun's method.\"\"\"\n    x = torch.randn(shape, device=device)\n    dt = -self.T / num_steps\n\n    for k in range(num_steps):\n        t = self.T - k * abs(dt)\n\n        # Predictor step\n        drift_1 = self.sde_drift(x, t)\n        noise = torch.randn_like(x)\n        diffusion = self.sde_diffusion(t) * torch.sqrt(torch.abs(dt)) * noise\n        x_pred = x + drift_1 * dt + diffusion\n\n        # Corrector step\n        drift_2 = self.sde_drift(x_pred, t + dt)\n        x = x + 0.5 * (drift_1 + drift_2) * dt + diffusion\n\n    return x\n</code></pre>"},{"location":"SDE/03_sde_sampling/#connection-to-ddpm-and-ddim","title":"Connection to DDPM and DDIM","text":""},{"location":"SDE/03_sde_sampling/#ddpm-as-discretized-reverse-sde","title":"DDPM as Discretized Reverse SDE","text":"<p>DDPM update rule:</p> \\[ x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left(x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t)\\right) + \\sigma_t z \\] <p>This is equivalent to Euler-Maruyama discretization of the reverse VP-SDE with specific time discretization.</p>"},{"location":"SDE/03_sde_sampling/#ddim-as-discretized-probability-flow-ode","title":"DDIM as Discretized Probability Flow ODE","text":"<p>DDIM update rule:</p> \\[ x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}} \\left(\\frac{x_t - \\sqrt{1-\\bar{\\alpha}_t} \\epsilon_\\theta(x_t, t)}{\\sqrt{\\bar{\\alpha}_t}}\\right) + \\sqrt{1-\\bar{\\alpha}_{t-1}} \\epsilon_\\theta(x_t, t) \\] <p>This is equivalent to Euler discretization of the probability flow ODE.</p>"},{"location":"SDE/03_sde_sampling/#the-parameter","title":"The \u03b7 Parameter","text":"<p>DDIM can interpolate between deterministic and stochastic:</p> \\[ x_{t-1} = \\text{deterministic part} + \\eta \\sigma_t z \\] <ul> <li>\\(\\eta = 0\\): Pure ODE (deterministic)</li> <li>\\(\\eta = 1\\): Full SDE (stochastic, like DDPM)</li> <li>\\(0 &lt; \\eta &lt; 1\\): Hybrid</li> </ul>"},{"location":"SDE/03_sde_sampling/#sampling-strategies","title":"Sampling Strategies","text":""},{"location":"SDE/03_sde_sampling/#number-of-steps","title":"Number of Steps","text":"<p>Trade-off: Quality vs speed</p> <p>Reverse SDE (Ancestral):</p> <ul> <li>High quality: 1000 steps</li> <li>Medium quality: 500 steps</li> <li>Fast: 250 steps</li> </ul> <p>Probability Flow ODE (DDIM):</p> <ul> <li>High quality: 100 steps</li> <li>Medium quality: 50 steps</li> <li>Fast: 20-25 steps</li> </ul> <p>Rule of thumb: ODE needs 5-10\u00d7 fewer steps than SDE for similar quality.</p>"},{"location":"SDE/03_sde_sampling/#non-uniform-time-discretization","title":"Non-Uniform Time Discretization","text":"<p>Uniform spacing (simple): <pre><code>times = torch.linspace(T, 0, num_steps)\n</code></pre></p> <p>Quadratic spacing (more steps at high noise): <pre><code>times = T * (1 - torch.linspace(0, 1, num_steps)**2)\n</code></pre></p> <p>Cosine spacing (from improved DDPM): <pre><code>s = 0.008\ntimes = T * torch.cos((torch.linspace(0, 1, num_steps) + s) / (1 + s) * math.pi / 2)**2\n</code></pre></p>"},{"location":"SDE/03_sde_sampling/#adaptive-step-sizes","title":"Adaptive Step Sizes","text":"<p>Idea: Use larger steps when error is small, smaller when error is large.</p> <p>Simple adaptive scheme: 1. Take a full step 2. Take two half steps 3. Compare results 4. If difference is small, accept; otherwise, reduce step size</p>"},{"location":"SDE/03_sde_sampling/#conditional-generation","title":"Conditional Generation","text":""},{"location":"SDE/03_sde_sampling/#classifier-guidance","title":"Classifier Guidance","text":"<p>Modify the score to incorporate class information:</p> \\[ \\nabla_x \\log p(x_t \\mid y) = \\nabla_x \\log p(x_t) + s \\nabla_x \\log p(y \\mid x_t) \\] <p>where \\(s\\) is the guidance scale.</p> <p>Implementation: <pre><code># Unconditional score\nepsilon_uncond = model(x, t)\nscore_uncond = -epsilon_uncond / sqrt(1 - alpha_bar_t)\n\n# Classifier gradient\nwith torch.enable_grad():\n    x_in = x.detach().requires_grad_(True)\n    logits = classifier(x_in, t)\n    log_prob = F.log_softmax(logits, dim=-1)[..., class_label]\n    classifier_grad = torch.autograd.grad(log_prob.sum(), x_in)[0]\n\n# Guided score\nscore_guided = score_uncond + s * classifier_grad\n</code></pre></p>"},{"location":"SDE/03_sde_sampling/#classifier-free-guidance","title":"Classifier-Free Guidance","text":"<p>Train a conditional model \\(\\epsilon_\\theta(x_t, t, c)\\) where \\(c\\) is the condition.</p> <p>During sampling, interpolate between conditional and unconditional:</p> \\[ \\tilde{\\epsilon}_\\theta(x_t, t, c) = (1 + w) \\epsilon_\\theta(x_t, t, c) - w \\epsilon_\\theta(x_t, t, \\emptyset) \\] <p>where \\(w\\) is the guidance weight.</p> <p>Implementation: <pre><code># Conditional prediction\nepsilon_cond = model(x, t, condition)\n\n# Unconditional prediction (null condition)\nepsilon_uncond = model(x, t, null_condition)\n\n# Guided prediction\nepsilon_guided = (1 + w) * epsilon_cond - w * epsilon_uncond\n</code></pre></p>"},{"location":"SDE/03_sde_sampling/#practical-considerations","title":"Practical Considerations","text":""},{"location":"SDE/03_sde_sampling/#memory-optimization","title":"Memory Optimization","text":"<p>Gradient checkpointing during sampling: <pre><code>from torch.utils.checkpoint import checkpoint\n\ndef model_forward(x, t):\n    return checkpoint(model, x, t, use_reentrant=False)\n</code></pre></p> <p>Batch sampling: <pre><code># Sample in batches to avoid OOM\nall_samples = []\nfor i in range(0, total_samples, batch_size):\n    batch_samples = sampler.sample_ode(shape=(batch_size, C, H, W))\n    all_samples.append(batch_samples)\nsamples = torch.cat(all_samples, dim=0)\n</code></pre></p>"},{"location":"SDE/03_sde_sampling/#deterministic-sampling","title":"Deterministic Sampling","text":"<p>For reproducibility, set random seed: <pre><code>torch.manual_seed(42)\nsamples = sampler.sample_ode(shape=(16, 3, 32, 32))\n</code></pre></p> <p>ODE sampling is deterministic given the same initial noise.</p>"},{"location":"SDE/03_sde_sampling/#interpolation","title":"Interpolation","text":"<p>Linear interpolation in latent space: <pre><code># Start from two different noise samples\nz1 = torch.randn(1, C, H, W)\nz2 = torch.randn(1, C, H, W)\n\n# Interpolate\nalphas = torch.linspace(0, 1, 10)\ninterpolated_samples = []\n\nfor alpha in alphas:\n    z_interp = (1 - alpha) * z1 + alpha * z2\n    sample = sampler.sample_ode_from_noise(z_interp)\n    interpolated_samples.append(sample)\n</code></pre></p>"},{"location":"SDE/03_sde_sampling/#comparison-sde-vs-ode-sampling","title":"Comparison: SDE vs ODE Sampling","text":"Aspect Reverse SDE Probability Flow ODE Stochasticity Stochastic Deterministic Steps needed 500-1000 20-100 Speed Slower Faster Diversity Higher Lower Reproducibility Different each time Same given same noise Interpolation Harder Easier Likelihood Cannot compute Can compute Corresponds to DDPM DDIM <p>When to use SDE:</p> <ul> <li>Need maximum sample diversity</li> <li>Quality is critical, speed is not</li> </ul> <p>When to use ODE:</p> <ul> <li>Need fast sampling</li> <li>Want deterministic generation</li> <li>Need to compute likelihoods</li> <li>Want smooth interpolations</li> </ul>"},{"location":"SDE/03_sde_sampling/#advanced-topics","title":"Advanced Topics","text":""},{"location":"SDE/03_sde_sampling/#exact-likelihood-computation","title":"Exact Likelihood Computation","text":"<p>Probability flow ODE allows exact likelihood via change of variables:</p> \\[ \\log p_0(x_0) = \\log p_T(x_T) - \\int_0^T \\nabla \\cdot f_\\theta(x_t, t)\\,dt \\] <p>where \\(f_\\theta\\) is the ODE drift.</p> <p>Implementation (expensive): <pre><code>from torchdiffeq import odeint\n\ndef compute_likelihood(x_0):\n    # Encode to noise\n    x_T = odeint(lambda t, x: -ode_drift(x, t), x_0, torch.tensor([0., T]))[-1]\n\n    # Compute divergence integral\n    def augmented_dynamics(t, state):\n        x, logp = state[0], state[1]\n        with torch.enable_grad():\n            x = x.requires_grad_(True)\n            drift = ode_drift(x, t)\n            divergence = compute_divergence(drift, x)\n        return drift, -divergence\n\n    _, neg_log_likelihood = odeint(augmented_dynamics, (x_0, 0.), torch.tensor([0., T]))\n\n    return -neg_log_likelihood + log_p_T(x_T)\n</code></pre></p>"},{"location":"SDE/03_sde_sampling/#inpainting","title":"Inpainting","text":"<p>Idea: Constrain known pixels during sampling.</p> <pre><code>def sample_with_inpainting(known_pixels, mask, num_steps=50):\n    x = torch.randn(shape)\n    dt = -T / num_steps\n\n    for k in range(num_steps):\n        t = T - k * abs(dt)\n\n        # Normal ODE step\n        epsilon_pred = model(x, t)\n        drift = compute_ode_drift(x, t, epsilon_pred)\n        x = x + drift * dt\n\n        # Project onto constraint\n        x = mask * known_pixels + (1 - mask) * x\n\n    return x\n</code></pre>"},{"location":"SDE/03_sde_sampling/#key-takeaways","title":"Key Takeaways","text":""},{"location":"SDE/03_sde_sampling/#conceptual","title":"Conceptual","text":"<ol> <li>Sampling requires SDE/ODE solvers \u2014 unlike training which uses closed-form</li> <li>Two strategies: Stochastic (SDE) vs deterministic (ODE)</li> <li>Same marginals: SDE and ODE produce same distributions</li> <li>DDPM = discretized reverse SDE, DDIM = discretized probability flow ODE</li> </ol>"},{"location":"SDE/03_sde_sampling/#practical","title":"Practical","text":"<ol> <li>Use ODE for speed \u2014 5-10\u00d7 fewer steps than SDE</li> <li>Use SDE for diversity \u2014 when quality matters more than speed</li> <li>Higher-order solvers help \u2014 RK4 can reduce steps by 2-3\u00d7</li> <li>Non-uniform spacing \u2014 allocate more steps to high-noise regions</li> </ol>"},{"location":"SDE/03_sde_sampling/#mathematical","title":"Mathematical","text":"<ol> <li>Reverse SDE: \\(dx = [f - g^2 \\nabla \\log p]dt + g\\,d\\bar{w}\\)</li> <li>Probability flow ODE: \\(dx = [f - \\frac{1}{2}g^2 \\nabla \\log p]dt\\)</li> <li>Discretization: Euler-Maruyama (SDE), Euler/RK4 (ODE)</li> <li>Connection: DDPM/DDIM are specific discretizations</li> </ol>"},{"location":"SDE/03_sde_sampling/#related-documents","title":"Related Documents","text":""},{"location":"SDE/03_sde_sampling/#sde-documentation","title":"SDE Documentation","text":"<ul> <li>00_sde_overview.md \u2014 High-level SDE introduction</li> <li>01_diffusion_sde_view.md \u2014 Detailed SDE formulation</li> <li>02_sde_training.md \u2014 How training works (no solvers!)</li> <li>03a_reverse_time_sde_and_proba_flow_ode.md \u2014 Theoretical derivations</li> </ul>"},{"location":"SDE/03_sde_sampling/#related-topics","title":"Related Topics","text":"<ul> <li>DDPM Sampling \u2014 Discrete version</li> <li>Flow Matching Sampling \u2014 Alternative approach</li> </ul>"},{"location":"SDE/03_sde_sampling/#summary","title":"Summary","text":"<p>Sampling from diffusion models requires numerical solvers:</p> <ol> <li>Reverse SDE: Stochastic, 500-1000 steps, high diversity</li> <li>Probability Flow ODE: Deterministic, 20-100 steps, fast</li> <li>Discretization methods: Euler, Heun, RK4, adaptive</li> <li>DDPM/DDIM: Specific discretizations of SDE/ODE</li> </ol> <p>Key distinction from training:</p> <ul> <li>Training: Uses closed-form marginals, no solver needed</li> <li>Sampling: Requires numerical integration of reverse process</li> </ul> <p>Practical recommendation: Use probability flow ODE (DDIM) with 50 steps for good balance of quality and speed.</p>"},{"location":"SDE/03_solving_vpsde/","title":"Solving the VP-SDE: Closed-Form Marginals","text":"<p>This document provides the exact solution to the variance-preserving SDE (VP-SDE), showing how the continuous-time formulation connects to DDPM's discrete notation.</p>"},{"location":"SDE/03_solving_vpsde/#overview","title":"Overview","text":"<p>This is the final \"dictionary page\" that makes DDPM notation (\\(\\alpha_t\\), \\(\\bar{\\alpha}_t\\)) line up exactly with the VP-SDE notation (\\(\\beta(t)\\), integrals).</p>"},{"location":"SDE/03_solving_vpsde/#what-well-show","title":"What We'll Show","text":"<p>We'll derive the closed-form marginal:</p> \\[ x(t) = \\exp\\left(-\\frac{1}{2}\\int_0^t \\beta(s)\\,ds\\right) x(0) + \\text{Gaussian noise} \\] <p>and show how \\(\\bar{\\alpha}_t\\) in DDPM corresponds to \\(\\exp\\left(-\\int_0^t \\beta(s)\\,ds\\right)\\) in continuous time.</p> <p>This is the key connection that makes discrete and continuous notations line up perfectly.</p>"},{"location":"SDE/03_solving_vpsde/#roadmap","title":"Roadmap","text":"<ol> <li>Solve the VP-SDE: Get the closed-form marginal \\(q(x_t \\mid x_0)\\)</li> <li>Identify \\(\\bar{\\alpha}(t)\\): Continuous-time analogue of DDPM's \\(\\bar{\\alpha}_t\\)</li> <li>Connect discrete and continuous: Show how products become integrals</li> </ol>"},{"location":"SDE/03_solving_vpsde/#notation","title":"Notation","text":""},{"location":"SDE/03_solving_vpsde/#continuous-time","title":"Continuous Time","text":"<ul> <li>Time: \\(t \\in [0, T]\\)</li> <li>VP-SDE:</li> </ul> <p>$$   dx(t) = -\\frac{1}{2}\\beta(t) x(t)\\,dt + \\sqrt{\\beta(t)}\\,dw(t)   $$</p> <p>where \\(w(t)\\) is Brownian motion and \\(\\beta(t) \\geq 0\\)</p>"},{"location":"SDE/03_solving_vpsde/#ddpm-discrete-time","title":"DDPM Discrete Time","text":"<ul> <li>Time steps: \\(k = 0, 1, \\ldots, N\\) with step size \\(\\Delta t\\)</li> <li>Notation:</li> <li>\\(\\beta_k\\): discrete noise amount</li> <li>\\(\\alpha_k := 1 - \\beta_k\\): signal retention</li> <li>\\(\\bar{\\alpha}_k := \\prod_{i=1}^k \\alpha_i\\): cumulative signal retention</li> </ul>"},{"location":"SDE/03_solving_vpsde/#step-1-solve-the-vp-sde","title":"Step 1: Solve the VP-SDE","text":"<p>The VP-SDE is a linear SDE (similar to an Ornstein\u2013Uhlenbeck process with time-varying rate). The standard technique is an integrating factor.</p>"},{"location":"SDE/03_solving_vpsde/#define-the-integrating-factor","title":"Define the Integrating Factor","text":"<p>Let:</p> \\[ \\gamma(t) := \\frac{1}{2}\\int_0^t \\beta(s)\\,ds \\] <p>Define the transformed variable:</p> \\[ y(t) := e^{\\gamma(t)} x(t) \\] <p>Strategy: We'll compute \\(dy(t)\\) and show that the drift term cancels, leaving only a pure diffusion.</p>"},{"location":"SDE/03_solving_vpsde/#differentiate-yt","title":"Differentiate \\(y(t)\\)","text":"<p>Using the product rule (since \\(e^{\\gamma(t)}\\) is deterministic):</p> \\[ dy(t) = e^{\\gamma(t)}\\,dx(t) + x(t)\\,d(e^{\\gamma(t)}) \\] <p>The differential of the exponential is:</p> \\[ d(e^{\\gamma(t)}) = e^{\\gamma(t)}\\,d\\gamma(t) = e^{\\gamma(t)} \\cdot \\frac{1}{2}\\beta(t)\\,dt \\] <p>Substitute \\(dx(t)\\) from the VP-SDE:</p> \\[ dx(t) = -\\frac{1}{2}\\beta(t) x(t)\\,dt + \\sqrt{\\beta(t)}\\,dw(t) \\] <p>Therefore:</p> \\[ \\begin{align} dy(t) &amp;= e^{\\gamma(t)}\\left(-\\frac{1}{2}\\beta(t) x(t)\\,dt + \\sqrt{\\beta(t)}\\,dw(t)\\right) + x(t) e^{\\gamma(t)} \\frac{1}{2}\\beta(t)\\,dt \\\\ &amp;= e^{\\gamma(t)}\\sqrt{\\beta(t)}\\,dw(t) \\end{align} \\] <p>Key observation: The drift terms cancel perfectly! This is why we chose this particular integrating factor.</p>"},{"location":"SDE/03_solving_vpsde/#integrate-from-0-to-t","title":"Integrate from 0 to \\(t\\)","text":"<p>Integrating both sides:</p> \\[ y(t) - y(0) = \\int_0^t e^{\\gamma(s)}\\sqrt{\\beta(s)}\\,dw(s) \\] <p>Since \\(y(0) = e^{\\gamma(0)} x(0) = x(0)\\) (because \\(\\gamma(0) = 0\\)):</p> \\[ y(t) = x(0) + \\int_0^t e^{\\gamma(s)}\\sqrt{\\beta(s)}\\,dw(s) \\] <p>Substitute back \\(x(t) = e^{-\\gamma(t)} y(t)\\):</p> \\[ \\boxed{x(t) = e^{-\\gamma(t)} x(0) + e^{-\\gamma(t)} \\int_0^t e^{\\gamma(s)}\\sqrt{\\beta(s)}\\,dw(s)} \\] <p>This is the exact solution to the VP-SDE.</p>"},{"location":"SDE/03_solving_vpsde/#step-2-the-marginal-distribution-is-gaussian","title":"Step 2: The Marginal Distribution is Gaussian","text":"<p>The second term in our solution is an It\u00f4 integral of deterministic coefficients against Brownian motion, so it's Gaussian with mean zero.</p>"},{"location":"SDE/03_solving_vpsde/#mean","title":"Mean","text":"\\[ \\mathbb{E}[x(t) \\mid x(0)] = e^{-\\gamma(t)} x(0) \\]"},{"location":"SDE/03_solving_vpsde/#variance","title":"Variance","text":"<p>Define the noise term:</p> \\[ \\eta(t) := e^{-\\gamma(t)} \\int_0^t e^{\\gamma(s)}\\sqrt{\\beta(s)}\\,dw(s) \\] <p>The covariance of \\(\\eta(t)\\) is:</p> \\[ \\text{Cov}[\\eta(t)] = e^{-2\\gamma(t)} \\int_0^t e^{2\\gamma(s)} \\beta(s)\\,ds \\cdot I \\]"},{"location":"SDE/03_solving_vpsde/#beautiful-simplification","title":"Beautiful Simplification","text":"<p>Notice that:</p> \\[ \\frac{d}{ds}\\left(e^{2\\gamma(s)}\\right) = e^{2\\gamma(s)} \\cdot 2\\gamma'(s) = e^{2\\gamma(s)} \\beta(s) \\] <p>Therefore:</p> \\[ \\int_0^t e^{2\\gamma(s)} \\beta(s)\\,ds = e^{2\\gamma(t)} - 1 \\] <p>Substituting:</p> \\[ \\text{Cov}[\\eta(t)] = e^{-2\\gamma(t)}(e^{2\\gamma(t)} - 1) I = (1 - e^{-2\\gamma(t)}) I \\]"},{"location":"SDE/03_solving_vpsde/#result","title":"Result","text":"\\[ \\boxed{q(x_t \\mid x_0) = \\mathcal{N}\\left(e^{-\\gamma(t)} x_0, (1 - e^{-2\\gamma(t)}) I\\right)} \\]"},{"location":"SDE/03_solving_vpsde/#step-3-identify-the-continuous-time-baralphat","title":"Step 3: Identify the Continuous-Time \\(\\bar{\\alpha}(t)\\)","text":""},{"location":"SDE/03_solving_vpsde/#compare-with-ddpm","title":"Compare with DDPM","text":"<p>Recall the DDPM marginal:</p> \\[ q(x_k \\mid x_0) = \\mathcal{N}\\left(\\sqrt{\\bar{\\alpha}_k} x_0, (1 - \\bar{\\alpha}_k) I\\right) \\] <p>From the SDE solution, we have:</p> <ul> <li>Mean coefficient: \\(e^{-\\gamma(t)}\\)</li> <li>Variance: \\(1 - e^{-2\\gamma(t)}\\)</li> </ul>"},{"location":"SDE/03_solving_vpsde/#define-baralphat","title":"Define \\(\\bar{\\alpha}(t)\\)","text":"<p>To match the DDPM form, define:</p> \\[ \\boxed{\\bar{\\alpha}(t) := e^{-2\\gamma(t)} = \\exp\\left(-\\int_0^t \\beta(s)\\,ds\\right)} \\] <p>Then:</p> \\[ e^{-\\gamma(t)} = \\sqrt{\\bar{\\alpha}(t)}, \\qquad 1 - e^{-2\\gamma(t)} = 1 - \\bar{\\alpha}(t) \\]"},{"location":"SDE/03_solving_vpsde/#unified-form","title":"Unified Form","text":"<p>The SDE marginal becomes:</p> \\[ \\boxed{q(x_t \\mid x_0) = \\mathcal{N}\\left(\\sqrt{\\bar{\\alpha}(t)} x_0, (1 - \\bar{\\alpha}(t)) I\\right)} \\] <p>where:</p> \\[ \\bar{\\alpha}(t) = \\exp\\left(-\\int_0^t \\beta(s)\\,ds\\right) \\] <p>This is the exact continuous-time version of DDPM's closed-form marginal.</p>"},{"location":"SDE/03_solving_vpsde/#step-4-discrete-products-become-integrals","title":"Step 4: Discrete Products Become Integrals","text":"<p>Now we connect the discrete \\(\\bar{\\alpha}_k = \\prod_{i=1}^k \\alpha_i\\) to the continuous form.</p>"},{"location":"SDE/03_solving_vpsde/#setup","title":"Setup","text":"<p>Recall:</p> <ul> <li>\\(\\alpha_i = 1 - \\beta_i\\)</li> <li>In the SDE scaling: \\(\\beta_i = \\beta(t_i) \\Delta t\\) (small)</li> </ul>"},{"location":"SDE/03_solving_vpsde/#take-logarithms","title":"Take Logarithms","text":"\\[ \\log \\bar{\\alpha}_k = \\sum_{i=1}^k \\log(1 - \\beta_i) \\]"},{"location":"SDE/03_solving_vpsde/#taylor-expansion","title":"Taylor Expansion","text":"<p>For small \\(\\beta_i\\):</p> \\[ \\log(1 - \\beta_i) \\approx -\\beta_i \\] <p>Therefore:</p> \\[ \\log \\bar{\\alpha}_k \\approx -\\sum_{i=1}^k \\beta_i = -\\sum_{i=1}^k \\beta(t_i) \\Delta t \\]"},{"location":"SDE/03_solving_vpsde/#riemann-sum-integral","title":"Riemann Sum \u2192 Integral","text":"<p>As \\(\\Delta t \\to 0\\), the Riemann sum becomes an integral:</p> \\[ \\log \\bar{\\alpha}_k \\approx -\\int_0^{t_k} \\beta(s)\\,ds \\]"},{"location":"SDE/03_solving_vpsde/#exponentiate","title":"Exponentiate","text":"\\[ \\boxed{\\bar{\\alpha}_k \\approx \\exp\\left(-\\int_0^{t_k} \\beta(s)\\,ds\\right) = \\bar{\\alpha}(t_k)} \\] <p>This is the precise \"product \u2192 integral\" dictionary.</p>"},{"location":"SDE/03_solving_vpsde/#the-complete-ddpm-vp-sde-dictionary","title":"The Complete DDPM \u2194 VP-SDE Dictionary","text":""},{"location":"SDE/03_solving_vpsde/#discrete-ddpm","title":"Discrete (DDPM)","text":"\\[ \\bar{\\alpha}_k = \\prod_{i=1}^k (1 - \\beta_i) \\]"},{"location":"SDE/03_solving_vpsde/#continuous-vp-sde","title":"Continuous (VP-SDE)","text":"\\[ \\bar{\\alpha}(t) = \\exp\\left(-\\int_0^t \\beta(s)\\,ds\\right) \\]"},{"location":"SDE/03_solving_vpsde/#marginal-both-cases","title":"Marginal (Both Cases)","text":"\\[ x \\approx \\sqrt{\\bar{\\alpha}} x_0 + \\sqrt{1 - \\bar{\\alpha}} \\epsilon \\]"},{"location":"SDE/03_solving_vpsde/#interpretation","title":"Interpretation","text":"<p>\\(\\bar{\\alpha}\\) represents the \"survival of signal\" coefficient:</p> <ul> <li>Discrete time: Multiplicative (product of retention factors)</li> <li>Continuous time: Exponential of integral (accumulated decay)</li> </ul> <p>As \\(\\Delta t \\to 0\\), products converge to exponentials of integrals\u2014this is a fundamental connection in stochastic calculus.</p>"},{"location":"SDE/03_solving_vpsde/#summary","title":"Summary","text":"<p>We derived the exact solution to the VP-SDE:</p> <ol> <li>Integrating factor method: Transforms the SDE into pure diffusion</li> <li>Closed-form marginal: Gaussian with explicit mean and variance</li> <li>Connection to DDPM: \\(\\bar{\\alpha}(t) = \\exp\\left(-\\int_0^t \\beta(s)\\,ds\\right)\\)</li> <li>Discrete-continuous bridge: Products become integrals in the limit</li> </ol> <p>Key insight: The continuous VP-SDE and discrete DDPM describe the same underlying process, just in different time parameterizations.</p>"},{"location":"SDE/03_solving_vpsde/#related-documents","title":"Related Documents","text":"<ul> <li>Reverse-Time SDE and Probability Flow ODE \u2014 How to sample (next)</li> <li>DDPM to VP-SDE \u2014 Deriving the SDE from DDPM</li> <li>VP-SDE to DDPM \u2014 Deriving DDPM from the SDE</li> <li>SDE View Overview \u2014 Conceptual introduction</li> </ul>"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/","title":"Reverse-Time SDE and Probability Flow ODE","text":"<p>This document connects the complete story of diffusion model sampling, showing how DDPM and DDIM emerge as discretizations of continuous-time processes.</p>"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#overview","title":"Overview","text":"<p>Now we connect the whole story:</p> <ul> <li>VP-SDE forward: Continuous-time noising process</li> <li>Reverse-time SDE: Stochastic sampler (DDPM-like)</li> <li>Probability flow ODE: Deterministic sampler (DDIM-like)</li> <li>DDIM \\(\\eta\\) parameter: Interpolates between ODE and SDE</li> </ul> <p>Key insight: The same learned score function can be used for both stochastic and deterministic sampling.</p>"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#notation","title":"Notation","text":""},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#time-convention","title":"Time Convention","text":"<ul> <li>Continuous time: \\(t \\in [0, T]\\)</li> <li>Convention: \\(t = 0\\) is clean data, \\(t = T\\) is pure noise</li> </ul>"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#vp-sde-forward-process","title":"VP-SDE Forward Process","text":"\\[ dx = -\\frac{1}{2}\\beta(t) x\\,dt + \\sqrt{\\beta(t)}\\,dw \\]"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#signal-coefficient","title":"Signal Coefficient","text":"\\[ \\bar{\\alpha}(t) := \\exp\\left(-\\int_0^t \\beta(s)\\,ds\\right) \\] <p>Forward marginal:</p> \\[ q(x_t \\mid x_0) = \\mathcal{N}\\left(\\sqrt{\\bar{\\alpha}(t)} x_0, (1 - \\bar{\\alpha}(t)) I\\right) \\]"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#score-function","title":"Score Function","text":"\\[ s(x, t) := \\nabla_x \\log p_t(x) \\] <p>Learned network: \\(s_\\theta(x, t) \\approx s(x, t)\\)</p> <p>Note: Equivalently, you can predict noise \\(\\epsilon_\\theta\\); we'll connect these later.</p>"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#step-1-reverse-time-sde-for-vp-sde","title":"Step 1: Reverse-Time SDE for VP-SDE","text":""},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#general-theorem-anderson-1982","title":"General Theorem (Anderson, 1982)","text":"<p>For a forward It\u00f4 SDE:</p> \\[ dx = f(x, t)\\,dt + g(t)\\,dw \\] <p>the reverse-time SDE (running from \\(T \\to 0\\)) is:</p> \\[ dx = \\left[f(x, t) - g(t)^2 \\nabla_x \\log p_t(x)\\right]dt + g(t)\\,d\\bar{w} \\] <p>where \\(\\bar{w}\\) is reverse-time Brownian motion.</p>"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#apply-to-vp-sde","title":"Apply to VP-SDE","text":"<p>For the VP-SDE:</p> <ul> <li>\\(f(x, t) = -\\frac{1}{2}\\beta(t) x\\)</li> <li>\\(g(t) = \\sqrt{\\beta(t)}\\)</li> <li>\\(g(t)^2 = \\beta(t)\\)</li> </ul> <p>Therefore, the reverse VP-SDE is:</p> \\[ \\boxed{dx = \\left[-\\frac{1}{2}\\beta(t) x - \\beta(t) s(x, t)\\right]dt + \\sqrt{\\beta(t)}\\,d\\bar{w}} \\]"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#interpretation","title":"Interpretation","text":"<p>Drift terms:</p> <ol> <li>\\(-\\frac{1}{2}\\beta(t) x\\): Same \"shrink toward zero\" as forward process</li> <li>\\(-\\beta(t) s(x, t)\\): Score correction that pushes toward high-density regions</li> </ol> <p>Diffusion term: \\(\\sqrt{\\beta(t)}\\,d\\bar{w}\\) (same magnitude as forward, but reverse-time)</p> <p>This is the continuous-time object that corresponds to DDPM sampling (stochastic).</p>"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#step-2-probability-flow-ode","title":"Step 2: Probability Flow ODE","text":""},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#the-surprising-result-song-et-al-2021","title":"The Surprising Result (Song et al., 2021)","text":"<p>There exists a deterministic ODE whose solution has exactly the same marginal distributions \\(p_t(x)\\) as the reverse SDE.</p> <p>That ODE is:</p> \\[ \\boxed{\\frac{dx}{dt} = f(x, t) - \\frac{1}{2}g(t)^2 s(x, t)} \\]"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#apply-to-vp-sde_1","title":"Apply to VP-SDE","text":"<p>For the VP-SDE:</p> \\[ \\boxed{\\frac{dx}{dt} = -\\frac{1}{2}\\beta(t) x - \\frac{1}{2}\\beta(t) s(x, t)} \\]"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#key-distinction","title":"Key Distinction","text":"Property Reverse SDE Probability Flow ODE Trajectories Stochastic Deterministic Marginals \\(p_t(x)\\) \\(p_t(x)\\) (same!) Noise Yes (\\(\\sqrt{\\beta(t)}\\,d\\bar{w}\\)) No Sampling DDPM-like DDIM-like <p>This ODE is the continuous-time conceptual ancestor of DDIM.</p>"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Same score function: Both use \\(s(x, t)\\)</li> <li>Different dynamics: ODE has half the score correction, no noise</li> <li>Same marginals: Generate from the same distribution</li> <li>Different paths: Individual trajectories differ, but statistics match</li> </ul>"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#step-3-ddpm-sampling-as-sde-discretization","title":"Step 3: DDPM Sampling as SDE Discretization","text":""},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#discretize-the-reverse-sde","title":"Discretize the Reverse SDE","text":"<p>To generate samples, discretize time: \\(t_N = T &gt; t_{N-1} &gt; \\cdots &gt; t_0 = 0\\)</p> <p>Let \\(\\Delta t_k = t_{k-1} - t_k\\) (negative, since we go backward).</p>"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#eulermaruyama-step","title":"Euler\u2013Maruyama Step","text":"<p>A simple Euler\u2013Maruyama discretization (backward in time):</p> \\[ x_{k-1} = x_k + \\left[-\\frac{1}{2}\\beta(t_k) x_k - \\beta(t_k) s_\\theta(x_k, t_k)\\right]\\Delta t_k + \\sqrt{\\beta(t_k) |\\Delta t_k|}\\, z_k \\] <p>where \\(z_k \\sim \\mathcal{N}(0, I)\\).</p>"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#the-ddpm-connection","title":"The DDPM Connection","text":"<p>The last term is the distinctive \"DDPM-ness\": fresh Gaussian noise at every step.</p> <p>DDPM typically presents this as a Gaussian transition:</p> \\[ p_\\theta(x_{k-1} \\mid x_k) = \\mathcal{N}(x_{k-1}; \\mu_\\theta(x_k, t_k), \\sigma_k^2 I) \\] <p>But mathematically, a one-step Euler\u2013Maruyama update is exactly a Gaussian transition:</p> <ul> <li>Mean: \\(x_k + \\text{drift} \\cdot \\Delta t\\)</li> <li>Variance: \\(\\text{diffusion}^2 \\cdot |\\Delta t|\\)</li> </ul>"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#key-insight","title":"Key Insight","text":"\\[ \\boxed{\\text{DDPM sampling} \\approx \\text{Euler\u2013Maruyama discretization of reverse SDE}} \\]"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#step-4-ddim-sampling-as-ode-discretization","title":"Step 4: DDIM Sampling as ODE Discretization","text":""},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#discretize-the-probability-flow-ode","title":"Discretize the Probability Flow ODE","text":"<p>Now discretize the probability flow ODE:</p> \\[ \\frac{dx}{dt} = -\\frac{1}{2}\\beta(t) x - \\frac{1}{2}\\beta(t) s_\\theta(x, t) \\]"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#euler-step","title":"Euler Step","text":"<p>A simple Euler discretization:</p> \\[ x_{k-1} = x_k + \\left[-\\frac{1}{2}\\beta(t_k) x_k - \\frac{1}{2}\\beta(t_k) s_\\theta(x_k, t_k)\\right]\\Delta t_k \\] <p>Notice: No randomness term!</p>"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#the-ddim-connection","title":"The DDIM Connection","text":"\\[ \\boxed{\\text{DDIM sampling} \\approx \\text{Euler discretization of probability flow ODE}} \\] <p>Key difference from DDPM: Deterministic trajectories, no added noise.</p>"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#step-5-ddim-update-in-baralpha-notation","title":"Step 5: DDIM Update in \\(\\bar{\\alpha}\\) Notation","text":"<p>This bridges back to the discrete DDPM/DDIM formulas you see in code.</p>"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#discrete-time-formulation","title":"Discrete-Time Formulation","text":"<p>In discrete-time DDPM notation (steps \\(t \\in \\{1, \\ldots, T\\}\\)):</p> \\[ x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon \\]"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#predict-x_0","title":"Predict \\(x_0\\)","text":"<p>A network predicts \\(\\epsilon_\\theta(x_t, t)\\). Form an estimate of \\(x_0\\):</p> \\[ \\boxed{\\hat{x}_0(x_t, t) = \\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_t}\\, \\epsilon_\\theta(x_t, t)}{\\sqrt{\\bar{\\alpha}_t}}} \\]"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#ddim-deterministic-update-eta-0","title":"DDIM Deterministic Update (\\(\\eta = 0\\))","text":"\\[ \\boxed{x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}}\\, \\hat{x}_0 + \\sqrt{1 - \\bar{\\alpha}_{t-1}}\\, \\epsilon_\\theta(x_t, t)} \\]"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#why-this-form-is-natural","title":"Why This Form is Natural","text":"<ol> <li>Keeps the noise direction: Uses \\(\\epsilon_\\theta(x_t, t)\\) predicted at time \\(t\\)</li> <li>Changes the noise scale: From \\(\\sqrt{1 - \\bar{\\alpha}_t}\\) to \\(\\sqrt{1 - \\bar{\\alpha}_{t-1}}\\)</li> <li>No new randomness: Deterministic update</li> </ol> <p>Interpretation: Follow one consistent flow line rather than resampling noise at each step.</p> <p>This is exactly what a deterministic probability flow ODE discretization does.</p>"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#step-6-the-eta-parameter-interpolating-ode-and-sde","title":"Step 6: The \\(\\eta\\) Parameter (Interpolating ODE and SDE)","text":"<p>DDIM is often written with a parameter \\(\\eta \\in [0, 1]\\) that controls extra noise:</p> \\[ x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}}\\, \\hat{x}_0 + \\sqrt{1 - \\bar{\\alpha}_{t-1} - \\sigma_t^2}\\, \\epsilon_\\theta(x_t, t) + \\sigma_t z \\] <p>where \\(z \\sim \\mathcal{N}(0, I)\\) and \\(\\sigma_t\\) is chosen based on \\(\\eta\\):</p>"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#the-spectrum","title":"The Spectrum","text":"\\(\\eta\\) \\(\\sigma_t\\) Behavior Corresponds to \\(0\\) \\(0\\) Deterministic Probability flow ODE \\(1\\) DDPM variance Stochastic Reverse SDE \\((0, 1)\\) Intermediate Hybrid Interpolation"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#intuitive-mapping","title":"Intuitive Mapping","text":"<ul> <li>Probability flow ODE \\(\\Leftrightarrow\\) DDIM (\\(\\eta = 0\\))</li> <li>Reverse SDE \\(\\Leftrightarrow\\) DDPM (stochastic, \"full noise\")</li> </ul> <p>Key insight: The \\(\\eta\\) parameter lets you trade off between:</p> <ul> <li>Determinism (faster, reproducible, good for interpolation)</li> <li>Stochasticity (more diverse samples, better mode coverage)</li> </ul>"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#summary-the-conceptual-triangle","title":"Summary: The Conceptual Triangle","text":"<p>Here's the clean mental model:</p>"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#the-learned-object","title":"The Learned Object","text":"<p>Score field: \\(s_\\theta(x, t) \\approx \\nabla_x \\log p_t(x)\\)</p>"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#two-ways-to-sample","title":"Two Ways to Sample","text":"<p>You can generate samples by evolving \\(x\\) backward using either:</p> <ol> <li>Reverse SDE (stochastic)</li> <li>Adds noise every step</li> <li>DDPM-like sampling</li> <li> <p>Equation: \\(dx = [f - g^2 s]\\,dt + g\\,d\\bar{w}\\)</p> </li> <li> <p>Probability Flow ODE (deterministic)</p> </li> <li>No added noise</li> <li>DDIM-like sampling</li> <li>Equation: \\(\\frac{dx}{dt} = f - \\frac{1}{2}g^2 s\\)</li> </ol>"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#key-insights","title":"Key Insights","text":"<ul> <li>Same score: Both use \\(s_\\theta(x, t)\\)</li> <li>Different dynamics: SDE adds noise, ODE doesn't</li> <li>Same marginals: Generate from the same distribution \\(p_t(x)\\)</li> <li>Different trajectories: Individual paths differ</li> </ul>"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#the-complete-picture","title":"The Complete Picture","text":"<pre><code>Forward SDE \u2192 Marginals p_t(x) \u2192 Score s(x,t) \u2192 Reverse SDE (DDPM)\n                                              \u2198\n                                                Probability Flow ODE (DDIM)\n</code></pre>"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#related-documents","title":"Related Documents","text":"<ul> <li>Solving the VP-SDE \u2014 Forward process solution</li> <li>DDPM from VP-SDE \u2014 Discrete-time derivation</li> <li>DDPM Foundations \u2014 Variational perspective</li> <li>SDE View Overview \u2014 Conceptual introduction</li> </ul>"},{"location":"SDE/03a_reverse_time_sde_and_proba_flow_ode/#references","title":"References","text":"<ol> <li>Anderson, B. D. O. (1982). Reverse-time diffusion equation models. Stochastic Processes and their Applications.</li> <li>Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., &amp; Poole, B. (2021). Score-Based Generative Modeling through Stochastic Differential Equations. ICLR.</li> <li>Song, J., Meng, C., &amp; Ermon, S. (2021). Denoising Diffusion Implicit Models. ICLR.</li> </ol>"},{"location":"SDE/03b_ddim_update_coeff/","title":"DDIM Update Coefficients: From Continuous SDE to Code","text":"<p>This document pins the continuous-time VP-SDE all the way down to the exact coefficients you see in DDIM code: the \\(\\sqrt{\\bar{\\alpha}_t}\\), \\(\\sqrt{1-\\bar{\\alpha}_t}\\), and the step-to-step update from \\(t\\) to \\(t'\\).</p>"},{"location":"SDE/03b_ddim_update_coeff/#overview","title":"Overview","text":"<p>We'll derive the exact DDIM update formula through a clean \"dictionary derivation\":</p> <ol> <li>Choose a schedule: Concrete \\(\\beta(t)\\) and define \\(\\bar{\\alpha}(t)\\)</li> <li>Discretize time: Get discrete \\(\\bar{\\alpha}_k\\) values</li> <li>Forward conditional: Show how \\(q(x_{t'} \\mid x_t, x_0)\\) gives the DDIM form</li> <li>DDIM update: Derive the deterministic update and the \\(\\eta\\) noise term</li> </ol> <p>Goal: Understand why DDIM code has those specific square root coefficients and how they emerge from the continuous SDE.</p>"},{"location":"SDE/03b_ddim_update_coeff/#notation","title":"Notation","text":""},{"location":"SDE/03b_ddim_update_coeff/#continuous-time","title":"Continuous Time","text":"<ul> <li>Time: \\(t \\in [0, T]\\)</li> <li>VP-SDE forward:</li> </ul> <p>$$   dx = -\\frac{1}{2}\\beta(t) x\\,dt + \\sqrt{\\beta(t)}\\,dw   $$</p>"},{"location":"SDE/03b_ddim_update_coeff/#signal-survival-coefficient","title":"Signal Survival Coefficient","text":"\\[ \\boxed{\\bar{\\alpha}(t) := \\exp\\left(-\\int_0^t \\beta(s)\\,ds\\right)} \\]"},{"location":"SDE/03b_ddim_update_coeff/#forward-marginal-exact","title":"Forward Marginal (Exact)","text":"\\[ \\boxed{q(x_t \\mid x_0) = \\mathcal{N}\\left(\\sqrt{\\bar{\\alpha}(t)} x_0, (1 - \\bar{\\alpha}(t)) I\\right)} \\]"},{"location":"SDE/03b_ddim_update_coeff/#discrete-sampling-times","title":"Discrete Sampling Times","text":"<p>Pick any decreasing sequence:</p> \\[ t_N = T &gt; t_{N-1} &gt; \\cdots &gt; t_0 = 0 \\] <p>In code, these are often integer indices, but conceptually they're just a grid in \\([0, T]\\).</p>"},{"location":"SDE/03b_ddim_update_coeff/#shorthand","title":"Shorthand","text":"\\[ \\bar{\\alpha}_k := \\bar{\\alpha}(t_k), \\quad x_k := x(t_k) \\]"},{"location":"SDE/03b_ddim_update_coeff/#step-1-concrete-schedule-examples","title":"Step 1: Concrete Schedule Examples","text":"<p>To make \\(\\bar{\\alpha}(t)\\) tangible, let's look at two common schedules.</p>"},{"location":"SDE/03b_ddim_update_coeff/#example-a-constant-betat-beta_0","title":"Example A: Constant \\(\\beta(t) = \\beta_0\\)","text":"<p>Then:</p> \\[ \\bar{\\alpha}(t) = \\exp(-\\beta_0 t) \\] <p>Between two times \\(t\\) and \\(t'\\):</p> \\[ \\frac{\\bar{\\alpha}(t')}{\\bar{\\alpha}(t)} = e^{-\\beta_0 (t' - t)} \\] <p>Simple exponential decay of signal.</p>"},{"location":"SDE/03b_ddim_update_coeff/#example-b-linear-betat-beta_min-beta_max-beta_minfractt","title":"Example B: Linear \\(\\beta(t) = \\beta_{\\min} + (\\beta_{\\max} - \\beta_{\\min})\\frac{t}{T}\\)","text":"<p>Then:</p> \\[ \\int_0^t \\beta(s)\\,ds = \\beta_{\\min} t + \\frac{\\beta_{\\max} - \\beta_{\\min}}{2T} t^2 \\] <p>So:</p> \\[ \\bar{\\alpha}(t) = \\exp\\left(-\\beta_{\\min} t - \\frac{\\beta_{\\max} - \\beta_{\\min}}{2T} t^2\\right) \\] <p>Quadratic in the exponent (common in DDPM).</p>"},{"location":"SDE/03b_ddim_update_coeff/#connection-to-code","title":"Connection to Code","text":"<p>In practice, implementations often directly specify discrete \\(\\beta_k\\) and compute \\(\\bar{\\alpha}_k\\) by products:</p> \\[ \\bar{\\alpha}_k = \\prod_{i=1}^k (1 - \\beta_i) \\] <p>The continuous view tells you these products are approximating the integral.</p>"},{"location":"SDE/03b_ddim_update_coeff/#step-2-discretization-how-baralpha_k-becomes-code-constants","title":"Step 2: Discretization (How \\(\\bar{\\alpha}_k\\) Becomes Code Constants)","text":""},{"location":"SDE/03b_ddim_update_coeff/#continuous-formula","title":"Continuous Formula","text":"<p>On your chosen time grid, compute:</p> \\[ \\bar{\\alpha}_k = \\exp\\left(-\\int_0^{t_k} \\beta(s)\\,ds\\right) \\]"},{"location":"SDE/03b_ddim_update_coeff/#discrete-approximation","title":"Discrete Approximation","text":"<p>If you only have discrete \\(\\beta_k\\) at steps, approximate the integral:</p> \\[ \\int_0^{t_k} \\beta(s)\\,ds \\approx \\sum_{i=1}^k \\beta(t_i) \\Delta t_i \\] <p>Therefore:</p> \\[ \\bar{\\alpha}_k \\approx \\exp\\left(-\\sum_{i=1}^k \\beta(t_i) \\Delta t_i\\right) \\]"},{"location":"SDE/03b_ddim_update_coeff/#ddpm-product-form","title":"DDPM Product Form","text":"<p>In DDPM notation with small per-step \\(\\beta_i\\):</p> \\[ \\bar{\\alpha}_k = \\prod_{i=1}^k (1 - \\beta_i) \\approx \\exp\\left(-\\sum_{i=1}^k \\beta_i\\right) \\] <p>This matches the exponential-of-integral story!</p>"},{"location":"SDE/03b_ddim_update_coeff/#why-code-has-alphas_cumprod","title":"Why Code Has <code>alphas_cumprod</code>","text":"\\[ \\boxed{\\text{alphas\\_cumprod}[k] = \\bar{\\alpha}_k} \\] <p>This array is the discretized signal survival coefficient.</p>"},{"location":"SDE/03b_ddim_update_coeff/#step-3-the-key-conditional-heart-of-ddim","title":"Step 3: The Key Conditional (Heart of DDIM)","text":"<p>This is where DDIM's deterministic nature emerges.</p>"},{"location":"SDE/03b_ddim_update_coeff/#forward-marginal-at-time-t","title":"Forward Marginal at Time \\(t\\)","text":"\\[ x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I) \\]"},{"location":"SDE/03b_ddim_update_coeff/#solve-for-epsilon","title":"Solve for \\(\\epsilon\\)","text":"\\[ \\boxed{\\epsilon = \\frac{x_t - \\sqrt{\\bar{\\alpha}_t} x_0}{\\sqrt{1 - \\bar{\\alpha}_t}}} \\]"},{"location":"SDE/03b_ddim_update_coeff/#same-expression-at-earlier-time-t-t","title":"Same Expression at Earlier Time \\(t' &lt; t\\)","text":"\\[ x_{t'} = \\sqrt{\\bar{\\alpha}_{t'}} x_0 + \\sqrt{1 - \\bar{\\alpha}_{t'}} \\epsilon \\] <p>Key insight: Use the same \\(\\epsilon\\) at both times!</p>"},{"location":"SDE/03b_ddim_update_coeff/#substitute-epsilon","title":"Substitute \\(\\epsilon\\)","text":"\\[ x_{t'} = \\sqrt{\\bar{\\alpha}_{t'}} x_0 + \\sqrt{1 - \\bar{\\alpha}_{t'}} \\cdot \\frac{x_t - \\sqrt{\\bar{\\alpha}_t} x_0}{\\sqrt{1 - \\bar{\\alpha}_t}} \\]"},{"location":"SDE/03b_ddim_update_coeff/#group-terms","title":"Group Terms","text":"<p>Rearranging:</p> \\[ \\boxed{x_{t'} = \\underbrace{\\left(\\sqrt{\\bar{\\alpha}_{t'}} - \\sqrt{1 - \\bar{\\alpha}_{t'}} \\frac{\\sqrt{\\bar{\\alpha}_t}}{\\sqrt{1 - \\bar{\\alpha}_t}}\\right)}_{\\text{coefficient on } x_0} x_0 + \\underbrace{\\frac{\\sqrt{1 - \\bar{\\alpha}_{t'}}}{\\sqrt{1 - \\bar{\\alpha}_t}}}_{\\text{coefficient on } x_t} x_t} \\] <p>This is a deterministic mapping from \\(x_t\\) to \\(x_{t'}\\) given \\(x_0\\).</p> <p>But in generation, we don't have \\(x_0\\)\u2014so we estimate it!</p>"},{"location":"SDE/03b_ddim_update_coeff/#step-4-enter-the-model-estimate-x_0-or-epsilon","title":"Step 4: Enter the Model (Estimate \\(x_0\\) or \\(\\epsilon\\))","text":"<p>Most code predicts \\(\\epsilon_\\theta(x_t, t)\\). Define the estimated clean image:</p> \\[ \\boxed{\\hat{x}_0(x_t, t) = \\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_t}\\, \\epsilon_\\theta(x_t, t)}{\\sqrt{\\bar{\\alpha}_t}}} \\] <p>Equivalently, view it as defining a \"consistent\" noise:</p> \\[ \\hat{\\epsilon}_\\theta(x_t, t) := \\epsilon_\\theta(x_t, t) \\]"},{"location":"SDE/03b_ddim_update_coeff/#the-ddim-idea","title":"The DDIM Idea","text":"<p>Use the same \"direction\" \\(\\epsilon_\\theta\\) to move between noise levels, without injecting fresh noise.</p>"},{"location":"SDE/03b_ddim_update_coeff/#deterministic-ddim-update","title":"Deterministic DDIM Update","text":"<p>The update from \\(t\\) to \\(t'\\) is:</p> \\[ \\boxed{x_{t'} = \\sqrt{\\bar{\\alpha}_{t'}}\\, \\hat{x}_0(x_t, t) + \\sqrt{1 - \\bar{\\alpha}_{t'}}\\, \\epsilon_\\theta(x_t, t)} \\] <p>This is the classic DDIM update (deterministic, \\(\\eta = 0\\)).</p>"},{"location":"SDE/03b_ddim_update_coeff/#why-this-is-clean","title":"Why This is Clean","text":"<ol> <li>Replace signal amplitude: \\(\\sqrt{\\bar{\\alpha}_t} \\to \\sqrt{\\bar{\\alpha}_{t'}}\\)</li> <li>Replace noise amplitude: \\(\\sqrt{1 - \\bar{\\alpha}_t} \\to \\sqrt{1 - \\bar{\\alpha}_{t'}}\\)</li> <li>Keep the same predicted noise: \\(\\epsilon_\\theta\\)</li> </ol> <p>This is exactly \"ODE-like\": Follow one consistent path.</p>"},{"location":"SDE/03b_ddim_update_coeff/#fast-sampling","title":"Fast Sampling","text":"<ul> <li>If \\(t' = t - 1\\) (one step): Familiar single-step form</li> <li>If \\(t'\\) skips many steps: Fast DDIM sampler (e.g., 50 steps instead of 1000)</li> </ul>"},{"location":"SDE/03b_ddim_update_coeff/#step-5-add-the-eta-noise-knob-interpolating-toward-ddpm","title":"Step 5: Add the \\(\\eta\\) Noise Knob (Interpolating Toward DDPM)","text":"<p>DDPM-style sampling injects new Gaussian noise each step. DDIM introduces parameter \\(\\eta\\) to add controlled randomness.</p>"},{"location":"SDE/03b_ddim_update_coeff/#ddim-with-eta-parameter","title":"DDIM with \\(\\eta\\) Parameter","text":"\\[ \\boxed{x_{t'} = \\sqrt{\\bar{\\alpha}_{t'}}\\, \\hat{x}_0 + \\sqrt{1 - \\bar{\\alpha}_{t'} - \\sigma^2}\\, \\epsilon_\\theta + \\sigma z} \\] <p>where \\(z \\sim \\mathcal{N}(0, I)\\).</p>"},{"location":"SDE/03b_ddim_update_coeff/#noise-variance-sigma","title":"Noise Variance \\(\\sigma\\)","text":"<p>A common choice for \\(\\sigma\\) as a function of \\(\\eta\\) and the pair \\((t, t')\\):</p> \\[ \\boxed{\\sigma = \\eta \\sqrt{\\frac{1 - \\bar{\\alpha}_{t'}}{1 - \\bar{\\alpha}_t} \\left(1 - \\frac{\\bar{\\alpha}_t}{\\bar{\\alpha}_{t'}}\\right)}} \\]"},{"location":"SDE/03b_ddim_update_coeff/#the-spectrum","title":"The Spectrum","text":"\\(\\eta\\) \\(\\sigma\\) Behavior Sampling Type \\(0\\) \\(0\\) Deterministic Probability flow ODE \\((0, 1)\\) Intermediate Hybrid Interpolation \\(1\\) DDPM-like Stochastic Close to reverse SDE"},{"location":"SDE/03b_ddim_update_coeff/#what-this-does","title":"What This Does","text":"<ul> <li>\\(\\eta = 0 \\Rightarrow \\sigma = 0\\): Deterministic DDIM (probability flow ODE discretization)</li> <li>\\(\\eta &gt; 0\\): Injects some noise each step (more \"SDE-like\")</li> <li>\\(\\eta = 1\\): Lands close to DDPM stochasticity (though exact DDPM reverse variance has its own specific form)</li> </ul>"},{"location":"SDE/03b_ddim_update_coeff/#step-6-the-continuous-time-interpretation","title":"Step 6: The Continuous-Time Interpretation","text":"<p>Now we see the complete picture:</p>"},{"location":"SDE/03b_ddim_update_coeff/#the-chain","title":"The Chain","text":"<ol> <li>Continuous-time VP-SDE gives:</li> </ol> <p>$$    \\bar{\\alpha}(t) = \\exp\\left(-\\int_0^t \\beta(s)\\,ds\\right)    $$</p> <ol> <li>Discrete sampler chooses a grid \\(t_k\\) and uses:</li> </ol> <p>$$</p> <p>\\bar{\\alpha}_k = \\bar{\\alpha}(t_k)    $$</p> <ol> <li>DDIM is essentially:</li> <li>Use the model's score/noise direction</li> <li>Transport from \\(\\bar{\\alpha}_t\\) to \\(\\bar{\\alpha}_{t'}\\)</li> <li> <p>Without adding new randomness (ODE)</p> </li> <li> <p>DDPM corresponds to:</p> </li> <li>Stochastic discretization (SDE)</li> <li>Injecting noise consistent with the diffusion coefficient</li> </ol>"},{"location":"SDE/03b_ddim_update_coeff/#the-complete-connection","title":"The Complete Connection","text":"<pre><code>Continuous VP-SDE \u2192 \u0304\u03b1(t) = exp(-\u222b\u03b2) \u2192 Discrete \u0304\u03b1_k \u2192 DDIM/DDPM coefficients\n</code></pre>"},{"location":"SDE/03b_ddim_update_coeff/#mental-model-why-the-coefficients-feel-inevitable","title":"Mental Model: Why the Coefficients Feel Inevitable","text":""},{"location":"SDE/03b_ddim_update_coeff/#signal-survival-interpretation","title":"Signal Survival Interpretation","text":"<p>\\(\\bar{\\alpha}(t)\\) represents \"how much of \\(x_0\\) survives\" at noise level \\(t\\).</p> <p>Therefore:</p> <ul> <li>\\(\\sqrt{\\bar{\\alpha}(t)}\\): Signal amplitude</li> <li>\\(\\sqrt{1 - \\bar{\\alpha}(t)}\\): Noise amplitude</li> </ul>"},{"location":"SDE/03b_ddim_update_coeff/#the-ddim-insight","title":"The DDIM Insight","text":"<p>DDIM says: Keep the same estimated \\(x_0\\) and \\(\\epsilon\\), but change the amplitudes to match the new time.</p> \\[ \\text{Old time } t: \\quad x_t = \\sqrt{\\bar{\\alpha}_t}\\, x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\, \\epsilon \\] \\[ \\text{New time } t': \\quad x_{t'} = \\sqrt{\\bar{\\alpha}_{t'}}\\, x_0 + \\sqrt{1 - \\bar{\\alpha}_{t'}}\\, \\epsilon \\] <p>That's why those square roots look so clean\u2014they're literally amplitudes.</p>"},{"location":"SDE/03b_ddim_update_coeff/#summary","title":"Summary","text":"<p>We derived the exact DDIM update coefficients:</p> <ol> <li>Schedule: \\(\\bar{\\alpha}(t) = \\exp(-\\int_0^t \\beta(s)\\,ds)\\)</li> <li>Discretization: \\(\\bar{\\alpha}_k = \\bar{\\alpha}(t_k)\\) (the <code>alphas_cumprod</code> array)</li> <li>Key conditional: Same \\(\\epsilon\\) at different times</li> <li>DDIM update: \\(x_{t'} = \\sqrt{\\bar{\\alpha}_{t'}}\\, \\hat{x}_0 + \\sqrt{1 - \\bar{\\alpha}_{t'}}\\, \\epsilon_\\theta\\)</li> <li>\\(\\eta\\) parameter: Interpolates between ODE (deterministic) and SDE (stochastic)</li> </ol> <p>Key insight: The continuous VP-SDE theory directly determines the exact coefficients you see in DDIM code.</p>"},{"location":"SDE/03b_ddim_update_coeff/#related-documents","title":"Related Documents","text":"<ul> <li>Solving VP-SDE \u2014 Derivation of \\(\\bar{\\alpha}(t)\\)</li> <li>Reverse SDE &amp; Probability Flow ODE \u2014 Conceptual sampling framework</li> <li>DDPM from VP-SDE \u2014 Discrete-time perspective</li> <li>DDPM Foundations \u2014 Variational formulation</li> </ul>"},{"location":"SDE/03b_ddim_update_coeff/#references","title":"References","text":"<ol> <li>Song, J., Meng, C., &amp; Ermon, S. (2021). Denoising Diffusion Implicit Models. ICLR.</li> <li>Ho, J., Jain, A., &amp; Abbeel, P. (2020). Denoising Diffusion Probabilistic Models. NeurIPS.</li> <li>Song, Y., et al. (2021). Score-Based Generative Modeling through Stochastic Differential Equations. ICLR.</li> </ol>"},{"location":"VAE/","title":"Variational Autoencoders (VAE)","text":"<p>Comprehensive guide to VAEs for gene expression and count data modeling.</p>"},{"location":"VAE/#overview","title":"Overview","text":"<p>Variational Autoencoders (VAEs) are powerful generative models that learn compressed representations of high-dimensional data. For computational biology, VAEs are particularly useful for:</p> <ul> <li>Dimensionality reduction: Learn low-dimensional latent spaces for gene expression</li> <li>Denoising: Remove technical noise from single-cell data</li> <li>Generation: Create synthetic samples for data augmentation</li> <li>Perturbation prediction: Model drug responses and genetic perturbations</li> </ul> <p>This series covers VAE theory, implementation, and specialized variants for biological count data.</p>"},{"location":"VAE/#document-series","title":"Document Series","text":""},{"location":"VAE/#core-theory","title":"Core Theory","text":"Document Topic Key Concepts VAE-01: Overview Introduction to VAEs Encoder-decoder, latent space, variational inference VAE-02: ELBO Evidence Lower Bound Reconstruction + KL divergence, variational objective VAE-03: Inference Inference &amp; generation Posterior q(z|x), prior p(z), sampling"},{"location":"VAE/#gradient-estimation","title":"Gradient Estimation","text":"Document Topic Key Concepts VAE-04: Reparameterization Reparameterization trick Backprop through stochastic nodes VAE-05: Pathwise Derivative Pathwise gradient estimator Score function vs. pathwise VAE-05a: Pathwise Details Implementation details Practical considerations"},{"location":"VAE/#training-optimization","title":"Training &amp; Optimization","text":"Document Topic Key Concepts VAE-06: Optimization Training strategies KL annealing, batch normalization, regularization VAE Model Training Implementation guide PyTorch training loops, hyperparameters"},{"location":"VAE/#count-data-biology","title":"Count Data &amp; Biology","text":"Document Topic Key Concepts VAE-07: NB &amp; ZINB Count data decoders Negative Binomial, Zero-Inflated NB VAE-08: NB Likelihood NB loss derivation Gamma-Poisson mixture, dispersion"},{"location":"VAE/#applications","title":"Applications","text":"Document Topic Key Concepts VAE for Prediction Predictive modeling Conditioning, perturbation response VAE-09: Roadmap Extensions &amp; future work Hierarchical VAE, disentanglement, causal"},{"location":"VAE/#quick-start-guide","title":"Quick Start Guide","text":""},{"location":"VAE/#1-start-with-the-basics","title":"1. Start with the Basics","text":"<p>Read in order: 1. VAE-01: Overview - Understand the overall framework 2. VAE-02: ELBO - Learn the training objective 3. VAE-03: Inference - Understand latent space and sampling</p>"},{"location":"VAE/#2-understand-gradients","title":"2. Understand Gradients","text":"<p>Essential for implementation: 1. VAE-04: Reparameterization - The key trick for backprop 2. VAE-05: Pathwise Derivative - Why it works</p>"},{"location":"VAE/#3-train-your-first-vae","title":"3. Train Your First VAE","text":"<ol> <li>VAE-06: Optimization - Training strategies</li> <li>VAE Model Training - Hands-on implementation</li> </ol>"},{"location":"VAE/#4-handle-count-data","title":"4. Handle Count Data","text":"<p>For scRNA-seq and bulk RNA-seq: 1. VAE-07: NB &amp; ZINB - Specialized decoders 2. VAE-08: NB Likelihood - Mathematical details</p>"},{"location":"VAE/#5-build-applications","title":"5. Build Applications","text":"<ol> <li>VAE for Prediction - Perturbation modeling</li> <li>VAE-09: Roadmap - Advanced topics</li> </ol>"},{"location":"VAE/#vae-variants-implemented","title":"VAE Variants Implemented","text":""},{"location":"VAE/#conditional-vae-cvae","title":"Conditional VAE (CVAE)","text":"<p>Use case: Conditional generation (e.g., cell type \u2192 expression)</p> <pre><code>from genailab.model.vae import CVAE\n\nmodel = CVAE(\n    input_dim=2000,      # genes\n    latent_dim=10,       # compressed representation\n    condition_dim=5,     # cell types\n    hidden_dims=[512, 256]\n)\n</code></pre>"},{"location":"VAE/#cvae-with-negative-binomial-cvae_nb","title":"CVAE with Negative Binomial (CVAE_NB)","text":"<p>Use case: Single-cell RNA-seq (count data with overdispersion)</p> <pre><code>from genailab.model.vae import CVAE_NB\n\nmodel = CVAE_NB(\n    input_dim=2000,\n    latent_dim=10,\n    condition_dim=5\n)\n# Predicts mean \u03bc and dispersion r for NB distribution\n</code></pre>"},{"location":"VAE/#cvae-with-zero-inflated-nb-cvae_zinb","title":"CVAE with Zero-Inflated NB (CVAE_ZINB)","text":"<p>Use case: scRNA-seq with dropout (many zeros)</p> <pre><code>from genailab.model.vae import CVAE_ZINB\n\nmodel = CVAE_ZINB(\n    input_dim=2000,\n    latent_dim=10,\n    condition_dim=5\n)\n# Predicts \u03bc, r, and dropout probability \u03c0\n</code></pre>"},{"location":"VAE/#key-concepts","title":"Key Concepts","text":""},{"location":"VAE/#encoder-recognition-network","title":"Encoder (Recognition Network)","text":"<p>Purpose: Map data x to latent distribution q(z|x)</p> <pre><code>x (gene expression) \u2192 Neural Net \u2192 \u03bc_z, \u03c3_z \u2192 z ~ N(\u03bc_z, \u03c3_z\u00b2)\n</code></pre>"},{"location":"VAE/#decoder-generative-network","title":"Decoder (Generative Network)","text":"<p>Purpose: Map latent z back to data distribution p(x|z)</p> <pre><code>z (latent code) \u2192 Neural Net \u2192 reconstruction x\u0302\n</code></pre> <p>Decoder types: - Gaussian: MSE loss, for continuous data - Negative Binomial: For count data with variance &gt; mean - Zero-Inflated NB: For sparse count data (scRNA-seq)</p>"},{"location":"VAE/#elbo-evidence-lower-bound","title":"ELBO (Evidence Lower Bound)","text":"<p>Training objective:</p> \\[ \\mathcal{L} = \\underbrace{\\mathbb{E}_{q(z|x)}[\\log p(x|z)]}_{\\text{Reconstruction}} - \\underbrace{KL[q(z|x) || p(z)]}_{\\text{Regularization}} \\] <ul> <li>Reconstruction: How well can we regenerate x from z?</li> <li>KL divergence: How close is q(z|x) to prior p(z)?</li> </ul>"},{"location":"VAE/#applications-in-computational-biology","title":"Applications in Computational Biology","text":""},{"location":"VAE/#1-denoising-scrna-seq","title":"1. Denoising scRNA-seq","text":"<p>Problem: Technical noise, dropout, batch effects Solution: VAE learns clean latent representation Model: CVAE_ZINB with batch/cell type conditioning</p>"},{"location":"VAE/#2-drug-response-prediction","title":"2. Drug Response Prediction","text":"<p>Problem: Predict perturbed expression from baseline Solution: Conditional VAE with drug embeddings Model: CVAE conditioned on [baseline expression, drug ID, dose]</p>"},{"location":"VAE/#3-data-augmentation","title":"3. Data Augmentation","text":"<p>Problem: Limited training samples Solution: Generate synthetic samples from learned distribution Model: Sample from p(z), decode to get new x</p>"},{"location":"VAE/#4-batch-correction","title":"4. Batch Correction","text":"<p>Problem: Technical variation across experiments Solution: Learn batch-invariant latent space Model: CVAE with adversarial batch discriminator</p>"},{"location":"VAE/#5-cell-type-discovery","title":"5. Cell Type Discovery","text":"<p>Problem: Identify novel cell types Solution: Cluster in learned latent space Model: VAE \u2192 t-SNE/UMAP on z \u2192 clustering</p>"},{"location":"VAE/#comparison-vae-vs-other-generative-models","title":"Comparison: VAE vs. Other Generative Models","text":"Model Pros Cons Best For VAE Fast, stable, explicit latent space Can be blurry, mode averaging Representation learning, denoising GAN Sharp samples, high quality Training instability, mode collapse Image generation Diffusion High quality, stable training Slow sampling State-of-the-art generation Flow Exact likelihood, invertible Complex architecture Density estimation <p>For gene expression: VAE is often preferred for its: - Interpretable latent space - Fast inference (single forward pass) - Stable training - Uncertainty quantification</p>"},{"location":"VAE/#related-topics","title":"Related Topics","text":""},{"location":"VAE/#within-this-project","title":"Within This Project","text":"<ul> <li>DDPM - Diffusion models (slower but higher quality)</li> <li>Flow Matching - Continuous normalizing flows</li> <li>Beta-VAE - Disentangled representations</li> <li>Foundation Models - Pre-trained encoders</li> </ul>"},{"location":"VAE/#external-resources","title":"External Resources","text":"<ul> <li>scVI - Industry-standard VAE for scRNA-seq</li> <li>CPA (Compositional Perturbation Autoencoder) - Perturbation prediction</li> <li>Geneformer - Foundation model (can replace VAE encoder)</li> </ul>"},{"location":"VAE/#implementation-status","title":"Implementation Status","text":"Component Status Location CVAE (Gaussian) \u2705 Complete <code>src/genailab/model/vae.py</code> CVAE_NB \u2705 Complete <code>src/genailab/model/vae.py</code> CVAE_ZINB \u2705 Complete <code>src/genailab/model/vae.py</code> Training scripts \u2705 Complete <code>scripts/</code> Evaluation metrics \u2705 Complete <code>src/genailab/eval/</code> Interactive notebooks \ud83d\udccb Planned <code>notebooks/vae/</code>"},{"location":"VAE/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"VAE/#when-should-i-use-vae-vs-diffusion","title":"When should I use VAE vs. Diffusion?","text":"<p>Use VAE when: - Fast inference is critical - You need interpretable latent representations - Working with small-to-medium datasets - Uncertainty quantification is important</p> <p>Use Diffusion when: - Generation quality is top priority - You have large datasets and compute - Slow sampling (100+ steps) is acceptable</p>"},{"location":"VAE/#how-to-choose-latent-dimension","title":"How to choose latent dimension?","text":"<p>Guidelines: - Gene expression: 10-50 dims (10-20 typical for scRNA-seq) - Rule of thumb: Start with 10-20, increase if reconstruction is poor - Validate: Plot reconstruction error vs. latent dim</p>"},{"location":"VAE/#what-decoder-should-i-use","title":"What decoder should I use?","text":"Data Type Decoder Reason Normalized (log-transformed) Gaussian (MSE) Simple, fast Raw counts (bulk RNA-seq) Negative Binomial Handles overdispersion scRNA-seq (sparse) ZINB Handles zeros and overdispersion <p>Questions or suggestions? Open an issue on GitHub</p>"},{"location":"VAE/VAE-01-overview/","title":"01: Overview","text":"<p>VAEs are the Rosetta Stone between \u201cclassical\u201d latent-variable models and the modern generative zoo\u2014diffusion, EBMs, JEPA, world models all inherit pieces of this logic, even when they loudly deny it.</p> <p>Let\u2019s walk this in a systematic, model-builder\u2019s way, using the screenshot as our concrete anchor rather than a fog of abstractions.</p>"},{"location":"VAE/VAE-01-overview/#1-what-problem-a-vae-is-actually-solving","title":"1. What problem a VAE is actually solving","text":"<p>At heart, a VAE is doing approximate Bayesian inference in a latent-variable model.</p> <p>We posit a generative story:</p> <ul> <li>Sample a latent variable \\(z \\sim p(z)\\), usually \\(\\mathcal{N}(0, I)\\)</li> <li>Generate data \\(x \\sim p_\\theta(x \\mid z)\\)</li> </ul> <p>This defines a joint distribution:</p> \\[ p_\\theta(x, z) = p_\\theta(x \\mid z) \\cdot p(z) \\] <p>What we want is the marginal likelihood:</p> \\[ p_\\theta(x) = \\int p_\\theta(x \\mid z) p(z) \\, dz \\] <p>That integral is the villain of the story. Deep nets make it intractable.</p>"},{"location":"VAE/VAE-01-overview/#2-variational-inference-replacing-the-impossible-with-the-trainable","title":"2. Variational inference: replacing the impossible with the trainable","text":"<p>Enter the encoder\u2014not as a compression device, but as an approximate posterior:</p> \\[ q_\\phi(z \\mid x) \\approx p_\\theta(z \\mid x) \\] <p>Instead of solving Bayes\u2019 rule exactly, we learn a distribution that\u2019s close.</p> <p>This gives us the ELBO (Evidence Lower Bound):</p> \\[ \\log p_\\theta(x) \\geq \\underbrace{\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]}_{\\text{reconstruction}} - \\underbrace{\\mathrm{KL}(q_\\phi(z|x) \\| p(z))}_{\\text{regularization}} \\] <p>This equation is the soul of the VAE. Everything in your screenshot is just bookkeeping around this identity.</p>"},{"location":"VAE/VAE-01-overview/#3-why-the-encoder-outputs-and-not-a-point","title":"3. Why the encoder outputs \u03bc and \u03c3, not a point","text":"<p>A deterministic autoencoder learns:</p> \\[ x \\mapsto z \\mapsto \\hat{x} \\] <p>A VAE learns:</p> \\[ x \\mapsto (\\mu(x), \\sigma(x)) \\Rightarrow q_\\phi(z \\mid x) \\] <p>Usually:</p> \\[ q_\\phi(z \\mid x) = \\mathcal{N}(\\mu(x), \\text{diag}(\\sigma^2(x))) \\] <p>This matters because:</p> <ul> <li>We are learning a distribution over latent causes, not a code</li> <li>The KL term forces this distribution to stay near the prior</li> <li>Sampling becomes meaningful: different \\(z\\)'s produce different but plausible \\(x\\)'s</li> </ul> <p>Your screenshot\u2019s green (\\(\\mu\\)) and purple (\\(\\sigma\\)) blocks are not decoration\u2014they are the model.</p>"},{"location":"VAE/VAE-01-overview/#4-the-reparameterization-trick-making-randomness-differentiable","title":"4. The reparameterization trick: making randomness differentiable","text":"<p>Sampling is not differentiable:</p> \\[ z \\sim \\mathcal{N}(\\mu, \\sigma^2) \\] <p>So we rewrite it as:</p> \\[ \\epsilon \\sim \\mathcal{N}(0, I), \\quad z = \\mu + \\sigma \\odot \\epsilon \\] <p>Now:</p> <ul> <li>Randomness is isolated in \\(\\epsilon\\)</li> <li>\\(\\mu\\) and \\(\\sigma\\) are deterministic functions of \\(x\\)</li> <li>Gradients flow cleanly through the graph</li> </ul> <p>This is why the trick is not a hack\u2014it\u2019s a change of variables.</p> <p>In your screenshot, the blue \u03b5 block feeding into \\(\\mu + \\epsilon \\odot \\sigma\\) is the mathematical pivot that makes VAEs trainable.</p>"},{"location":"VAE/VAE-01-overview/#5-why-the-kl-term-has-that-closed-form","title":"5. Why the KL term has that closed form","text":"<p>Because both distributions are Gaussians:</p> \\[ q_\\phi(z|x) = \\mathcal{N}(\\mu, \\sigma^2), \\quad p(z) = \\mathcal{N}(0, I) \\] <p>The KL divergence reduces to:</p> \\[ \\mathrm{KL} = \\frac{1}{2} \\sum_{i=1}^{d} \\left( \\mu_i^2 + \\sigma_i^2 - \\log \\sigma_i^2 - 1 \\right) \\] <p>This term:</p> <ul> <li>Penalizes large means (keeps latent centered)</li> <li>Penalizes tiny variances (prevents memorization)</li> <li>Encourages smooth, well-filled latent space</li> </ul> <p>This is why VAEs interpolate nicely but look blurry: you\u2019re paying for continuity with entropy.</p>"},{"location":"VAE/VAE-01-overview/#6-decoder-likelihood-not-reconstruction-error","title":"6. Decoder: likelihood, not reconstruction \u201cerror\u201d","text":"<p>One subtle but crucial point:</p> <p>The decoder defines a likelihood model, not just a pixel predictor.</p> <p>Examples:</p> <ul> <li>Bernoulli \u2192 binary images</li> <li>Gaussian \u2192 continuous data</li> <li>Categorical \u2192 tokens</li> </ul> <p>So the reconstruction term is:</p> \\[ \\mathbb{E}_{q(z|x)}[\\log p_\\theta(x \\mid z)] \\] <p>Mean squared error is just one special case of a Gaussian likelihood with fixed variance.</p>"},{"location":"VAE/VAE-01-overview/#7-conditional-vae-cvae-controlled-generation","title":"7. Conditional VAE (cVAE): controlled generation","text":"<p>Now we add a condition \\(y\\):</p> <p>Generative story:</p> \\[ z \\sim p(z), \\quad x \\sim p_\\theta(x \\mid z, y) \\] <p>Inference:</p> \\[ q_\\phi(z \\mid x, y) \\] <p>ELBO:</p> \\[ \\mathbb{E}_{q(z|x,y)}[\\log p_\\theta(x|z,y)] - \\mathrm{KL}(q(z|x,y) \\| p(z)) \\] <p>Practically:</p> <ul> <li>Concatenate \\(y\\) to encoder input</li> <li>Concatenate \\(y\\) to decoder input</li> <li>Sampling with fixed \\(y\\) gives conditional generation</li> </ul> <p>This is the conceptual ancestor of classifier-free guidance in diffusion.</p>"},{"location":"VAE/VAE-01-overview/#8-mental-model-what-vaes-get-rightand-wrong","title":"8. Mental model: what VAEs get right\u2014and wrong","text":"<p>VAEs give you:</p> <ul> <li>Explicit latent variables</li> <li>Amortized inference</li> <li>A proper probabilistic objective</li> <li>A bridge between Bayesian models and deep nets</li> </ul> <p>They struggle with:</p> <ul> <li>Sharp details (entropy pressure)</li> <li>Expressive posteriors (Gaussian bottleneck)</li> <li>Multi-modal likelihoods</li> </ul> <p>Which is why the field evolved toward:</p> <ul> <li>Diffusion (score matching instead of likelihood)</li> <li>EBMs / JEPA (implicit densities, no decoder likelihood)</li> <li>World models (latent dynamics + control)</li> </ul>"},{"location":"VAE/VAE-01-overview/#9-how-this-connects-forward-to-your-roadmap","title":"9. How this connects forward to your roadmap","text":"<p>You can think of modern generative models as VAEs that rebelled:</p> <ul> <li>Diffusion: remove decoder likelihood, learn score instead</li> <li>EBMs / JEPA: drop explicit density, learn energy or invariants</li> <li>World models: put dynamics inside the latent space</li> <li>Flow matching: continuous reparameterization without KL</li> </ul> <p>But the core tension\u2014latent structure vs tractability\u2014starts here.</p>"},{"location":"VAE/VAE-01-overview/#suggested-next-step-logical-not-trendy","title":"Suggested next step (logical, not trendy)","text":"<p>Before jumping to diffusion, a powerful intermediate stop is:</p> <ul> <li>\u03b2-VAE (disentanglement vs capacity)</li> <li>IWAE (tightening the ELBO)</li> <li>VAE with learned priors (VampPrior)</li> </ul> <p>Those sharpen your intuition for why diffusion took over\u2014and what it sacrificed.</p> <p>When you\u2019re ready, we can deliberately \u201cbreak\u201d the VAE and watch diffusion emerge from the wreckage.</p>"},{"location":"VAE/VAE-01-overview/#10-references-further-reading","title":"10. References &amp; Further Reading","text":""},{"location":"VAE/VAE-01-overview/#foundational-papers","title":"Foundational Papers","text":"<ol> <li>Kingma &amp; Welling (2014) \u2014 \"Auto-Encoding Variational Bayes\" \u2014 The original VAE paper</li> <li>Rezende et al. (2014) \u2014 \"Stochastic Backpropagation and Approximate Inference in Deep Generative Models\"</li> <li>Sohn et al. (2015) \u2014 \"Learning Structured Output Representation using Deep Conditional Generative Models\" \u2014 cVAE</li> </ol>"},{"location":"VAE/VAE-01-overview/#vae-variants","title":"VAE Variants","text":"<ol> <li>Higgins et al. (2017) \u2014 \"\u03b2-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework\"</li> <li>Burda et al. (2016) \u2014 \"Importance Weighted Autoencoders\" (IWAE)</li> <li>Tomczak &amp; Welling (2018) \u2014 \"VAE with a VampPrior\"</li> </ol>"},{"location":"VAE/VAE-01-overview/#bridge-to-diffusion","title":"Bridge to Diffusion","text":"<ol> <li><code>dev/references/Principles of diffusion models.pdf</code> \u2014 Comprehensive treatment of score matching and diffusion</li> <li>Song &amp; Ermon (2019) \u2014 \"Generative Modeling by Estimating Gradients of the Data Distribution\"</li> <li>Ho et al. (2020) \u2014 \"Denoising Diffusion Probabilistic Models\" (DDPM)</li> </ol>"},{"location":"VAE/VAE-01-overview/#biology-applications","title":"Biology Applications","text":"<ol> <li>Lopez et al. (2018) \u2014 \"Deep generative modeling for single-cell transcriptomics\" (scVI)</li> <li>Lotfollahi et al. (2019) \u2014 \"scGen: Predicting single-cell perturbation responses\"</li> </ol>"},{"location":"VAE/VAE-01-overview/#learning-roadmap","title":"Learning Roadmap","text":"<p>See ROADMAP.md for the full progression from VAE \u2192 Diffusion \u2192 EBMs \u2192 JEPA \u2192 World Models.</p>"},{"location":"VAE/VAE-02-elbo/","title":"The ELBO: Derivation and Intuition","text":"<p>This document expands on the Evidence Lower Bound (ELBO), the central objective in Variational Autoencoders.</p>"},{"location":"VAE/VAE-02-elbo/#notation-reference","title":"Notation Reference","text":"Symbol Name Description \\(x\\) Data Observed data point (e.g., gene expression vector) \\(z\\) Latent Unobserved latent variable \\(\\theta\\) Decoder parameters Weights of the generative model \\(\\phi\\) Encoder parameters Weights of the inference network \\(p_\\theta(x \\mid z)\\) Likelihood Probability of data given latent (decoder) \\(p(z)\\) Prior Prior distribution over latents, typically \\(\\mathcal{N}(0, I)\\) \\(p_\\theta(x, z)\\) Joint Joint distribution \\(= p_\\theta(x \\mid z) \\cdot p(z)\\) \\(p_\\theta(x)\\) Marginal likelihood Evidence; what we want but can't compute \\(q_\\phi(z \\mid x)\\) Approximate posterior Encoder's guess at \\(p_\\theta(z \\mid x)\\) \\(\\mathrm{KL}(\\cdot \\| \\cdot)\\) KL divergence Measures \"distance\" between distributions"},{"location":"VAE/VAE-02-elbo/#1-the-elbo-equation","title":"1. The ELBO Equation","text":"<p>In words: The log-probability of the data is at least as large as the expected reconstruction quality minus the KL penalty.</p> <p>In math:</p> \\[ \\log p_\\theta(x) \\geq \\underbrace{\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]}_{\\text{reconstruction term}} - \\underbrace{\\mathrm{KL}(q_\\phi(z|x) \\| p(z))}_{\\text{regularization term}} \\] <p>Explanation:</p> <ul> <li> <p>Left side: \\(\\log p_\\theta(x)\\) is the log marginal likelihood (or \"evidence\"). This is what we want to maximize\u2014it measures how well our model explains the data.</p> </li> <li> <p>Right side: The ELBO (Evidence Lower BOund). Since we can't compute the left side directly, we maximize this lower bound instead.</p> </li> <li> <p>Reconstruction term: \\(\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]\\) asks: \"If I sample latents from my encoder \\(q_\\phi(z|x)\\), how well does my decoder \\(p_\\theta(x|z)\\) reconstruct the original data?\"</p> </li> <li> <p>KL term: \\(\\mathrm{KL}(q_\\phi(z|x) \\| p(z))\\) measures how far the encoder's distribution is from the prior. It penalizes encoders that stray too far from \\(\\mathcal{N}(0, I)\\).</p> </li> </ul>"},{"location":"VAE/VAE-02-elbo/#2-deriving-the-elbo-step-by-step","title":"2. Deriving the ELBO (Step by Step)","text":""},{"location":"VAE/VAE-02-elbo/#step-1-start-with-the-marginal-likelihood","title":"Step 1: Start with the marginal likelihood","text":"<p>In words: The probability of data \\(x\\) is obtained by integrating over all possible latent values \\(z\\).</p> <p>In math:</p> \\[ \\log p_\\theta(x) = \\log \\int p_\\theta(x, z) \\, dz \\] <p>Explanation: This integral is intractable for deep networks because we'd need to evaluate the decoder for every possible \\(z\\).</p>"},{"location":"VAE/VAE-02-elbo/#step-2-introduce-the-approximate-posterior","title":"Step 2: Introduce the approximate posterior","text":"<p>In words: Multiply and divide by \\(q_\\phi(z|x)\\) inside the integral\u2014this doesn't change the value.</p> <p>In math:</p> \\[ = \\log \\int q_\\phi(z|x) \\cdot \\frac{p_\\theta(x, z)}{q_\\phi(z|x)} \\, dz \\] <p>Explanation: This is a mathematical trick. We're rewriting the integral in a form that lets us use importance sampling.</p>"},{"location":"VAE/VAE-02-elbo/#step-3-rewrite-as-an-expectation","title":"Step 3: Rewrite as an expectation","text":"<p>In words: The integral over \\(q_\\phi(z|x)\\) is just an expectation under that distribution.</p> <p>In math:</p> \\[ = \\log \\mathbb{E}_{q_\\phi(z|x)} \\left[ \\frac{p_\\theta(x, z)}{q_\\phi(z|x)} \\right] \\] <p>Explanation: We've converted the integral into an expectation, which we can estimate by sampling.</p>"},{"location":"VAE/VAE-02-elbo/#step-4-apply-jensens-inequality","title":"Step 4: Apply Jensen's inequality","text":"<p>In words: The log of an expectation is at least as large as the expectation of the log.</p> <p>In math:</p> \\[ \\geq \\mathbb{E}_{q_\\phi(z|x)} \\left[ \\log \\frac{p_\\theta(x, z)}{q_\\phi(z|x)} \\right] \\] <p>Explanation: Jensen's inequality states that for a concave function (like \\(\\log\\)):</p> \\[ \\log \\mathbb{E}[X] \\geq \\mathbb{E}[\\log X] \\] <p>This is where the \"lower bound\" comes from\u2014we're trading equality for tractability.</p>"},{"location":"VAE/VAE-02-elbo/#step-5-expand-the-log-ratio","title":"Step 5: Expand the log ratio","text":"<p>In words: Split the log of a ratio into a difference of logs.</p> <p>In math:</p> \\[ = \\mathbb{E}_{q_\\phi(z|x)} \\left[ \\log p_\\theta(x, z) - \\log q_\\phi(z|x) \\right] \\] <p>Explanation: Using \\(\\log(a/b) = \\log a - \\log b\\).</p>"},{"location":"VAE/VAE-02-elbo/#step-6-factor-the-joint-distribution","title":"Step 6: Factor the joint distribution","text":"<p>In words: The joint \\(p_\\theta(x, z)\\) equals the likelihood times the prior.</p> <p>In math:</p> \\[ = \\mathbb{E}_{q_\\phi(z|x)} \\left[ \\log p_\\theta(x|z) + \\log p(z) - \\log q_\\phi(z|x) \\right] \\] <p>Explanation: We used \\(p_\\theta(x, z) = p_\\theta(x|z) \\cdot p(z)\\).</p>"},{"location":"VAE/VAE-02-elbo/#step-7-rearrange-into-elbo-form","title":"Step 7: Rearrange into ELBO form","text":"<p>In words: Group the terms to reveal reconstruction and KL components.</p> <p>In math:</p> \\[ = \\underbrace{\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]}_{\\text{reconstruction}} - \\underbrace{\\mathbb{E}_{q_\\phi(z|x)}\\left[\\log \\frac{q_\\phi(z|x)}{p(z)}\\right]}_{\\text{KL divergence}} \\] \\[ = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - \\mathrm{KL}(q_\\phi(z|x) \\| p(z)) \\] <p>This is the ELBO.</p>"},{"location":"VAE/VAE-02-elbo/#3-the-gap-what-are-we-losing","title":"3. The Gap: What Are We Losing?","text":"<p>In words: The difference between the true log-likelihood and the ELBO is exactly the KL divergence between the approximate and true posteriors.</p> <p>In math:</p> \\[ \\log p_\\theta(x) - \\text{ELBO} = \\mathrm{KL}(q_\\phi(z|x) \\| p_\\theta(z|x)) \\] <p>Explanation:</p> <ul> <li>\\(p_\\theta(z|x)\\) is the true posterior\u2014what Bayes' rule would give us if we could compute it.</li> <li>\\(q_\\phi(z|x)\\) is our approximate posterior\u2014what the encoder outputs.</li> <li>The gap is always \\(\\geq 0\\) (KL is non-negative).</li> <li>When \\(q_\\phi = p_\\theta\\), the gap is zero and ELBO equals the true log-likelihood.</li> </ul> <p>Implication: Maximizing the ELBO simultaneously: 1. Increases the marginal likelihood \\(p_\\theta(x)\\) 2. Pushes \\(q_\\phi(z|x)\\) toward the true posterior \\(p_\\theta(z|x)\\)</p>"},{"location":"VAE/VAE-02-elbo/#4-why-keep-qzx-close-to-pz","title":"4. Why Keep \\(q(z|x)\\) Close to \\(p(z)\\)?","text":"<p>Three practical reasons: sampling, generalization, and geometry.</p>"},{"location":"VAE/VAE-02-elbo/#reason-1-so-generation-is-possible-at-all","title":"Reason 1: So generation is possible at all","text":"<p>In words: At test time, we sample from the prior \\(p(z)\\), not from any encoder.</p> <p>In math:</p> \\[ z \\sim p(z) = \\mathcal{N}(0, I) \\quad \\Rightarrow \\quad x \\sim p_\\theta(x|z) \\] <p>Explanation: If the encoder learns to place latents in some weird region far from the prior, then sampling from \\(p(z)\\) lands you in \"dead space\" the decoder never saw. Result: garbage samples.</p> <p>Intuition: The KL term says: \"Don't hide all your data in a secret corner of latent space.\"</p>"},{"location":"VAE/VAE-02-elbo/#reason-2-it-prevents-memorization","title":"Reason 2: It prevents memorization","text":"<p>In words: Without KL, the encoder could assign each datapoint its own unique, sharply-peaked latent.</p> <p>Explanation: </p> <ul> <li>The encoder could make \\(q_\\phi(z|x)\\) extremely sharp (tiny \\(\\sigma\\)) and well-separated for each datapoint.</li> <li>This is like a lookup table: perfect reconstruction, but no generalization.</li> <li>The decoder learns to memorize, not to generate.</li> </ul> <p>What KL penalizes:</p> <ul> <li>Large \\(\\mu(x)\\): moving the mean far from the prior center</li> <li>Tiny \\(\\sigma(x)\\): collapsing the variance (over-confidence)</li> </ul>"},{"location":"VAE/VAE-02-elbo/#reason-3-it-makes-the-latent-space-smooth","title":"Reason 3: It makes the latent space smooth","text":"<p>In words: We want nearby latents to produce similar outputs.</p> <p>Explanation:</p> <ul> <li>If nearby \\(z\\)'s correspond to wildly different \\(x\\)'s, interpolation fails.</li> <li>Keeping \\(q\\) near a simple, smooth prior encourages a globally consistent latent geometry.</li> <li>This is why VAEs can interpolate between samples\u2014the latent space is \"well-organized.\"</li> </ul>"},{"location":"VAE/VAE-02-elbo/#5-the-two-terms-a-balancing-act","title":"5. The Two Terms: A Balancing Act","text":"Term Wants Risk if too strong Reconstruction Perfect reproduction of input Memorization, sharp posteriors KL Posteriors match prior Posterior collapse, blurry outputs <p>Posterior collapse: When KL dominates, the encoder learns \\(q_\\phi(z|x) \\approx p(z)\\) for all \\(x\\). The latent carries no information, and the decoder ignores it.</p> <p>The \u03b2-VAE insight: Multiply KL by \\(\\beta\\) to control this trade-off explicitly.</p>"},{"location":"VAE/VAE-02-elbo/#6-summary","title":"6. Summary","text":"<ol> <li>ELBO = Reconstruction \u2212 KL</li> <li>Reconstruction encourages the decoder to explain the data</li> <li>KL keeps the encoder honest and the latent space usable</li> <li>The gap between ELBO and true likelihood measures posterior approximation quality</li> <li>Maximizing ELBO improves both the generative model and the inference network</li> </ol>"},{"location":"VAE/VAE-02-elbo/#references","title":"References","text":"<ul> <li>VAE-01-overview.md \u2014 Main VAE theory</li> <li>VAE-03-inference.md \u2014 Why we introduce q(z|x)</li> <li>beta_vae.md \u2014 How \u03b2 controls the reconstruction-KL trade-off</li> <li>Kingma &amp; Welling (2014) \u2014 \"Auto-Encoding Variational Bayes\"</li> </ul>"},{"location":"VAE/VAE-03-inference/","title":"VAE: Why We Introduce q(z|x)","text":"<p>Addressing the fundamental question: \"Why are we allowed to just introduce a distribution over latent variables?\"</p>"},{"location":"VAE/VAE-03-inference/#1-the-natural-objection","title":"1. The Natural Objection","text":"<p>We introduced the posterior \\(q(z|x)\\) in order to derive the ELBO.</p> <p>And that immediately raises the natural objection:</p> <p>\"If our goal is to learn latent variables \\(z\\), why are we allowed to just introduce a distribution over them?\"</p> <p>This is the right question.</p>"},{"location":"VAE/VAE-03-inference/#2-the-key-clarification","title":"2. The Key Clarification","text":""},{"location":"VAE/VAE-03-inference/#we-do-not-introduce-qzx-because-we-want-it","title":"We do NOT introduce \\(q(z|x)\\) because we want it","text":"<p>We introduce it because we cannot compute the true posterior.</p> <p>In words: The true object of interest is the posterior \\(p_\\theta(z|x)\\).</p> <p>In math:</p> \\[ p_\\theta(z|x) = \\frac{p_\\theta(x|z) \\cdot p(z)}{p_\\theta(x)} \\] <p>The problem: The denominator requires:</p> \\[ p_\\theta(x) = \\int p_\\theta(x|z) \\, p(z) \\, dz \\] <p>which is intractable.</p> <p>So the logic is not:</p> <p>\"Let's introduce \\(q\\) because it's convenient.\"</p> <p>It is:</p> <p>\"Exact inference is impossible, so we must approximate it.\"</p> <p>This is classical variational inference, not a neural-network trick.</p>"},{"location":"VAE/VAE-03-inference/#3-what-is-actually-being-optimized","title":"3. What Is Actually Being Optimized","text":"<p>Let's separate variables, distributions, and parameters.</p>"},{"location":"VAE/VAE-03-inference/#latent-variable-z","title":"Latent variable \\(z\\)","text":"<ul> <li>\\(z\\) is a random variable</li> <li>It is not a parameter we optimize directly</li> </ul>"},{"location":"VAE/VAE-03-inference/#true-posterior-unavailable","title":"True posterior (unavailable)","text":"<ul> <li>\\(p_\\theta(z|x)\\) \u2014 what Bayes' rule would give us</li> <li>Depends on unknown normalization \\(p_\\theta(x)\\)</li> <li>Cannot be evaluated or sampled from</li> </ul>"},{"location":"VAE/VAE-03-inference/#variational-posterior-introduced","title":"Variational posterior (introduced)","text":"<ul> <li>\\(q_\\phi(z|x)\\) \u2014 our approximation</li> <li>Tractable, learnable</li> <li>Parameterized by a neural network (the encoder)</li> </ul> <p>The crucial distinction: We introduce \\(q(z|x)\\) not to replace \\(z\\), but to replace inference about \\(z\\).</p>"},{"location":"VAE/VAE-03-inference/#4-what-learning-z-really-means","title":"4. What \"Learning z\" Really Means","text":"<p>This sentence usually causes confusion:</p> <p>\"VAEs learn latent variables.\"</p> <p>What they actually do is:</p> <p>VAEs learn a conditional distribution over latent variables given data.</p> <p>In math:</p> \\[ z \\sim q_\\phi(z|x) \\] <p>So instead of learning:</p> <ul> <li>A single latent code \\(z_i\\) per datapoint</li> </ul> <p>We learn:</p> <ul> <li>A function that maps \\(x \\to\\) distribution over plausible \\(z\\)'s</li> </ul> <p>This is Bayesian inference, amortized across the dataset.</p>"},{"location":"VAE/VAE-03-inference/#5-why-introducing-qzx-is-mathematically-legitimate","title":"5. Why Introducing \\(q(z|x)\\) Is Mathematically Legitimate","text":"<p>Here's the clean justification:</p> <p>\"We introduce \\(q(z|x)\\) as an auxiliary distribution. This does not change the likelihood. It only allows us to rewrite it in a form we can optimize.\"</p> <p>Nothing is assumed about the data at this step \u2014 only about tractability.</p> <p>In math: The ELBO derivation starts with an exact identity:</p> \\[ \\log p(x) = \\log \\mathbb{E}_{q(z|x)} \\left[ \\frac{p(x, z)}{q(z|x)} \\right] \\] <p>This identity is exact, before Jensen's inequality.</p> <p>The approximation only enters when we lower-bound this expression, not when we introduce \\(q\\).</p>"},{"location":"VAE/VAE-03-inference/#6-what-assumptions-are-actually-being-made","title":"6. What Assumptions Are Actually Being Made","text":""},{"location":"VAE/VAE-03-inference/#assumptions-about-the-model","title":"Assumptions about the model","text":"<ul> <li>Data is generated from latent variables:</li> </ul> \\[ z \\sim p(z), \\quad x \\sim p_\\theta(x|z) \\] <ul> <li>The prior \\(p(z)\\) is simple (e.g., Gaussian)</li> </ul>"},{"location":"VAE/VAE-03-inference/#assumptions-about-inference","title":"Assumptions about inference","text":"<ul> <li>Exact posterior inference is intractable</li> <li>A parametric family \\(q_\\phi(z|x)\\) is expressive enough to approximate it</li> </ul>"},{"location":"VAE/VAE-03-inference/#what-is-not-assumed","title":"What is NOT assumed","text":"<ul> <li>That there is a single \"true\" latent code</li> <li>That the posterior is Gaussian in reality</li> <li>That \\(q(z|x)\\) is correct \u2014 only that it is optimizable</li> </ul>"},{"location":"VAE/VAE-03-inference/#7-known-vs-unknown-final-summary","title":"7. Known vs Unknown (Final Summary)","text":""},{"location":"VAE/VAE-03-inference/#known-fixed","title":"Known / Fixed","text":"What Value Observed data \\(x\\) Prior \\(p(z) = \\mathcal{N}(0, I)\\) Network architectures Encoder and decoder structure"},{"location":"VAE/VAE-03-inference/#unknown-learned","title":"Unknown / Learned","text":"What Learned by Decoder parameters \\(\\theta\\) Maximizing ELBO Encoder parameters \\(\\phi\\) Maximizing ELBO Latent geometry Emerges from training"},{"location":"VAE/VAE-03-inference/#random-sampled-not-learned","title":"Random (Sampled, Not Learned)","text":"What Role \\(z\\) Latent variable, sampled from $q_\\phi(z \\(\\epsilon\\) Noise for reparameterization, sampled from \\(\\mathcal{N}(0, I)\\)"},{"location":"VAE/VAE-03-inference/#8-the-one-sentence-that-resolves-everything","title":"8. The One Sentence That Resolves Everything","text":"<p>If you remember only one sentence, make it this:</p> <p>We are not learning latent variables directly \u2014 we are learning how to perform inference over latent variables.</p> <p>That sentence dissolves the apparent contradiction.</p>"},{"location":"VAE/VAE-03-inference/#9-why-this-matters-for-what-comes-next","title":"9. Why This Matters for What Comes Next","text":"<p>Once this clicks, the evolution of generative models becomes obvious:</p> Model Approach to Inference VAE Explicit approximate posterior $q_\\phi(z Diffusion Implicit posterior via score matching EBMs / JEPA No normalized posterior at all World models Latent inference + dynamics <p>But they all inherit this core move:</p> <p>Replace intractable inference with learnable inference.</p>"},{"location":"VAE/VAE-03-inference/#references","title":"References","text":"<ul> <li>VAE-01-overview.md \u2014 Main VAE theory</li> <li>VAE-02-elbo.md \u2014 ELBO derivation</li> <li>ROADMAP.md \u2014 Learning path to diffusion and beyond</li> </ul>"},{"location":"VAE/VAE-04-reparameterization/","title":"The Reparameterization Trick","text":"<p>How to make sampling differentiable for end-to-end training.</p>"},{"location":"VAE/VAE-04-reparameterization/#notation-reference","title":"Notation Reference","text":"Symbol Meaning \\(z\\) Latent variable (what we sample) \\(\\epsilon\\) Random noise from a fixed distribution (e.g., \\(\\mathcal{N}(0, I)\\)) \\(\\mu_\\phi(x)\\) Mean of the approximate posterior, output by encoder with parameters \\(\\phi\\) \\(\\sigma_\\phi(x)\\) Standard deviation of the approximate posterior \\(q_\\phi(z \\mid x)\\) Approximate posterior distribution \\(f(z)\\) Any function of \\(z\\) (e.g., \\(\\log p_\\theta(x \\mid z)\\)) \\(g_\\phi(\\epsilon)\\) Deterministic transformation: \\(g_\\phi(\\epsilon) = \\mu_\\phi + \\sigma_\\phi \\cdot \\epsilon\\)"},{"location":"VAE/VAE-04-reparameterization/#1-the-core-problem","title":"1. The Core Problem","text":"<p>In a VAE, we want to optimize an objective like:</p> \\[ \\mathbb{E}_{q_\\phi(z \\mid x)}[f(z)] \\] <p>In words: The expected value of some function \\(f(z)\\) where \\(z\\) is sampled from a distribution \\(q_\\phi(z \\mid x)\\) that depends on learnable parameters \\(\\phi\\).</p> <p>The problem: How do we differentiate this expectation with respect to \\(\\phi\\)?</p> <p>If you sample directly:</p> \\[ z \\sim \\mathcal{N}(\\mu_\\phi(x), \\sigma_\\phi^2(x)) \\] <p>then the sampling operation breaks the computational graph. Autograd sees a random number that \"appeared from nowhere\"\u2014no gradient path exists.</p>"},{"location":"VAE/VAE-04-reparameterization/#2-why-naive-sampling-is-not-differentiable","title":"2. Why Na\u00efve Sampling Is Not Differentiable","text":"<p>Consider this code:</p> <pre><code>z = Normal(mu, sigma).sample()\nloss = f(z)\n</code></pre> <p>The mapping \\((\\mu, \\sigma) \\to z\\) is stochastic, not deterministic.</p> <p>There is no path for:</p> \\[ \\frac{\\partial z}{\\partial \\mu}, \\quad \\frac{\\partial z}{\\partial \\sigma} \\] <p>because the randomness is inside the operation. Gradients stop at the sampling step.</p>"},{"location":"VAE/VAE-04-reparameterization/#3-the-reparameterization-trick-the-exact-move","title":"3. The Reparameterization Trick (The Exact Move)","text":"<p>Instead of sampling from the parameterized distribution, we rewrite the random variable as a deterministic transformation of parameter-free noise.</p> <p>Before (not differentiable):</p> \\[ z \\sim \\mathcal{N}(\\mu, \\sigma^2) \\] <p>After (differentiable):</p> \\[ \\epsilon \\sim \\mathcal{N}(0, I), \\quad z = \\mu + \\sigma \\odot \\epsilon \\] <p>Now:</p> <ul> <li>\\(\\epsilon\\) carries all the randomness and has no learnable parameters</li> <li>\\(\\mu\\) and \\(\\sigma\\) are deterministic outputs of the encoder</li> <li>\\(z\\) is a deterministic function of \\((\\mu, \\sigma, \\epsilon)\\)</li> </ul> <p>This restores the gradient path:</p> \\[ \\frac{\\partial z}{\\partial \\mu} = 1, \\quad \\frac{\\partial z}{\\partial \\sigma} = \\epsilon \\] <p>Gradients flow through the sample.</p>"},{"location":"VAE/VAE-04-reparameterization/#4-why-this-is-not-a-hack","title":"4. Why This Is Not a Hack","text":"<p>This is a change of variables, not a trick in the pejorative sense.</p>"},{"location":"VAE/VAE-04-reparameterization/#what-moving-randomness-means","title":"What \"moving randomness\" means","text":"<p>Consider the VAE encoder-decoder pipeline:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  BEFORE (randomness inside the model):                          \u2502\n\u2502                                                                  \u2502\n\u2502  x \u2192 Encoder \u2192 (\u03bc, \u03c3) \u2192 [SAMPLE z ~ N(\u03bc,\u03c3\u00b2)] \u2192 Decoder \u2192 x\u0302     \u2502\n\u2502                              \u2191                                   \u2502\n\u2502                         randomness here                          \u2502\n\u2502                         (breaks gradients)                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  AFTER (randomness at the input):                               \u2502\n\u2502                                                                  \u2502\n\u2502  \u03b5 ~ N(0,I) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                          \u2502\n\u2502                                      \u2193                          \u2502\n\u2502  x \u2192 Encoder \u2192 (\u03bc, \u03c3) \u2192 [z = \u03bc + \u03c3\u00b7\u03b5] \u2192 Decoder \u2192 x\u0302            \u2502\n\u2502                              \u2191                                   \u2502\n\u2502                    deterministic function                        \u2502\n\u2502                    (gradients flow through)                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>\"Inside the model\" = the sampling step \\(z \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) occurs between encoder and decoder, blocking gradients.</p> <p>\"Input of the model\" = the noise \\(\\epsilon\\) is sampled before any computation, then passed through as a regular input.</p> <p>The key insight: sampling \\(z \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) is mathematically equivalent to sampling \\(\\epsilon \\sim \\mathcal{N}(0, 1)\\) and computing \\(z = \\mu + \\sigma \\cdot \\epsilon\\). We've just rewritten the same random variable in a differentiable form.</p>"},{"location":"VAE/VAE-04-reparameterization/#5-what-the-reparameterization-trick-enables","title":"5. What the Reparameterization Trick Enables","text":""},{"location":"VAE/VAE-04-reparameterization/#the-key-identity","title":"The Key Identity","text":"<p>Define the reparameterization function:</p> \\[ g_\\phi(\\epsilon) = \\mu_\\phi + \\sigma_\\phi \\cdot \\epsilon \\] <p>This is a deterministic function that transforms noise \\(\\epsilon\\) into a sample \\(z\\) using the encoder parameters \\(\\phi\\).</p>"},{"location":"VAE/VAE-04-reparameterization/#derivation","title":"Derivation","text":"<p>We want to compute:</p> \\[ \\nabla_\\phi \\, \\mathbb{E}_{q_\\phi(z \\mid x)}[f(z)] \\] <p>Step 1: Rewrite \\(z\\) using the reparameterization:</p> \\[ z = g_\\phi(\\epsilon) = \\mu_\\phi + \\sigma_\\phi \\cdot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I) \\] <p>Step 2: Substitute into the expectation. Since \\(z\\) is now a deterministic function of \\(\\epsilon\\):</p> \\[ \\mathbb{E}_{q_\\phi(z \\mid x)}[f(z)] = \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0, I)}[f(g_\\phi(\\epsilon))] \\] <p>Step 3: Move the gradient inside the expectation (valid because \\(\\epsilon\\) doesn't depend on \\(\\phi\\)):</p> \\[ \\nabla_\\phi \\, \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0, I)}[f(g_\\phi(\\epsilon))] = \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0, I)}[\\nabla_\\phi f(g_\\phi(\\epsilon))] \\]"},{"location":"VAE/VAE-04-reparameterization/#the-result","title":"The Result","text":"\\[ \\nabla_\\phi \\, \\mathbb{E}_{q_\\phi(z \\mid x)}[f(z)] = \\mathbb{E}_{\\epsilon \\sim p(\\epsilon)}\\left[\\nabla_\\phi f(g_\\phi(\\epsilon))\\right] \\] <p>In words: The gradient of an expectation over a parameterized distribution equals the expectation of the gradient, computed by:</p> <ol> <li>Sampling noise \\(\\epsilon\\) from a fixed distribution</li> <li>Transforming it to \\(z = g_\\phi(\\epsilon)\\)</li> <li>Computing \\(\\nabla_\\phi f(z)\\) via standard backpropagation</li> </ol> <p>This gives us:</p> <ul> <li>Low-variance gradients (compared to REINFORCE)</li> <li>End-to-end backpropagation through the sampling step</li> <li>Scalable variational inference</li> </ul> <p>Without this, VAEs would not work in practice.</p>"},{"location":"VAE/VAE-04-reparameterization/#6-other-reparameterizable-distributions","title":"6. Other Reparameterizable Distributions","text":"<p>The trick generalizes beyond Gaussians.</p>"},{"location":"VAE/VAE-04-reparameterization/#location-scale-families","title":"Location-Scale Families","text":"<p>Any distribution of the form:</p> \\[ z = \\mu + \\sigma \\cdot \\epsilon \\] <p>Examples: Gaussian, Logistic, Laplace\u2014all reparameterizable.</p>"},{"location":"VAE/VAE-04-reparameterization/#log-normal","title":"Log-Normal","text":"\\[ z \\sim \\text{LogNormal}(\\mu, \\sigma) \\quad \\Rightarrow \\quad z = \\exp(\\mu + \\sigma \\cdot \\epsilon), \\quad \\epsilon \\sim \\mathcal{N}(0, 1) \\]"},{"location":"VAE/VAE-04-reparameterization/#gumbel-distribution","title":"Gumbel Distribution","text":"\\[ g = -\\log(-\\log u), \\quad u \\sim \\text{Uniform}(0, 1) \\] <p>This underlies Gumbel-Softmax for differentiable categorical sampling.</p>"},{"location":"VAE/VAE-04-reparameterization/#7-discrete-variables-when-reparameterization-fails","title":"7. Discrete Variables: When Reparameterization Fails","text":"<p>Discrete sampling (e.g., categorical variables) is not reparameterizable in the same clean way.</p>"},{"location":"VAE/VAE-04-reparameterization/#gumbel-softmax-concrete-distribution","title":"Gumbel-Softmax (Concrete Distribution)","text":"<p>An approximation that:</p> <ul> <li>Samples continuous noise</li> <li>Applies softmax with temperature</li> </ul> <p>This gives approximate differentiability with biased but low-variance gradients. It's inspired by reparameterization but not exact.</p>"},{"location":"VAE/VAE-04-reparameterization/#8-contrast-with-reinforce-score-function-estimator","title":"8. Contrast with REINFORCE (Score-Function Estimator)","text":"<p>Before reparameterization, people used the score-function estimator:</p> \\[ \\nabla_\\phi \\, \\mathbb{E}_{q_\\phi(z)}[f(z)] = \\mathbb{E}_{q_\\phi(z)}\\left[f(z) \\cdot \\nabla_\\phi \\log q_\\phi(z)\\right] \\] <p>This works for any distribution, but has:</p> <ul> <li>Extremely high variance</li> <li>Unstable training</li> <li>Slow convergence</li> </ul> <p>Reparameterization trades generality for smooth, low-variance gradients. This is why VAEs beat earlier variational methods in practice.</p>"},{"location":"VAE/VAE-04-reparameterization/#9-where-this-idea-shows-up-elsewhere","title":"9. Where This Idea Shows Up Elsewhere","text":""},{"location":"VAE/VAE-04-reparameterization/#normalizing-flows","title":"Normalizing Flows","text":"<p>Flows are pure reparameterization:</p> \\[ z = f_\\theta(\\epsilon), \\quad \\epsilon \\sim \\mathcal{N}(0, I) \\] <p>All expressivity comes from the transformation \\(f_\\theta\\).</p>"},{"location":"VAE/VAE-04-reparameterization/#diffusion-models","title":"Diffusion Models","text":"<p>Reverse diffusion is effectively sampling via deterministic transforms driven by injected noise. The conceptual lineage is direct.</p>"},{"location":"VAE/VAE-04-reparameterization/#policy-gradients-in-rl","title":"Policy Gradients in RL","text":"<p>In continuous-action RL (SAC, some PPO variants):</p> \\[ a = \\mu_\\theta(s) + \\sigma_\\theta(s) \\cdot \\epsilon \\] <p>This gives lower-variance gradients than REINFORCE.</p>"},{"location":"VAE/VAE-04-reparameterization/#bayesian-neural-networks","title":"Bayesian Neural Networks","text":"<p>Weight uncertainty:</p> \\[ w = \\mu + \\sigma \\cdot \\epsilon \\] <p>Makes uncertainty differentiable.</p>"},{"location":"VAE/VAE-04-reparameterization/#10-summary","title":"10. Summary","text":"<p>The reparameterization trick works by moving randomness from inside the model to the input, turning stochastic nodes into deterministic functions of noise.</p>"},{"location":"VAE/VAE-04-reparameterization/#11-connection-to-other-generative-models","title":"11. Connection to Other Generative Models","text":"Model How It Uses Reparameterization VAE Reparameterize latent sampling Flows Reparameterize entire distributions Diffusion Reparameterize generation as noise removal World Models Reparameterize uncertainty over dynamics <p>They all share the same theme: make uncertainty differentiable.</p>"},{"location":"VAE/VAE-04-reparameterization/#references","title":"References","text":"<ul> <li>VAE-01-overview.md \u2014 Main VAE theory</li> <li>VAE-02-elbo.md \u2014 ELBO derivation</li> <li>Kingma &amp; Welling (2014) \u2014 \"Auto-Encoding Variational Bayes\"</li> </ul>"},{"location":"VAE/VAE-05-pathwise-derivative/","title":"Pathwise Derivatives, Stochastic Calculus, and Autodiff","text":"<p>This document clarifies:</p> <ol> <li>Determinism vs. randomness \u2014 what backprop actually requires</li> <li>Pathwise derivatives vs. distributional derivatives \u2014 two different mathematical operations</li> <li>Why SDEs are not a counterexample \u2014 stochastic calculus and autodiff serve different purposes</li> <li>What diffusion models actually do \u2014 and how they relate to reparameterization</li> </ol>"},{"location":"VAE/VAE-05-pathwise-derivative/#1-the-precise-requirement-for-backpropagation","title":"1. The Precise Requirement for Backpropagation","text":""},{"location":"VAE/VAE-05-pathwise-derivative/#the-question","title":"The Question","text":"<p>\"For \\(\\partial z / \\partial \\mu\\) to be computable, does \\(\\mu\\) need to be deterministic?\"</p>"},{"location":"VAE/VAE-05-pathwise-derivative/#the-correct-answer","title":"The Correct Answer","text":"<p>Not quite. The precise statement is:</p> <p>For \\(\\partial z / \\partial \\mu\\) to be computable by backprop, \\(z\\) must be a deterministic function of \\(\\mu\\) given the source of randomness.</p> <p>This is called pathwise determinism.</p>"},{"location":"VAE/VAE-05-pathwise-derivative/#what-this-means","title":"What This Means","text":"<ul> <li>Randomness is allowed</li> <li>But it must be externalized \u2014 treated as an input, not generated inside the computation</li> <li>Given a fixed noise sample \\(\\epsilon\\), the mapping \\(\\mu \\to z\\) must be deterministic</li> </ul>"},{"location":"VAE/VAE-05-pathwise-derivative/#2-what-actually-breaks-gradients-in-naive-sampling","title":"2. What Actually Breaks Gradients in Na\u00efve Sampling","text":""},{"location":"VAE/VAE-05-pathwise-derivative/#the-problematic-code","title":"The Problematic Code","text":"<pre><code>z = Normal(mu, sigma).sample()\nloss = f(z)\n</code></pre> <p>Mathematically, this means:</p> \\[ z \\sim \\mathcal{N}(\\mu, \\sigma^2) \\]"},{"location":"VAE/VAE-05-pathwise-derivative/#why-autodiff-fails","title":"Why Autodiff Fails","text":"<p>The sampling operator is opaque to the computational graph:</p> <ul> <li>Autograd treats <code>sample()</code> as a black box</li> <li>There is no explicit functional relationship \\(z = g(\\mu, \\sigma, \\text{noise})\\)</li> <li>The graph sees no edge connecting \\(\\mu\\) to \\(z\\)</li> </ul> <pre><code>Computational graph (na\u00efve sampling):\n\n    \u03bc \u2500\u2500\u2500\u2500\u2500\u2500?\u2500\u2500\u2500\u2500\u2500\u2500&gt; z \u2500\u2500\u2500\u2500\u2500\u2500&gt; f(z) \u2500\u2500\u2500\u2500\u2500\u2500&gt; loss\n              \u2191\n         no defined path\n         (sampling is opaque)\n</code></pre> <p>Therefore:</p> \\[ \\frac{\\partial z}{\\partial \\mu} \\quad \\text{is undefined in autodiff} \\] <p>Not because math forbids it \u2014 but because the path is not represented in the graph.</p>"},{"location":"VAE/VAE-05-pathwise-derivative/#3-what-the-reparameterization-trick-actually-does","title":"3. What the Reparameterization Trick Actually Does","text":"<p>Rewrite the same random variable as:</p> \\[ \\epsilon \\sim \\mathcal{N}(0, I) \\quad \\text{(sampled once, treated as input)} \\] \\[ z = \\mu + \\sigma \\cdot \\epsilon \\quad \\text{(deterministic given } \\epsilon \\text{)} \\]"},{"location":"VAE/VAE-05-pathwise-derivative/#key-properties","title":"Key Properties","text":"<ul> <li>Randomness is now explicit and external</li> <li>\\(z\\) is a deterministic function of \\((\\mu, \\sigma, \\epsilon)\\)</li> <li>\\(\\epsilon\\) is treated as an input, not a parameter to differentiate through</li> </ul> <pre><code>Computational graph (reparameterized):\n\n    \u03b5 (external input)\n         \u2193\n    \u03bc \u2500\u2500\u2500\u2500\u2500\u2500&gt; z = \u03bc + \u03c3\u00b7\u03b5 \u2500\u2500\u2500\u2500\u2500\u2500&gt; f(z) \u2500\u2500\u2500\u2500\u2500\u2500&gt; loss\n         \u2191\n    \u03c3 \u2500\u2500\u2500\u2518\n\n    Clear paths: \u2202z/\u2202\u03bc = 1, \u2202z/\u2202\u03c3 = \u03b5\n</code></pre> <p>Now the gradient is well-defined:</p> \\[ \\frac{\\partial z}{\\partial \\mu} = 1, \\quad \\frac{\\partial z}{\\partial \\sigma} = \\epsilon \\]"},{"location":"VAE/VAE-05-pathwise-derivative/#the-critical-distinction","title":"The Critical Distinction","text":"<p>We are not differentiating through randomness. We are differentiating through a deterministic function that uses randomness as input.</p> <p>This distinction is everything.</p>"},{"location":"VAE/VAE-05-pathwise-derivative/#4-two-different-notions-of-stochastic-derivative","title":"4. Two Different Notions of \"Stochastic Derivative\"","text":"<p>This is where confusion often arises. There are two fundamentally different operations that both involve \"derivatives\" and \"randomness\":</p>"},{"location":"VAE/VAE-05-pathwise-derivative/#a-pathwise-derivatives-what-ml-uses","title":"(A) Pathwise Derivatives \u2014 What ML Uses","text":"<p>Goal: Compute \\(\\nabla_\\theta \\mathbb{E}_{z \\sim p_\\theta}[f(z)]\\)</p> <p>Method:</p> <ol> <li>Reparameterize: \\(z = g_\\theta(\\epsilon)\\) where \\(\\epsilon \\sim p(\\epsilon)\\) is fixed</li> <li>Differentiate the deterministic function \\(g_\\theta\\)</li> <li>Average over noise samples</li> </ol> \\[ \\nabla_\\theta \\mathbb{E}_{z \\sim p_\\theta}[f(z)] = \\mathbb{E}_{\\epsilon}\\left[\\nabla_\\theta f(g_\\theta(\\epsilon))\\right] \\] <p>Key property: Given \\(\\epsilon\\), everything is deterministic. Standard chain rule applies.</p>"},{"location":"VAE/VAE-05-pathwise-derivative/#b-stochastic-calculus-what-sdes-use","title":"(B) Stochastic Calculus \u2014 What SDEs Use","text":"<p>Goal: Define dynamics driven by continuous noise (Brownian motion)</p> <p>Method: It\u00f4 or Stratonovich calculus \u2014 special rules for integrating against nowhere-differentiable processes</p> \\[ dX_t = b(X_t)\\,dt + \\sigma(X_t)\\,dW_t \\] <p>Key property: \\(W_t\\) is nowhere differentiable. \"Derivatives\" are defined in an integral/distributional sense.</p>"},{"location":"VAE/VAE-05-pathwise-derivative/#comparison-table","title":"Comparison Table","text":"Aspect Pathwise (ML) Stochastic Calculus (SDEs) Noise Fixed sample \\(\\epsilon\\) Continuous process \\(W_t\\) Derivative Standard chain rule It\u00f4's lemma Result Deterministic gradient Distribution over paths Use case Backprop, VAEs, RL Physics, finance, diffusion"},{"location":"VAE/VAE-05-pathwise-derivative/#5-how-sdes-actually-handle-randomness","title":"5. How SDEs Actually Handle Randomness","text":"<p>Consider a stochastic differential equation:</p> \\[ dX_t = b(X_t)\\,dt + \\sigma(X_t)\\,dW_t \\]"},{"location":"VAE/VAE-05-pathwise-derivative/#key-facts-about-sdes","title":"Key Facts About SDEs","text":"<ol> <li>\\(W_t\\) (Brownian motion) is nowhere differentiable \u2014 you cannot write \\(dW_t/dt\\)</li> <li>Individual sample paths are not classically differentiable</li> <li>SDEs are defined in an integral sense, not as pointwise derivatives</li> </ol>"},{"location":"VAE/VAE-05-pathwise-derivative/#what-the-notation-actually-means","title":"What the Notation Actually Means","text":"<p>When we write \\(dX_t\\), we are not taking a derivative. We are defining:</p> \\[ X_t = X_0 + \\int_0^t b(X_s)\\,ds + \\int_0^t \\sigma(X_s)\\,dW_s \\] <p>The second integral is a stochastic integral (It\u00f4 integral), which requires special rules because \\(W_t\\) has infinite variation.</p>"},{"location":"VAE/VAE-05-pathwise-derivative/#itos-lemma-the-chain-rule-for-sdes","title":"It\u00f4's Lemma \u2014 The Chain Rule for SDEs","text":"<p>For a function \\(f(X_t)\\) where \\(X_t\\) follows an SDE:</p> \\[ df = \\frac{\\partial f}{\\partial x}\\,dX + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}\\sigma^2\\,dt \\] <p>The extra \\(\\frac{1}{2}\\sigma^2 f''\\) term is the It\u00f4 correction \u2014 it arises because \\(dW_t \\cdot dW_t = dt\\) (quadratic variation).</p> <p>This is fundamentally different from the standard chain rule.</p>"},{"location":"VAE/VAE-05-pathwise-derivative/#6-why-sde-theory-doesnt-help-naive-backprop","title":"6. Why SDE Theory Doesn't Help Na\u00efve Backprop","text":""},{"location":"VAE/VAE-05-pathwise-derivative/#what-autodiff-requires","title":"What Autodiff Requires","text":"<ul> <li>Explicit functional dependencies in a computational graph</li> <li>Pathwise gradients via chain rule</li> <li>Deterministic mappings given all inputs</li> </ul>"},{"location":"VAE/VAE-05-pathwise-derivative/#what-sde-theory-provides","title":"What SDE Theory Provides","text":"<ul> <li>Weak/distributional derivatives</li> <li>Distributions over paths</li> <li>Expectation-level results (e.g., Fokker-Planck equations)</li> </ul> <p>These live in different mathematical worlds.</p> <p>SDE machinery does not automatically give you gradients for:</p> <pre><code>z = sample(mu)  # Still opaque to autodiff\n</code></pre> <p>You still need to reparameterize to get pathwise gradients.</p>"},{"location":"VAE/VAE-05-pathwise-derivative/#7-how-diffusion-models-actually-work","title":"7. How Diffusion Models Actually Work","text":"<p>Diffusion models use SDE language but train with pathwise derivatives.</p>"},{"location":"VAE/VAE-05-pathwise-derivative/#the-forward-process-adding-noise","title":"The Forward Process (Adding Noise)","text":"\\[ dx_t = -\\frac{1}{2}\\beta(t)x_t\\,dt + \\sqrt{\\beta(t)}\\,dW_t \\] <p>This is an SDE \u2014 but we don't backprop through it. We just use it to generate noisy training data.</p>"},{"location":"VAE/VAE-05-pathwise-derivative/#the-reverse-process-denoising","title":"The Reverse Process (Denoising)","text":"\\[ dx_t = \\left[-\\frac{1}{2}\\beta(t)x_t - \\beta(t)\\nabla_x \\log p_t(x_t)\\right]dt + \\sqrt{\\beta(t)}\\,d\\bar{W}_t \\] <p>The score \\(\\nabla_x \\log p_t(x)\\) is approximated by a neural network \\(s_\\theta(x, t)\\).</p>"},{"location":"VAE/VAE-05-pathwise-derivative/#how-training-works","title":"How Training Works","text":"<p>Key insight: Training does NOT backprop through the SDE.</p> <p>Instead, it uses a denoising score matching objective:</p> \\[ \\mathcal{L}(\\theta) = \\mathbb{E}_{t, x_0, \\epsilon}\\left[\\|s_\\theta(x_t, t) - \\nabla_{x_t} \\log p(x_t | x_0)\\|^2\\right] \\] <p>where \\(x_t = \\alpha_t x_0 + \\sigma_t \\epsilon\\) is a reparameterized noisy sample.</p>"},{"location":"VAE/VAE-05-pathwise-derivative/#the-connection-to-reparameterization","title":"The Connection to Reparameterization","text":"<pre><code># Diffusion training (simplified)\nepsilon = torch.randn_like(x0)           # External noise\nx_t = alpha_t * x0 + sigma_t * epsilon   # Reparameterized!\npredicted_noise = model(x_t, t)\nloss = F.mse_loss(predicted_noise, epsilon)\nloss.backward()                          # Pathwise gradient!\n</code></pre> <p>This is the reparameterization trick at scale:</p> <ul> <li>Noise \\(\\epsilon\\) is sampled externally</li> <li>\\(x_t\\) is a deterministic function of \\((x_0, \\epsilon, t)\\)</li> <li>Gradients flow through the deterministic path</li> </ul>"},{"location":"VAE/VAE-05-pathwise-derivative/#sampling-inference","title":"Sampling (Inference)","text":"<p>At inference, we do solve an SDE/ODE:</p> \\[ dx_t = \\left[-\\frac{1}{2}\\beta(t)x_t - \\beta(t)s_\\theta(x_t, t)\\right]dt \\] <p>But we don't need gradients here \u2014 just forward simulation.</p>"},{"location":"VAE/VAE-05-pathwise-derivative/#8-reparameterization-in-other-domains","title":"8. Reparameterization in Other Domains","text":""},{"location":"VAE/VAE-05-pathwise-derivative/#reinforcement-learning-sac-td3","title":"Reinforcement Learning (SAC, TD3)","text":"\\[ a = \\mu_\\theta(s) + \\sigma_\\theta(s) \\cdot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I) \\] <p>Exactly the same trick \u2014 externalize noise, differentiate through the deterministic path.</p>"},{"location":"VAE/VAE-05-pathwise-derivative/#bayesian-neural-networks","title":"Bayesian Neural Networks","text":"\\[ w = \\mu + \\sigma \\cdot \\epsilon \\] <p>Weight uncertainty becomes differentiable.</p>"},{"location":"VAE/VAE-05-pathwise-derivative/#normalizing-flows","title":"Normalizing Flows","text":"\\[ z = f_\\theta(\\epsilon), \\quad \\epsilon \\sim \\mathcal{N}(0, I) \\] <p>Pure reparameterization \u2014 all expressivity in the transformation.</p>"},{"location":"VAE/VAE-05-pathwise-derivative/#9-summary-two-worlds-one-bridge","title":"9. Summary: Two Worlds, One Bridge","text":""},{"location":"VAE/VAE-05-pathwise-derivative/#the-core-insight","title":"The Core Insight","text":"<p>Backpropagation requires deterministic paths in the computational graph. The reparameterization trick works by making randomness an explicit input, so that the mapping from parameters to samples is deterministic conditioned on that noise.</p>"},{"location":"VAE/VAE-05-pathwise-derivative/#the-companion-insight","title":"The Companion Insight","text":"<p>Stochastic calculus defines derivatives in distribution or expectation, not in the pathwise sense required by autodiff.</p> <p>No contradiction \u2014 just different tools for different jobs.</p>"},{"location":"VAE/VAE-05-pathwise-derivative/#visual-summary","title":"Visual Summary","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  PATHWISE DERIVATIVES (what ML uses)                            \u2502\n\u2502                                                                  \u2502\n\u2502  1. Sample noise \u03b5 ~ N(0,I) once                                \u2502\n\u2502  2. Compute z = g_\u03b8(\u03b5) deterministically                        \u2502\n\u2502  3. Backprop through g_\u03b8 using standard chain rule              \u2502\n\u2502  4. Average gradients over many \u03b5 samples                       \u2502\n\u2502                                                                  \u2502\n\u2502  Result: \u2207_\u03b8 E[f(z)] \u2248 (1/N) \u03a3 \u2207_\u03b8 f(g_\u03b8(\u03b5\u1d62))                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  STOCHASTIC CALCULUS (what SDEs use)                            \u2502\n\u2502                                                                  \u2502\n\u2502  1. Define dynamics: dX = b(X)dt + \u03c3(X)dW                       \u2502\n\u2502  2. W_t is nowhere differentiable                               \u2502\n\u2502  3. Use It\u00f4's lemma (modified chain rule)                       \u2502\n\u2502  4. Results are distributions over paths                        \u2502\n\u2502                                                                  \u2502\n\u2502  Result: Fokker-Planck equations, path distributions            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"VAE/VAE-05-pathwise-derivative/#10-intuition-check","title":"10. Intuition Check","text":"<p>Think of randomness like an input image:</p> <ul> <li>You don't differentiate with respect to the pixels</li> <li>You differentiate with respect to the parameters that process the image</li> </ul> <p>Reparameterization treats noise the same way:</p> <ul> <li>\\(\\epsilon\\) is like input data \u2014 fixed during the forward/backward pass</li> <li>\\(\\theta\\) is what we optimize \u2014 gradients flow through the deterministic transformation</li> </ul>"},{"location":"VAE/VAE-05-pathwise-derivative/#references","title":"References","text":"<ul> <li>VAE-04-reparameterization.md \u2014 The reparameterization trick</li> <li>VAE-QA.md \u2014 Why the prior matters</li> <li>Kingma &amp; Welling (2014) \u2014 \"Auto-Encoding Variational Bayes\"</li> <li>Song et al. (2021) \u2014 \"Score-Based Generative Modeling through SDEs\"</li> </ul>"},{"location":"VAE/VAE-05a-pathwise-gradient-estimator/","title":"Deriving the Pathwise Gradient Estimator","text":"<p>This document builds the pathwise (reparameterization) gradient estimator step by step, then explains why the simple derivative \\(\\partial z / \\partial \\mu = 1\\) is the key insight.</p>"},{"location":"VAE/VAE-05a-pathwise-gradient-estimator/#step-0-the-object-we-want-to-differentiate","title":"Step 0: The Object We Want to Differentiate","text":"<p>We want gradients of an expectation where the distribution depends on parameters:</p> \\[ J(\\phi) = \\mathbb{E}_{z \\sim q_\\phi(z)}\\left[f(z)\\right] \\] <p>For VAEs: \\(f(z) = \\log p_\\theta(x \\mid z)\\), and \\(q_\\phi\\) is \\(q_\\phi(z \\mid x)\\).</p>"},{"location":"VAE/VAE-05a-pathwise-gradient-estimator/#step-1-reparameterize-the-random-variable","title":"Step 1: Reparameterize the Random Variable","text":"<p>Assume we can write samples as a deterministic transform of parameter-free noise:</p> \\[ \\epsilon \\sim p(\\epsilon) \\quad \\text{(does NOT depend on } \\phi \\text{)} \\] \\[ z = g_\\phi(\\epsilon) \\] <p>Example (Gaussian): \\(g_\\phi(\\epsilon) = \\mu_\\phi + \\sigma_\\phi \\odot \\epsilon\\), with \\(\\epsilon \\sim \\mathcal{N}(0, I)\\).</p> <p>Then:</p> \\[ J(\\phi) = \\mathbb{E}_{\\epsilon \\sim p(\\epsilon)}\\left[f(g_\\phi(\\epsilon))\\right] \\]"},{"location":"VAE/VAE-05a-pathwise-gradient-estimator/#step-2-differentiate-under-the-expectation-the-pathwise-move","title":"Step 2: Differentiate Under the Expectation (The \"Pathwise\" Move)","text":"<p>Because \\(p(\\epsilon)\\) doesn't depend on \\(\\phi\\), we can move the gradient inside:</p> \\[ \\nabla_\\phi J(\\phi) = \\mathbb{E}_{\\epsilon \\sim p(\\epsilon)}\\left[\\nabla_\\phi f(g_\\phi(\\epsilon))\\right] \\] <p>Now apply the chain rule:</p> \\[ \\nabla_\\phi f(g_\\phi(\\epsilon)) = \\underbrace{\\nabla_z f(z)}_{\\text{gradient of downstream loss}} \\bigg|_{z=g_\\phi(\\epsilon)} \\cdot \\underbrace{\\nabla_\\phi g_\\phi(\\epsilon)}_{\\text{how sample moves w.r.t. } \\phi} \\] <p>So the pathwise gradient estimator is:</p> \\[ \\nabla_\\phi J(\\phi) = \\mathbb{E}_{\\epsilon \\sim p(\\epsilon)}\\left[\\nabla_z f(g_\\phi(\\epsilon)) \\cdot \\nabla_\\phi g_\\phi(\\epsilon)\\right] \\] <p>And the Monte Carlo estimator with \\(K\\) samples is:</p> \\[ \\nabla_\\phi J(\\phi) \\approx \\frac{1}{K} \\sum_{k=1}^{K} \\nabla_z f(g_\\phi(\\epsilon_k)) \\cdot \\nabla_\\phi g_\\phi(\\epsilon_k), \\quad \\epsilon_k \\sim p(\\epsilon) \\] <p>That's the whole method: differentiate through the sampling path.</p>"},{"location":"VAE/VAE-05a-pathwise-gradient-estimator/#exercise-the-gaussian-case","title":"Exercise: The Gaussian Case","text":"<p>Take the Gaussian case:</p> \\[ z = \\mu + \\sigma \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 1) \\] <p>Assume \\(\\phi\\) parameterizes \\(\\mu\\) and \\(\\sigma\\).</p> <p>Question: What is \\(\\frac{\\partial z}{\\partial \\mu}\\)?</p> <p>Answer: It's 1.</p> <p>And that simple answer is actually the entire reason the reparameterization trick works.</p>"},{"location":"VAE/VAE-05a-pathwise-gradient-estimator/#1-why-partial-z-partial-mu-1-matters","title":"1. Why \\(\\partial z / \\partial \\mu = 1\\) Matters","text":"<p>You have:</p> \\[ z = \\mu + \\sigma \\epsilon \\] <p>Treat \\(\\epsilon\\) as a fixed input (a sampled number) during backprop.</p> <p>Then:</p> \\[ \\frac{\\partial z}{\\partial \\mu} = 1, \\qquad \\frac{\\partial z}{\\partial \\sigma} = \\epsilon \\] <p>These derivatives are:</p> <ul> <li>Well-defined</li> <li>Finite</li> <li>Independent of probability theory</li> </ul> <p>They are just ordinary calculus.</p> <p>This is the key:</p> <p>Once the randomness is made explicit, the gradient is completely classical.</p>"},{"location":"VAE/VAE-05a-pathwise-gradient-estimator/#2-contrast-with-naive-sampling","title":"2. Contrast with Na\u00efve Sampling","text":"<p>If instead you wrote:</p> \\[ z \\sim \\mathcal{N}(\\mu, \\sigma^2) \\] <p>there is no expression for:</p> \\[ \\frac{\\partial z}{\\partial \\mu} \\] <p>because:</p> <ul> <li>\"Sampling\" is not a mathematical function</li> <li>It hides randomness inside an opaque operation</li> </ul> <p>Autodiff has nothing to differentiate.</p> <p>So it's not that the derivative \"should\" be 1 mathematically \u2014 it's that you never gave the system a function to differentiate.</p>"},{"location":"VAE/VAE-05a-pathwise-gradient-estimator/#3-what-gradient-actually-flows-in-a-vae","title":"3. What Gradient Actually Flows in a VAE","text":"<p>Putting it together, the gradient w.r.t. \\(\\mu\\) becomes:</p> \\[ \\nabla_\\mu \\mathbb{E}[f(z)] \\approx \\frac{1}{K} \\sum_{k=1}^{K} \\underbrace{\\nabla_z f(z_k)}_{\\text{decoder gradient}} \\cdot \\underbrace{\\frac{\\partial z_k}{\\partial \\mu}}_{= 1} \\] <p>So gradients flow straight through the sample.</p> <ul> <li>No likelihood ratios</li> <li>No REINFORCE</li> <li>No variance explosion</li> </ul>"},{"location":"VAE/VAE-05a-pathwise-gradient-estimator/#4-why-its-called-a-pathwise-derivative","title":"4. Why It's Called a \"Pathwise\" Derivative","text":"<p>Each sampled \\(\\epsilon_k\\) defines a path:</p> \\[ \\mu \\longrightarrow z_k \\longrightarrow f(z_k) \\] <p>You differentiate along that path.</p> <p>That's why the name \"pathwise gradient estimator\" is more descriptive than \"reparameterization trick\".</p>"},{"location":"VAE/VAE-05a-pathwise-gradient-estimator/#5-a-subtle-but-crucial-point","title":"5. A Subtle but Crucial Point","text":"<p>During backprop:</p> <ul> <li>\\(\\epsilon\\) is treated as a constant</li> <li>We are not differentiating randomness</li> <li>We are differentiating a deterministic computation conditioned on noise</li> </ul> <p>This is why the intuition about \"determinism\" is right, but needs refinement: pathwise determinism, not absolute determinism.</p>"},{"location":"VAE/VAE-05a-pathwise-gradient-estimator/#6-where-this-fails","title":"6. Where This Fails","text":"<p>This only works when:</p> <ul> <li>You can write \\(z = g_\\phi(\\epsilon)\\)</li> <li>With \\(\\epsilon\\) independent of \\(\\phi\\)</li> </ul> <p>That's why:</p> <ul> <li>Discrete sampling breaks it \u2014 no continuous path to differentiate</li> <li>Gumbel-Softmax is an approximation \u2014 continuous relaxation of discrete</li> <li>Score-function estimators exist as a fallback \u2014 REINFORCE for non-reparameterizable cases</li> </ul>"},{"location":"VAE/VAE-05a-pathwise-gradient-estimator/#7-differentiating-sample-paths-in-sdes-clarification","title":"7. Differentiating Sample Paths in SDEs (Clarification)","text":"<p>There's potential confusion between \"pathwise derivatives\" in ML and \"sample paths\" in SDEs. Let's clarify.</p>"},{"location":"VAE/VAE-05a-pathwise-gradient-estimator/#in-ml-pathwise-along-a-fixed-noise-realization","title":"In ML: \"Pathwise\" = Along a Fixed Noise Realization","text":"<p>Given a fixed \\(\\epsilon\\), we have a deterministic path:</p> \\[ \\theta \\xrightarrow{g_\\theta} z \\xrightarrow{f} \\text{loss} \\] <p>We differentiate this path using the standard chain rule. The word \"path\" refers to the computational graph path.</p>"},{"location":"VAE/VAE-05a-pathwise-gradient-estimator/#in-sdes-sample-path-a-realization-of-a-stochastic-process","title":"In SDEs: \"Sample Path\" = A Realization of a Stochastic Process","text":"<p>A sample path \\(\\{X_t(\\omega)\\}_{t \\geq 0}\\) is one realization of the process, indexed by outcome \\(\\omega\\).</p> <p>Key differences:</p> Aspect ML Pathwise SDE Sample Path What varies Parameters \\(\\theta\\) Time \\(t\\) Noise Fixed \\(\\epsilon\\) Continuous \\(W_t(\\omega)\\) Differentiability Standard calculus Nowhere differentiable in \\(t\\) Goal \\(\\nabla_\\theta \\mathbb{E}[f(z)]\\) Describe evolution \\(X_t\\)"},{"location":"VAE/VAE-05a-pathwise-gradient-estimator/#why-sdes-need-special-calculus","title":"Why SDEs Need Special Calculus","text":"<p>For an SDE sample path \\(X_t(\\omega)\\):</p> \\[ X_t = X_0 + \\int_0^t b(X_s)\\,ds + \\int_0^t \\sigma(X_s)\\,dW_s \\] <ul> <li>The path \\(t \\mapsto X_t(\\omega)\\) is continuous but nowhere differentiable</li> <li>You cannot write \\(dX_t/dt\\) in the classical sense</li> <li>It\u00f4 calculus provides rules for manipulating these objects</li> </ul>"},{"location":"VAE/VAE-05a-pathwise-gradient-estimator/#the-confusion-resolved","title":"The Confusion Resolved","text":"<p>When ML papers say \"pathwise gradient,\" they mean:</p> <p>Differentiate w.r.t. parameters along a fixed noise realization</p> <p>When SDE papers say \"sample path,\" they mean:</p> <p>One realization of a continuous-time stochastic process</p> <p>These are different uses of the word \"path\":</p> <ul> <li>ML: path through the computational graph</li> <li>SDEs: path through state space over time</li> </ul>"},{"location":"VAE/VAE-05a-pathwise-gradient-estimator/#can-we-differentiate-sde-sample-paths-wrt-parameters","title":"Can We Differentiate SDE Sample Paths w.r.t. Parameters?","text":"<p>Yes! This is called sensitivity analysis or pathwise sensitivity:</p> \\[ \\frac{\\partial X_t}{\\partial \\theta} \\] <p>where \\(\\theta\\) parameterizes the drift \\(b_\\theta\\) or diffusion \\(\\sigma_\\theta\\).</p> <p>This requires solving an auxiliary SDE (the sensitivity equation):</p> \\[ d\\left(\\frac{\\partial X_t}{\\partial \\theta}\\right) = \\frac{\\partial b}{\\partial x}\\frac{\\partial X_t}{\\partial \\theta}\\,dt + \\frac{\\partial b}{\\partial \\theta}\\,dt + \\text{(diffusion terms)} \\] <p>This is computationally expensive and rarely used in ML. Instead, diffusion models use:</p> <ul> <li>Denoising score matching \u2014 avoids differentiating through the SDE</li> <li>Reparameterization at each timestep \u2014 \\(x_t = \\alpha_t x_0 + \\sigma_t \\epsilon\\)</li> </ul>"},{"location":"VAE/VAE-05a-pathwise-gradient-estimator/#8-summary","title":"8. Summary","text":"<p>The reparameterization trick works because it turns sampling into a differentiable computation graph with respect to model parameters.</p> <p>The key insight: \\(\\partial z / \\partial \\mu = 1\\) is just ordinary calculus once you externalize the noise.</p>"},{"location":"VAE/VAE-05a-pathwise-gradient-estimator/#references","title":"References","text":"<ul> <li>VAE-05-followup-1.md \u2014 Pathwise derivatives vs. stochastic calculus</li> <li>VAE-04-reparameterization.md \u2014 The reparameterization trick</li> <li>Kingma &amp; Welling (2014) \u2014 \"Auto-Encoding Variational Bayes\"</li> <li>Mohamed et al. (2020) \u2014 \"Monte Carlo Gradient Estimation in Machine Learning\"</li> </ul>"},{"location":"VAE/VAE-06-optimization/","title":"VAE Optimization: The ELBO and Gradient Flow","text":"<p>Core question: How does a VAE actually learn? What objective is being optimized, and how do gradients flow through both encoder and decoder?</p> <p>This document unpacks the optimization mechanics of VAEs \u2014 the single equation that governs training, how both networks are updated jointly, and why the reparameterization trick is essential. Understanding this is prerequisite for diagnosing issues like posterior collapse and for extending VAEs to conditional or \u03b2-VAE variants.</p>"},{"location":"VAE/VAE-06-optimization/#1-the-optimization-objective-the-one-equation-that-rules-them-all","title":"1. The Optimization Objective (The One Equation That Rules Them All)","text":"<p>A VAE is trained by maximizing the Evidence Lower Bound (ELBO) on the data likelihood.</p> <p>We start from the quantity we wish we could maximize directly:</p> \\[ \\log p_\\theta(x) \\] <p>This is the log probability that the generative model assigns to a datapoint \\(x\\) (gene expression vector, cell profile, etc.). Unfortunately, it involves an intractable integral over latent variables \\(z\\).</p> <p>So we introduce an approximate posterior \\(q_\\phi(z \\mid x)\\) and derive a lower bound:</p> \\[ \\log p_\\theta(x) \\;\\geq\\; \\mathbb{E}_{q_\\phi(z \\mid x)}[\\log p_\\theta(x \\mid z)] - \\mathrm{KL}\\left(q_\\phi(z \\mid x) \\,\\|\\, p(z)\\right) \\] <p>This right-hand side is the ELBO.</p> <p>Spelled out in words:</p> <ul> <li>First term (Reconstruction): How well can the decoder reconstruct the data from the latent code?</li> <li>Second term (KL Regularization): How close is the encoder's posterior to the prior we want the latent space to follow?</li> </ul> <p>Training a VAE means maximizing ELBO, or equivalently minimizing the negative ELBO as a loss.</p>"},{"location":"VAE/VAE-06-optimization/#2-is-elbo-used-in-both-encoder-and-decoder","title":"2. Is ELBO Used in Both Encoder and Decoder?","text":"<p>Yes \u2014 and this point is subtle but crucial.</p> <p>There is one single scalar objective (ELBO), but both networks appear inside it, so both are optimized jointly.</p>"},{"location":"VAE/VAE-06-optimization/#parameter-breakdown","title":"Parameter Breakdown","text":"<ul> <li>Decoder parameters \\(\\theta\\) appear in \\(\\log p_\\theta(x \\mid z)\\)</li> <li>Encoder parameters \\(\\phi\\) appear in \\(q_\\phi(z \\mid x)\\), which affects:</li> <li>The expectation (where we sample \\(z\\) from)</li> <li>The KL divergence term</li> </ul>"},{"location":"VAE/VAE-06-optimization/#gradient-flow-during-backpropagation","title":"Gradient Flow During Backpropagation","text":"<ul> <li>Gradients flow into decoder weights via reconstruction quality</li> <li>Gradients flow into encoder weights via:</li> <li>Reconstruction quality (through sampled \\(z\\))</li> <li>KL regularization</li> </ul> <p>No separate objectives, no alternating optimization by default. One ELBO, one joint training loop.</p>"},{"location":"VAE/VAE-06-optimization/#3-walking-through-optimization-step-by-step","title":"3. Walking Through Optimization Step-by-Step","text":"<p>Let's narrate a single forward + backward pass as if we were the optimizer.</p>"},{"location":"VAE/VAE-06-optimization/#step-1-encoder-outputs-q_phiz-mid-x","title":"Step 1: Encoder Outputs \\(q_\\phi(z \\mid x)\\)","text":"<p>You feed in a data point \\(x\\) (say a gene expression vector).</p> <p>The encoder outputs parameters of a distribution:</p> \\[ q_\\phi(z \\mid x) = \\mathcal{N}\\left(\\mu_\\phi(x), \\sigma_\\phi^2(x)\\right) \\] <p>Important: The encoder does not output \\(z\\). It outputs a distribution over possible latent codes. This is what makes the model Bayesian.</p>"},{"location":"VAE/VAE-06-optimization/#step-2-reparameterization-trick-so-gradients-dont-die","title":"Step 2: Reparameterization Trick (So Gradients Don't Die)","text":"<p>Sampling is not differentiable. So we rewrite:</p> \\[ z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\odot \\varepsilon \\quad\\text{where}\\quad \\varepsilon \\sim \\mathcal{N}(0, I) \\] <p>Now:</p> <ul> <li>Randomness is isolated in \\(\\varepsilon\\)</li> <li>\\(z\\) is a deterministic function of \\(\\phi\\)</li> </ul> <p>This lets gradients flow through the sample back into the encoder.</p>"},{"location":"VAE/VAE-06-optimization/#step-3-decoder-outputs-p_thetax-mid-z","title":"Step 3: Decoder Outputs \\(p_\\theta(x \\mid z)\\)","text":"<p>The sampled \\(z\\) is fed into the decoder.</p> <p>The decoder outputs parameters of a likelihood distribution:</p> Likelihood Use Case Gaussian Real-valued expression Poisson / Negative Binomial RNA-seq counts ZINB scRNA-seq with dropouts <p>So the decoder is not saying \"here is \\(x\\)\" \u2014 it is saying:</p> <p>\"Given \\(z\\), this is the probability distribution over possible \\(x\\).\"</p>"},{"location":"VAE/VAE-06-optimization/#step-4-compute-the-elbo","title":"Step 4: Compute the ELBO","text":"<p>Now we compute two quantities:</p>"},{"location":"VAE/VAE-06-optimization/#a-reconstruction-term","title":"(a) Reconstruction Term","text":"\\[ \\mathbb{E}_{q_\\phi(z \\mid x)}[\\log p_\\theta(x \\mid z)] \\] <p>In practice:</p> <ul> <li>One or a few Monte Carlo samples of \\(z\\)</li> <li>Log-likelihood under the decoder distribution</li> </ul> <p>This term:</p> <ul> <li>Trains the decoder to reconstruct data</li> <li>Trains the encoder to produce useful latent codes</li> </ul>"},{"location":"VAE/VAE-06-optimization/#b-kl-divergence","title":"(b) KL Divergence","text":"\\[ \\mathrm{KL}\\left(q_\\phi(z \\mid x) \\,\\|\\, p(z)\\right) \\] <p>This:</p> <ul> <li>Pulls latent representations toward the prior (usually \\(\\mathcal{N}(0,I)\\))</li> <li>Prevents pathological memorization</li> <li>Gives latent space semantic structure</li> </ul>"},{"location":"VAE/VAE-06-optimization/#step-5-backpropagation-the-unifying-moment","title":"Step 5: Backpropagation (The Unifying Moment)","text":"<p>The total loss is:</p> \\[ \\mathcal{L} = -\\mathbb{E}_{q_\\phi(z \\mid x)}[\\log p_\\theta(x \\mid z)] + \\mathrm{KL}\\left(q_\\phi(z \\mid x) \\,\\|\\, p(z)\\right) \\] <p>Gradients flow:</p> <ul> <li>From reconstruction \u2192 decoder parameters \\(\\theta\\)</li> <li>Through \\(z\\) \u2192 encoder parameters \\(\\phi\\)</li> <li>From KL \u2192 encoder parameters \\(\\phi\\)</li> </ul> <p>Everything is learned jointly via standard SGD/Adam.</p>"},{"location":"VAE/VAE-06-optimization/#4-conceptual-summary","title":"4. Conceptual Summary","text":"<p>A VAE is not:</p> <ul> <li>\"Encoder learns posterior, decoder learns reconstruction\" (too shallow)</li> </ul> <p>A VAE is:</p> <p>A single probabilistic model trained by maximizing a variational bound, where the encoder proposes latent explanations and the decoder judges how well those explanations generate the data, under a global pressure to keep the latent space simple and continuous.</p> <p>This framing scales beautifully to:</p> <ul> <li>scRNA-seq (scVI, scGen)</li> <li>Conditional VAEs (tissue, disease, batch)</li> <li>\u03b2-VAEs (disentanglement)</li> <li>Semi-supervised VAEs</li> <li>Diffusion\u2013VAE hybrids</li> </ul> <p>And it sets you up perfectly for score matching and diffusion, where the \"posterior\" idea gets replaced by something even sneakier.</p>"},{"location":"VAE/VAE-06-optimization/#next-steps","title":"Next Steps","text":"<p>Next natural topic: Why the KL term causes posterior collapse, and why scRNA-seq VAEs often weaken or reshape it.</p>"},{"location":"VAE/VAE-07-NB-ZINB/","title":"Choosing the Likelihood: NB vs ZINB for Gene Expression","text":"<p>This document explains why the choice of \\(p(x \\mid z)\\) matters for VAEs in computational biology, and when to use Negative Binomial (NB) vs Zero-Inflated Negative Binomial (ZINB).</p>"},{"location":"VAE/VAE-07-NB-ZINB/#1-the-core-principle","title":"1. The Core Principle","text":"<p>Choosing \\(p(x \\mid z)\\) means choosing a statistical noise model for your data.</p> <p>Once you choose it, maximum likelihood training forces a specific loss function.</p> <p>This is not an ML trick \u2014 it is straight probability theory.</p>"},{"location":"VAE/VAE-07-NB-ZINB/#2-what-is-x-in-gene-expression-modeling","title":"2. What is \\(x\\) in Gene Expression Modeling?","text":"<p>For RNA-seq, your observed data \\(x\\) is usually one of:</p> <ul> <li>Raw counts: non-negative integers (e.g., gene A has 0, 3, 17 reads)</li> <li>Normalized/transformed values: log1p(TPM), logCPM, etc. (continuous)</li> </ul> <p>This choice alone already restricts what likelihoods make sense.</p>"},{"location":"VAE/VAE-07-NB-ZINB/#3-why-gaussian-mse-is-often-wrong-for-counts","title":"3. Why Gaussian (\u2192 MSE) is Often Wrong for Counts","text":"<p>A Gaussian likelihood assumes:</p> <ul> <li>Data is continuous</li> <li>Symmetric noise</li> <li>Variance independent of the mean</li> <li>Can take negative values</li> </ul> <p>Raw RNA-seq counts violate all of these.</p> <p>If you write:</p> \\[ p(x \\mid z) = \\mathcal{N}(\\mu(z), \\sigma^2) \\] <p>you are implicitly saying:</p> <p>\"Gene expression fluctuates symmetrically around a mean, with constant variance.\"</p> <p>This is biologically false for counts.</p> <p>MSE is only acceptable after heavy preprocessing (log transforms), and even then it's an approximation.</p>"},{"location":"VAE/VAE-07-NB-ZINB/#4-the-negative-binomial-distribution-standard-interpretation","title":"4. The Negative Binomial Distribution: Standard Interpretation","text":""},{"location":"VAE/VAE-07-NB-ZINB/#41-classical-definition","title":"4.1 Classical Definition","text":"<p>The Negative Binomial distribution has two standard interpretations:</p> <p>Interpretation 1: Counting Failures</p> <p>Count the number of failures before observing \\(r\\) successes, where each trial has success probability \\(p\\).</p> \\[ X \\sim \\text{NB}(r, p) \\quad \\Rightarrow \\quad P(X = k) = \\binom{k + r - 1}{k} p^r (1-p)^k \\] <p>Interpretation 2: Gamma-Poisson Mixture (more relevant for ML)</p> <p>A Poisson distribution whose rate \\(\\lambda\\) is itself random, drawn from a Gamma distribution.</p> \\[ \\lambda \\sim \\text{Gamma}(\\theta, \\theta/\\mu) \\quad \\Rightarrow \\quad X \\mid \\lambda \\sim \\text{Poisson}(\\lambda) \\] <p>Marginalizing out \\(\\lambda\\) gives \\(X \\sim \\text{NB}(\\mu, \\theta)\\).</p> <p>This second interpretation is why NB is called an overdispersed Poisson \u2014 it adds extra variance beyond what Poisson allows.</p>"},{"location":"VAE/VAE-07-NB-ZINB/#42-why-nb-is-used-in-ml","title":"4.2 Why NB is Used in ML","text":"<p>NB appears whenever you have:</p> <ul> <li>Count data with more variance than Poisson predicts</li> <li>Heterogeneity in the underlying rate (different cells, users, events)</li> <li>Clustering or \"burstiness\" in arrivals</li> </ul> <p>Common applications:</p> Domain Phenomenon Modeled Genomics Gene expression counts (RNA-seq) NLP Word counts in documents Epidemiology Disease case counts E-commerce Purchase counts per user Insurance Claim counts per policy Ecology Species abundance counts <p>In all cases, the key property is:</p> \\[ \\text{Var}(X) = \\mu + \\frac{\\mu^2}{\\theta} &gt; \\mu \\] <p>This overdispersion (variance &gt; mean) is ubiquitous in real count data.</p>"},{"location":"VAE/VAE-07-NB-ZINB/#43-nb-in-neural-networks","title":"4.3 NB in Neural Networks","text":"<p>In deep learning, NB is used as the output distribution when:</p> <ol> <li>The target is non-negative integer counts</li> <li>A Poisson assumption underestimates variance</li> <li>You want a proper probabilistic model (not just MSE)</li> </ol> <p>The network predicts the parameters \\((\\mu, \\theta)\\), and the loss is the NB negative log-likelihood.</p>"},{"location":"VAE/VAE-07-NB-ZINB/#5-why-negative-binomial-nb-is-the-default-for-rna-seq","title":"5. Why Negative Binomial (NB) is the Default for RNA-seq","text":""},{"location":"VAE/VAE-07-NB-ZINB/#51-what-nb-models","title":"5.1 What NB Models","text":"<p>Negative Binomial models:</p> <ul> <li>Non-negative integer counts</li> <li>Variance that grows with the mean (overdispersion)</li> </ul> <p>This matches RNA-seq extremely well.</p> <p>Formally, for one gene \\(g\\):</p> \\[ x_g \\sim \\text{NB}(\\mu_g, \\alpha_g) \\] <p>where: * \\(\\mu_g\\) = mean (predicted by decoder) * \\(\\alpha_g\\) = dispersion parameter</p> <p>Key property \u2014 the mean-variance relationship:</p> \\[ \\text{Var}(x_g) = \\mu_g + \\alpha_g \\cdot \\mu_g^2 \\] <p>This captures:</p> <ul> <li>Biological variability</li> <li>Technical noise</li> <li>Sequencing depth effects</li> </ul>"},{"location":"VAE/VAE-07-NB-ZINB/#52-how-nb-becomes-the-loss-function","title":"5.2 How NB Becomes the Loss Function","text":"<p>In a VAE/cVAE, the decoder predicts \\(\\mu_g(z, y)\\) (and possibly \\(\\alpha_g\\)).</p> <p>The training objective includes:</p> \\[ \\log p(x \\mid z, y) = \\sum_{g} \\log \\text{NB}(x_g \\mid \\mu_g(z, y), \\alpha_g) \\] <p>So the reconstruction loss is the negative log-likelihood of the NB distribution.</p> <p>This replaces MSE. There is no freedom here:</p> <ul> <li>If you assume NB \u2192 maximum likelihood forces this loss</li> </ul>"},{"location":"VAE/VAE-07-NB-ZINB/#6-why-zinb-exists-and-when-you-need-it","title":"6. Why ZINB Exists (and When You Need It)","text":""},{"location":"VAE/VAE-07-NB-ZINB/#61-the-zero-problem","title":"6.1 The Zero Problem","text":"<p>In single-cell RNA-seq, you see many zeros:</p> <ul> <li>Some are true biological zeros (gene not expressed)</li> <li>Many are dropout (gene expressed but not captured)</li> </ul> <p>NB alone often underestimates zeros.</p>"},{"location":"VAE/VAE-07-NB-ZINB/#62-zero-inflated-nb-zinb","title":"6.2 Zero-Inflated NB (ZINB)","text":"<p>ZINB says:</p> <p>\"Some zeros come from the NB process, but others come from an extra 'always zero' process.\"</p> <p>Mathematically, for each gene:</p> \\[ p(x_g = 0) = \\pi_g + (1 - \\pi_g) \\cdot \\text{NB}(0 \\mid \\mu_g, \\alpha_g) \\] \\[ p(x_g = k) = (1 - \\pi_g) \\cdot \\text{NB}(k \\mid \\mu_g, \\alpha_g), \\quad k &gt; 0 \\] <p>The decoder now predicts three quantities:</p> Parameter Meaning \\(\\mu_g(z, y)\\) Mean expression \\(\\alpha_g\\) Dispersion \\(\\pi_g(z, y)\\) Zero-inflation probability <p>The likelihood of the observed expression is computed gene by gene using a zero-inflated Negative Binomial distribution whose parameters are predicted by the decoder network</p>"},{"location":"VAE/VAE-07-NB-ZINB/#7-when-to-use-nb-vs-zinb","title":"7. When to Use NB vs ZINB","text":""},{"location":"VAE/VAE-07-NB-ZINB/#use-nb-if","title":"Use NB if:","text":"<ul> <li>Bulk RNA-seq</li> <li>Pseudo-bulk scRNA</li> <li>UMI-based scRNA with good depth</li> <li>Zeros are mostly explained by low expression</li> </ul>"},{"location":"VAE/VAE-07-NB-ZINB/#use-zinb-if","title":"Use ZINB if:","text":"<ul> <li>Sparse scRNA-seq</li> <li>Very shallow sequencing</li> <li>Dropout dominates zero counts</li> <li>NB underfits zeros badly</li> </ul> <p>Practical advice: Many modern pipelines start with NB and only escalate to ZINB if diagnostics demand it.</p>"},{"location":"VAE/VAE-07-NB-ZINB/#8-where-do-nbzinb-apply-decoder-only","title":"8. Where Do NB/ZINB Apply? (Decoder Only)","text":"<p>Important clarification: NB and ZINB are used only in the decoder, not the encoder.</p>"},{"location":"VAE/VAE-07-NB-ZINB/#why","title":"Why?","text":"<p>The VAE has two parts:</p> Component Input Output Distribution Encoder \\(x\\) (observed data) \\(z\\) (latent) \\(q_\\phi(z \\mid x) = \\mathcal{N}(\\mu_\\phi(x), \\sigma_\\phi(x))\\) Decoder \\(z\\) (latent) \\(x\\) (reconstructed) \\(p_\\theta(x \\mid z) = \\text{NB}(\\mu_\\theta(z), \\alpha)\\) <ul> <li>Encoder: Maps data to latent space. The latent \\(z\\) is continuous and Gaussian \u2014 this is required for the reparameterization trick.</li> <li>Decoder: Maps latent back to data space. The likelihood \\(p(x \\mid z)\\) must match the data type \u2014 hence NB/ZINB for counts.</li> </ul>"},{"location":"VAE/VAE-07-NB-ZINB/#the-encoder-stays-gaussian","title":"The Encoder Stays Gaussian","text":"<p>Even when modeling count data:</p> \\[ q_\\phi(z \\mid x) = \\mathcal{N}(\\mu_\\phi(x), \\text{diag}(\\sigma_\\phi^2(x))) \\] <p>This is because:</p> <ol> <li>Reparameterization requires continuous distributions \u2014 you can't easily reparameterize discrete distributions</li> <li>The latent space is a learned representation \u2014 it doesn't need to match the data distribution</li> <li>KL divergence is tractable \u2014 Gaussian-to-Gaussian KL has a closed form</li> </ol>"},{"location":"VAE/VAE-07-NB-ZINB/#summary","title":"Summary","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  ENCODER: q(z|x)                                            \u2502\n\u2502  \u2022 Always Gaussian (for reparameterization)                 \u2502\n\u2502  \u2022 Input: count data x                                      \u2502\n\u2502  \u2022 Output: continuous latent z                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  DECODER: p(x|z)                                            \u2502\n\u2502  \u2022 NB or ZINB (matches data type)                           \u2502\n\u2502  \u2022 Input: continuous latent z                               \u2502\n\u2502  \u2022 Output: count data x                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"VAE/VAE-07-NB-ZINB/#why-gaussian-prior-even-with-nbzinb-decoder","title":"Why Gaussian Prior Even with NB/ZINB Decoder?","text":"<p>You might wonder: if we're modeling count data with NB/ZINB, why is the KL divergence still computed against a Gaussian prior \\(p(z) = \\mathcal{N}(0, I)\\)?</p> <p>Key insight: The latent space \\(z\\) is separate from the data space \\(x\\).</p> <p>The NB/ZINB likelihood only affects \\(p(x \\mid z)\\) \u2014 how we model the output. The latent \\(z\\) is a learned representation, not the data itself.</p> <p>Why keep the latent Gaussian?</p> <ol> <li>Reparameterization trick requires continuous, differentiable sampling</li> <li>You can't easily backpropagate through discrete distributions</li> <li> <p>Gaussian allows \\(z = \\mu + \\sigma \\cdot \\epsilon\\) where \\(\\epsilon \\sim \\mathcal{N}(0, I)\\)</p> </li> <li> <p>The prior \\(p(z) = \\mathcal{N}(0, I)\\) is a regularizer</p> </li> <li>Prevents the encoder from \"cheating\" by encoding each sample at a unique, far-away point</li> <li>Forces the latent space to be smooth and interpolable</li> <li> <p>Without it, the VAE degenerates into a deterministic autoencoder</p> </li> <li> <p>Tractable KL divergence</p> </li> <li>Gaussian-to-Gaussian KL has a closed form:      $\\(\\text{KL}(q \\| p) = -\\frac{1}{2} \\sum_j \\left(1 + \\log \\sigma_j^2 - \\mu_j^2 - \\sigma_j^2\\right)\\)$</li> <li>No sampling needed for the KL term</li> </ol> <p>What would happen without the Gaussian prior?</p> <ul> <li>The encoder could map each training sample to a unique, isolated point</li> <li>The latent space would have \"holes\" \u2014 regions with no training data</li> <li>Sampling from \\(p(z)\\) would generate garbage</li> <li>No smooth interpolation between samples</li> </ul> <p>Summary: The decoder likelihood (NB/ZINB) determines how we model the data. The encoder prior (Gaussian) determines how we regularize the latent space. These are independent design choices.</p>"},{"location":"VAE/VAE-07-NB-ZINB/#9-summary-likelihood-loss","title":"9. Summary: Likelihood \u2192 Loss","text":"Data Representation Likelihood \\(p(x \\mid z)\\) Loss Function log1p(TPM) Gaussian MSE Counts (bulk) NB NB NLL Counts (scRNA) ZINB ZINB NLL <p>You are not choosing a \"loss function\" \u2014 you are choosing a data-generating assumption.</p> <p>Note on <code>log1p(TPM)</code> notation:</p> <p><code>log1p(x) = log(1 + x)</code></p> <p>Why use <code>log1p</code> instead of <code>log</code>?</p> <ul> <li>Handles zeros: <code>log1p(0) = log(1) = 0</code> (vs <code>log(0) = -\u221e</code>)</li> <li>Reduces skewness: Compresses high values more than low values</li> <li>Stabilizes variance: Makes variance less dependent on the mean</li> </ul>"},{"location":"VAE/VAE-07-NB-ZINB/#10-why-this-motivates-score-matching-preview","title":"10. Why This Motivates Score Matching (Preview)","text":"<p>You're now seeing the pressure point:</p> <ul> <li>Biology data is messy</li> <li>NB vs ZINB is already a modeling compromise</li> <li>Wrong likelihood \u2192 biased gradients everywhere</li> </ul> <p>Score matching later says:</p> <p>\"Let's stop committing to a precise likelihood.\"</p> <p>But you cannot appreciate that move until you fully understand NB/ZINB \u2014 which is what this document covers.</p>"},{"location":"VAE/VAE-07-NB-ZINB/#11-key-takeaway","title":"11. Key Takeaway","text":"<p>Choosing \\(p(x \\mid z)\\) is choosing how you believe noise enters your biological measurements; maximum likelihood then forces the corresponding loss.</p>"},{"location":"VAE/VAE-07-NB-ZINB/#next-steps","title":"Next Steps","text":"<p>The next document (VAE-08-NB-likelihood.md) covers:</p> <ol> <li>Write out the NB log-likelihood term explicitly</li> <li>Compare it side-by-side with MSE</li> <li>Interpret each term biologically (mean, dispersion, zeros)</li> </ol>"},{"location":"VAE/VAE-07-NB-ZINB/#references","title":"References","text":"<ul> <li>VAE-02-elbo.md \u2014 ELBO derivation</li> <li>VAE-08-NB-likelihood.md \u2014 NB log-likelihood derivation</li> <li>Lopez et al. (2018) \u2014 \"Deep generative modeling for single-cell transcriptomics\" (scVI)</li> <li>Eraslan et al. (2019) \u2014 \"Single-cell RNA-seq denoising using a deep count autoencoder\" (DCA)</li> <li>Cameron &amp; Trivedi (2013) \u2014 \"Regression Analysis of Count Data\"</li> </ul>"},{"location":"VAE/VAE-08-NB-likelihood/","title":"The Negative Binomial Log-Likelihood: Derivation and Biological Interpretation","text":"<p>This document derives the NB log-likelihood explicitly, compares it with MSE, and interprets each term biologically.</p>"},{"location":"VAE/VAE-08-NB-likelihood/#1-the-negative-binomial-distribution","title":"1. The Negative Binomial Distribution","text":""},{"location":"VAE/VAE-08-NB-likelihood/#11-probability-mass-function","title":"1.1 Probability Mass Function","text":"<p>For a count \\(x \\in \\{0, 1, 2, \\ldots\\}\\), the Negative Binomial PMF is:</p> \\[ \\text{NB}(x \\mid \\mu, \\alpha) = \\binom{x + r - 1}{x} \\left(\\frac{r}{r + \\mu}\\right)^r \\left(\\frac{\\mu}{r + \\mu}\\right)^x \\] <p>where: * \\(\\mu\\) = mean * \\(r = 1/\\alpha\\) = \"number of failures\" parameter (inverse dispersion) * \\(\\alpha\\) = dispersion parameter</p>"},{"location":"VAE/VAE-08-NB-likelihood/#12-alternative-parameterization-more-common-in-ml","title":"1.2 Alternative Parameterization (More Common in ML)","text":"<p>Using \\(\\theta = 1/\\alpha\\) (inverse dispersion):</p> \\[ \\text{NB}(x \\mid \\mu, \\theta) = \\frac{\\Gamma(x + \\theta)}{\\Gamma(\\theta) \\cdot x!} \\left(\\frac{\\theta}{\\theta + \\mu}\\right)^\\theta \\left(\\frac{\\mu}{\\theta + \\mu}\\right)^x \\]"},{"location":"VAE/VAE-08-NB-likelihood/#13-mean-and-variance","title":"1.3 Mean and Variance","text":"\\[ \\mathbb{E}[x] = \\mu \\] \\[ \\text{Var}(x) = \\mu + \\frac{\\mu^2}{\\theta} = \\mu + \\alpha \\mu^2 \\] <p>Key insight: Variance grows faster than the mean. This is overdispersion.</p>"},{"location":"VAE/VAE-08-NB-likelihood/#2-the-nb-log-likelihood","title":"2. The NB Log-Likelihood","text":"<p>Taking the log of the PMF:</p> \\[ \\log \\text{NB}(x \\mid \\mu, \\theta) = \\log \\Gamma(x + \\theta) - \\log \\Gamma(\\theta) - \\log(x!) + \\theta \\log\\left(\\frac{\\theta}{\\theta + \\mu}\\right) + x \\log\\left(\\frac{\\mu}{\\theta + \\mu}\\right) \\]"},{"location":"VAE/VAE-08-NB-likelihood/#21-simplified-form","title":"2.1 Simplified Form","text":"<p>Let \\(p = \\frac{\\theta}{\\theta + \\mu}\\) (probability of \"failure\" in the NB interpretation). Then:</p> \\[ \\log \\text{NB}(x \\mid \\mu, \\theta) = \\log \\binom{x + \\theta - 1}{x} + \\theta \\log p + x \\log(1 - p) \\]"},{"location":"VAE/VAE-08-NB-likelihood/#22-the-loss-function","title":"2.2 The Loss Function","text":"<p>For a VAE, the reconstruction loss for one gene \\(g\\) is:</p> \\[ \\mathcal{L}_{\\text{recon}}^{(g)} = -\\log \\text{NB}(x_g \\mid \\mu_g, \\theta_g) \\] <p>Summing over all \\(G\\) genes:</p> \\[ \\mathcal{L}_{\\text{recon}} = -\\sum_{g=1}^{G} \\log \\text{NB}(x_g \\mid \\mu_g, \\theta_g) \\]"},{"location":"VAE/VAE-08-NB-likelihood/#3-side-by-side-comparison-nb-vs-mse","title":"3. Side-by-Side Comparison: NB vs MSE","text":""},{"location":"VAE/VAE-08-NB-likelihood/#31-mse-gaussian-likelihood","title":"3.1 MSE (Gaussian Likelihood)","text":"<p>If \\(p(x \\mid z) = \\mathcal{N}(\\mu, \\sigma^2)\\), the log-likelihood is:</p> \\[ \\log p(x \\mid z) = -\\frac{1}{2\\sigma^2}(x - \\mu)^2 - \\frac{1}{2}\\log(2\\pi\\sigma^2) \\] <p>Ignoring constants, the loss is:</p> \\[ \\mathcal{L}_{\\text{MSE}} = \\frac{1}{2\\sigma^2} \\sum_g (x_g - \\mu_g)^2 \\]"},{"location":"VAE/VAE-08-NB-likelihood/#32-nb-negative-log-likelihood","title":"3.2 NB Negative Log-Likelihood","text":"\\[ \\mathcal{L}_{\\text{NB}} = -\\sum_g \\left[\\log \\Gamma(x_g + \\theta_g) - \\log \\Gamma(\\theta_g) - \\log(x_g!) + \\theta_g \\log\\left(\\frac{\\theta_g}{\\theta_g + \\mu_g}\\right) + x_g \\log\\left(\\frac{\\mu_g}{\\theta_g + \\mu_g}\\right)\\right] \\]"},{"location":"VAE/VAE-08-NB-likelihood/#33-key-differences","title":"3.3 Key Differences","text":"Aspect MSE (Gaussian) NB NLL Penalizes Squared deviation \\((x - \\mu)^2\\) Log-probability of count Variance model Constant \\(\\sigma^2\\) \\(\\mu + \\mu^2/\\theta\\) (grows with mean) Zero handling No special treatment Natural probability mass at 0 Domain \\(x \\in \\mathbb{R}\\) \\(x \\in \\{0, 1, 2, \\ldots\\}\\) Gradient behavior Linear in residual Depends on \\(\\mu\\), \\(\\theta\\), and \\(x\\)"},{"location":"VAE/VAE-08-NB-likelihood/#4-biological-interpretation-of-each-term","title":"4. Biological Interpretation of Each Term","text":"<p>Let's read the NB log-likelihood term by term:</p>"},{"location":"VAE/VAE-08-NB-likelihood/#term-1-log-gammax-theta-log-gammatheta-logx","title":"Term 1: \\(\\log \\Gamma(x + \\theta) - \\log \\Gamma(\\theta) - \\log(x!)\\)","text":"<p>What it is: Combinatorial normalization</p> <p>Biological meaning: Accounts for the number of ways to observe \\(x\\) counts given the underlying process. This term ensures the distribution sums to 1.</p>"},{"location":"VAE/VAE-08-NB-likelihood/#term-2-theta-logleftfracthetatheta-muright","title":"Term 2: \\(\\theta \\log\\left(\\frac{\\theta}{\\theta + \\mu}\\right)\\)","text":"<p>What it is: Contribution from the dispersion parameter</p> <p>Biological meaning:  * When \\(\\theta\\) is large (low dispersion), this term dominates \u2192 distribution is more Poisson-like * When \\(\\theta\\) is small (high dispersion), variance is large \u2192 accounts for biological heterogeneity</p>"},{"location":"VAE/VAE-08-NB-likelihood/#term-3-x-logleftfracmutheta-muright","title":"Term 3: \\(x \\log\\left(\\frac{\\mu}{\\theta + \\mu}\\right)\\)","text":"<p>What it is: Contribution from the observed count</p> <p>Biological meaning: * Penalizes mismatch between predicted mean \\(\\mu\\) and observed count \\(x\\) * Larger \\(x\\) \u2192 stronger pull toward higher \\(\\mu\\) * This is where the decoder's prediction \\(\\mu_g(z, y)\\) enters</p>"},{"location":"VAE/VAE-08-NB-likelihood/#5-why-nb-handles-zeros-better-than-gaussian","title":"5. Why NB Handles Zeros Better Than Gaussian","text":""},{"location":"VAE/VAE-08-NB-likelihood/#51-probability-of-zero-under-nb","title":"5.1 Probability of Zero Under NB","text":"\\[ \\text{NB}(0 \\mid \\mu, \\theta) = \\left(\\frac{\\theta}{\\theta + \\mu}\\right)^\\theta \\] <p>This is a proper probability mass at zero.</p> <ul> <li>When \\(\\mu\\) is small \u2192 \\(P(x=0)\\) is large (expected)</li> <li>When \\(\\mu\\) is large \u2192 \\(P(x=0)\\) is small (rare event)</li> </ul>"},{"location":"VAE/VAE-08-NB-likelihood/#52-probability-of-zero-under-gaussian","title":"5.2 Probability of Zero Under Gaussian","text":"\\[ \\mathcal{N}(0 \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{\\mu^2}{2\\sigma^2}\\right) \\] <p>This is a probability density, not a mass. The Gaussian: * Assigns positive density to negative values (impossible for counts) * Treats \\(x=0\\) the same as any other point * Has no special structure for zeros</p>"},{"location":"VAE/VAE-08-NB-likelihood/#6-the-dispersion-parameter-theta-or-alpha-1theta","title":"6. The Dispersion Parameter \\(\\theta\\) (or \\(\\alpha = 1/\\theta\\))","text":""},{"location":"VAE/VAE-08-NB-likelihood/#61-what-it-controls","title":"6.1 What It Controls","text":"\\[ \\text{Var}(x) = \\mu + \\frac{\\mu^2}{\\theta} \\] <ul> <li>Large \\(\\theta\\) (small \\(\\alpha\\)): Variance \u2248 \\(\\mu\\) \u2192 Poisson-like</li> <li>Small \\(\\theta\\) (large \\(\\alpha\\)): Variance \u226b \\(\\mu\\) \u2192 highly overdispersed</li> </ul>"},{"location":"VAE/VAE-08-NB-likelihood/#62-how-its-learned","title":"6.2 How It's Learned","text":"<p>In practice: * Gene-specific \\(\\theta_g\\): Each gene has its own dispersion (more flexible) * Shared \\(\\theta\\): One dispersion for all genes (simpler) * Predicted \\(\\theta_g(z)\\): Dispersion depends on latent state (most flexible)</p> <p>scVI uses gene-specific dispersion learned during training.</p>"},{"location":"VAE/VAE-08-NB-likelihood/#7-gradient-behavior-why-nb-trains-differently","title":"7. Gradient Behavior: Why NB Trains Differently","text":""},{"location":"VAE/VAE-08-NB-likelihood/#71-mse-gradient","title":"7.1 MSE Gradient","text":"\\[ \\frac{\\partial \\mathcal{L}_{\\text{MSE}}}{\\partial \\mu_g} = \\frac{\\mu_g - x_g}{\\sigma^2} \\] <p>Linear in the residual. Large counts \u2192 large gradients.</p>"},{"location":"VAE/VAE-08-NB-likelihood/#72-nb-gradient","title":"7.2 NB Gradient","text":"\\[ \\frac{\\partial \\mathcal{L}_{\\text{NB}}}{\\partial \\mu_g} = \\frac{\\theta_g}{\\theta_g + \\mu_g} - \\frac{x_g}{\\mu_g} \\] <p>This gradient: * Is bounded as \\(\\mu_g \\to \\infty\\) * Naturally handles the mean-variance relationship * Doesn't explode for large counts</p>"},{"location":"VAE/VAE-08-NB-likelihood/#8-implementation-note-log-space-stability","title":"8. Implementation Note: Log-Space Stability","text":"<p>In practice, we often predict \\(\\log \\mu\\) instead of \\(\\mu\\) directly:</p> <pre><code>log_mu = decoder(z)  # Predict log-mean\nmu = torch.exp(log_mu)  # Ensures mu &gt; 0\n</code></pre> <p>This avoids numerical issues with negative means and improves optimization stability.</p>"},{"location":"VAE/VAE-08-NB-likelihood/#9-summary-table","title":"9. Summary Table","text":"Component Symbol Biological Meaning Mean \\(\\mu_g\\) Expected expression level of gene \\(g\\) Dispersion \\(\\theta_g\\) (or \\(\\alpha_g = 1/\\theta_g\\)) Gene-specific noise level Variance \\(\\mu_g + \\mu_g^2/\\theta_g\\) Total variability (Poisson + overdispersion) \\(P(x=0)\\) \\((\\theta/(\\theta+\\mu))^\\theta\\) Probability of zero counts"},{"location":"VAE/VAE-08-NB-likelihood/#10-key-takeaway","title":"10. Key Takeaway","text":"<p>The NB log-likelihood is not just a \"different loss function\" \u2014 it encodes a specific belief about how count data is generated, with mean-variance coupling and proper handling of zeros.</p> <p>When you minimize NB NLL, you are finding parameters that maximize the probability of observing your data under this generative model.</p>"},{"location":"VAE/VAE-08-NB-likelihood/#next-steps","title":"Next Steps","text":"<p>The next document will cover: * ZINB log-likelihood \u2014 adding the zero-inflation component * Comparing NB vs ZINB diagnostics \u2014 when to switch</p>"},{"location":"VAE/VAE-08-NB-likelihood/#references","title":"References","text":"<ul> <li>VAE-07-NB-ZINB.md \u2014 Overview of likelihood choices</li> <li>Lopez et al. (2018) \u2014 \"Deep generative modeling for single-cell transcriptomics\"</li> <li>Hilbe (2011) \u2014 \"Negative Binomial Regression\"</li> </ul>"},{"location":"VAE/VAE-09-roadmap-discussion/","title":"Roadmap Discussion: Adapting GenAI for Gene Expression","text":"<p>This document discusses how to adapt the ROADMAP.md for gene expression data (bulk RNA-seq and scRNA-seq).</p> <p>The roadmap is strong as a \"generative AI curriculum\" and mostly workable for gene expression. However, a few stages and metrics need domain-specific rewiring to avoid optimizing the wrong thing.</p>"},{"location":"VAE/VAE-09-roadmap-discussion/#whats-already-a-great-fit","title":"What's Already a Great Fit","text":""},{"location":"VAE/VAE-09-roadmap-discussion/#vae-cvae","title":"VAE \u2192 cVAE","text":"<p>This is exactly the right foundation if you treat gene expression as counts (NB/ZINB) rather than \"pixels\" (Gaussian/MSE).</p> <ul> <li>Bulk RNA-seq: NB is usually the right default; ZINB rarely needed</li> <li>scRNA-seq: NB is often sufficient for UMI data; ZINB can help for extreme sparsity, but it's not automatically \"better\"</li> </ul>"},{"location":"VAE/VAE-09-roadmap-discussion/#score-matching-ddpm","title":"Score Matching \u2192 DDPM","text":"<p>Good next chapter if you pick the right representation. Directly diffusing raw counts is awkward; practical pipelines diffuse:</p> <ul> <li>Log-normalized expression, or</li> <li>Learned latent space (latent diffusion)</li> </ul>"},{"location":"VAE/VAE-09-roadmap-discussion/#jepa-world-models","title":"JEPA / World Models","text":"<p>Especially relevant for biology use cases:</p> <ul> <li>Perturbation prediction (action = drug/KO)</li> <li>Trajectory modeling (action = time)</li> <li>Counterfactuals</li> </ul> <p>Keeping JEPA/world models in later stages is sensible.</p>"},{"location":"VAE/VAE-09-roadmap-discussion/#what-to-adjust-for-gene-expression","title":"What to Adjust for Gene Expression","text":""},{"location":"VAE/VAE-09-roadmap-discussion/#put-scvi-style-likelihoods-library-size-into-stage-12","title":"Put scVI-Style Likelihoods + Library Size into Stage 1\u20132","text":"<p>For real gene expression work, the first serious milestone should be:</p> <ul> <li>NB decoder for counts (bulk + scRNA)</li> <li>Explicit handling of library size / sequencing depth (as offset or covariate)</li> <li>ZINB only after diagnostics show NB underfits zeros</li> </ul> <p>This is the difference between \"VAE toy demo\" and \"biology-grade model.\"</p>"},{"location":"VAE/VAE-09-roadmap-discussion/#replace-vision-metrics-with-biology-metrics","title":"Replace Vision Metrics with Biology Metrics","text":"<p>FID/IS aren't natural for gene expression (they rely on pretrained vision feature extractors). Better metrics:</p> Category Metric Likelihood Held-out NB/ZINB log-likelihood (or ELBO) Distribution Gene-wise mean/variance + zero rate matching (per condition) Structure Condition-separation (do generated samples preserve tissue/disease structure?) Utility Downstream classifier trained on real+synthetic \u2192 tested on real"},{"location":"VAE/VAE-09-roadmap-discussion/#iwae-only-if-you-hit-posterior-collapse","title":"IWAE: Only if You Hit Posterior Collapse","text":"<p>IWAE is a great learning milestone, but for expression you'll get more value from:</p> <ul> <li>KL warmup / free bits</li> <li>Decoder likelihood correctness (NB)</li> <li>Conditioning hygiene</li> </ul> <p>IWAE becomes useful when studying inference quality, but it's not the highest ROI \"next step\" unless you see issues.</p>"},{"location":"VAE/VAE-09-roadmap-discussion/#flow-matching-after-deciding-data-space-vs-latent-space","title":"Flow Matching: After Deciding Data Space vs Latent Space","text":"<p>Flow matching works well if you work in:</p> <ul> <li>Continuous normalized expression space, or</li> <li>Latent space from a trained encoder</li> </ul> <p>Tie it explicitly to the representation choice.</p>"},{"location":"VAE/VAE-09-roadmap-discussion/#a-two-track-roadmap-for-gene-expression","title":"A Two-Track Roadmap for Gene Expression","text":""},{"location":"VAE/VAE-09-roadmap-discussion/#track-a-count-faithful-representation-learning","title":"Track A: Count-Faithful Representation Learning","text":"<ol> <li>cVAE with NB decoder (bulk + scRNA)</li> <li>Add conditions (tissue/disease/batch) + counterfactual swap</li> <li>\u03b2-VAE only if you want disentangled residual factors (with good diagnostics)</li> </ol>"},{"location":"VAE/VAE-09-roadmap-discussion/#track-b-high-fidelity-generation","title":"Track B: High-Fidelity Generation","text":"<ol> <li>Learn a good continuous representation (normalized or latent)</li> <li>Score matching / diffusion in that space</li> <li>Conditional sampling (classifier-free guidance for metadata)</li> </ol>"},{"location":"VAE/VAE-09-roadmap-discussion/#unification","title":"Unification","text":"<p>Both tracks converge for:</p> <ul> <li>Perturbation response (world model)</li> <li>JEPA-style predictive objectives</li> </ul> <p>This preserves the \"VAE \u2192 score matching \u2192 diffusion \u2192 JEPA/world models\" arc, but makes it biology-native.</p>"},{"location":"VAE/VAE-09-roadmap-discussion/#the-key-decision-what-are-you-modeling","title":"The Key Decision: What Are You Modeling?","text":"<p>For your first real dataset, choose one:</p> Representation Likelihood Pros Cons Raw counts NB/ZINB Biologically faithful Harder to model Log-normalized Gaussian Easier diffusion/flow Loses count structure Learned latent Gaussian Best of both worlds Requires good encoder first <p>Recommendation: Start with raw counts \u2014 it forces you to confront the actual generative problem (library size, overdispersion, sparsity, batch, confounding).</p>"},{"location":"VAE/VAE-09-roadmap-discussion/#guardrails-for-raw-count-modeling","title":"Guardrails for Raw Count Modeling","text":""},{"location":"VAE/VAE-09-roadmap-discussion/#use-nb-first-not-zinb","title":"Use NB First, Not ZINB","text":"<p>Your MVP should be NB:</p> \\[ x_g \\sim \\text{NB}(\\mu_g, \\alpha_g) \\] <p>ZINB adds an extra head (\\(\\pi\\)) and can soak up modeling mistakes (\"everything becomes dropout\"). Upgrade to ZINB only if NB fails clear diagnostics.</p>"},{"location":"VAE/VAE-09-roadmap-discussion/#pick-real-but-manageable-datasets","title":"Pick Real but Manageable Datasets","text":"<p>Datasets should be:</p> <ul> <li>Public, well-described</li> <li>Not enormous</li> <li>Have clean metadata (tissue/disease/batch)</li> <li>Used in prior work (for sanity-checking)</li> </ul>"},{"location":"VAE/VAE-09-roadmap-discussion/#minimum-data-hygiene-for-raw-counts","title":"Minimum Data Hygiene for Raw Counts","text":"<p>If you skip these, NB models will look worse than they are:</p> <ul> <li>Gene filtering: Remove genes expressed in ~0 cells/samples (or keep HVGs)</li> <li>Library size factor (must-have): Total counts per sample/cell</li> </ul> <p>Typical NB parameterization:</p> <p>$$   \\mu_g = \\ell \\cdot \\exp(\\eta_g)   $$</p> <p>where \\(\\ell\\) is library size and \\(\\eta_g\\) is what the decoder predicts from \\((z, y)\\).</p> <ul> <li>Batch covariate: Include batch in \\(y\\) if present (even if you later want invariance)</li> </ul>"},{"location":"VAE/VAE-09-roadmap-discussion/#evaluation-metrics-for-real-data","title":"Evaluation Metrics for Real Data","text":"<p>Forget FID. For counts, track:</p> <p>Likelihood-Fit Diagnostics</p> <ul> <li>Held-out NB log-likelihood / ELBO</li> <li>Gene-wise mean/variance vs real (per condition)</li> <li>Zero rate vs real (per gene; per condition)</li> </ul> <p>Structure Diagnostics</p> <ul> <li>Train a probe on inferred \\(z\\) to predict batch/tissue (detect leakage/confounding)</li> <li>Latent collapse monitoring: average KL, active dimensions</li> </ul> <p>Usefulness Diagnostics</p> <ul> <li>Downstream classifier trained on synthetic + real, tested on real</li> <li>DE signature preservation: effect size correlation (real vs generated)</li> </ul>"},{"location":"VAE/VAE-09-roadmap-discussion/#where-different-methods-shine","title":"Where Different Methods Shine","text":"<p>| Method | Best For | Caveats | |--------|----------|---------|| | cVAE (NB) | Controlled generation + counterfactual swaps; latents for world-modeling | \u2014 | | \u03b2-VAE | Disentangled residual factors | Can hurt reconstruction/log-likelihood | | Diffusion/Score | High-fidelity generation | Awkward on discrete counts; use latent space |</p>"},{"location":"VAE/VAE-09-roadmap-discussion/#practical-staged-plan-raw-counts","title":"Practical Staged Plan (Raw Counts)","text":"<ol> <li>NB cVAE on a real dataset with a single clean condition (tissue only OR disease only)</li> <li>Add library size modeling explicitly and confirm fit improves</li> <li>Add batch conditioning and test counterfactual consistency</li> <li>Test ZINB only if NB underfits zeros (check held-out likelihood + zero-rate calibration)</li> <li>Add \u03b2 (\u03b2-VAE) only for specific goals: disentangled factors, stable latents</li> </ol>"},{"location":"VAE/VAE-09-roadmap-discussion/#bulk-vs-scrna-which-first","title":"Bulk vs scRNA: Which First?","text":"<p>For your first real experiment:</p> Data Type Pros Cons Bulk RNA-seq Cleaner per sample; simpler Fewer samples; harder to train deep models scRNA-seq More data points; tests sparsity handling More nuisance variation <p>Recommendation: Start with scRNA-seq (PBMC 3k) because:</p> <ol> <li>Smaller, faster iteration \u2014 3k cells vs thousands of samples</li> <li>Well-documented \u2014 extensively used in tutorials (scanpy, scVI)</li> <li>Clear ground truth \u2014 known cell types for validation</li> <li>Directly tests NB/ZINB \u2014 sparsity is real</li> <li>Preprocessing script ready \u2014 <code>src/genailab/data/sc_preprocess.py</code></li> </ol>"},{"location":"VAE/VAE-09-roadmap-discussion/#recommended-datasets","title":"Recommended Datasets","text":""},{"location":"VAE/VAE-09-roadmap-discussion/#scrna-seq-start-here","title":"scRNA-seq (Start Here)","text":"Dataset Description Size Link PBMC 3k Classic starter dataset ~3k cells 10x Genomics Tabula Sapiens Multi-tissue human atlas ~500k cells Tabula Sapiens"},{"location":"VAE/VAE-09-roadmap-discussion/#bulk-rna-seq-later","title":"Bulk RNA-seq (Later)","text":"Dataset Description Size Link GTEx Multi-tissue, healthy baseline ~17k samples GTEx Portal recount3 Uniformly processed public RNA-seq Massive recount3 <p>Note: For bulk, NB is typically sufficient. For UMI scRNA, NB is often enough; add ZINB only if NB badly underfits zeros.</p>"},{"location":"VAE/VAE-09-roadmap-discussion/#references","title":"References","text":"<ul> <li>ROADMAP.md \u2014 Original GenAI roadmap</li> <li>VAE-07-NB-ZINB.md \u2014 NB vs ZINB likelihood choice</li> <li>VAE-08-NB-likelihood.md \u2014 NB log-likelihood derivation</li> <li>Lopez et al. (2018) \u2014 scVI: Deep generative modeling for single-cell transcriptomics</li> <li>Eraslan et al. (2019) \u2014 DCA: Single-cell RNA-seq denoising using a deep count autoencoder</li> </ul>"},{"location":"VAE/VAE-QA/","title":"VAE Q&amp;A: Why Keep the Posterior Close to the Prior?","text":"<p>Clarifying the role of the KL divergence term and the prior assumption in VAEs.</p>"},{"location":"VAE/VAE-QA/#the-question","title":"The Question","text":"<p>In VAEs, the KL divergence term \\(\\mathrm{KL}(q(z|x) \\| p(z))\\) encourages the approximate posterior \\(q(z|x)\\) to stay close to the prior \\(p(z)\\). One justification given is that this makes the latent space \"smooth\"\u2014nearby latents produce similar outputs.</p> <p>But the prior \\(p(z) = \\mathcal{N}(0, I)\\) is our assumption. How do we know it's a good assumption? If the prior doesn't match reality, why is pushing \\(q(z|x)\\) toward it beneficial? Is this just mathematical intuition, or is there a principled reason?</p>"},{"location":"VAE/VAE-QA/#1-the-prior-is-an-assumptionand-a-strong-one","title":"1. The Prior Is an Assumption\u2014and a Strong One","text":"<p>Let's be explicit:</p> <p>The prior \\(p(z)\\) is not discovered from data. It is imposed by us.</p> <p>Typically:</p> \\[ p(z) = \\mathcal{N}(0, I) \\] <p>There is no guarantee that the true latent causes of your data are Gaussian, isotropic, or even unimodal.</p> <p>So if anyone claims \"the prior matches the true data-generating process\"\u2014that's false in general.</p> <p>The real question is:</p> <p>If the prior is arbitrary, why force the posterior toward it at all?</p>"},{"location":"VAE/VAE-QA/#2-the-prior-is-not-a-beliefit-is-a-coordinate-system","title":"2. The Prior Is Not a Belief\u2014It Is a Coordinate System","text":"<p>This is the key mental pivot.</p> <p>In VAEs, the prior is not primarily a probabilistic belief about reality. It is a chosen reference measure\u2014a coordinate system in which we want the latent space to live.</p> <p>Think of it this way:</p> <ul> <li>We are not saying \"the world is Gaussian\"</li> <li>We are saying \"we choose to represent latent causes in a Gaussian coordinate system\"</li> </ul> <p>This is analogous to choosing:</p> <ul> <li>Euclidean vs. polar coordinates</li> <li>A basis in linear algebra</li> <li>A gauge in physics</li> </ul> <p>The KL term enforces compatibility with that coordinate system.</p>"},{"location":"VAE/VAE-QA/#3-why-compatibility-matters-the-unavoidable-constraint","title":"3. Why Compatibility Matters (The Unavoidable Constraint)","text":"<p>The unavoidable fact:</p> <p>At generation time, we must sample from something.</p> <p>We do not know the aggregate posterior:</p> \\[ \\int q(z|x) \\, p_{\\text{data}}(x) \\, dx \\] <p>So we choose a simple distribution we can sample from. This forces a constraint:</p> <p>The latent space must be arranged so that sampling from a simple distribution lands us in \"valid\" regions.</p> <p>The KL term is the enforcement mechanism.</p> <p>Without it:</p> <ul> <li>Each datapoint can occupy a disconnected island in latent space</li> <li>There is no globally meaningful geometry</li> <li>Sampling becomes undefined behavior</li> </ul> <p>This is not aesthetic\u2014it is operational necessity.</p>"},{"location":"VAE/VAE-QA/#4-smoothness-is-about-learnability-not-truth","title":"4. Smoothness Is About Learnability, Not Truth","text":"<p>The smoothness argument deserves clarification:</p> <p>Smoothness is not about correctness of the prior. It is about controlling the hypothesis class of the decoder.</p> <p>The decoder is a continuous function:</p> \\[ x = f_\\theta(z) \\] <p>If nearby \\(z\\)'s map to wildly different \\(x\\)'s, then \\(f_\\theta\\) must be extremely non-smooth, which makes generalization impossible.</p> <p>The KL term forces the decoder to operate in a regime where:</p> <ul> <li>Small changes in \\(z\\) matter locally</li> <li>Global structure is shared</li> </ul> <p>This is regularization, not epistemology.</p>"},{"location":"VAE/VAE-QA/#5-the-precise-statement","title":"5. The Precise Statement","text":"<p>Here is the non-hand-wavy statement you can defend:</p> <p>The KL term does not encode a belief that the prior is true. It enforces that the encoder and decoder agree on a common latent reference distribution so that generation, interpolation, and generalization are possible.</p> <p>No mysticism required.</p>"},{"location":"VAE/VAE-QA/#6-why-not-learn-the-prior-instead","title":"6. Why Not Learn the Prior Instead?","text":"<p>Indeed\u2014and people do. Examples:</p> <ul> <li>VampPrior (Tomczak &amp; Welling, 2018)</li> <li>Hierarchical VAEs</li> <li>Mixture priors</li> <li>Normalizing flow priors</li> </ul> <p>These relax the Gaussian assumption while keeping the same logic:</p> <p>The posterior must stay close to some tractable reference distribution.</p> <p>The principle survives:</p> <ul> <li>The form of the prior can change</li> <li>The role of the prior cannot</li> </ul>"},{"location":"VAE/VAE-QA/#7-why-geometry-matters-even-with-a-wrong-prior","title":"7. Why Geometry Matters Even with a \"Wrong\" Prior","text":"<p>Even if the prior is \"wrong\":</p> <ul> <li>Forcing consistency produces a shared latent manifold</li> <li>The decoder learns relative structure, not absolute coordinates</li> <li>Many different priors yield equivalent expressive power up to reparameterization</li> </ul> <p>In fact:</p> <p>Any continuous latent model is only identifiable up to smooth transformations.</p> <p>So the \"true\" geometry is unobservable anyway. This is a deep but underappreciated fact.</p>"},{"location":"VAE/VAE-QA/#8-the-trade-off-vaes-make","title":"8. The Trade-Off VAEs Make","text":"<p>Here is the sentence most papers avoid saying explicitly:</p> <p>VAEs trade representational faithfulness for controllability.</p> <p>The KL term is the price we pay to:</p> <ul> <li>Sample from the model</li> <li>Interpolate in latent space</li> <li>Generalize to new data</li> <li>Reason about uncertainty</li> </ul> <p>Diffusion models later drop this constraint\u2014and gain fidelity\u2014but lose explicit latents. This is not an accident. It's a fundamental trade-off.</p>"},{"location":"VAE/VAE-QA/#9-summary","title":"9. Summary","text":"<p>To answer the question precisely:</p> <ul> <li>No, it is not because the prior is assumed to be true</li> <li>No, it is not just vague intuition</li> <li>Yes, it is a deliberate inductive bias</li> <li>The bias is chosen because it makes learning, inference, and generation possible at all</li> </ul> <p>One sentence you can defend publicly:</p> <p>We keep \\(q(z|x)\\) close to \\(p(z)\\) not because the prior is correct, but because it defines a shared, tractable latent coordinate system that makes sampling, generalization, and learning feasible.</p>"},{"location":"VAE/VAE-QA/#connection-to-other-models","title":"Connection to Other Models","text":"<p>This clarifies the evolution of generative models:</p> Model Approach VAE Impose global latent geometry via KL Diffusion No global latent; iterative denoising instead EBMs No normalized distribution; learn energy landscape <p>Each makes a different trade-off between tractability and expressiveness.</p>"},{"location":"VAE/VAE-QA/#follow-up-vaes-vs-diffusion-vs-ebms","title":"Follow-Up: VAEs vs Diffusion vs EBMs","text":"<p>Contrasting VAEs with diffusion models and EBMs, which refuse to impose a global latent geometry\u2014and pay the computational price instead.</p>"},{"location":"VAE/VAE-QA/#the-fork-in-the-road-impose-geometry-vs-refuse-geometry","title":"The Fork in the Road: Impose Geometry vs. Refuse Geometry","text":"<p>At a high level, generative models must answer one unavoidable question:</p> <p>Where do we put structure?</p> <p>There are two fundamentally different answers.</p>"},{"location":"VAE/VAE-QA/#path-a-vaes-impose-a-latent-coordinate-system","title":"Path A: VAEs \u2014 Impose a Latent Coordinate System","text":""},{"location":"VAE/VAE-QA/#core-commitment","title":"Core Commitment","text":"<p>VAEs say:</p> <p>\"We will represent data through a low-dimensional latent variable \\(z\\), and we will force that latent space to live in a simple, shared geometry.\"</p> <p>That geometry is defined by the prior \\(p(z)\\).</p> <p>The KL term enforces:</p> <ul> <li>Global consistency \u2014 all data points share the same latent space</li> <li>Smoothness \u2014 nearby latents produce similar outputs</li> <li>Sampleability \u2014 we can generate new data by sampling from the prior</li> </ul>"},{"location":"VAE/VAE-QA/#consequences","title":"Consequences","text":"<p>Benefits:</p> <ul> <li>Explicit latent representations</li> <li>Fast sampling (single forward pass)</li> <li>Interpolation and controllable generation</li> </ul> <p>Costs:</p> <ul> <li>Decoder must explain everything through a constrained latent bottleneck</li> <li>Likelihood pressure + KL pressure \u2192 blurred outputs</li> <li>Mismatch between imposed geometry and true data complexity</li> </ul> <p>This is not a bug\u2014it's the cost of choosing structure up front.</p>"},{"location":"VAE/VAE-QA/#path-b-diffusion-ebms-refuse-a-global-latent-geometry","title":"Path B: Diffusion / EBMs \u2014 Refuse a Global Latent Geometry","text":"<p>Diffusion models and EBMs make a radically different choice:</p> <p>\"We will not assume there exists a simple latent coordinate system at all.\"</p> <p>Instead:</p> <ul> <li>Generation happens in data space</li> <li>Uncertainty is modeled directly</li> <li>Structure emerges implicitly</li> </ul> <p>No global \\(z\\) that must look Gaussian. No KL to a prior.</p>"},{"location":"VAE/VAE-QA/#diffusion-models-structure-via-noise-not-coordinates","title":"Diffusion Models: Structure via Noise, Not Coordinates","text":""},{"location":"VAE/VAE-QA/#what-diffusion-replaces","title":"What Diffusion Replaces","text":"<p>Diffusion models drop:</p> <ul> <li>Explicit latent variables</li> <li>Amortized inference</li> <li>KL divergence to a prior</li> </ul> <p>And replace them with:</p> <ul> <li>A Markov chain of noising and denoising steps</li> <li>Score matching instead of likelihood maximization</li> </ul>"},{"location":"VAE/VAE-QA/#the-key-philosophical-move","title":"The Key Philosophical Move","text":"<p>Instead of asking:</p> <p>\"What latent variable caused this data?\"</p> <p>Diffusion asks:</p> <p>\"How does this data locally deform probability mass?\"</p> <p>Geometry is learned implicitly via gradients of the log density (the score function):</p> \\[ \\nabla_x \\log p(x) \\]"},{"location":"VAE/VAE-QA/#consequences_1","title":"Consequences","text":"<p>Benefits:</p> <ul> <li>Extremely high sample quality</li> <li>No pressure to compress information into a bottleneck</li> <li>No need for a \"correct\" prior</li> </ul> <p>Costs:</p> <ul> <li>Sampling is slow (many iterative steps)</li> <li>No explicit, compact latent representation</li> <li>Control is indirect (classifier guidance, conditioning tricks)</li> </ul>"},{"location":"VAE/VAE-QA/#energy-based-models-ebms-no-normalization-no-geometry","title":"Energy-Based Models (EBMs): No Normalization, No Geometry","text":"<p>EBMs go even further. They say:</p> <p>\"We won't even define a normalized probability distribution.\"</p> <p>They learn an energy function:</p> \\[ E_\\theta(x) \\] <p>This defines an energy landscape over data:</p> <ul> <li>Low energy = plausible data</li> <li>High energy = implausible data</li> </ul> <p>The (unnormalized) probability is:</p> \\[ p_\\theta(x) \\propto \\exp(-E_\\theta(x)) \\] <p>No latent space. No decoder likelihood. No tractable partition function.</p>"},{"location":"VAE/VAE-QA/#consequences_2","title":"Consequences","text":"<p>Benefits:</p> <ul> <li>Maximum flexibility</li> <li>No imposed geometry</li> <li>Very expressive</li> </ul> <p>Costs:</p> <ul> <li>Training is hard (contrastive divergence, noise contrastive estimation)</li> <li>Sampling requires MCMC or Langevin dynamics</li> <li>Inference is expensive</li> </ul> <p>EBMs trade everything for expressiveness.</p>"},{"location":"VAE/VAE-QA/#why-diffusion-beat-vaes-in-vision","title":"Why Diffusion Beat VAEs in Vision","text":"<p>Here's the uncomfortable truth:</p> <p>Natural images do not live on a globally smooth, low-dimensional manifold.</p> <p>They have:</p> <ul> <li>Sharp edges</li> <li>Multi-scale structure</li> <li>Combinatorial variation</li> </ul> <p>Forcing all of that through:</p> \\[ z \\sim \\mathcal{N}(0, I) \\] <p>is an extreme compression.</p> <p>Diffusion avoids that compression entirely. That's why it wins on fidelity.</p>"},{"location":"VAE/VAE-QA/#why-vaes-are-still-essential","title":"Why VAEs Are Still Essential","text":"<p>For applications like world models, JEPA, and structured reasoning, VAEs provide critical capabilities:</p> <p>What VAEs give you:</p> <ul> <li>Explicit latent variables</li> <li>Amortized inference (fast encoding)</li> <li>Compact representations</li> <li>Fast rollout for planning</li> </ul> <p>What these enable:</p> <ul> <li>World models and dynamics learning</li> <li>Planning and reasoning</li> <li>Controllable generation</li> <li>Uncertainty quantification</li> </ul> <p>Diffusion struggles with these use cases. That's why modern systems often combine both approaches.</p>"},{"location":"VAE/VAE-QA/#modern-hybrids-the-synthesis-phase","title":"Modern Hybrids: The Synthesis Phase","text":"<p>Current research is converging on hybrids:</p> <ul> <li>VAE-style latents for structure and fast inference</li> <li>Diffusion-style decoders for high-fidelity generation</li> <li>Learned priors instead of fixed Gaussians</li> <li>Latent diffusion (run diffusion in latent space, not pixel space)</li> </ul> <p>This is not regression\u2014it's reconciliation.</p>"},{"location":"VAE/VAE-QA/#summary-the-core-trade-off","title":"Summary: The Core Trade-Off","text":"<p>VAEs assume a simple latent geometry and pay with fidelity. Diffusion refuses a global latent geometry and pays with computation.</p> <p>This captures the fundamental design choice in generative modeling.</p>"},{"location":"VAE/VAE-QA/#the-meta-lesson","title":"The Meta-Lesson","text":"<p>The question asked earlier:</p> <p>\"Is the prior just intuition?\"</p> <p>turns out to be the central design question of generative modeling.</p> <p>Every modern model answers it differently:</p> Model Answer to \"Where is structure?\" VAE In an explicit latent space with imposed geometry Diffusion In the score function, learned implicitly EBM In the energy landscape, no normalization Latent Diffusion Hybrid: latent structure + diffusion fidelity <p>And that's why this field is still very much alive.</p>"},{"location":"VAE/VAE-QA/#where-to-go-next","title":"Where to Go Next","text":"<p>The natural continuation:</p> <ol> <li>Latent Diffusion \u2014 the explicit bridge between VAEs and diffusion</li> <li>World Models / JEPA \u2014 where the latent is learned by prediction, not reconstruction</li> </ol> <p>These build directly on the concepts covered here.</p>"},{"location":"VAE/VAE-QA/#references","title":"References","text":"<ul> <li>VAE-01-overview.md \u2014 Main VAE theory</li> <li>VAE-02-elbo.md \u2014 ELBO derivation</li> <li>VAE-03-inference.md \u2014 Why we introduce \\(q(z|x)\\)</li> <li>reparameterization-trick.md \u2014 The reparameterization trick</li> </ul>"},{"location":"VAE/VAE-for-prediction/","title":"Using VAEs for Prediction: From Generative Models to Downstream Tasks","text":"<p>VAEs, cVAEs, and \u03b2-VAEs are generative models\u2014they learn to reconstruct data, not predict labels. Yet they are remarkably effective for prediction tasks when used correctly.</p> <p>This document explains how to leverage VAE-family models for classification, regression, ranking, and anomaly detection in computational biology applications.</p>"},{"location":"VAE/VAE-for-prediction/#the-core-insight","title":"The Core Insight","text":"<p>VAEs don't predict labels by default, but they manufacture representations that can be turned into powerful predictors. The key is understanding what the latent space captures and how to extract predictions from it.</p> <p>The pattern:</p> <ol> <li>Train a VAE to model gene expression</li> <li>Extract latent representations \\(z\\) for each sample</li> <li>Use \\(z\\) as features for downstream prediction tasks</li> </ol> <p>This two-stage approach often outperforms end-to-end discriminative models because the VAE learns robust, denoised representations of the underlying biology.</p>"},{"location":"VAE/VAE-for-prediction/#1-what-a-vae-optimizes-and-what-it-doesnt","title":"1. What a VAE Optimizes (and What It Doesn't)","text":"<p>A vanilla VAE learns:</p> <ul> <li>An encoder \\(q_\\phi(z | x)\\): gene expression \u2192 latent state</li> <li>A decoder \\(p_\\theta(x | z)\\): latent state \u2192 reconstructed expression</li> </ul> <p>The objective is the ELBO:</p> <ul> <li>Reconstruct \\(x\\) well</li> <li>Keep \\(z\\) close to a simple prior (usually \\(\\mathcal{N}(0, I)\\))</li> </ul> <p>No labels. No prediction target. Just \"explain the data compactly.\"</p> <p>By default, a VAE is:</p> <ul> <li>Not a classifier</li> <li>Not a regressor</li> <li>Not a ranker</li> </ul> <p>But that's like saying PCA can't predict anything. True\u2014and also missing the point.</p>"},{"location":"VAE/VAE-for-prediction/#2-prediction-lives-in-latent-space","title":"2. Prediction Lives in Latent Space","text":"<p>Once you have a latent variable \\(z\\), three prediction strategies become available.</p>"},{"location":"VAE/VAE-for-prediction/#strategy-a-latent-downstream-predictor","title":"Strategy A: Latent \u2192 Downstream Predictor","text":"<p>This is the most common and most powerful approach:</p> <pre><code>gene expression x\n   \u2193 encoder\nlatent z\n   \u2193 classifier / regressor / ranker\nprediction \u0177\n</code></pre> <p>This is not a hack\u2014it's the intended use in biology.</p> <p>Examples:</p> <ul> <li>scRNA-seq: latent \u2192 cell type classification</li> <li>Bulk RNA-seq: latent \u2192 disease state prediction</li> <li>Transcript modeling: latent \u2192 reliability score</li> </ul> <p>The VAE performs representation learning, not prediction. The predictor is a separate model trained on \\(z\\).</p> <p>Why this works well:</p> <ul> <li>Expression data is noisy; VAEs denoise and compress</li> <li>Reliability is latent and indirect; \\(z\\) captures the underlying state</li> <li>The latent space is smooth and continuous</li> </ul> <p>You're essentially asking: \"What is the biological state of this sample?\" rather than \"What are the raw counts?\"</p>"},{"location":"VAE/VAE-for-prediction/#3-cvae-controlled-inference-via-conditioning","title":"3. cVAE: Controlled Inference via Conditioning","text":"<p>A conditional VAE models:</p> \\[ p(x | z, c) \\] <p>where \\(c\\) might be:</p> <ul> <li>Tissue type</li> <li>Disease state</li> <li>Batch ID</li> <li>Perturbation condition</li> </ul> <p>Now the latent \\(z\\) is forced to explain what remains after conditioning. For gene expression, this is powerful.</p>"},{"location":"VAE/VAE-for-prediction/#why-cvae-excels-for-downstream-prediction","title":"Why cVAE Excels for Downstream Prediction","text":"<p>You can:</p> <ul> <li>Condition on tissue and disease</li> <li>Force \\(z\\) to capture sample-specific variation</li> <li>Decouple biological signal from confounders</li> </ul> <pre><code>z = encoder(x, condition)\ny_pred = prediction_head(z)\n</code></pre> <p>The prediction now answers: \"What is the state of this sample given its biological context?\"</p>"},{"location":"VAE/VAE-for-prediction/#4-vae-interpretability-through-disentanglement","title":"4. \u03b2-VAE: Interpretability Through Disentanglement","text":"<p>\u03b2-VAE modifies the ELBO:</p> \\[ \\mathcal{L}_{\\beta\\text{-VAE}} = \\mathbb{E}[\\log p(x|z)] - \\beta \\cdot \\text{KL}(q(z|x) \\| p(z)) \\] <p>Increasing \\(\\beta\\):</p> <ul> <li>Enforces factorized latent dimensions</li> <li>Sacrifices reconstruction fidelity</li> <li>Gains disentanglement</li> </ul> <p>For biology, this often means:</p> <ul> <li>One latent dimension \u2248 expression strength</li> <li>Another \u2248 variability across conditions</li> <li>Another \u2248 batch susceptibility</li> </ul> <p>Now prediction becomes not just possible, but explainable:</p> <p>\"This sample is unusual because latent factor 3 (expression instability) is high.\"</p> <p>This pairs well with:</p> <ul> <li>Ranking samples by specific latent factors</li> <li>Thresholding on interpretable dimensions</li> <li>Model interpretation and feature attribution</li> </ul>"},{"location":"VAE/VAE-for-prediction/#5-joint-models-prediction-inside-the-vae","title":"5. Joint Models: Prediction Inside the VAE","text":"<p>You can also bake prediction directly into the VAE.</p>"},{"location":"VAE/VAE-for-prediction/#approach-1-vae-supervised-head","title":"Approach 1: VAE + Supervised Head","text":"<p>Loss:</p> \\[ \\mathcal{L} = \\mathcal{L}_{\\text{ELBO}}(x) + \\lambda \\cdot \\mathcal{L}_{\\text{pred}}(y, f(z)) \\] <p>This gives you:</p> <ul> <li>Generative modeling of expression</li> <li>Predictive latent space</li> <li>Semi-supervised learning (labels optional)</li> </ul> <p>This is attractive when:</p> <ul> <li>Labels are sparse</li> <li>Unlabeled samples dominate</li> <li>Labels are noisy or partial</li> </ul>"},{"location":"VAE/VAE-for-prediction/#approach-2-probabilistic-prediction","title":"Approach 2: Probabilistic Prediction","text":"<p>Instead of predicting a point estimate, model:</p> \\[ p(y | z) \\] <p>Now the model outputs uncertainty:</p> <p>\"This sample has a 0.78 \u00b1 0.12 probability of belonging to class A.\"</p> <p>This enables Bayesian decision-making and calibrated ranking.</p>"},{"location":"VAE/VAE-for-prediction/#6-ranking-and-anomaly-detection","title":"6. Ranking and Anomaly Detection","text":"<p>VAEs are surprisingly effective for unsupervised ranking.</p>"},{"location":"VAE/VAE-for-prediction/#reconstruction-error-as-anomaly-score","title":"Reconstruction Error as Anomaly Score","text":"<p>If a sample:</p> <ul> <li>Reconstructs poorly</li> <li>Has high posterior uncertainty</li> <li>Lives far from the latent manifold</li> </ul> <p>...that's a signal worth investigating.</p> <p>This makes VAEs naturally suited for:</p> <ul> <li>Outlier detection</li> <li>Quality screening</li> <li>Novelty detection</li> </ul>"},{"location":"VAE/VAE-for-prediction/#latent-likelihood-as-confidence","title":"Latent Likelihood as Confidence","text":"<p>You can rank samples by:</p> <ul> <li>ELBO (higher = more typical)</li> <li>Marginal likelihood</li> <li>Posterior entropy (lower = more confident)</li> </ul> <p>This is ranking without explicit labels\u2014purely from the generative model.</p>"},{"location":"VAE/VAE-for-prediction/#7-practical-downstream-tasks-for-evaluation","title":"7. Practical Downstream Tasks for Evaluation","text":"<p>To evaluate generative models like VAEs, we need downstream prediction tasks. Here are practical options:</p> Task Labels Difficulty Biological Relevance Cell type classification Cell type annotations Easy High Disease state prediction Case/control labels Medium High Batch prediction Batch IDs Easy Low (sanity check) Perturbation response Treatment labels Hard Very high Trajectory position Pseudotime Medium High"},{"location":"VAE/VAE-for-prediction/#multiple-complementary-approaches","title":"Multiple Complementary Approaches","text":"<ol> <li>Latent \u2192 classifier: Supervised prediction from \\(z\\)</li> <li>Latent uncertainty \u2192 quality proxy: High variance = low confidence</li> <li>Reconstruction error \u2192 anomaly score: Poorly modeled = unusual</li> <li>Condition-invariant factors: What's consistent across contexts?</li> <li>Semi-supervised learning: Leverage unlabeled data</li> </ol> <p>A VAE is not competing with discriminative models\u2014it's a substrate for building them.</p>"},{"location":"VAE/VAE-for-prediction/#8-mental-model","title":"8. Mental Model","text":"<p>Think of VAEs as answering:</p> <p>\"What kind of thing is this sample?\"</p> <p>Not:</p> <p>\"What is the label for this sample?\"</p> <p>But once you know what kind of thing it is, the second question becomes much easier\u2014and much more robust.</p>"},{"location":"VAE/VAE-for-prediction/#9-advanced-directions","title":"9. Advanced Directions","text":"<p>Next steps for more sophisticated modeling:</p> <ul> <li>Hierarchical VAEs: Model structure at multiple scales (gene \u2192 pathway \u2192 program)</li> <li>Mixture-of-VAEs: Capture distinct regimes or subpopulations</li> <li>Semi-supervised VAE: Joint generative + discriminative training</li> <li>Combining with other methods: Use VAE latents as features for GMMs, PU learning, etc.</li> </ul> <p>The generative story gives you structure. The predictive layer gives you decisions.</p>"},{"location":"VAE/VAE-for-prediction/#summary","title":"Summary","text":"VAE Variant Best For VAE Unsupervised representation learning cVAE Controlled generation, removing confounders \u03b2-VAE Interpretable, disentangled representations Semi-supervised VAE Sparse labels, leveraging unlabeled data <p>All can be used for downstream prediction by training classifiers/regressors on the latent space \\(z\\).</p>"},{"location":"VAE/VAE-model-training/","title":"VAE Model Training: Interpreting Loss Curves and Diagnostics","text":"<p>This document explains how to interpret VAE training output, diagnose common issues like posterior collapse, and understand the interplay between reconstruction and KL divergence.</p>"},{"location":"VAE/VAE-model-training/#the-elbo-loss","title":"The ELBO Loss","text":"<p>VAEs maximize the Evidence Lower Bound (ELBO), or equivalently minimize the negative ELBO:</p> \\[ \\mathcal{L} = \\underbrace{-\\mathbb{E}_{q(z|x)}[\\log p(x|z)]}_{\\text{reconstruction loss}} + \\underbrace{\\text{KL}(q(z|x) \\| p(z))}_{\\text{regularization}} \\] <p>In practice, with a \u03b2-VAE formulation:</p> \\[ \\mathcal{L} = \\mathcal{L}_{\\text{recon}} + \\beta \\cdot \\mathcal{L}_{\\text{KL}} \\] <p>where:</p> <ul> <li>Reconstruction loss: How well the decoder reconstructs the input (MSE for Gaussian, NLL for NB/ZINB)</li> <li>KL divergence: How much the learned posterior \\(q(z|x)\\) deviates from the prior \\(p(z) = \\mathcal{N}(0, I)\\)</li> <li>\u03b2: Weight controlling the trade-off (\u03b2 = 1 is standard VAE, \u03b2 &gt; 1 encourages disentanglement)</li> </ul>"},{"location":"VAE/VAE-model-training/#interpreting-training-output","title":"Interpreting Training Output","text":"<p>A typical training log looks like:</p> <pre><code>epoch 001 | train loss=0.8684 recon=0.8440 kl=0.0488 | val loss=0.7531\nepoch 005 | train loss=0.5926 recon=0.4825 kl=0.2203 | val loss=0.5779\nepoch 010 | train loss=0.5083 recon=0.3807 kl=0.2554 | val loss=0.4987\nepoch 018 | train loss=0.4374 recon=0.2755 kl=0.3238 | val loss=0.4253\n</code></pre>"},{"location":"VAE/VAE-model-training/#what-each-metric-means","title":"What Each Metric Means","text":"Metric Formula Interpretation loss recon + \u03b2 \u00d7 kl Total objective (lower = better) recon MSE or NLL Reconstruction quality kl KL(q || p) Information encoded in latent space"},{"location":"VAE/VAE-model-training/#healthy-training-signs","title":"Healthy Training Signs","text":"<ol> <li>Total loss decreasing \u2014 The model is learning</li> <li>Reconstruction improving \u2014 The decoder is getting better</li> <li>KL increasing then stabilizing \u2014 The latent space is being used</li> <li>Train \u2248 Val \u2014 No significant overfitting</li> </ol>"},{"location":"VAE/VAE-model-training/#the-kl-term-why-it-matters","title":"The KL Term: Why It Matters","text":""},{"location":"VAE/VAE-model-training/#what-kl-measures","title":"What KL Measures","text":"<p>The KL divergence measures how different the learned posterior \\(q_\\phi(z|x)\\) is from the prior \\(p(z) = \\mathcal{N}(0, I)\\).</p> \\[ \\text{KL}(q(z|x) \\| p(z)) = \\frac{1}{2} \\sum_{j=1}^{d} \\left( \\mu_j^2 + \\sigma_j^2 - \\log \\sigma_j^2 - 1 \\right) \\] <p>where \\(\\mu_j\\) and \\(\\sigma_j\\) are the encoder outputs for latent dimension \\(j\\).</p>"},{"location":"VAE/VAE-model-training/#interpreting-kl-values","title":"Interpreting KL Values","text":"KL Value What It Means \u2248 0 Posterior equals prior \u2014 encoder ignores input 0.1 \u2013 1.0 Healthy range \u2014 latent encodes meaningful variation &gt; 2.0 Posterior far from prior \u2014 may indicate underfitting or need for higher \u03b2"},{"location":"VAE/VAE-model-training/#posterior-collapse-the-kl-0-problem","title":"Posterior Collapse: The KL \u2248 0 Problem","text":""},{"location":"VAE/VAE-model-training/#what-is-posterior-collapse","title":"What Is Posterior Collapse?","text":"<p>When \\(\\text{KL}(q(z|x) \\| p(z)) \\approx 0\\), it means:</p> \\[ q_\\phi(z|x) \\approx p(z) = \\mathcal{N}(0, I) \\quad \\text{for all } x \\] <p>This looks mathematically fine, but the problem is: the encoder outputs the same distribution regardless of the input.</p>"},{"location":"VAE/VAE-model-training/#why-its-bad","title":"Why It's Bad","text":"<p>If \\(q(z|x) \\approx \\mathcal{N}(0, I)\\) for all inputs:</p> <ul> <li>The encoder has learned to ignore the input</li> <li>Every sample maps to roughly the same latent distribution</li> <li>The latent code \\(z\\) carries no information about \\(x\\)</li> <li>The latent space is useless for downstream tasks</li> </ul>"},{"location":"VAE/VAE-model-training/#why-it-happens","title":"Why It Happens","text":"<p>The decoder becomes so powerful that it can reconstruct \\(x\\) without using \\(z\\). It essentially memorizes the data distribution.</p> <p>The model finds an \"easy\" solution: minimize KL by setting \\(q(z|x) = p(z)\\), and let the decoder do all the work.</p>"},{"location":"VAE/VAE-model-training/#how-to-detect-it","title":"How to Detect It","text":"Symptom Healthy Collapsed KL during training Increases, then stabilizes (0.1\u20131.0) Stays near 0 Latent space (UMAP) Clusters by meaningful factors All points in one blob Downstream classification Good accuracy Near random"},{"location":"VAE/VAE-model-training/#how-to-fix-it","title":"How to Fix It","text":"<ol> <li>Lower \u03b2 \u2014 Reduce KL penalty (e.g., \u03b2 = 0.1 or 0.5)</li> <li>KL annealing \u2014 Start with \u03b2 = 0, gradually increase</li> <li>Free bits \u2014 Allow minimum KL per dimension before penalizing</li> <li>Weaker decoder \u2014 Reduce decoder capacity</li> <li>Cyclical annealing \u2014 Periodically reset \u03b2 to 0</li> </ol>"},{"location":"VAE/VAE-model-training/#the-parameter","title":"The \u03b2 Parameter","text":""},{"location":"VAE/VAE-model-training/#effect-of-different-values","title":"Effect of Different \u03b2 Values","text":"\u03b2 Value Effect \u03b2 &lt; 1 Prioritize reconstruction, allow higher KL \u03b2 = 1 Standard VAE (balanced) \u03b2 &gt; 1 Prioritize regularization, encourage disentanglement"},{"location":"VAE/VAE-model-training/#practical-guidance","title":"Practical Guidance","text":"<ul> <li>Start with \u03b2 = 0.5 for gene expression data</li> <li>If KL stays near 0, reduce \u03b2 further</li> <li>If reconstruction is poor, reduce \u03b2</li> <li>If latent space is unstructured, increase \u03b2</li> </ul>"},{"location":"VAE/VAE-model-training/#kl-annealing","title":"KL Annealing","text":"<p>A common technique to avoid posterior collapse:</p> <pre><code>def kl_annealing_schedule(epoch, warmup_epochs=10, max_beta=1.0):\n    \"\"\"Linear KL annealing.\"\"\"\n    return min(max_beta, max_beta * epoch / warmup_epochs)\n</code></pre> <p>This allows the model to first learn good reconstructions, then gradually enforce the prior constraint.</p>"},{"location":"VAE/VAE-model-training/#example-healthy-training-curves","title":"Example: Healthy Training Curves","text":"<p>The figure below shows training curves from a cVAE trained on synthetic bulk RNA-seq data (<code>examples/01_bulk_cvae.ipynb</code>):</p> <p></p> <p>What to observe:</p> <ul> <li>Total Loss (left): Both train and val decrease smoothly, converging together</li> <li>Reconstruction Loss (center): Decreases rapidly early, then plateaus \u2014 the decoder is learning</li> <li>KL Divergence (right): Increases from ~0.05 to ~0.32, then stabilizes \u2014 the latent space is being used</li> </ul> <p>This is a textbook example of healthy VAE training: good reconstruction, no posterior collapse, no overfitting.</p>"},{"location":"VAE/VAE-model-training/#example-diagnosing-your-training","title":"Example: Diagnosing Your Training","text":"<p>Given this output:</p> <pre><code>epoch 001 | recon=0.84 kl=0.05\nepoch 010 | recon=0.38 kl=0.26\nepoch 018 | recon=0.28 kl=0.32\n</code></pre> <p>Diagnosis:</p> <ul> <li>\u2705 Reconstruction improving (0.84 \u2192 0.28)</li> <li>\u2705 KL increasing (0.05 \u2192 0.32) \u2014 latent space is being used</li> <li>\u2705 KL in healthy range (0.1\u20131.0)</li> <li>\u2705 No posterior collapse</li> </ul> <p>Conclusion: Training is healthy. The model is learning meaningful latent representations.</p>"},{"location":"VAE/VAE-model-training/#summary","title":"Summary","text":"Metric Want to See Red Flag Total loss Decreasing, train \u2248 val Diverging train/val Reconstruction Decreasing Stuck high KL Increases then stabilizes (0.1\u20131.0) Stays near 0 <p>The key insight: KL \u2248 0 means the latent space is useless, even though it technically satisfies the prior constraint. A healthy VAE has moderate KL, indicating the encoder is learning to compress input-specific information into the latent space.</p>"},{"location":"VAE/VAE-slides/","title":"Variational Autoencoders: A Visual Derivation","text":"<p>A 10-slide presentation on the ELBO derivation and VAE intuition.</p>"},{"location":"VAE/VAE-slides/#slide-1-latent-variable-model-notation","title":"Slide 1: Latent Variable Model &amp; Notation","text":"<p>Goal: Model data \\(x\\) as generated from latent variables \\(z\\).</p> \\[ p_\\theta(x, z) = p_\\theta(x | z) \\cdot p(z) \\] \\[ p_\\theta(x) = \\int p_\\theta(x, z) \\, dz \\] <p>Notation:</p> Symbol Meaning \\(x\\) Observed data \\(z\\) Latent variable \\(p(z)\\) Prior over latents, e.g., \\(\\mathcal{N}(0, I)\\) $p_\\theta(x z)$ $q_\\phi(z x)$"},{"location":"VAE/VAE-slides/#slide-2-why-direct-likelihood-is-hard","title":"Slide 2: Why Direct Likelihood is Hard","text":"<p>The problem: Computing \\(p_\\theta(x)\\) requires an intractable integral.</p> \\[ \\log p_\\theta(x) = \\log \\int p_\\theta(x, z) \\, dz \\] <p>\u26a0\ufe0f This integral is the bottleneck.</p> <p>The true posterior \\(p_\\theta(z|x)\\) is complex and unknown.</p> <p>Key idea: Instead of solving exactly, introduce a tractable approximation.</p>"},{"location":"VAE/VAE-slides/#slide-3-variational-approximation","title":"Slide 3: Variational Approximation","text":"<p>Solution: Learn an encoder network to approximate the true posterior.</p> \\[ q_\\phi(z|x) \\approx p_\\theta(z|x) \\] <p>Question: How do we use \\(q_\\phi\\) to get a tractable learning objective?</p>"},{"location":"VAE/VAE-slides/#slide-4-rewriting-the-likelihood","title":"Slide 4: Rewriting the Likelihood","text":"<p>Step 1: Multiply and divide by \\(q_\\phi(z|x)\\) inside the integral.</p> \\[ \\log p_\\theta(x) = \\log \\int p_\\theta(x, z) \\, dz \\] \\[ = \\log \\int q_\\phi(z|x) \\cdot \\frac{p_\\theta(x, z)}{q_\\phi(z|x)} \\, dz \\] <p>This doesn't change the value\u2014it just rewrites the integral in a useful form.</p>"},{"location":"VAE/VAE-slides/#slide-5-expectation-under-the-encoder","title":"Slide 5: Expectation Under the Encoder","text":"<p>Step 2: Recognize this as an expectation under \\(q_\\phi(z|x)\\).</p> \\[ = \\log \\mathbb{E}_{q_\\phi(z|x)} \\left[ \\frac{p_\\theta(x, z)}{q_\\phi(z|x)} \\right] \\] <p>Why this matters: We can now estimate this by sampling from \\(q_\\phi(z|x)\\).</p>"},{"location":"VAE/VAE-slides/#slide-6-lower-bounding-the-log-likelihood","title":"Slide 6: Lower Bounding the Log-Likelihood","text":"<p>Step 3: Apply Jensen's inequality.</p> \\[ \\log \\mathbb{E}[X] \\geq \\mathbb{E}[\\log X] \\] <p>Therefore:</p> \\[ \\log p_\\theta(x) \\geq \\mathbb{E}_{q_\\phi(z|x)} \\left[ \\log p_\\theta(x, z) - \\log q_\\phi(z|x) \\right] \\] <p>This is where \"lower bound\" comes from.</p>"},{"location":"VAE/VAE-slides/#slide-7-decomposing-the-bound","title":"Slide 7: Decomposing the Bound","text":"<p>Step 4: Expand the joint distribution \\(p_\\theta(x, z) = p_\\theta(x|z) \\cdot p(z)\\).</p> \\[ = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - \\mathrm{KL}(q_\\phi(z|x) \\| p(z)) \\] Term Interpretation $\\mathbb{E}{q\\phi(z x)}[\\log p_\\theta(x $\\mathrm{KL}(q_\\phi(z x) | p(z))$"},{"location":"VAE/VAE-slides/#slide-8-evidence-lower-bound-elbo","title":"Slide 8: Evidence Lower Bound (ELBO)","text":"<p>The VAE objective:</p> \\[ \\mathcal{L}(\\theta, \\phi; x) = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - \\mathrm{KL}(q_\\phi(z|x) \\| p(z)) \\] <p>Maximizing ELBO simultaneously:</p> <ol> <li>Improves reconstruction quality</li> <li>Keeps the latent distribution well-structured</li> </ol>"},{"location":"VAE/VAE-slides/#slide-9-why-the-kl-term-matters","title":"Slide 9: Why the KL Term Matters","text":"<p>Three reasons to keep \\(q(z|x)\\) close to \\(p(z)\\):</p> Reason Explanation Enables generation At test time, we sample \\(z \\sim p(z)\\). If encoder pushes latents away from prior, decoder never sees those regions. Prevents memorization Without KL, encoder could assign each datapoint a unique, sharp latent code (lookup table). Smooth latent space KL encourages nearby \\(z\\)'s to produce similar \\(x\\)'s, enabling interpolation. <p>Intuition: \"Force all data to live in the same coordinate system.\"</p>"},{"location":"VAE/VAE-slides/#slide-10-what-does-elbo-really-optimize","title":"Slide 10: What Does ELBO Really Optimize?","text":"<p>The gap between true likelihood and ELBO:</p> \\[ \\log p_\\theta(x) - \\mathcal{L} = \\mathrm{KL}(q_\\phi(z|x) \\| p_\\theta(z|x)) \\] <p>Interpretation:</p> <ul> <li>The gap is the KL between approximate and true posterior</li> <li>Gap \\(\\geq 0\\) always (KL is non-negative)</li> <li>When \\(q_\\phi = p_\\theta\\), gap is zero</li> </ul> <p>Maximizing ELBO simultaneously:</p> <ol> <li>Learns the generative model \\(p_\\theta\\)</li> <li>Performs approximate Bayesian inference</li> </ol>"},{"location":"VAE/VAE-slides/#summary-the-vae-story","title":"Summary: The VAE Story","text":"<pre><code>1. Goal         \u2192  Maximize log p(x)\n2. Obstacle     \u2192  Intractable integral over z\n3. Solution     \u2192  Introduce approximate posterior q(z|x)\n4. Derivation   \u2192  One inequality (Jensen's), one decomposition\n5. Result       \u2192  ELBO = Reconstruction \u2212 KL\n</code></pre> <p>The ELBO is a principled objective that balances:</p> <ul> <li>Explaining the data (reconstruction)</li> <li>Maintaining a usable latent space (regularization)</li> </ul>"},{"location":"VAE/VAE-slides/#references","title":"References","text":"<ul> <li>Kingma &amp; Welling (2014) \u2014 \"Auto-Encoding Variational Bayes\"</li> <li>VAE-01-overview.md \u2014 Detailed theory</li> <li>VAE-02-elbo.md \u2014 Step-by-step derivation</li> </ul>"},{"location":"beta-VAE/beta_vae/","title":"\u03b2-VAE: Disentanglement and the Information Bottleneck","text":"<p>Building on the VAE foundation, \u03b2-VAE introduces a single hyperparameter that controls the trade-off between reconstruction quality and latent space structure.</p>"},{"location":"beta-VAE/beta_vae/#1-motivation-why-modify-the-vae","title":"1. Motivation: Why Modify the VAE?","text":"<p>Standard VAEs optimize:</p> \\[ \\mathcal{L}_{\\text{VAE}} = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - \\mathrm{KL}(q(z|x) \\| p(z)) \\] <p>This treats reconstruction and regularization equally. But what if we want:</p> <ul> <li>More structured latent space \u2192 increase KL weight</li> <li>Better reconstruction \u2192 decrease KL weight</li> </ul> <p>\u03b2-VAE makes this explicit.</p>"},{"location":"beta-VAE/beta_vae/#2-the-vae-objective","title":"2. The \u03b2-VAE Objective","text":"<p>Simply multiply the KL term by \u03b2:</p> \\[ \\mathcal{L}_{\\beta\\text{-VAE}} = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - \\beta \\cdot \\mathrm{KL}(q(z|x) \\| p(z)) \\] \u03b2 value Effect \u03b2 = 1 Standard VAE \u03b2 &gt; 1 Stronger regularization \u2192 more disentanglement, worse reconstruction \u03b2 &lt; 1 Weaker regularization \u2192 better reconstruction, less structure"},{"location":"beta-VAE/beta_vae/#3-information-bottleneck-interpretation","title":"3. Information Bottleneck Interpretation","text":"<p>\u03b2-VAE can be understood through the information bottleneck framework:</p> \\[ \\max_{q(z|x)} \\; I(z; y) - \\beta \\cdot I(z; x) \\] <p>Where: - \\(I(z; y)\\) \u2014 mutual information between latent and target (reconstruction) - \\(I(z; x)\\) \u2014 mutual information between latent and input (compression)</p> <p>Higher \u03b2 forces the model to: 1. Compress more aggressively 2. Keep only the most informative features 3. Discard nuisance variation</p>"},{"location":"beta-VAE/beta_vae/#4-disentanglement-what-does-it-mean","title":"4. Disentanglement: What Does It Mean?","text":"<p>A disentangled representation has:</p> <ul> <li>Each latent dimension captures one independent factor of variation</li> <li>Changing one dimension changes one semantic attribute</li> <li>Dimensions are statistically independent</li> </ul>"},{"location":"beta-VAE/beta_vae/#example-images","title":"Example (Images)","text":"Dimension Controls \\(z_1\\) Rotation \\(z_2\\) Scale \\(z_3\\) Color"},{"location":"beta-VAE/beta_vae/#example-gene-expression","title":"Example (Gene Expression)","text":"Dimension Controls \\(z_1\\) Cell type identity \\(z_2\\) Cell cycle phase \\(z_3\\) Batch effect"},{"location":"beta-VAE/beta_vae/#5-why-1-encourages-disentanglement","title":"5. Why \u03b2 &gt; 1 Encourages Disentanglement","text":"<p>The KL term can be decomposed:</p> \\[ \\mathrm{KL}(q(z|x) \\| p(z)) = \\underbrace{I(z; x)}_{\\text{mutual info}} + \\underbrace{\\mathrm{KL}(q(z) \\| p(z))}_{\\text{marginal matching}} \\] <p>Increasing \u03b2: 1. Reduces \\(I(z; x)\\) \u2014 forces compression 2. Pushes \\(q(z)\\) toward \\(p(z) = \\mathcal{N}(0, I)\\) \u2014 encourages independence</p> <p>The factorial prior \\(p(z) = \\prod_i p(z_i)\\) induces statistical independence between dimensions.</p>"},{"location":"beta-VAE/beta_vae/#6-the-reconstruction-disentanglement-trade-off","title":"6. The Reconstruction-Disentanglement Trade-off","text":"<p>This is the fundamental tension:</p> <pre><code>\u03b2 small \u2190\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2192 \u03b2 large\nBetter reconstruction          Better disentanglement\nEntangled latents              Worse reconstruction\nOverfitting risk               Posterior collapse risk\n</code></pre>"},{"location":"beta-VAE/beta_vae/#posterior-collapse","title":"Posterior Collapse","text":"<p>When \u03b2 is too high: - \\(q(z|x) \\approx p(z)\\) for all \\(x\\) - Latent carries no information - Decoder ignores \\(z\\), generates \"average\" output</p>"},{"location":"beta-VAE/beta_vae/#7-disentanglement-metrics","title":"7. Disentanglement Metrics","text":""},{"location":"beta-VAE/beta_vae/#dci-disentanglement-completeness-informativeness","title":"DCI (Disentanglement, Completeness, Informativeness)","text":"<ul> <li>Disentanglement: Does each code capture at most one factor?</li> <li>Completeness: Is each factor captured by at most one code?</li> <li>Informativeness: Can factors be predicted from codes?</li> </ul>"},{"location":"beta-VAE/beta_vae/#mig-mutual-information-gap","title":"MIG (Mutual Information Gap)","text":"\\[ \\text{MIG} = \\frac{1}{K} \\sum_{k=1}^{K} \\frac{1}{H(v_k)} \\left( I(z_{j^{(k)}}; v_k) - \\max_{j \\neq j^{(k)}} I(z_j; v_k) \\right) \\] <p>Measures the gap between the most and second-most informative latent for each factor.</p>"},{"location":"beta-VAE/beta_vae/#sap-separated-attribute-predictability","title":"SAP (Separated Attribute Predictability)","text":"<p>Trains classifiers to predict factors from individual latents.</p>"},{"location":"beta-VAE/beta_vae/#8-implementation","title":"8. Implementation","text":""},{"location":"beta-VAE/beta_vae/#loss-function","title":"Loss Function","text":"<pre><code>def beta_vae_loss(x, x_recon, mu, logvar, beta=4.0):\n    \"\"\"\u03b2-VAE loss with configurable \u03b2.\"\"\"\n    # Reconstruction (negative log-likelihood)\n    recon_loss = F.mse_loss(x_recon, x, reduction='sum')\n\n    # KL divergence\n    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n\n    # \u03b2-weighted ELBO\n    return recon_loss + beta * kl_loss\n</code></pre>"},{"location":"beta-VAE/beta_vae/#annealing-strategies","title":"Annealing Strategies","text":"<p>Rather than fixed \u03b2, gradually increase it:</p> <pre><code>def get_beta(epoch, warmup_epochs=10, target_beta=4.0):\n    \"\"\"Linear \u03b2 annealing.\"\"\"\n    if epoch &lt; warmup_epochs:\n        return target_beta * epoch / warmup_epochs\n    return target_beta\n</code></pre> <p>This helps avoid posterior collapse early in training.</p>"},{"location":"beta-VAE/beta_vae/#9-variants-and-extensions","title":"9. Variants and Extensions","text":""},{"location":"beta-VAE/beta_vae/#-tcvae-total-correlation-vae","title":"\u03b2-TCVAE (Total Correlation VAE)","text":"<p>Decomposes KL into three terms:</p> \\[ \\mathrm{KL}(q(z|x) \\| p(z)) = \\underbrace{I(z; x)}_{\\text{index-code MI}} + \\underbrace{\\mathrm{TC}(z)}_{\\text{total correlation}} + \\underbrace{\\sum_i \\mathrm{KL}(q(z_i) \\| p(z_i))}_{\\text{dimension-wise KL}} \\] <p>Only penalizes the total correlation term, which directly measures dependence between dimensions.</p>"},{"location":"beta-VAE/beta_vae/#factor-vae","title":"Factor VAE","text":"<p>Adds an adversarial term to encourage factorial \\(q(z)\\):</p> \\[ \\mathcal{L} = \\mathcal{L}_{\\text{VAE}} + \\gamma \\cdot \\mathrm{KL}(q(z) \\| \\bar{q}(z)) \\] <p>Where \\(\\bar{q}(z) = \\prod_i q(z_i)\\) is the factorial approximation.</p>"},{"location":"beta-VAE/beta_vae/#dip-vae-disentangled-inferred-prior","title":"DIP-VAE (Disentangled Inferred Prior)","text":"<p>Matches moments of \\(q(z)\\) to the prior:</p> \\[ \\mathcal{L} = \\mathcal{L}_{\\text{VAE}} + \\lambda_{\\text{od}} \\sum_{i \\neq j} [\\text{Cov}(z)]_{ij}^2 + \\lambda_d \\sum_i ([\\text{Cov}(z)]_{ii} - 1)^2 \\]"},{"location":"beta-VAE/beta_vae/#10-application-to-gene-expression","title":"10. Application to Gene Expression","text":""},{"location":"beta-VAE/beta_vae/#why-disentanglement-matters","title":"Why Disentanglement Matters","text":"<p>In single-cell biology, we want latents that separate: - Biological signal (cell type, state) - Technical noise (batch, sequencing depth)</p> <p>A disentangled model enables: 1. Batch correction: Zero out batch dimensions 2. Counterfactuals: Change disease dimension, keep cell type 3. Interpretability: Each dimension has biological meaning</p>"},{"location":"beta-VAE/beta_vae/#practical-considerations","title":"Practical Considerations","text":"Challenge Solution No ground truth factors Use known covariates (batch, donor) as proxies High dimensionality Start with PCA-reduced input Sparse data Use negative binomial likelihood"},{"location":"beta-VAE/beta_vae/#11-experiments-to-run","title":"11. Experiments to Run","text":""},{"location":"beta-VAE/beta_vae/#experiment-1-sweep","title":"Experiment 1: \u03b2 Sweep","text":"<pre><code>betas = [0.1, 0.5, 1.0, 2.0, 4.0, 10.0]\nfor beta in betas:\n    model = BetaVAE(beta=beta)\n    train(model)\n    evaluate_reconstruction(model)\n    evaluate_disentanglement(model)\n</code></pre>"},{"location":"beta-VAE/beta_vae/#experiment-2-latent-traversal","title":"Experiment 2: Latent Traversal","text":"<p>For each dimension \\(i\\): 1. Encode a sample: \\(\\mu, \\sigma = \\text{encode}(x)\\) 2. Vary \\(z_i\\) from \\(-3\\) to \\(+3\\) 3. Decode and visualize</p>"},{"location":"beta-VAE/beta_vae/#experiment-3-condition-prediction","title":"Experiment 3: Condition Prediction","text":"<p>Train linear classifiers to predict tissue/disease from individual latent dimensions.</p>"},{"location":"beta-VAE/beta_vae/#12-connection-to-diffusion","title":"12. Connection to Diffusion","text":"<p>\u03b2-VAE's insight\u2014that compression induces structure\u2014reappears in diffusion:</p> <ul> <li>Diffusion adds noise (compression) then learns to denoise</li> <li>The noise schedule is analogous to \u03b2 annealing</li> <li>Both trade reconstruction for latent regularity</li> </ul> <p>This is why understanding \u03b2-VAE deeply prepares you for diffusion.</p>"},{"location":"beta-VAE/beta_vae/#13-references","title":"13. References","text":"<ol> <li>Higgins et al. (2017) \u2014 \"\u03b2-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework\"</li> <li>Chen et al. (2018) \u2014 \"Isolating Sources of Disentanglement in VAEs\" (\u03b2-TCVAE)</li> <li>Kim &amp; Mnih (2018) \u2014 \"Disentangling by Factorising\" (Factor VAE)</li> <li>Kumar et al. (2018) \u2014 \"Variational Inference of Disentangled Latent Concepts from Unlabeled Observations\" (DIP-VAE)</li> <li>Locatello et al. (2019) \u2014 \"Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations\"</li> </ol>"},{"location":"beta-VAE/beta_vae/#next-steps","title":"Next Steps","text":"<p>After \u03b2-VAE: 1. IWAE \u2014 Tighter bounds without changing the objective structure 2. Score matching \u2014 The bridge to diffusion models</p> <p>See ROADMAP.md for the full learning path.</p>"},{"location":"datasets/","title":"Datasets","text":"<p>This directory documents datasets used in genai-lab for training and evaluating generative models, along with their preprocessing pipelines and related code.</p>"},{"location":"datasets/#overview","title":"Overview","text":"Category Datasets Use Cases Gene Expression PBMC 3k/68k, bulk RNA-seq VAE training, latent diffusion Medical Imaging Chest X-ray (synthetic &amp; real) Diffusion models, DiT Perturbation scPerturb, Replogle, Norman scPPDM, JEPA, perturbation prediction"},{"location":"datasets/#gene-expression-datasets","title":"Gene Expression Datasets","text":"<p>Single-cell and bulk RNA-seq datasets for generative modeling.</p> Document Description PBMC.md PBMC 3k/68k dataset guide data_preparation.md RNA-seq preprocessing workflows <p>Related code:</p> <ul> <li><code>src/genailab/data/</code> \u2014 Data loading utilities</li> <li><code>src/genailab/data/sc_dataset.py</code> \u2014 Single-cell dataset classes</li> <li><code>notebooks/diffusion/04_gene_expression_diffusion/</code> \u2014 Gene expression diffusion demo</li> </ul>"},{"location":"datasets/#medical-imaging-datasets","title":"Medical Imaging Datasets","text":"<p>Datasets for training diffusion models on medical images.</p> Document Description chest_xray.md Chest X-ray datasets (synthetic &amp; real) <p>Related code:</p> <ul> <li><code>src/genailab/diffusion/datasets.py</code> \u2014 <code>SyntheticXRayDataset</code>, <code>ChestXRayDataset</code></li> <li><code>notebooks/diffusion/03_medical_imaging_diffusion/</code> \u2014 Medical imaging diffusion demo</li> </ul>"},{"location":"datasets/#perturbation-datasets","title":"Perturbation Datasets","text":"<p>Perturb-seq and CRISPR screening datasets for perturbation prediction models.</p> Document Description scperturb.md scPerturb harmonized collection perturb_seq_guide.md General Perturb-seq data handling <p>Target applications:</p> <ul> <li>scPPDM: Single-cell Perturbation Prediction via Diffusion Models</li> <li>JEPA: Joint Embedding Predictive Architecture for perturbation response</li> <li>Counterfactual generation: \"What if\" perturbation scenarios</li> </ul>"},{"location":"datasets/#data-pipeline-pattern","title":"Data Pipeline Pattern","text":"<p>Each dataset follows a consistent pipeline:</p> <pre><code>Raw Data \u2192 Preprocessing \u2192 PyTorch Dataset \u2192 DataLoader \u2192 Model\n              \u2193\n         Normalization\n         Quality Control\n         Train/Val/Test Split\n</code></pre> <p>Key considerations:</p> <ol> <li>Gene expression: Log-transform, HVG selection, NB/ZINB for counts</li> <li>Medical imaging: Resize, normalize to [-1, 1], augmentation</li> <li>Perturbation: Control vs treated pairing, batch correction</li> </ol>"},{"location":"datasets/#adding-new-datasets","title":"Adding New Datasets","text":"<p>When documenting a new dataset:</p> <ol> <li>Create a markdown file in the appropriate subdirectory</li> <li>Include:</li> <li>Source: Where to download, licensing</li> <li>Description: What the data contains</li> <li>Preprocessing: Required transformations</li> <li>Code: Related modules and notebooks</li> <li>Example usage: Code snippets</li> <li>Update this README with a link</li> </ol>"},{"location":"datasets/#related-documentation","title":"Related Documentation","text":"<ul> <li>ROADMAP.md \u2014 Learning progression</li> <li>Latent Diffusion + NB/ZINB \u2014 Count data handling</li> <li>scPPDM concepts \u2014 Perturbation modeling</li> </ul>"},{"location":"datasets/gene_expression/","title":"Gene Expression Datasets","text":"<p>Datasets for training generative models on gene expression data (single-cell and bulk RNA-seq).</p>"},{"location":"datasets/gene_expression/#available-datasets","title":"Available Datasets","text":"Dataset Type Cells/Samples Genes Document PBMC 3k scRNA-seq ~2,700 ~13,000 PBMC.md PBMC 68k scRNA-seq ~68,000 ~13,000 PBMC.md"},{"location":"datasets/gene_expression/#key-considerations-for-gene-expression","title":"Key Considerations for Gene Expression","text":""},{"location":"datasets/gene_expression/#count-data-challenge","title":"Count Data Challenge","text":"<p>Gene expression data consists of counts, not continuous values. This requires special handling:</p> <ol> <li>Preprocessing: <code>log1p</code> transform, normalization</li> <li>Model output: NB/ZINB decoder (not MSE reconstruction)</li> <li>Diffusion: Run in latent space, not raw counts</li> </ol> <p>See Latent Diffusion + NB/ZINB for details.</p>"},{"location":"datasets/gene_expression/#typical-pipeline","title":"Typical Pipeline","text":"<pre><code># 1. Load and preprocess\nadata = sc.read_h5ad(\"pbmc3k.h5ad\")\nsc.pp.normalize_total(adata, target_sum=1e4)\nsc.pp.log1p(adata)\nsc.pp.highly_variable_genes(adata, n_top_genes=2000)\n\n# 2. Create PyTorch dataset\nfrom genailab.data.sc_dataset import AnnDataDataset\ndataset = AnnDataDataset(adata, use_hvg=True)\n\n# 3. Train VAE with NB decoder\nfrom genailab.model.vae import GeneVAE_NB\nvae = GeneVAE_NB(n_genes=2000, latent_dim=128)\n</code></pre>"},{"location":"datasets/gene_expression/#related-code","title":"Related Code","text":"Module Purpose <code>src/genailab/data/sc_dataset.py</code> AnnData \u2192 PyTorch Dataset <code>src/genailab/model/decoders.py</code> NB/ZINB decoders <code>src/genailab/objectives/losses.py</code> <code>nb_loss</code>, <code>zinb_loss</code>, <code>elbo_loss_nb</code>"},{"location":"datasets/gene_expression/#related-notebooks","title":"Related Notebooks","text":"<ul> <li><code>notebooks/diffusion/04_gene_expression_diffusion/</code> \u2014 Latent diffusion for gene expression</li> <li><code>examples/01_bulk_cvae.ipynb</code> \u2014 CVAE on bulk RNA-seq</li> <li><code>examples/02_pbmc3k_cvae_nb.ipynb</code> \u2014 CVAE with NB decoder on PBMC</li> </ul>"},{"location":"datasets/gene_expression/#documents","title":"Documents","text":"<ul> <li>PBMC.md \u2014 PBMC 3k/68k dataset guide</li> <li>data_preparation.md \u2014 General RNA-seq preprocessing</li> </ul>"},{"location":"datasets/gene_expression/PBMC/","title":"PBMC Datasets for Generative Modeling","text":"<p>PBMC 3k and 68k are the MNIST of single-cell biology \u2014 standardized, clean, and the right starting point for VAE, cVAE, and score-based models.</p>"},{"location":"datasets/gene_expression/PBMC/#what-are-pbmc-datasets","title":"What Are PBMC Datasets?","text":"<p>PBMC stands for Peripheral Blood Mononuclear Cells \u2014 immune cells circulating in blood: T cells, B cells, NK cells, monocytes, and dendritic cells.</p> <p>Biologically, they're appealing because:</p> <ul> <li>They're diverse but well-studied</li> <li>They have strong, stereotyped transcriptional programs</li> <li>Cell types are separable yet overlapping (a perfect stress test for latent models)</li> </ul> <p>Technologically, PBMC datasets come from 10x Genomics and are widely used as benchmarks in single-cell analysis.</p>"},{"location":"datasets/gene_expression/PBMC/#pbmc-3k-the-hello-world-of-scrna-seq","title":"PBMC 3k: The \"Hello World\" of scRNA-seq","text":"<p>PBMC 3k contains ~2,700 cells from a healthy donor, sequenced using droplet-based scRNA-seq.</p>"},{"location":"datasets/gene_expression/PBMC/#what-you-get-per-cell","title":"What You Get Per Cell","text":"<ul> <li>A vector of raw UMI counts (~13,000\u201320,000 genes after filtering)</li> <li>Extreme sparsity (90\u201395% zeros)</li> <li>A total count per cell (library size)</li> <li>Latent biological structure visible even with simple models</li> </ul>"},{"location":"datasets/gene_expression/PBMC/#why-its-perfect-for-this-roadmap","title":"Why It's Perfect for This Roadmap","text":"<ul> <li>Small enough to iterate fast</li> <li>Large enough to expose overdispersion</li> <li>Cell types are known and interpretable</li> <li>Ideal for validating NB vs ZINB likelihoods</li> <li>Excellent for latent space visualization (UMAP/t-SNE)</li> </ul> <p>PBMC 3k lets you verify that your ELBO, KL term, decoder parameterization, and library-size handling are correct before scaling.</p> <p>From a generative-modeling perspective, PBMC 3k is where you learn to respect the data manifold.</p>"},{"location":"datasets/gene_expression/PBMC/#pbmc-68k-same-biology-different-regime","title":"PBMC 68k: Same Biology, Different Regime","text":"<p>PBMC 68k is the same kind of data, but in a different computational universe: ~68,000 cells.</p> <p>What changes is not biology \u2014 it's statistics and scaling.</p>"},{"location":"datasets/gene_expression/PBMC/#key-differences","title":"Key Differences","text":"<ul> <li>Many more rare cell states</li> <li>Much sharper estimates of dispersion parameters</li> <li>Clearer separation between biological and technical noise</li> <li>Enough data to expose posterior collapse issues in VAEs</li> <li>Enough scale to make diffusion / score models meaningful</li> </ul>"},{"location":"datasets/gene_expression/PBMC/#where-it-matters","title":"Where It Matters","text":"<ul> <li>Amortized inference actually matters</li> <li>Batch effects start to bite</li> <li>Latent dimensionality becomes nontrivial</li> <li>Conditional models (cVAE) become obviously useful</li> </ul> <p>If PBMC 3k asks \"does this work?\", PBMC 68k asks \"does this still work under pressure?\"</p>"},{"location":"datasets/gene_expression/PBMC/#why-pbmc-is-ideal-for-generative-expression-modeling","title":"Why PBMC Is Ideal for Generative Expression Modeling","text":"<p>A core goal in computational biology \u2014 pursued by companies like Synthesize Bio, insitro, and others \u2014 is to:</p> <p>Generate biologically realistic gene expression states under different conditions</p> <p>This enables studying treatment responses, predicting perturbation effects, and simulating counterfactual scenarios in silico.</p> <p>PBMC datasets let you practice exactly that in miniature:</p> Concept PBMC Mapping Condition Cell type (T cell, monocyte, etc.) Latent Cell state, activation, continuous variation Generation Simulate realistic immune profiles Counterfactual \"What if this monocyte were a T cell?\" <p>That's not toy modeling \u2014 that's the same abstraction used in scVI, scGen, and industry-scale expression simulators.</p>"},{"location":"datasets/gene_expression/PBMC/#what-pbmc-teaches-you","title":"What PBMC Teaches You","text":"<ul> <li>How much structure comes from the condition</li> <li>How much must live in the latent</li> <li>When the model cheats by encoding labels in z</li> <li>How overdispersion actually looks in practice</li> </ul>"},{"location":"datasets/gene_expression/PBMC/#how-pbmc-fits-into-our-roadmap","title":"How PBMC Fits Into Our Roadmap","text":"<p>Mapping this directly onto our genai-lab roadmap:</p>"},{"location":"datasets/gene_expression/PBMC/#vae-stage-1","title":"VAE (Stage 1)","text":"<ul> <li>PBMC 3k: Learn reconstruction + KL balance</li> <li>PBMC 68k: Test stability, latent capacity</li> </ul>"},{"location":"datasets/gene_expression/PBMC/#cvae","title":"cVAE","text":"<ul> <li>Condition on cell type</li> <li>Generate realistic expression given (z, cell_type)</li> <li>Perform label swapping to test disentanglement</li> </ul>"},{"location":"datasets/gene_expression/PBMC/#-vae","title":"\u03b2-VAE","text":"<ul> <li>Explore how much biological variation survives stronger bottlenecks</li> </ul>"},{"location":"datasets/gene_expression/PBMC/#score-matching-diffusion-stage-3","title":"Score Matching / Diffusion (Stage 3+)","text":"<ul> <li>PBMC 68k gives enough density to meaningfully estimate scores</li> <li>Noise \u2192 denoise \u2192 recover biological structure</li> </ul> <p>At each stage, PBMC acts as a known reference system. If the model fails here, it won't magically succeed on GTEx or TCGA.</p>"},{"location":"datasets/gene_expression/PBMC/#critical-point-keep-raw-counts","title":"Critical Point: Keep Raw Counts","text":"<p>PBMC data must stay in raw count space if you're serious about generative modeling.</p> <p>The correct approach (as in <code>data_preparation.md</code>):</p> <ul> <li>No log1p</li> <li>No CPM</li> <li>No normalization before modeling</li> <li>Library size treated explicitly</li> </ul> <p>This is not pedantry. If you normalize first, you destroy the generative story and force the model to learn artifacts instead of biology.</p> <p>PBMC is forgiving enough to show you this mistake clearly \u2014 another reason it's a great teacher.</p>"},{"location":"datasets/gene_expression/PBMC/#big-picture-takeaway","title":"Big Picture Takeaway","text":"<p>PBMC 3k and 68k are not just datasets \u2014 they're didactic instruments.</p> <p>They teach you:</p> <ul> <li>How generative assumptions meet biological reality</li> <li>How overdispersion emerges naturally</li> <li>How conditioning interacts with latent structure</li> <li>How scale changes model behavior</li> </ul> <p>Once PBMC feels intuitive, moving to Tabula Sapiens, GTEx, or disease cohorts becomes an engineering problem, not a conceptual one.</p>"},{"location":"datasets/gene_expression/PBMC/#next-steps","title":"Next Steps","text":"<ul> <li>Walk through PBMC preprocessing line-by-line</li> <li>Sketch the exact cVAE computational graph for PBMC</li> <li>Design evaluation metrics for realistic expression generation</li> </ul>"},{"location":"datasets/gene_expression/PBMC/#is-it-okay-to-use-normalized-counts","title":"Is It Okay to Use Normalized Counts?","text":"<p>This is a really important question, and the confusion is extremely common \u2014 even among people who use NB/ZINB every day.</p> <p>Short answer: Normalized counts look like counts, but they are no longer generated by a count process.</p> <p>The long answer is where the insight lives.</p>"},{"location":"datasets/gene_expression/PBMC/#1-what-nbzinb-are-actually-modeling","title":"1. What NB/ZINB Are Actually Modeling","text":"<p>A Negative Binomial model is not just a curve that fits integers. It encodes a physical data-generation process:</p> <ol> <li>True expression rate of gene g in cell i</li> <li>Sequencing depth / capture efficiency of that cell (library size)</li> <li>Sampling noise + biological variability</li> </ol> <p>Mathematically, the canonical scRNA-seq NB story is:</p> <p>\"Given a cell with library size \\(\\ell_i\\), gene g produces counts \\(y_{ig}\\) drawn from an NB distribution with mean proportional to \\(\\ell_i\\).\"</p> <p>Written plainly:</p> <ul> <li>Counts increase when you sequence deeper</li> <li>Variance increases with the mean (overdispersion)</li> <li>Zero inflation comes from biology + dropout, not arithmetic tricks</li> </ul> <p>That story is true in the wet lab. That's why NB works so well.</p>"},{"location":"datasets/gene_expression/PBMC/#2-what-normalization-actually-does","title":"2. What Normalization Actually Does","text":"<p>Normalization (CPM, TPM, size-factor normalization, log1p, etc.) rewrites the data to answer a different question:</p> <p>\"What would expression look like if all cells had the same depth?\"</p> <p>That is a deterministic transformation of the raw counts.</p> <p>Example (CPM):</p> <pre><code>y_ig  \u2192  y_ig / \u2113_i \u00d7 10\u2076\n</code></pre> <p>Key observation:</p> <ul> <li>\\(\\ell_i\\) (library size) is removed</li> <li>Depth variability is collapsed</li> <li>Values are now ratios, not samples from a counting process</li> </ul> <p>You didn't just rescale the data. You changed the random variable.</p>"},{"location":"datasets/gene_expression/PBMC/#3-why-theyre-still-counts-is-misleading","title":"3. Why \"They're Still Counts\" Is Misleading","text":"<p>After normalization:</p> <ul> <li>Values may still be non-negative</li> <li>They may even be integers (if you round)</li> <li>They may look \"count-like\"</li> </ul> <p>But generatively, they are no longer counts.</p> Question Answer Type \"How many molecules did I observe?\" True count \"What fraction of my sequencing budget went to this gene?\" Normalized value <p>Those are not the same random experiment.</p> <p>NB/ZINB likelihoods assume:</p> <ul> <li>Randomness comes from molecular sampling</li> <li>Variance depends on the mean and depth</li> <li>Library size is a latent or observed covariate</li> </ul> <p>After normalization:</p> <ul> <li>Depth is fixed by construction</li> <li>Variance is artificially homogenized</li> <li>Mean\u2013variance coupling is broken</li> </ul> <p>So the likelihood is wrong, even if the curve \"fits\".</p>"},{"location":"datasets/gene_expression/PBMC/#4-the-hidden-problem-double-using-library-size","title":"4. The Hidden Problem: Double-Using Library Size","text":"<p>This is the subtle killer.</p> <p>If you:</p> <ol> <li>Normalize counts to remove library size</li> <li>Then train a model with an NB decoder</li> </ol> <p>You've implicitly told the model:</p> <p>\"Pretend depth doesn't matter \u2014 but also explain variance as if it did.\"</p> <p>That contradiction forces the model to:</p> <ul> <li>Invent fake dispersion</li> <li>Misuse the latent space</li> <li>Leak technical effects into z</li> <li>Blur biology and noise</li> </ul> <p>This is one of the main reasons people see:</p> <ul> <li>Posterior collapse</li> <li>Mushy latent spaces</li> <li>Poor counterfactuals</li> </ul> <p>The model is trying to explain artifacts you injected.</p>"},{"location":"datasets/gene_expression/PBMC/#5-why-normalization-is-fine-for-some-tasks","title":"5. Why Normalization Is Fine for Some Tasks","text":"<p>Normalization isn't evil. It's just task-specific.</p> <p>Appropriate for (descriptive tasks):</p> <ul> <li>Clustering</li> <li>Visualization (UMAP / PCA)</li> <li>Differential expression heuristics</li> <li>Linear models assuming homoskedasticity</li> </ul> <p>Not appropriate for (generative tasks):</p> <ul> <li>Asserting a data-generating process</li> <li>Likelihood-based evaluation</li> <li>Counterfactuals that must respect physics</li> </ul> <p>That's why scVI, scGen, and industry-scale models all operate on raw counts with explicit size factors.</p>"},{"location":"datasets/gene_expression/PBMC/#6-the-correct-way-to-normalize-in-generative-models","title":"6. The Correct Way to \"Normalize\" in Generative Models","text":"<p>Instead of transforming the data, you transform the model.</p> <p>The canonical trick:</p> <p>Keep raw counts; include library size as an offset or covariate</p> <p>Conceptually:</p> <ul> <li>Decoder predicts a rate</li> <li>Library size scales that rate</li> <li>NB handles overdispersion naturally</li> </ul> <p>This preserves:</p> <ul> <li>Correct variance structure</li> <li>Biological signal</li> <li>Valid sampling semantics</li> </ul> <p>Nothing is thrown away.</p>"},{"location":"datasets/gene_expression/PBMC/#7-a-mental-checksum","title":"7. A Mental Checksum","text":"<p>Ask this question:</p> <p>\"Could I plausibly simulate raw sequencing data from this representation?\"</p> <ul> <li>Raw counts \u2192 yes</li> <li>Normalized counts \u2192 no</li> </ul> <p>If you can't simulate sequencing, you're not doing generative biology \u2014 you're doing regression on a convenience transform.</p>"},{"location":"datasets/gene_expression/PBMC/#8-why-this-matters-for-diffusion-models","title":"8. Why This Matters for Diffusion Models","text":"<p>This becomes even more critical for:</p> <ul> <li>Score matching</li> <li>Diffusion models</li> <li>Likelihood-based evaluation</li> </ul> <p>Those methods assume:</p> <ul> <li>The data distribution is real</li> <li>Noise has a physical interpretation</li> <li>The model can walk backward to plausible samples</li> </ul> <p>Normalized data breaks that chain completely.</p>"},{"location":"datasets/gene_expression/PBMC/#bottom-line","title":"Bottom Line","text":"<p>Normalization answers analytical questions. Raw counts answer generative questions.</p> <p>If your goal is:</p> <ul> <li>Counterfactual gene expression</li> <li>In silico experiments</li> <li>Realistic sampling</li> <li>Industry-grade generative models</li> </ul> <p>Then raw counts + explicit library size is not a preference \u2014 it's a requirement.</p> <p>This distinction is one of the big conceptual thresholds between using generative models and understanding them.</p>"},{"location":"datasets/gene_expression/data_preparation/","title":"Data Preparation for Generative Models","text":"<p>This document describes how to obtain and preprocess real-world gene expression datasets for training and evaluating generative models (VAE, diffusion, etc.).</p>"},{"location":"datasets/gene_expression/data_preparation/#1-why-real-data-matters","title":"1. Why Real Data Matters","text":"<p>To objectively compare different generative approaches (VAE vs diffusion, NB vs ZINB, etc.), we need:</p> <ul> <li>Real count distributions with overdispersion and sparsity</li> <li>Meaningful conditions (tissue, disease, cell type) for conditional generation</li> <li>Held-out test sets for likelihood-based evaluation</li> </ul>"},{"location":"datasets/gene_expression/data_preparation/#2-recommended-datasets","title":"2. Recommended Datasets","text":""},{"location":"datasets/gene_expression/data_preparation/#21-scrna-seq-start-here","title":"2.1 scRNA-seq (Start Here)","text":"Dataset Description Size Conditions Link PBMC 3k Classic starter dataset ~2,700 cells Cell type 10x Genomics PBMC 68k Larger PBMC dataset ~68,000 cells Cell type 10x Genomics Tabula Sapiens Multi-tissue human atlas ~500k cells Tissue, cell type, donor Portal Tabula Muris Multi-tissue mouse atlas ~100k cells Tissue, cell type Portal <p>Note: For UMI-based scRNA-seq, NB is often sufficient. Add ZINB only if NB badly underfits zeros.</p>"},{"location":"datasets/gene_expression/data_preparation/#22-bulk-rna-seq-later","title":"2.2 Bulk RNA-seq (Later)","text":"Dataset Description Size Conditions Link GTEx Multi-tissue, healthy baseline ~17k samples Tissue, sex, age GTEx Portal recount3 Uniformly processed public RNA-seq Massive Study-dependent recount3 TCGA Cancer transcriptomes ~11k samples Cancer type, stage GDC Portal <p>Note: For bulk RNA-seq, NB is typically the right likelihood; ZINB is rarely necessary.</p>"},{"location":"datasets/gene_expression/data_preparation/#3-preprocessing-scripts","title":"3. Preprocessing Scripts","text":""},{"location":"datasets/gene_expression/data_preparation/#31-scrna-seq-python-scanpy","title":"3.1 scRNA-seq (Python + Scanpy)","text":"<p>Script: <code>src/genailab/data/sc_preprocess.py</code></p> <p>What it does:</p> <ol> <li>Loads 10x MTX format or downloads PBMC3k directly</li> <li>Computes QC metrics (n_counts, n_genes, mito %)</li> <li>Filters low-quality cells and genes</li> <li>Computes library size for NB models</li> <li>Saves raw counts as <code>.h5ad</code></li> </ol> <p>Key principle: Do NOT normalize or log-transform if using NB/ZINB likelihood.</p>"},{"location":"datasets/gene_expression/data_preparation/#32-bulk-rna-seq-r-recount3","title":"3.2 Bulk RNA-seq (R + recount3)","text":"<p>Script: <code>src/genailab/data/bulk_recount3_preprocess.R</code></p> <p>What it does:</p> <ol> <li>Downloads uniformly processed counts from recount3</li> <li>Extracts counts matrix and sample metadata</li> <li>Filters lowly-expressed genes</li> <li>Saves as RDS (can convert to CSV for Python)</li> </ol> <p>Reference: recount3 quickstart</p>"},{"location":"datasets/gene_expression/data_preparation/#33-bulk-rna-seq-python-alternative","title":"3.3 Bulk RNA-seq (Python Alternative)","text":"<p>Script: <code>src/genailab/data/bulk_preprocess.py</code></p> <p>What it does:</p> <ol> <li>Loads counts from CSV files (exported from R or downloaded from portals)</li> <li>Optionally downloads from GEO using GEOparse</li> <li>Computes library size for NB models</li> <li>Filters lowly-expressed genes</li> <li>Converts to AnnData format (same as scRNA-seq)</li> </ol> <p>Usage examples:</p> <pre><code># From CSV files (e.g., exported from R/recount3)\npython -m genailab.data.bulk_preprocess csv \\\n    --counts bulk_counts.csv \\\n    --metadata bulk_metadata.csv \\\n    --output bulk.h5ad\n\n# From GEO (requires: pip install GEOparse)\npython -m genailab.data.bulk_preprocess geo \\\n    --geo-id GSE12345 \\\n    --output bulk.h5ad\n</code></pre> <p>Workflow: Use R/recount3 to download uniformly processed counts, export to CSV, then use Python for ML pipeline</p>"},{"location":"datasets/gene_expression/data_preparation/#4-wiring-conditions-for-cvae","title":"4. Wiring Conditions for cVAE","text":"<p>Once you have counts and metadata, create a condition table:</p>"},{"location":"datasets/gene_expression/data_preparation/#41-bulk-rna-seq-conditions","title":"4.1 Bulk RNA-seq Conditions","text":"Condition Type Example Values <code>tissue</code> Categorical \"liver\", \"brain\", \"heart\" <code>disease_status</code> Categorical \"healthy\", \"tumor\", \"treated\" <code>batch</code> Categorical \"batch1\", \"batch2\" <code>sex</code> Categorical \"M\", \"F\" <code>age</code> Continuous 25, 45, 67"},{"location":"datasets/gene_expression/data_preparation/#42-scrna-seq-conditions","title":"4.2 scRNA-seq Conditions","text":"Condition Type Example Values <code>cell_type</code> Categorical \"T cell\", \"B cell\", \"Monocyte\" <code>tissue</code> Categorical \"blood\", \"lung\", \"liver\" <code>donor</code> Categorical \"donor1\", \"donor2\" <code>batch</code> Categorical \"10x_v2\", \"10x_v3\" <p>These become categorical IDs \u2192 embedding tables in the cVAE encoder/decoder.</p>"},{"location":"datasets/gene_expression/data_preparation/#5-critical-checklist-for-nbzinb-models","title":"5. Critical Checklist for NB/ZINB Models","text":"<ul> <li> Keep raw counts in the training tensor (no normalization)</li> <li> Compute library size (total counts per sample/cell) as offset or covariate</li> <li> Start with NB, upgrade to ZINB only if NB underfits zeros on held-out data</li> <li> Include batch as a condition (even if you later want invariance)</li> <li> Filter genes: Remove genes expressed in &lt;3 cells/samples</li> <li> Filter cells/samples: Remove outliers by QC metrics</li> </ul>"},{"location":"datasets/gene_expression/data_preparation/#6-library-size-why-it-matters","title":"6. Library Size: Why It Matters","text":"<p>Library size (total counts per cell/sample) varies due to technical factors, not biology.</p> <p>For NB models, the typical parameterization is:</p> \\[ \\mu_g = \\ell \\cdot \\exp(\\eta_g) \\] <p>where:</p> <ul> <li>\\(\\ell\\) = library size (or learned size factor)</li> <li>\\(\\eta_g\\) = what the decoder predicts from \\((z, y)\\)</li> </ul> <p>How to compute:</p> <pre><code># scRNA-seq (scanpy)\nadata.obs[\"library_size\"] = np.array(adata.X.sum(axis=1)).ravel()\n\n# Bulk RNA-seq (pandas)\nlibrary_size = counts.sum(axis=0)  # sum over genes\n</code></pre>"},{"location":"datasets/gene_expression/data_preparation/#7-output-format-for-ml","title":"7. Output Format for ML","text":"<p>Both scRNA-seq and bulk RNA-seq should produce:</p> File Contents <code>counts.h5ad</code> or <code>counts.csv</code> Raw count matrix (genes \u00d7 samples/cells) <code>metadata.csv</code> Sample/cell metadata with conditions <code>library_size.npy</code> Precomputed library sizes <p>The <code>.h5ad</code> format (AnnData) is preferred because it stores counts, metadata, and gene info together.</p>"},{"location":"datasets/gene_expression/data_preparation/#references","title":"References","text":"<ul> <li>Scanpy tutorials</li> <li>recount3 quickstart</li> <li>GTEx Portal</li> <li>10x Genomics Datasets</li> </ul>"},{"location":"datasets/medical_imaging/","title":"Medical Imaging Datasets","text":"<p>Datasets for training diffusion models and other generative models on medical images.</p>"},{"location":"datasets/medical_imaging/#available-datasets","title":"Available Datasets","text":"Dataset Type Size Resolution Document Synthetic X-Ray Generated Configurable Configurable chest_xray.md Kaggle Chest X-Ray Real ~5,800 Variable chest_xray.md"},{"location":"datasets/medical_imaging/#use-cases","title":"Use Cases","text":"<p>Medical imaging datasets in genai-lab are used for:</p> <ol> <li>Diffusion model development: Testing score networks, noise schedules, sampling</li> <li>DiT training: Demonstrating Transformer-based diffusion</li> <li>Flow matching: Rectified flow on realistic image data</li> <li>Benchmarking: Comparing architectures on structured data</li> </ol>"},{"location":"datasets/medical_imaging/#related-code","title":"Related Code","text":"Module Purpose <code>src/genailab/diffusion/datasets.py</code> <code>SyntheticXRayDataset</code>, <code>ChestXRayDataset</code> <code>src/genailab/diffusion/architectures.py</code> <code>UNet2D</code>, <code>UNet3D</code> for image diffusion <code>src/genailab/diffusion/training.py</code> <code>train_image_diffusion</code>"},{"location":"datasets/medical_imaging/#related-notebooks","title":"Related Notebooks","text":"<ul> <li><code>notebooks/diffusion/03_medical_imaging_diffusion/</code> \u2014 Full pipeline demo</li> </ul>"},{"location":"datasets/medical_imaging/#documents","title":"Documents","text":"<ul> <li>chest_xray.md \u2014 Chest X-ray datasets (synthetic and real)</li> </ul>"},{"location":"datasets/medical_imaging/chest_xray/","title":"Chest X-Ray Datasets","text":"<p>This document covers chest X-ray datasets available in genai-lab for training diffusion models and other generative architectures.</p>"},{"location":"datasets/medical_imaging/chest_xray/#1-synthetic-x-ray-dataset","title":"1. Synthetic X-Ray Dataset","text":"<p>Purpose: Testing and development without requiring real medical data.</p>"},{"location":"datasets/medical_imaging/chest_xray/#description","title":"Description","text":"<p><code>SyntheticXRayDataset</code> generates diverse synthetic chest X-ray images with randomized anatomical structures:</p> <ul> <li>Lungs: Elliptical shapes with variable position, size, darkness</li> <li>Heart: Circular structure between lungs</li> <li>Ribs: Curved horizontal lines with variable spacing</li> <li>Spine: Optional vertical structure</li> <li>Clavicles: Optional horizontal structures at top</li> </ul>"},{"location":"datasets/medical_imaging/chest_xray/#usage","title":"Usage","text":"<pre><code>from genailab.diffusion.datasets import SyntheticXRayDataset\n\n# Create dataset\ndataset = SyntheticXRayDataset(\n    n_samples=1000,    # Number of images to generate\n    img_size=128,      # Image dimensions (square)\n    seed=42            # Reproducibility\n)\n\n# Access images\nimg = dataset[0]  # Returns tensor of shape (1, 128, 128) in [-1, 1]\n</code></pre>"},{"location":"datasets/medical_imaging/chest_xray/#characteristics","title":"Characteristics","text":"Property Value Output shape <code>(1, H, W)</code> grayscale Value range <code>[-1, 1]</code> (normalized) Generation On-init (all images pre-generated) Randomization Per-image anatomical variation"},{"location":"datasets/medical_imaging/chest_xray/#when-to-use","title":"When to Use","text":"<ul> <li>Development: Fast iteration without data download</li> <li>Testing: Verify pipeline correctness</li> <li>Debugging: Reproducible synthetic data</li> <li>Demos: Quick demonstrations</li> </ul>"},{"location":"datasets/medical_imaging/chest_xray/#2-real-chest-x-ray-dataset-kaggle","title":"2. Real Chest X-Ray Dataset (Kaggle)","text":"<p>Purpose: Training on real medical images for realistic diffusion models.</p>"},{"location":"datasets/medical_imaging/chest_xray/#source","title":"Source","text":"<p>Kaggle Chest X-Ray Images (Pneumonia)</p> <ul> <li>URL: https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia</li> <li>License: CC BY 4.0</li> <li>Size: ~1.2 GB</li> </ul>"},{"location":"datasets/medical_imaging/chest_xray/#dataset-structure","title":"Dataset Structure","text":"<pre><code>chest_xray/\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 NORMAL/      (~1,300 images)\n\u2502   \u2514\u2500\u2500 PNEUMONIA/   (~3,900 images)\n\u251c\u2500\u2500 val/\n\u2502   \u251c\u2500\u2500 NORMAL/\n\u2502   \u2514\u2500\u2500 PNEUMONIA/\n\u2514\u2500\u2500 test/\n    \u251c\u2500\u2500 NORMAL/\n    \u2514\u2500\u2500 PNEUMONIA/\n</code></pre>"},{"location":"datasets/medical_imaging/chest_xray/#usage_1","title":"Usage","text":"<pre><code>from genailab.diffusion.datasets import ChestXRayDataset\n\n# Load real X-rays (normal cases)\ndataset = ChestXRayDataset(\n    root_dir=\"data/chest_xray/train/NORMAL\",\n    img_size=128\n)\n\n# Or use factory function\nfrom genailab.diffusion.datasets import get_dataset\n\ndataset = get_dataset(\n    name='chest_xray',\n    root_dir=\"data/chest_xray/train/NORMAL\",\n    img_size=128\n)\n</code></pre>"},{"location":"datasets/medical_imaging/chest_xray/#download-instructions","title":"Download Instructions","text":"<ol> <li>Create Kaggle account and API token</li> <li>Install kaggle CLI: <code>pip install kaggle</code></li> <li>Download:</li> </ol> <pre><code>kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\nunzip chest-xray-pneumonia.zip -d data/\n</code></pre>"},{"location":"datasets/medical_imaging/chest_xray/#characteristics_1","title":"Characteristics","text":"Property Value Original resolution Variable (typically 1000-2000 px) Output shape <code>(1, H, W)</code> grayscale (resized) Value range <code>[-1, 1]</code> (normalized) Classes NORMAL, PNEUMONIA"},{"location":"datasets/medical_imaging/chest_xray/#3-factory-function","title":"3. Factory Function","text":"<p>Use <code>get_dataset()</code> for unified access:</p> <pre><code>from genailab.diffusion.datasets import get_dataset\n\n# Synthetic\nsynthetic_ds = get_dataset('synthetic', img_size=128, n_samples=1000)\n\n# Real\nreal_ds = get_dataset('chest_xray', root_dir='data/chest_xray/train/NORMAL', img_size=128)\n</code></pre>"},{"location":"datasets/medical_imaging/chest_xray/#4-integration-with-training","title":"4. Integration with Training","text":""},{"location":"datasets/medical_imaging/chest_xray/#with-diffusion-training","title":"With Diffusion Training","text":"<pre><code>from torch.utils.data import DataLoader\nfrom genailab.diffusion.datasets import get_dataset\nfrom genailab.diffusion.training import train_image_diffusion\n\n# Create dataset and loader\ndataset = get_dataset('synthetic', img_size=64, n_samples=500)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Train diffusion model\nmodel, losses = train_image_diffusion(\n    dataloader=dataloader,\n    img_size=64,\n    n_epochs=50,\n    device='cuda'\n)\n</code></pre>"},{"location":"datasets/medical_imaging/chest_xray/#see-also","title":"See Also","text":"<ul> <li><code>notebooks/diffusion/03_medical_imaging_diffusion/</code> \u2014 Full tutorial</li> </ul>"},{"location":"datasets/medical_imaging/chest_xray/#5-extending-with-new-datasets","title":"5. Extending with New Datasets","text":"<p>To add a new medical imaging dataset:</p> <ol> <li>Create a new <code>Dataset</code> class in <code>src/genailab/diffusion/datasets.py</code></li> <li>Implement <code>__len__</code> and <code>__getitem__</code></li> <li>Ensure output is <code>(C, H, W)</code> tensor in <code>[-1, 1]</code></li> <li>Add to <code>get_dataset()</code> factory function</li> <li>Document here</li> </ol>"},{"location":"datasets/medical_imaging/chest_xray/#related-code","title":"Related Code","text":"File Contents <code>src/genailab/diffusion/datasets.py</code> <code>SyntheticXRayDataset</code>, <code>ChestXRayDataset</code>, <code>get_dataset</code> <code>src/genailab/diffusion/training.py</code> <code>train_image_diffusion</code> <code>src/genailab/diffusion/architectures.py</code> <code>UNet2D</code> for image diffusion"},{"location":"datasets/perturbation/","title":"Perturbation Datasets","text":"<p>Datasets for training perturbation prediction models (scPPDM, JEPA) on CRISPR screening and Perturb-seq data.</p>"},{"location":"datasets/perturbation/#overview","title":"Overview","text":"<p>Perturbation datasets capture cellular responses to genetic interventions (CRISPR knockouts, knockdowns, overexpression). These are essential for:</p> <ul> <li>scPPDM: Single-cell Perturbation Prediction via Diffusion Models</li> <li>JEPA: Joint Embedding Predictive Architecture for perturbation response</li> <li>Counterfactual generation: Predicting \"what if\" scenarios</li> </ul>"},{"location":"datasets/perturbation/#key-datasets","title":"Key Datasets","text":"Dataset Cells Perturbations Cell Type Document scPerturb Multiple Multiple Various scperturb.md Replogle 2022 ~2.5M &gt;5,000 K562 scperturb.md Norman 2019 ~100k ~300 (combinatorial) K562 scperturb.md Adamson 2016 ~10k ~100 K562 scperturb.md"},{"location":"datasets/perturbation/#data-structure","title":"Data Structure","text":"<p>Perturb-seq data typically includes:</p> <pre><code>AnnData object:\n\u251c\u2500\u2500 X: Expression matrix (cells \u00d7 genes)\n\u251c\u2500\u2500 obs: Cell metadata\n\u2502   \u251c\u2500\u2500 perturbation: Gene targeted\n\u2502   \u251c\u2500\u2500 is_control: Boolean (NT control)\n\u2502   \u2514\u2500\u2500 cell_type, batch, etc.\n\u2514\u2500\u2500 var: Gene metadata\n    \u2514\u2500\u2500 gene_name, highly_variable, etc.\n</code></pre>"},{"location":"datasets/perturbation/#key-fields","title":"Key Fields","text":"<ul> <li>Control cells: Non-targeting (NT) guide, baseline expression</li> <li>Perturbed cells: Expression after intervention</li> <li>Perturbation label: Which gene was targeted</li> </ul>"},{"location":"datasets/perturbation/#use-cases-in-genai-lab","title":"Use Cases in genai-lab","text":""},{"location":"datasets/perturbation/#1-scppdm-diffusion-based","title":"1. scPPDM (Diffusion-based)","text":"<p>Train diffusion model to predict post-perturbation expression:</p> <pre><code>Control expression + Perturbation embedding \u2192 Diffusion \u2192 Perturbed expression\n</code></pre>"},{"location":"datasets/perturbation/#2-jepa-embedding-based","title":"2. JEPA (Embedding-based)","text":"<p>Predict perturbation effects in latent space:</p> <pre><code>Control latent + Perturbation \u2192 Predictor \u2192 Perturbed latent\n</code></pre>"},{"location":"datasets/perturbation/#3-counterfactual-generation","title":"3. Counterfactual Generation","text":"<p>Generate \"what if\" scenarios:</p> <ul> <li>What if gene X was knocked out in this cell?</li> <li>What is the distribution of possible outcomes?</li> </ul>"},{"location":"datasets/perturbation/#documents","title":"Documents","text":"<ul> <li>scperturb.md \u2014 scPerturb harmonized collection</li> <li>perturb_seq_guide.md \u2014 General Perturb-seq data handling</li> </ul>"},{"location":"datasets/perturbation/#related-documentation","title":"Related Documentation","text":"<ul> <li>Generative AI for Perturbation Modeling</li> <li>Joint Latent Spaces and JEPA</li> <li>Alternative Backbones for Biology</li> </ul>"},{"location":"datasets/perturbation/scperturb/","title":"scPerturb: Harmonized Perturbation Datasets","text":"<p>scPerturb is a harmonized collection of single-cell perturbation datasets, providing a unified resource for training and benchmarking perturbation prediction models.</p>"},{"location":"datasets/perturbation/scperturb/#overview","title":"Overview","text":"Property Value Source scperturb.org Paper Peidli et al. (2024) \"scPerturb: Harmonized Single-Cell Perturbation Data\" Format AnnData (h5ad) License Various (check individual datasets)"},{"location":"datasets/perturbation/scperturb/#available-datasets","title":"Available Datasets","text":""},{"location":"datasets/perturbation/scperturb/#recommended-for-scppdm-jepa","title":"Recommended for scPPDM / JEPA","text":"Dataset Perturbations Cells Cell Type Modality Notes Replogle 2022 (K562) &gt;5,000 genes ~2.5M K562 CRISPRi Largest, recommended for training Norman 2019 ~300 (combinatorial) ~100k K562 CRISPRa Combinatorial perturbations Adamson 2016 ~100 (UPR) ~10k K562 CRISPRi Focused on UPR pathway Dixit 2016 ~24 ~10k Dendritic CRISPRi Original Perturb-seq paper"},{"location":"datasets/perturbation/scperturb/#additional-datasets","title":"Additional Datasets","text":"Dataset Focus Size Frangieh 2021 Cancer immunotherapy ~200k cells Papalexi 2021 ECCITE-seq (protein + RNA) ~40k cells Gasperini 2019 Enhancer perturbations ~200k cells"},{"location":"datasets/perturbation/scperturb/#data-access","title":"Data Access","text":""},{"location":"datasets/perturbation/scperturb/#option-1-scperturb-portal","title":"Option 1: scPerturb Portal","text":"<p>Download pre-processed h5ad files from scperturb.org.</p>"},{"location":"datasets/perturbation/scperturb/#option-2-python-api","title":"Option 2: Python API","text":"<pre><code># Install scperturb\npip install scperturb\n\n# Download dataset\nimport scperturb\nadata = scperturb.load_dataset(\"replogle_2022_k562\")\n</code></pre>"},{"location":"datasets/perturbation/scperturb/#option-3-direct-download","title":"Option 3: Direct Download","text":"<pre><code># Replogle 2022 K562 (large, ~5GB)\nwget https://scperturb.org/data/replogle_2022_k562.h5ad\n\n# Norman 2019 (smaller, good for testing)\nwget https://scperturb.org/data/norman_2019.h5ad\n</code></pre>"},{"location":"datasets/perturbation/scperturb/#data-structure","title":"Data Structure","text":"<p>All scPerturb datasets follow a consistent schema:</p> <pre><code>import scanpy as sc\n\nadata = sc.read_h5ad(\"replogle_2022_k562.h5ad\")\n\n# Key fields in adata.obs:\n# - perturbation: Target gene name (e.g., \"TP53\", \"non-targeting\")\n# - is_control: Boolean, True for non-targeting controls\n# - cell_type: Cell type annotation\n# - perturbation_type: \"CRISPRi\", \"CRISPRa\", \"CRISPRko\"\n\n# Expression matrix\nX = adata.X  # (n_cells, n_genes)\n\n# Get control cells\ncontrols = adata[adata.obs['is_control']]\n\n# Get perturbed cells for specific gene\ntp53_ko = adata[adata.obs['perturbation'] == 'TP53']\n</code></pre>"},{"location":"datasets/perturbation/scperturb/#preprocessing-for-scppdm","title":"Preprocessing for scPPDM","text":""},{"location":"datasets/perturbation/scperturb/#standard-pipeline","title":"Standard Pipeline","text":"<pre><code>import scanpy as sc\nimport numpy as np\n\n# 1. Load data\nadata = sc.read_h5ad(\"replogle_2022_k562.h5ad\")\n\n# 2. Quality control\nsc.pp.filter_cells(adata, min_genes=200)\nsc.pp.filter_genes(adata, min_cells=3)\n\n# 3. Normalize\nsc.pp.normalize_total(adata, target_sum=1e4)\nsc.pp.log1p(adata)\n\n# 4. Select highly variable genes\nsc.pp.highly_variable_genes(adata, n_top_genes=2000)\nadata = adata[:, adata.var['highly_variable']]\n\n# 5. Split by perturbation\ntrain_perts = [...]  # List of perturbations for training\ntest_perts = [...]   # Held-out perturbations\n\ntrain_adata = adata[adata.obs['perturbation'].isin(train_perts + ['non-targeting'])]\ntest_adata = adata[adata.obs['perturbation'].isin(test_perts + ['non-targeting'])]\n</code></pre>"},{"location":"datasets/perturbation/scperturb/#creating-control-perturbed-pairs","title":"Creating Control-Perturbed Pairs","text":"<pre><code>def create_pairs(adata, perturbation):\n    \"\"\"Create (control, perturbed) pairs for training.\"\"\"\n    controls = adata[adata.obs['is_control']].X\n    perturbed = adata[adata.obs['perturbation'] == perturbation].X\n\n    # Random pairing (or use more sophisticated matching)\n    n_pairs = min(len(controls), len(perturbed))\n    ctrl_idx = np.random.choice(len(controls), n_pairs, replace=False)\n    pert_idx = np.random.choice(len(perturbed), n_pairs, replace=False)\n\n    return controls[ctrl_idx], perturbed[pert_idx]\n</code></pre>"},{"location":"datasets/perturbation/scperturb/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>Standard metrics for perturbation prediction:</p> Metric Description MSE Mean squared error on held-out perturbations Pearson r Correlation between predicted and true expression DEG overlap Overlap of differentially expressed genes Pathway enrichment Biological pathway consistency"},{"location":"datasets/perturbation/scperturb/#use-in-genai-lab","title":"Use in genai-lab","text":""},{"location":"datasets/perturbation/scperturb/#for-scppdm-diffusion","title":"For scPPDM (Diffusion)","text":"<pre><code># Train diffusion model to predict perturbation effects\n# Input: control expression + perturbation embedding\n# Output: perturbed expression distribution\n</code></pre>"},{"location":"datasets/perturbation/scperturb/#for-jepa","title":"For JEPA","text":"<pre><code># Train predictor in latent space\n# Input: control latent + perturbation token\n# Output: predicted perturbed latent\n</code></pre>"},{"location":"datasets/perturbation/scperturb/#recommended-starting-point","title":"Recommended Starting Point","text":"<p>For initial experiments, use Norman 2019:</p> <ul> <li>Smaller size (~100k cells)</li> <li>Well-characterized perturbations</li> <li>Combinatorial effects (interesting for modeling)</li> <li>Widely used benchmark</li> </ul> <p>For production training, use Replogle 2022:</p> <ul> <li>Largest dataset (&gt;5,000 perturbations)</li> <li>Comprehensive coverage</li> <li>High quality</li> </ul>"},{"location":"datasets/perturbation/scperturb/#references","title":"References","text":"<ul> <li>Peidli et al. (2024) \"scPerturb: Harmonized Single-Cell Perturbation Data\"</li> <li>Replogle et al. (2022) \"Mapping information-rich genotype-phenotype landscapes with genome-scale Perturb-seq\"</li> <li>Norman et al. (2019) \"Exploring genetic interaction manifolds constructed from rich single-cell phenotypes\"</li> <li>Dixit et al. (2016) \"Perturb-Seq: Dissecting Molecular Circuits with Scalable Single-Cell RNA Profiling of Pooled Genetic Screens\"</li> </ul>"},{"location":"datasets/perturbation/scperturb/#related-documentation","title":"Related Documentation","text":"<ul> <li>Perturbation Datasets Overview</li> <li>Generative AI for Perturbation Modeling</li> <li>JEPA for Perturb-seq</li> </ul>"},{"location":"diffusion/","title":"Diffusion Models: Theory and Background","text":"<p>Reference materials and deep-dive documents for understanding diffusion models from first principles.</p>"},{"location":"diffusion/#looking-for-sde-supplements","title":"\ud83c\udfaf Looking for SDE Supplements?","text":"<p>Most comprehensive SDE materials are now in: <code>notebooks/diffusion/02_sde_formulation/supplements/</code></p> <p>8 focused supplements covering:</p> <ul> <li>Forward SDE design choices (VP/VE/sub-VP)</li> <li>Fokker-Planck equation and probability evolution</li> <li>Dimensional analysis and units</li> <li>Training loss, reverse SDEs, and more</li> </ul> <p>Start here: <code>notebooks/diffusion/02_sde_formulation/README.md</code></p>"},{"location":"diffusion/#purpose","title":"Purpose","text":"<p>This directory contains theoretical background and mathematical foundations for diffusion models. These documents complement the interactive tutorials in <code>notebooks/diffusion/</code> with deeper mathematical rigor and historical context.</p> <p>For learning: Start with <code>notebooks/diffusion/</code> for hands-on tutorials, then return here for deeper understanding.</p>"},{"location":"diffusion/#documents","title":"Documents","text":""},{"location":"diffusion/#brownian_motion_tutorialmd-comprehensive-introduction","title":"brownian_motion_tutorial.md \u2014 Comprehensive Introduction","text":"<p>Complete tutorial on Brownian motion from physical origins to diffusion models</p> <p>Topics covered:</p> <ul> <li>Physical origin story (Robert Brown, Einstein, Wiener)</li> <li>Random walks vs Brownian motion</li> <li>The four defining properties (and what they really mean)</li> <li>The mysterious \\(\\sqrt{dt}\\) scaling explained</li> <li>Scaling limit: from discrete to continuous (Donsker's theorem)</li> <li>Why Brownian motion powers diffusion models</li> </ul> <p>When to read: After understanding basic SDEs, before diving deep into score matching</p> <p>Key takeaway: Brownian motion is the universal limit of random walks and the mathematical foundation that makes diffusion models work.</p>"},{"location":"diffusion/#brownian_motionmd-original-notes-archive","title":"brownian_motion.md \u2014 Original Notes (Archive)","text":"<p>Original Q&amp;A-style notes on Brownian motion</p> <p>Status: Archived. See <code>brownian_motion_tutorial.md</code> for the comprehensive rewrite.</p> <p>Contents:</p> <ul> <li>Random walk vs Brownian motion comparison</li> <li>Scaling limit derivation</li> <li>Origin story and physical interpretation</li> </ul>"},{"location":"diffusion/#brownian_motion_qamd-original-qa-archive","title":"brownian_motion_QA.md \u2014 Original Q&amp;A (Archive)","text":"<p>Original Q&amp;A focusing on the \\(\\sqrt{dt}\\) scaling</p> <p>Status: Archived. Content integrated into <code>brownian_motion_tutorial.md</code>.</p> <p>Contents:</p> <ul> <li>Why \\(dw(t) \\propto \\sqrt{dt} \\cdot \\varepsilon\\)</li> <li>Step-by-step explanation of variance scaling</li> <li>Connection to Euler-Maruyama numerics</li> </ul>"},{"location":"diffusion/#sde_qamd-sde-questions-draft","title":"sde_QA.md \u2014 SDE Questions (Draft)","text":"<p>Draft Q&amp;A on SDE concepts</p> <p>Status: Draft/archive. For comprehensive SDE coverage, see <code>notebooks/diffusion/02_sde_formulation/</code>.</p> <p>Contents:</p> <ul> <li>How SDEs are solved</li> <li>What models are learned</li> <li>Wiener process alternatives</li> </ul> <p>Note: This content overlaps with <code>notebooks/diffusion/02_sde_formulation/sde_QA.md</code> which is the canonical version.</p>"},{"location":"diffusion/#reverse_process-reverse-time-sde-theory","title":"reverse_process/ \u2014 Reverse-Time SDE Theory","text":"<p>Complete mathematical treatment of reversing diffusion processes</p> <p>This directory contains the theoretical foundation for generating samples from noise in diffusion models\u2014the reverse-time SDE and its derivation.</p>"},{"location":"diffusion/#reverse_processreverse_process_derivationmd-main-derivation","title":"reverse_process/reverse_process_derivation.md \u2014 Main Derivation","text":"<p>Complete derivation of the reverse-time SDE from first principles</p> <p>Topics covered:</p> <ul> <li>Anderson's theorem (1982) \u2014 the mathematical key to reversing SDEs</li> <li>Derivation via Fokker-Planck equation</li> <li>Why the score function \\(\\nabla_x \\log p_t(x)\\) appears</li> <li>Physical intuition: drift, diffusion, and effective drift</li> <li>Connection to generative modeling</li> </ul> <p>When to read: After understanding forward SDEs and Fokker-Planck equations</p> <p>Key result: \\(dx = [f(x,t) - g(t)^2 \\nabla_x \\log p_t(x)]\\,dt + g(t)\\,d\\bar{w}(t)\\)</p>"},{"location":"diffusion/#reverse_processreverse_process_examplemd-concrete-example","title":"reverse_process/reverse_process_example.md \u2014 Concrete Example","text":"<p>Detailed worked example: reversing a 1D Gaussian diffusion</p> <p>Topics covered:</p> <ul> <li>Complete setup: forward Brownian motion</li> <li>Step-by-step score calculation for Gaussian</li> <li>Physical interpretation of the score</li> <li>Reverse SDE with explicit coefficients</li> <li>Numerical verification with Python code</li> <li>Why the drift points outward (paradox explained)</li> </ul> <p>When to read: After reading the main derivation, for concrete intuition</p> <p>Key insight: For \\(p_t(x) = \\mathcal{N}(0, 2Dt)\\), the score is \\(\\nabla \\log p_t = -x/(2Dt)\\), which guides particles back to the origin.</p>"},{"location":"diffusion/#reverse_processfokker_planck_derivationmd-fokker-planck-equation","title":"reverse_process/fokker_planck_derivation.md \u2014 Fokker-Planck Equation","text":"<p>Derivation of the probability evolution equation from first principles</p> <p>Topics covered:</p> <ul> <li>Chapman-Kolmogorov equation</li> <li>Kramers-Moyal expansion</li> <li>Physical interpretation: drift vs. diffusion</li> <li>Conservation laws and probability current</li> <li>Examples: pure diffusion, Ornstein-Uhlenbeck</li> <li>Connection to reverse SDEs</li> </ul> <p>When to read: Before the reverse process derivation, as foundational background</p> <p>Key result: \\(\\frac{\\partial p_t}{\\partial t} = -\\nabla \\cdot (f p_t) + \\frac{1}{2}g^2 \\nabla^2 p_t\\)</p>"},{"location":"diffusion/#score_network-score-network-architecture-components","title":"score_network/ \u2014 Score Network Architecture Components","text":"<p>Deep-dive into the architectural components of modern score networks</p> <p>This directory explains how to build neural networks that estimate \\(\\nabla_x \\log p_t(x)\\) across different noise levels\u2014the core of diffusion models.</p>"},{"location":"diffusion/#score_networkadvanced_architecturesmd-architectures-for-realistic-data","title":"score_network/advanced_architectures.md \u2014 Architectures for Realistic Data","text":"<p>Advanced neural network architectures for complex, real-world data</p> <p>Topics covered:</p> <ul> <li>U-Net: Dominant architecture for images and medical imaging</li> <li>Multi-scale processing, skip connections</li> <li>Residual blocks with time conditioning</li> <li>Attention layers for long-range dependencies</li> <li>Vision Transformer (DiT): Scalable transformer-based architecture</li> <li>Adaptive Layer Normalization (AdaLN)</li> <li>When to use DiT vs U-Net</li> <li>Networks for Biological Data: Gene expression, scRNA-seq</li> <li>Deep residual MLPs for tabular data</li> <li>Graph Neural Networks for pathway structure</li> <li>Handling sparsity in single-cell data</li> </ul> <p>When to read: When moving beyond toy examples to realistic data</p> <p>Key insight: Match the architecture's inductive biases to your data's structure.</p>"},{"location":"diffusion/#score_networktime_embedding_and_filmmd-time-conditioning-components","title":"score_network/time_embedding_and_film.md \u2014 Time Conditioning Components","text":"<p>Deep-dive into time conditioning mechanisms used in score networks</p> <p>Topics covered:</p> <ul> <li>Time Embedding: Transform scalar \\(t\\) to high-dimensional representation</li> <li>Sinusoidal embeddings (multiple frequencies)</li> <li>Why networks struggle with raw scalar inputs</li> <li>Connection to Fourier basis</li> <li>FiLM (Feature-wise Linear Modulation): Condition layers on time</li> <li>Affine transformations: \\(\\gamma_{\\text{scale}} \\odot h + \\gamma_{\\text{shift}}\\)</li> <li>Why FiLM is more effective than concatenation</li> <li>Implementation patterns for MLPs and CNNs</li> <li>Comparison: FiLM vs. concatenation vs. attention</li> <li>Advanced topics: Adaptive Group Normalization, multi-scale embeddings</li> </ul> <p>When to read: When implementing score networks and needing to understand components</p> <p>Key insight: Time embedding provides multiple frequencies for the network to understand time at different scales, while FiLM allows layer-wise adaptation to noise levels.</p>"},{"location":"diffusion/#forward_process_derivationmd-forward-sde-solution","title":"forward_process_derivation.md \u2014 Forward SDE Solution","text":"<p>Deriving the forward diffusion process: from clean data to noise</p> <p>Topics covered:</p> <ul> <li>VP-SDE (Variance Preserving) as the canonical example</li> <li>Solution using integrating factors</li> <li>Derivation of \\(x_t = \\sqrt{\\bar{\\alpha}_t}\\,x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\,\\varepsilon\\)</li> <li>Connection to noise schedules \\(\\beta(t)\\)</li> </ul> <p>When to read: After understanding basic SDEs, before reverse processes</p> <p>Supplements:</p> <ul> <li><code>noise_schedules.md</code> \u2014 Common noise schedule choices</li> <li><code>integrating_factor.md</code> \u2014 Integrating factor technique</li> <li><code>alpha_definitions_derivation.md</code> \u2014 Origin of \\(\\alpha(t)\\) and \\(\\bar{\\alpha}_t\\)</li> </ul>"},{"location":"diffusion/#noise_schedulesmd-noise-schedule-design","title":"noise_schedules.md \u2014 Noise Schedule Design","text":"<p>Comprehensive guide to noise schedules in diffusion models</p> <p>Topics covered:</p> <ul> <li>Mathematical background: \\(\\beta(t)\\), \\(\\alpha(t)\\), \\(\\bar{\\alpha}_t\\)</li> <li>Common schedules: linear, cosine, polynomial, sigmoid</li> <li>Properties and trade-offs</li> <li>Why cosine often works better</li> </ul> <p>When to read: When implementing diffusion models or tuning performance</p>"},{"location":"diffusion/#integrating_factormd-integrating-factor-technique","title":"integrating_factor.md \u2014 Integrating Factor Technique","text":"<p>Explanation of integrating factors for solving linear differential equations</p> <p>Topics covered:</p> <ul> <li>What is an integrating factor</li> <li>How it simplifies differential equations</li> <li>Derivation of \\(\\frac{d\\mu}{dt}\\) for exponential integrating factors</li> <li>Application to the forward SDE</li> </ul> <p>When to read: When studying the forward process derivation</p>"},{"location":"diffusion/#alpha_definitions_derivationmd-origin-of-alpha-definitions","title":"alpha_definitions_derivation.md \u2014 Origin of Alpha Definitions","text":"<p>How \\(\\alpha(t)\\) and \\(\\bar{\\alpha}_t\\) definitions arise from integrating factors</p> <p>Topics covered:</p> <ul> <li>Connection between integrating factor and signal coefficients</li> <li>Why \\(\\bar{\\alpha}_t = \\exp(-\\int_0^t \\beta(s)\\,ds)\\)</li> <li>Physical meaning of alpha decay</li> </ul> <p>When to read: After forward process derivation, when seeking deeper understanding</p>"},{"location":"diffusion/#classifier_free_guidancemd-conditional-generation","title":"classifier_free_guidance.md \u2014 Conditional Generation","text":"<p>Comprehensive guide to classifier-free guidance for conditional diffusion models</p> <p>Topics covered:</p> <ul> <li>Problem: Why naive conditioning doesn't work well</li> <li>Classifier guidance (original approach with separate classifier)</li> <li>Classifier-free guidance (elegant solution without classifier)</li> <li>Training procedure (condition dropping)</li> <li>Guidance scale and fidelity vs. diversity trade-off</li> <li>Implementation in both DDPM and SDE views</li> <li>Variants: dynamic guidance, multi-conditional, negative prompting</li> </ul> <p>When to read: When building conditional diffusion models (text-to-image, class-conditional, etc.)</p> <p>Key insight: Train one model for both conditional and unconditional generation by randomly dropping conditions during training, then amplify the difference at sampling time.</p>"},{"location":"diffusion/#history-historical-development","title":"history/ \u2014 Historical Development","text":"<p>How diffusion models evolved and unified</p>"},{"location":"diffusion/#historydiffusion_models_developmentmd-complete-historical-timeline","title":"history/diffusion_models_development.md \u2014 Complete Historical Timeline","text":"<p>Traces the development of diffusion models from multiple perspectives</p> <p>Topics covered:</p> <ul> <li>Timeline: Score matching (2005) \u2192 DDPM (2020) \u2192 SDE view (2021)</li> <li>Was DDPM derived from SDEs? (No\u2014retrospective unification)</li> <li>Three views: Variational, Score-Based, Flow-Based</li> <li>How the SDE view unified them all</li> <li>Why understanding history clarifies the \"multiple views\" confusion</li> </ul> <p>When to read: After understanding both DDPM and SDE views, when seeking big-picture understanding</p> <p>Key insight: DDPM was developed independently as a discrete-time model. The SDE view came later and revealed that DDPM, NCSN, and flow-based models are all the same underlying process.</p> <p>Includes: Diagram showing the convergence of three perspectives to continuous-time formulation</p>"},{"location":"diffusion/#organization-strategy","title":"Organization Strategy","text":""},{"location":"diffusion/#docsdiffusion-vs-notebooksdiffusion","title":"<code>docs/diffusion/</code> vs <code>notebooks/diffusion/</code>","text":"<p>This directory (<code>docs/diffusion/</code>):</p> <ul> <li>Purpose: Reference materials, mathematical deep-dives, historical context</li> <li>Format: Markdown documents with rigorous derivations</li> <li>Style: Tutorial/blog style but with more mathematical detail</li> <li>Audience: Researchers, those seeking deeper understanding</li> </ul> <p><code>notebooks/diffusion/</code>:</p> <ul> <li>Purpose: Interactive learning, hands-on coding, visualization</li> <li>Format: Jupyter notebooks with executable code</li> <li>Style: Step-by-step tutorials with examples</li> <li>Audience: Practitioners, those learning by doing</li> </ul> <p>Relationship: These directories complement each other. Start with notebooks for intuition, come here for rigor.</p>"},{"location":"diffusion/#recommended-reading-order","title":"Recommended Reading Order","text":""},{"location":"diffusion/#for-understanding-brownian-motion","title":"For Understanding Brownian Motion","text":"<ol> <li>Start: <code>brownian_motion_tutorial.md</code> (this directory)</li> <li>Complete introduction from physics to mathematics</li> <li>Explains the \\(\\sqrt{dt}\\) scaling thoroughly</li> <li> <p>Connects to diffusion models</p> </li> <li> <p>Practice: <code>notebooks/diffusion/02_sde_formulation/02_sde_formulation.ipynb</code></p> </li> <li>Visualize Brownian motion paths</li> <li> <p>See the scaling in action with code</p> </li> <li> <p>Deep dive: <code>notebooks/diffusion/02_sde_formulation/supplements/02_brownian_motion_dimensionality.md</code></p> </li> <li>Why \\(w(t) \\in \\mathbb{R}^d\\), not scalar</li> </ol>"},{"location":"diffusion/#for-understanding-sdes","title":"For Understanding SDEs","text":"<ol> <li>Start: <code>notebooks/diffusion/02_sde_formulation/sde_formulation.md</code></li> <li> <p>Core SDE theory for diffusion models</p> </li> <li> <p>Clarify: <code>notebooks/diffusion/02_sde_formulation/sde_QA.md</code></p> </li> <li> <p>Common questions answered</p> </li> <li> <p>Supplements: <code>notebooks/diffusion/02_sde_formulation/supplements/</code></p> </li> <li>Eight focused deep-dives on specific topics</li> </ol>"},{"location":"diffusion/#for-understanding-forward-process-data-noise","title":"For Understanding Forward Process (Data \u2192 Noise)","text":"<ol> <li>Start: <code>forward_process_derivation.md</code> (this directory)</li> <li> <p>Complete derivation of \\(x_t = \\sqrt{\\bar{\\alpha}_t}\\,x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\,\\varepsilon\\)</p> </li> <li> <p>Background: <code>integrating_factor.md</code> (this directory)</p> </li> <li> <p>Mathematical technique used in the derivation</p> </li> <li> <p>Deep dive: <code>alpha_definitions_derivation.md</code> (this directory)</p> </li> <li> <p>Why the definitions look the way they do</p> </li> <li> <p>Practice: <code>noise_schedules.md</code> (this directory)</p> </li> <li>Choosing and tuning noise schedules</li> </ol>"},{"location":"diffusion/#for-understanding-reverse-process-noise-data","title":"For Understanding Reverse Process (Noise \u2192 Data)","text":"<ol> <li>Foundation: <code>reverse_process/fokker_planck_derivation.md</code></li> <li>How probability distributions evolve under SDEs</li> <li> <p>Essential background for understanding reverse processes</p> </li> <li> <p>Main theory: <code>reverse_process/reverse_process_derivation.md</code></p> </li> <li>Complete derivation of the reverse-time SDE</li> <li> <p>Anderson's theorem and the score function</p> </li> <li> <p>Concrete example: <code>reverse_process/reverse_process_example.md</code></p> </li> <li>Worked example with explicit calculations</li> <li> <p>Numerical verification code</p> </li> <li> <p>Practice: <code>notebooks/diffusion/02_sde_formulation/supplements/</code></p> </li> <li>Supplement 03: Training loss and denoising</li> <li>Supplement 04: Score matching</li> <li>Supplement 05: Reverse SDE implementation</li> </ol>"},{"location":"diffusion/#for-implementing-score-networks","title":"For Implementing Score Networks","text":"<ol> <li>Architecture basics: <code>dev/notebooks/diffusion/02_sde_formulation/score_network_architecture.md</code></li> <li>Activation functions (SiLU)</li> <li>Basic MLP implementation</li> <li> <p>Why <code>y.sum().backward()</code></p> </li> <li> <p>Advanced architectures: <code>score_network/advanced_architectures.md</code> (this directory)</p> </li> <li>U-Net for images and medical imaging</li> <li>Vision Transformer (DiT) for large-scale training</li> <li>Architectures for gene expression and scRNA-seq</li> <li> <p>When to use which architecture</p> </li> <li> <p>Component deep-dive: <code>score_network/time_embedding_and_film.md</code> (this directory)</p> </li> <li>Time embedding: sinusoidal representations</li> <li>FiLM: feature-wise linear modulation</li> <li> <p>Implementation patterns and debugging tips</p> </li> <li> <p>Practice: <code>notebooks/diffusion/02_sde_formulation/02_sde_formulation.ipynb</code></p> </li> <li>Train a score network on toy 2D data</li> <li>See time conditioning in action</li> <li>Sample from the reverse SDE</li> </ol>"},{"location":"diffusion/#related-resources","title":"Related Resources","text":""},{"location":"diffusion/#in-this-repository","title":"In This Repository","text":"<ul> <li>Interactive tutorials: <code>notebooks/diffusion/</code></li> <li><code>01_ddpm_basics.ipynb</code>: DDPM on gene expression</li> <li> <p><code>02_sde_formulation/</code>: Complete SDE tutorial package</p> </li> <li> <p>Theory documents: <code>docs/</code></p> </li> <li><code>score_matching/</code>: Score functions and Fisher/Stein scores</li> <li><code>EBM/</code>: Energy-based models</li> </ul>"},{"location":"diffusion/#external-references","title":"External References","text":"<ul> <li>Einstein (1905): Investigations on the Theory of the Brownian Movement</li> <li>Donsker (1951): Invariance principle for random walks</li> <li>Song et al. (2021): Score-Based Generative Modeling through SDEs</li> <li>\u00d8ksendal (2003): Stochastic Differential Equations textbook</li> </ul>"},{"location":"diffusion/#contributing","title":"Contributing","text":"<p>When adding new documents to this directory:</p> <ol> <li>Check for overlap: See if content fits better in <code>notebooks/diffusion/</code></li> <li>Maintain style: Tutorial/blog style with rigorous mathematics</li> <li>Add to this README: Update the documents list and reading order</li> <li>Cross-reference: Link to related notebooks and docs</li> </ol>"},{"location":"diffusion/#status","title":"Status","text":""},{"location":"diffusion/#core-theory-documents","title":"Core Theory Documents","text":"<ul> <li>\u2705 brownian_motion_tutorial.md: Complete comprehensive tutorial</li> <li>\u2705 forward_process_derivation.md: Complete with supplements</li> <li>\u2705 reverse_process/: Complete directory with all three documents</li> <li>\u2705 reverse_process_derivation.md: Full reverse SDE derivation</li> <li>\u2705 reverse_process_example.md: Worked 1D Gaussian example</li> <li>\u2705 fokker_planck_derivation.md: Probability evolution equation</li> <li>\u2705 score_network/: Architecture components directory</li> <li>\u2705 advanced_architectures.md: U-Net, DiT, and architectures for realistic data</li> <li>\u2705 time_embedding_and_film.md: Time embedding and FiLM conditioning components</li> </ul>"},{"location":"diffusion/#supplement-documents","title":"Supplement Documents","text":"<ul> <li>\u2705 noise_schedules.md: Complete guide to noise schedules</li> <li>\u2705 integrating_factor.md: Mathematical technique explained</li> <li>\u2705 alpha_definitions_derivation.md: Origin of alpha definitions</li> </ul>"},{"location":"diffusion/#archived-documents","title":"Archived Documents","text":"<ul> <li>\ud83d\udce6 brownian_motion.md: Archived (superseded by tutorial)</li> <li>\ud83d\udce6 brownian_motion_QA.md: Archived (content integrated into tutorial)</li> <li>\ud83d\udcdd sde_QA.md: Draft (see notebooks version for canonical content)</li> </ul> <p>Next Steps:</p> <ul> <li>After understanding Brownian motion \u2192 <code>notebooks/diffusion/02_sde_formulation/</code> for SDE basics</li> <li>After understanding forward/reverse processes \u2192 <code>notebooks/diffusion/02_sde_formulation/supplements/</code> for implementation details</li> <li>For practical implementation \u2192 Start with DDPM notebook, then move to SDE formulation</li> </ul>"},{"location":"diffusion/alpha_definitions_derivation/","title":"Where Do \\(\\alpha(t)\\) and \\(\\bar{\\alpha}_t\\) Come From?","text":""},{"location":"diffusion/alpha_definitions_derivation/#overview","title":"Overview","text":"<p>The coefficients \\(\\alpha(t)\\) and \\(\\bar{\\alpha}_t\\) appear throughout diffusion model theory, but their definitions with integrals in the exponent can seem mysterious:</p> \\[ \\alpha(t) = \\exp\\left(-\\frac{1}{2}\\int_0^t \\beta(s)\\,ds\\right), \\quad \\bar{\\alpha}_t = \\exp\\left(-\\int_0^t \\beta(s)\\,ds\\right) \\] <p>Key insight: These are not arbitrary definitions. They emerge naturally from solving the forward SDE using the integrating factor technique.</p> <p>This document explains where these definitions come from and why they have this specific form.</p>"},{"location":"diffusion/alpha_definitions_derivation/#referenced-from","title":"Referenced From","text":"<ul> <li><code>docs/diffusion/noise_schedules.md</code> \u2014 Uses these definitions extensively</li> <li><code>docs/diffusion/forward_process_derivation.md</code> \u2014 Full derivation of the forward process</li> </ul>"},{"location":"diffusion/alpha_definitions_derivation/#the-starting-point-the-vp-sde","title":"The Starting Point: The VP-SDE","text":"<p>The Variance-Preserving SDE describes how clean data \\(x_0\\) is corrupted over time:</p> \\[ dx = -\\frac{1}{2}\\beta(t)\\,x\\,dt + \\sqrt{\\beta(t)}\\,dw \\] <p>where:</p> <ul> <li>\\(x(t)\\) is the state at time \\(t\\) (starts at \\(x_0\\))</li> <li>\\(\\beta(t) &gt; 0\\) is the noise schedule</li> <li>\\(dw\\) is Brownian motion</li> </ul> <p>Question: What is the relationship between \\(x_t\\) and \\(x_0\\)?</p> <p>To answer this, we need to solve this SDE.</p>"},{"location":"diffusion/alpha_definitions_derivation/#solving-the-sde-via-integrating-factor","title":"Solving the SDE via Integrating Factor","text":""},{"location":"diffusion/alpha_definitions_derivation/#step-1-identify-the-sde-structure","title":"Step 1: Identify the SDE Structure","text":"<p>The VP-SDE has the form:</p> \\[ dx = a(t) x\\,dt + b(t)\\,dw \\] <p>with \\(a(t) = -\\frac{1}{2}\\beta(t)\\) and \\(b(t) = \\sqrt{\\beta(t)}\\).</p> <p>This is a linear SDE (the drift is linear in \\(x\\)), which can be solved using an integrating factor.</p>"},{"location":"diffusion/alpha_definitions_derivation/#step-2-define-the-integrating-factor","title":"Step 2: Define the Integrating Factor","text":"<p>For a linear SDE with drift coefficient \\(a(t)\\), the integrating factor is:</p> \\[ \\mu(t) = \\exp\\left(-\\int_0^t a(s)\\,ds\\right) \\] <p>In our case, \\(a(t) = -\\frac{1}{2}\\beta(t)\\), so:</p> \\[ \\mu(t) = \\exp\\left(-\\int_0^t \\left(-\\frac{1}{2}\\beta(s)\\right)\\,ds\\right) = \\exp\\left(\\frac{1}{2}\\int_0^t \\beta(s)\\,ds\\right) \\] <p>Why this choice? The integrating factor is designed so that \\(\\frac{d\\mu}{dt} = -a(t)\\mu(t)\\), which allows the drift term to cancel when we multiply through.</p>"},{"location":"diffusion/alpha_definitions_derivation/#step-3-apply-the-integrating-factor","title":"Step 3: Apply the Integrating Factor","text":"<p>Multiply both sides of the SDE by \\(\\mu(t)\\):</p> \\[ \\mu(t)\\,dx = \\mu(t) \\cdot \\left[-\\frac{1}{2}\\beta(t)x\\,dt + \\sqrt{\\beta(t)}\\,dw\\right] \\] <p>Using It\u00f4's lemma on \\(\\mu(t)x(t)\\), we get:</p> \\[ d(\\mu x) = \\mu\\,dx + x\\,d\\mu \\] <p>Since \\(d\\mu = -a(t)\\mu\\,dt = \\frac{1}{2}\\beta(t)\\mu\\,dt\\):</p> \\[ d(\\mu x) = \\mu\\left[-\\frac{1}{2}\\beta(t)x\\,dt + \\sqrt{\\beta(t)}\\,dw\\right] + x \\cdot \\frac{1}{2}\\beta(t)\\mu\\,dt \\] <p>The drift terms cancel:</p> \\[ d(\\mu x) = \\mu(t)\\sqrt{\\beta(t)}\\,dw \\]"},{"location":"diffusion/alpha_definitions_derivation/#step-4-integrate","title":"Step 4: Integrate","text":"<p>Integrate from \\(0\\) to \\(t\\):</p> \\[ \\mu(t)x(t) - \\mu(0)x(0) = \\int_0^t \\mu(s)\\sqrt{\\beta(s)}\\,dw(s) \\] <p>Since \\(\\mu(0) = 1\\):</p> \\[ \\mu(t)x(t) = x_0 + \\int_0^t \\mu(s)\\sqrt{\\beta(s)}\\,dw(s) \\]"},{"location":"diffusion/alpha_definitions_derivation/#step-5-solve-for-xt","title":"Step 5: Solve for \\(x(t)\\)","text":"\\[ x(t) = \\frac{1}{\\mu(t)} x_0 + \\frac{1}{\\mu(t)} \\int_0^t \\mu(s)\\sqrt{\\beta(s)}\\,dw(s) \\]"},{"location":"diffusion/alpha_definitions_derivation/#the-emergence-of-alphat","title":"The Emergence of \\(\\alpha(t)\\)","text":"<p>Define:</p> \\[ \\alpha(t) = \\frac{1}{\\mu(t)} = \\exp\\left(-\\frac{1}{2}\\int_0^t \\beta(s)\\,ds\\right) \\] <p>This is where \\(\\alpha(t)\\) comes from! It's the inverse of the integrating factor.</p> <p>Now the solution becomes:</p> \\[ x(t) = \\alpha(t) x_0 + \\alpha(t) \\int_0^t \\mu(s)\\sqrt{\\beta(s)}\\,dw(s) \\] <p>Physical meaning: </p> <ul> <li>\\(\\alpha(t)\\) is the signal decay coefficient</li> <li>The term \\(\\alpha(t) x_0\\) shows how the original signal scales over time</li> <li>As \\(t\\) increases and \\(\\beta(s) &gt; 0\\), \\(\\alpha(t)\\) decreases toward 0</li> </ul>"},{"location":"diffusion/alpha_definitions_derivation/#the-stochastic-integral-computing-the-variance","title":"The Stochastic Integral: Computing the Variance","text":"<p>The stochastic integral \\(\\int_0^t \\mu(s)\\sqrt{\\beta(s)}\\,dw(s)\\) is Gaussian with: - Mean: 0 - Variance: \\(\\int_0^t \\mu(s)^2 \\beta(s)\\,ds\\) (by It\u00f4 isometry)</p>"},{"location":"diffusion/alpha_definitions_derivation/#computing-the-variance","title":"Computing the Variance","text":"\\[ \\text{Var}\\left(\\int_0^t \\mu(s)\\sqrt{\\beta(s)}\\,dw(s)\\right) = \\int_0^t \\mu(s)^2 \\beta(s)\\,ds \\] <p>Since \\(\\mu(s) = \\exp\\left(\\frac{1}{2}\\int_0^s \\beta(u)\\,du\\right)\\):</p> \\[ \\mu(s)^2 = \\exp\\left(\\int_0^s \\beta(u)\\,du\\right) \\] <p>So:</p> \\[ \\text{Var} = \\int_0^t \\exp\\left(\\int_0^s \\beta(u)\\,du\\right) \\beta(s)\\,ds \\] <p>Trick: Let \\(\\Phi(s) = \\int_0^s \\beta(u)\\,du\\). Then \\(\\frac{d\\Phi}{ds} = \\beta(s)\\):</p> \\[ \\text{Var} = \\int_0^t e^{\\Phi(s)}\\,d\\Phi(s) = e^{\\Phi(t)} - e^{\\Phi(0)} = e^{\\Phi(t)} - 1 \\] <p>Since \\(\\Phi(t) = \\int_0^t \\beta(s)\\,ds\\) and \\(\\mu(t) = e^{\\Phi(t)/2}\\):</p> \\[ \\text{Var} = \\mu(t)^2 - 1 \\]"},{"location":"diffusion/alpha_definitions_derivation/#variance-of-xt","title":"Variance of \\(x(t)\\)","text":"<p>The noise term in \\(x(t)\\) is:</p> \\[ \\alpha(t) \\int_0^t \\mu(s)\\sqrt{\\beta(s)}\\,dw(s) \\] <p>Its variance is:</p> \\[ \\alpha(t)^2 \\cdot (\\mu(t)^2 - 1) = \\frac{1}{\\mu(t)^2} \\cdot (\\mu(t)^2 - 1) = 1 - \\frac{1}{\\mu(t)^2} \\] <p>Since \\(\\alpha(t) = 1/\\mu(t)\\):</p> \\[ \\text{Variance of noise term} = 1 - \\alpha(t)^2 \\]"},{"location":"diffusion/alpha_definitions_derivation/#the-emergence-of-baralpha_t","title":"The Emergence of \\(\\bar{\\alpha}_t\\)","text":"<p>Define:</p> \\[ \\bar{\\alpha}_t = \\alpha(t)^2 = \\left(\\frac{1}{\\mu(t)}\\right)^2 = \\exp\\left(-\\int_0^t \\beta(s)\\,ds\\right) \\] <p>This is where \\(\\bar{\\alpha}_t\\) comes from! It's the square of the signal coefficient.</p>"},{"location":"diffusion/alpha_definitions_derivation/#the-final-form","title":"The Final Form","text":"<p>The solution becomes:</p> \\[ x_t = \\alpha(t) x_0 + \\sqrt{1 - \\alpha(t)^2} \\cdot \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, I) \\] <p>Or equivalently:</p> \\[ \\boxed{x_t = \\sqrt{\\bar{\\alpha}_t}\\, x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\, \\varepsilon} \\] <p>where:</p> <ul> <li>\\(\\sqrt{\\bar{\\alpha}_t} = \\alpha(t)\\) is the signal coefficient</li> <li>\\(\\sqrt{1-\\bar{\\alpha}_t}\\) is the noise coefficient</li> </ul>"},{"location":"diffusion/alpha_definitions_derivation/#why-these-definitions","title":"Why These Definitions?","text":""},{"location":"diffusion/alpha_definitions_derivation/#not-arbitrary","title":"Not Arbitrary!","text":"<p>The definitions of \\(\\alpha(t)\\) and \\(\\bar{\\alpha}_t\\) are not chosen arbitrarily. They emerge naturally from:</p> <ol> <li>The SDE structure: The VP-SDE \\(dx = -\\frac{1}{2}\\beta(t)x\\,dt + \\sqrt{\\beta(t)}\\,dw\\)</li> <li>The integrating factor technique: \\(\\mu(t) = \\exp\\left(\\frac{1}{2}\\int_0^t \\beta(s)\\,ds\\right)\\)</li> <li>The solution process: \\(\\alpha(t) = 1/\\mu(t)\\) and \\(\\bar{\\alpha}_t = \\alpha(t)^2\\)</li> </ol>"},{"location":"diffusion/alpha_definitions_derivation/#the-integrating-factor-connection","title":"The Integrating Factor Connection","text":"Quantity Definition Origin \\(\\mu(t)\\) \\(\\exp\\left(\\frac{1}{2}\\int_0^t \\beta(s)\\,ds\\right)\\) Integrating factor for SDE \\(\\alpha(t)\\) \\(1/\\mu(t) = \\exp\\left(-\\frac{1}{2}\\int_0^t \\beta(s)\\,ds\\right)\\) Inverse of integrating factor (signal coefficient) \\(\\bar{\\alpha}_t\\) \\(\\alpha(t)^2 = \\exp\\left(-\\int_0^t \\beta(s)\\,ds\\right)\\) Square of signal coefficient <p>Key insight: The exponential with an integral in the exponent is exactly the integrating factor form from ODE/SDE theory.</p>"},{"location":"diffusion/alpha_definitions_derivation/#alternative-starting-from-baralpha_t","title":"Alternative: Starting from \\(\\bar{\\alpha}_t\\)","text":"<p>Some papers define \\(\\bar{\\alpha}_t\\) directly and derive \\(\\beta(t)\\) from it.</p>"},{"location":"diffusion/alpha_definitions_derivation/#forward-approach-this-document","title":"Forward Approach (This Document)","text":"\\[ \\beta(t) \\quad \\xrightarrow{\\text{solve SDE}} \\quad \\alpha(t) = \\exp\\left(-\\frac{1}{2}\\int_0^t \\beta(s)\\,ds\\right) \\quad \\xrightarrow{\\text{square}} \\quad \\bar{\\alpha}_t = \\alpha(t)^2 \\]"},{"location":"diffusion/alpha_definitions_derivation/#inverse-approach-also-valid","title":"Inverse Approach (Also Valid)","text":"\\[ \\bar{\\alpha}_t \\quad \\xrightarrow{\\text{differentiate}} \\quad \\beta(t) = -\\frac{d \\log \\bar{\\alpha}_t}{dt} \\] <p>Derivation: From \\(\\bar{\\alpha}_t = \\exp\\left(-\\int_0^t \\beta(s)\\,ds\\right)\\), take the log:</p> \\[ \\log \\bar{\\alpha}_t = -\\int_0^t \\beta(s)\\,ds \\] <p>Differentiate:</p> \\[ \\frac{d \\log \\bar{\\alpha}_t}{dt} = -\\beta(t) \\] <p>So:</p> \\[ \\beta(t) = -\\frac{d \\log \\bar{\\alpha}_t}{dt} \\] <p>Both approaches are equivalent\u2014you can start with \\(\\beta(t)\\) or \\(\\bar{\\alpha}_t\\).</p>"},{"location":"diffusion/alpha_definitions_derivation/#summary","title":"Summary","text":"Definition Formula Origin Integrating factor \\(\\mu(t) = \\exp\\left(\\frac{1}{2}\\int_0^t \\beta(s)\\,ds\\right)\\) Standard technique for linear SDEs Signal coefficient \\(\\alpha(t) = 1/\\mu(t) = \\exp\\left(-\\frac{1}{2}\\int_0^t \\beta(s)\\,ds\\right)\\) Inverse of integrating factor Cumulative coefficient \\(\\bar{\\alpha}_t = \\alpha(t)^2 = \\exp\\left(-\\int_0^t \\beta(s)\\,ds\\right)\\) Square of signal coefficient <p>The key point: These definitions are not ad hoc. They arise naturally from solving the VP-SDE using the integrating factor technique, which is why they have integrals in the exponent.</p>"},{"location":"diffusion/alpha_definitions_derivation/#references","title":"References","text":"<ul> <li>Forward Process Derivation: <code>docs/diffusion/forward_process_derivation.md</code> \u2014 Complete derivation</li> <li>Integrating Factor Technique: <code>docs/diffusion/integrating_factor.md</code> \u2014 General method</li> <li>\u00d8ksendal (2003): \"Stochastic Differential Equations\" \u2014 Chapter 5 on linear SDEs</li> <li>Ho et al. (2020): \"Denoising Diffusion Probabilistic Models\" \u2014 Uses these coefficients throughout</li> </ul>"},{"location":"diffusion/brownian_motion_tutorial/","title":"Understanding Brownian Motion: From Random Walks to Diffusion Models","text":"<p>A ground-up introduction to the stochastic process that powers modern diffusion models</p> <p>Brownian motion is the mathematical heartbeat of diffusion models. It's the reason why adding noise \"just works,\" why the \\(\\sqrt{dt}\\) scaling appears everywhere, and why continuous-time SDEs can describe discrete-time algorithms like DDPM.</p> <p>This tutorial builds intuition from physical observations, connects to rigorous mathematics, and shows exactly how Brownian motion enables generative modeling.</p>"},{"location":"diffusion/brownian_motion_tutorial/#table-of-contents","title":"Table of Contents","text":"<ol> <li>The Physical Origin Story</li> <li>Random Walk: The Discrete Cousin</li> <li>Brownian Motion: The Continuous Limit</li> <li>The Four Defining Properties (And What They Really Mean)</li> <li>The Mysterious \\(\\sqrt{dt}\\) Scaling</li> <li>From Random Walk to Brownian Motion: The Scaling Limit</li> <li>Why This Matters for Diffusion Models</li> </ol>"},{"location":"diffusion/brownian_motion_tutorial/#1-the-physical-origin-story","title":"1. The Physical Origin Story","text":""},{"location":"diffusion/brownian_motion_tutorial/#the-discovery-1827","title":"The Discovery (1827)","text":"<p>Robert Brown, a botanist, looked through a microscope at pollen grains suspended in water and noticed something uncanny:</p> <p>They jittered forever.</p> <p>No trend. No settling. No rest. The motion was real, persistent, and structureless.</p> <p>At first, he suspected life. Then he tried dust particles. Same thing. The motion was universal, relentless, and seemingly random.</p>"},{"location":"diffusion/brownian_motion_tutorial/#the-explanation-1905","title":"The Explanation (1905)","text":"<p>Decades later, Albert Einstein explained it: invisible molecular collisions from the surrounding fluid.</p> <p>The key insights: - Water molecules are in constant thermal motion - They bombard the visible particle from all directions - Each collision is tiny, but there are billions per second - The cumulative effect is visible, persistent random motion</p> <p>This wasn't just a curiosity\u2014Einstein used it to prove atoms exist and to measure Avogadro's number.</p>"},{"location":"diffusion/brownian_motion_tutorial/#the-mathematical-skeleton-1920s","title":"The Mathematical Skeleton (1920s)","text":"<p>Norbert Wiener gave the phenomenon its clean mathematical structure, now called the Wiener process or Brownian motion.</p> <p>This mathematical object became the foundation for: - Stochastic calculus (It\u00f4, Stratonovich) - Financial mathematics (Black-Scholes) - Statistical physics (Langevin dynamics) - Modern machine learning (diffusion models, score matching)</p>"},{"location":"diffusion/brownian_motion_tutorial/#2-random-walk-the-discrete-cousin","title":"2. Random Walk: The Discrete Cousin","text":"<p>Before understanding Brownian motion, let's start with something simpler: random walks.</p>"},{"location":"diffusion/brownian_motion_tutorial/#definition","title":"Definition","text":"<p>A random walk is defined step by step in discrete time.</p> <p>Time: \\(t = 0, 1, 2, \\ldots\\)</p> <p>At each step, the position changes by a random increment:</p> \\[ X_{n+1} = X_n + \\varepsilon_{n+1} \\] <p>where \\(\\varepsilon_n\\) are independent and identically distributed (i.i.d.) random variables.</p>"},{"location":"diffusion/brownian_motion_tutorial/#classic-example-simple-symmetric-random-walk","title":"Classic Example: Simple Symmetric Random Walk","text":"\\[ \\varepsilon_n = \\begin{cases} +1 &amp; \\text{with probability } \\tfrac{1}{2} \\\\ -1 &amp; \\text{with probability } \\tfrac{1}{2} \\end{cases} \\] <p>Interpretation:</p> <ul> <li>Flip a coin</li> <li>Heads \u2192 step right</li> <li>Tails \u2192 step left</li> <li>Repeat forever</li> </ul>"},{"location":"diffusion/brownian_motion_tutorial/#key-properties","title":"Key Properties","text":"<ul> <li>Discrete time and usually discrete space</li> <li>Independent increments: Each step doesn't depend on previous steps</li> <li>Paths look jagged, stair-like</li> <li>Easy to simulate, easy to analyze</li> </ul>"},{"location":"diffusion/brownian_motion_tutorial/#where-random-walks-appear","title":"Where Random Walks Appear","text":"<ul> <li>Gambling (gambler's ruin)</li> <li>Population genetics (genetic drift)</li> <li>Markov chains</li> <li>Graph theory (random walks on graphs)</li> <li>Toy models for diffusion</li> </ul>"},{"location":"diffusion/brownian_motion_tutorial/#3-brownian-motion-the-continuous-limit","title":"3. Brownian Motion: The Continuous Limit","text":"<p>Brownian motion is what you get when a random walk is pushed to the limit of infinite refinement.</p>"},{"location":"diffusion/brownian_motion_tutorial/#the-transition","title":"The Transition","text":"<p>Think of it this way:</p> <p>Random walk is flipping coins while walking on tiles.</p> <p>Brownian motion is what the walk looks like after you: - Shrink the tiles to dust - Speed up time - Step infinitely often</p> <p>The path never smooths out. Reality just stops showing you the individual steps.</p>"},{"location":"diffusion/brownian_motion_tutorial/#continuous-time","title":"Continuous Time","text":"<p>Brownian motion is defined in continuous time:</p> \\[ t \\in [0,\\infty) \\] <p>Unlike random walks where you count steps \\(n = 0, 1, 2, \\ldots\\), Brownian motion evolves continuously. You can ask \"where is the particle at time \\(t = \\pi\\)?\" and get a meaningful answer.</p>"},{"location":"diffusion/brownian_motion_tutorial/#4-the-four-defining-properties","title":"4. The Four Defining Properties","text":"<p>A process \\(B_t\\) is Brownian motion (or a Wiener process) if it satisfies four properties. Let's unpack what each one really means.</p>"},{"location":"diffusion/brownian_motion_tutorial/#conceptual-clarification-what-is-b_t","title":"Conceptual Clarification: What is \\(B_t\\)?","text":"<p>Before diving into the properties, let's clarify what we mean by \"process\" and how to think about \\(B_t\\).</p>"},{"location":"diffusion/brownian_motion_tutorial/#is-b_t-a-random-variable","title":"Is \\(B_t\\) a Random Variable?","text":"<p>Yes, but more precisely:</p> <ul> <li>For each fixed time \\(t\\): \\(B_t\\) is a random variable representing the position/displacement at time \\(t\\)</li> <li>For the collection \\(\\{B_t : t \\geq 0\\}\\): This is a stochastic process\u2014a family of random variables indexed by time</li> </ul> <p>Think of it this way:</p> <ul> <li>At time \\(t = 0.5\\), \\(B_{0.5}\\) is a random variable (could be any real number, with probabilities given by a Gaussian)</li> <li>At time \\(t = 1.0\\), \\(B_{1.0}\\) is a different random variable</li> <li>The process \\(B_t\\) is the entire collection: \\(\\{B_{0.5}, B_{1.0}, B_{2.3}, \\ldots\\}\\)</li> </ul> <p>In diffusion models: \\(B_t\\) represents the noise/displacement at time \\(t\\). For a d-dimensional process, \\(B_t \\in \\mathbb{R}^d\\) is a random vector.</p>"},{"location":"diffusion/brownian_motion_tutorial/#is-process-the-same-as-in-gaussian-process-dirichlet-process-etc","title":"Is \"Process\" the Same as in Gaussian Process, Dirichlet Process, etc.?","text":"<p>Yes! The word \"process\" here means stochastic process in the same sense as:</p> <ul> <li>Gaussian Process: A collection of random variables \\(\\{f(x) : x \\in \\mathcal{X}\\}\\) where any finite subset is jointly Gaussian</li> <li>Dirichlet Process: A stochastic process whose realizations are probability distributions</li> <li>Markov Process: A stochastic process with the Markov property</li> <li>Brownian Motion: A stochastic process with specific properties (the four we're about to define)</li> </ul> <p>Common structure: All are collections of random variables indexed by some parameter (time, space, etc.)</p> <p>Key difference: What properties they satisfy: - Brownian motion: Independent increments, Gaussian, variance = time - Gaussian process: Any finite subset is jointly Gaussian - Dirichlet process: Realizations are probability distributions</p> <p>In summary: </p> <ul> <li>\\(B_t\\) (for fixed \\(t\\)) = random variable (position at time \\(t\\))</li> <li>\\(\\{B_t : t \\geq 0\\}\\) = stochastic process (the entire trajectory)</li> <li>\"Process\" = same mathematical concept as GP, DP, etc., but with different properties</li> </ul>"},{"location":"diffusion/brownian_motion_tutorial/#property-1-b_0-0","title":"Property 1: \\(B_0 = 0\\)","text":"<p>\"Start at zero.\"</p> <p>Mathematical meaning: The process begins at the origin.</p> <p>Physical meaning: We measure displacement, not absolute position. The particle's initial location is irrelevant; only changes matter.</p> <p>Interpretation: Wherever the particle starts, call that position zero. This is a choice of reference frame, not a physical constraint.</p> <p>Why it matters: Origins are conventions, not truths. This mirrors how physics works\u2014we care about relative motion.</p>"},{"location":"diffusion/brownian_motion_tutorial/#property-2-independent-increments","title":"Property 2: Independent Increments","text":"\\[ B_t - B_s \\perp\\!\\!\\!\\perp B_u - B_v \\quad \\text{for disjoint intervals} \\] <p>Mathematical meaning: Changes over non-overlapping time intervals are independent random variables.</p> <p>Physical meaning: The future does not remember the past. Molecular collisions are so rapid and chaotic that past kicks don't influence future kicks.</p> <p>Why this is reasonable:</p> <ul> <li>Billions of collisions per second</li> <li>Correlations wash out almost instantly</li> <li>The surrounding fluid has no memory at the scale of observation</li> </ul> <p>Interpretation: Yesterday's shove tells you nothing about tomorrow's shove.</p> <p>This is the Markov assumption, but earned, not assumed. It's why Brownian motion became the backbone of Markov processes, diffusion, and SDEs.</p>"},{"location":"diffusion/brownian_motion_tutorial/#property-3-stationary-gaussian-increments","title":"Property 3: Stationary Gaussian Increments","text":"\\[ B_t - B_s \\sim \\mathcal{N}(0, t-s) \\] <p>This is the most information-dense property. Let's break it down.</p>"},{"location":"diffusion/brownian_motion_tutorial/#mean-0","title":"Mean = 0","text":"<p>No drift. No preferred direction.</p> <p>Physical meaning: The fluid is isotropic and at equilibrium. If there were a mean, you'd have wind, flow, or external force.</p>"},{"location":"diffusion/brownian_motion_tutorial/#variance-t-s","title":"Variance = \\(t-s\\)","text":"<p>Spread grows linearly with time.</p> <p>Physical meaning:</p> <ul> <li>Each collision adds a tiny displacement</li> <li>Variances add, not displacements</li> <li>This is diffusion, not ballistic motion</li> </ul> <p>Key insight: Distance \\(\\propto \\sqrt{\\text{time}}\\), not \\(\\propto \\text{time}\\).</p> <p>This single fact distinguishes diffusion from motion with velocity.</p>"},{"location":"diffusion/brownian_motion_tutorial/#gaussian-shape","title":"Gaussian Shape","text":"<p>Why normal distribution?</p> <p>Because the displacement over time is the sum of enormously many tiny, independent kicks, and the Central Limit Theorem kicks in mercilessly.</p> <p>Gaussianity is not sacred\u2014it's statistical inevitability under these conditions.</p>"},{"location":"diffusion/brownian_motion_tutorial/#property-4-continuous-paths-but-nowhere-differentiable","title":"Property 4: Continuous Paths (But Nowhere Differentiable)","text":"<p>Mathematical meaning: \\(B_t\\) is continuous, but has no well-defined velocity anywhere.</p> <p>Physical meaning:</p> <ul> <li>The particle never teleports</li> <li>But at every scale, motion is jittery</li> </ul> <p>The strange reality:</p> <p>Zoom in: Still jagged. Zoom in again: Still jagged. Zoom in infinitely: Still jagged.</p> <p>There is no smallest wiggle.</p> <p>Consequences: This forced mathematics to abandon classical calculus and invent: - It\u00f4 calculus - Stratonovich calculus - Stochastic integration</p> <p>Nature demanded new math.</p>"},{"location":"diffusion/brownian_motion_tutorial/#5-the-mysterious-sqrtdt-scaling","title":"5. The Mysterious \\(\\sqrt{dt}\\) Scaling","text":"<p>One of the most confusing aspects of Brownian motion is why noise scales as \\(\\sqrt{dt}\\), not \\(dt\\). Let's build intuition step by step.</p>"},{"location":"diffusion/brownian_motion_tutorial/#the-question","title":"The Question","text":"<p>In SDEs, we write:</p> \\[ dw(t) = \\sqrt{dt} \\cdot \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0,1) \\] <p>Why the square root? Why not just \\(dt\\) or some other power?</p>"},{"location":"diffusion/brownian_motion_tutorial/#intuition-1-diffusive-scaling","title":"Intuition 1: Diffusive Scaling","text":"<p>Imagine a particle getting random kicks over time.</p> <ul> <li>Over 1 second, it wanders a lot</li> <li>Over 0.01 seconds, it wanders a little</li> <li>Over 0 seconds, it doesn't move at all</li> </ul> <p>Key empirical fact: If you wait 4\u00d7 longer, the typical distance only doubles\u2014not quadruples.</p> \\[ \\text{typical distance} \\propto \\sqrt{\\text{time}} \\] <p>This is called diffusive scaling.</p>"},{"location":"diffusion/brownian_motion_tutorial/#intuition-2-variance-adds-not-displacements","title":"Intuition 2: Variance Adds, Not Displacements","text":"<p>Consider a simple random walk with \\(n\\) steps:</p> \\[ X_n = \\sum_{k=1}^n \\varepsilon_k, \\quad \\varepsilon_k \\sim \\mathcal{N}(0, 1) \\] <p>Variance:</p> \\[ \\text{Var}(X_n) = \\text{Var}\\left(\\sum_{k=1}^n \\varepsilon_k\\right) = n \\] <p>Why this formula? The variance of a sum of independent random variables equals the sum of their variances:</p> \\[ \\text{Var}\\left(\\sum_{k=1}^n \\varepsilon_k\\right) = \\sum_{k=1}^n \\text{Var}(\\varepsilon_k) = \\sum_{k=1}^n 1 = n \\] <p>General formula for variance of sum:</p> <p>For independent random variables \\(X_1, X_2, \\ldots, X_n\\):</p> <p>$$</p> <p>\\text{Var}\\left(\\sum_{k=1}^n X_k\\right) = \\sum_{k=1}^n \\text{Var}(X_k) $$</p> <p>For dependent random variables (with covariance):</p> <p>$$</p> <p>\\text{Var}\\left(\\sum_{k=1}^n X_k\\right) = \\sum_{k=1}^n \\text{Var}(X_k) + 2\\sum_{i &lt; j} \\text{Cov}(X_i, X_j) $$</p> <p>In our case, the \\(\\varepsilon_k\\) are independent (each step is independent), so the covariance terms are zero, and we get the simple sum.</p> <p>Standard deviation:</p> \\[ \\text{SD}(X_n) = \\sqrt{n} \\] <p>So: - Variance grows linearly with number of steps - Standard deviation (typical distance) grows like \\(\\sqrt{n}\\)</p>"},{"location":"diffusion/brownian_motion_tutorial/#from-steps-to-time","title":"From Steps to Time","text":"<p>Now connect steps to continuous time:</p> <ul> <li>Total time: \\(t\\)</li> <li>Step size: \\(\\Delta t\\)</li> <li>Number of steps: \\(n = t / \\Delta t\\)</li> </ul> <p>If each step had noise of fixed size, variance would blow up as \\(\\Delta t \\to 0\\). That's bad.</p> <p>Solution: Scale down the noise per step:</p> \\[ \\Delta x_k = \\sqrt{\\Delta t} \\cdot \\varepsilon_k \\] <p>Variance over time \\(t\\):</p> \\[ \\text{Var}(x_t) = \\sum_{k=1}^n \\text{Var}(\\sqrt{\\Delta t} \\cdot \\varepsilon_k) = \\sum_{k=1}^n \\Delta t = n \\cdot \\Delta t = t \\] <p>Perfect! We get: - Finite variance - Linear growth in time - Well-defined continuous limit</p> <p>This is why noise per step must scale as \\(\\sqrt{\\Delta t}\\).</p>"},{"location":"diffusion/brownian_motion_tutorial/#the-mathematical-statement","title":"The Mathematical Statement","text":"<p>Brownian motion is defined to satisfy:</p> \\[ w(t+\\Delta t) - w(t) \\sim \\mathcal{N}(0, \\Delta t) \\] <p>Since any \\(\\mathcal{N}(0, \\Delta t)\\) variable can be written as \\(\\sqrt{\\Delta t}\\) times a standard normal:</p> \\[ dw(t) = \\sqrt{dt} \\cdot \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0,1) \\] <p>This is not an approximation or modeling choice\u2014it's baked into the definition.</p>"},{"location":"diffusion/brownian_motion_tutorial/#why-not-other-scalings","title":"Why Not Other Scalings?","text":"<p>Let's check alternatives:</p>"},{"location":"diffusion/brownian_motion_tutorial/#if-noise-scaled-as-dt","title":"If noise scaled as \\(dt\\)","text":"\\[ dw \\sim dt \\] <p>Then: - Variance per step \\(\\sim dt^2\\) - Total variance over time \\(\\sim dt \\to 0\\)</p> <p>Result: Noise disappears. You just get an ODE.</p>"},{"location":"diffusion/brownian_motion_tutorial/#if-noise-scaled-as-constant","title":"If noise scaled as constant","text":"\\[ dw \\sim 1 \\] <p>Then: - Variance per unit time \\(\\to \\infty\\)</p> <p>Result: Process explodes. No limit exists.</p>"},{"location":"diffusion/brownian_motion_tutorial/#only-sqrtdt-works","title":"Only \\(\\sqrt{dt}\\) works","text":"<p>It's the unique scaling that gives: - Nontrivial randomness - Finite variance - A meaningful continuous-time limit</p> <p>This is why Brownian motion is so universal.</p>"},{"location":"diffusion/brownian_motion_tutorial/#the-key-insight-for-sdes","title":"The Key Insight for SDEs","text":"<p>In an SDE:</p> \\[ dx = f(x,t)\\,dt + g(t)\\,dw \\] <p>You can now clearly see: - Deterministic motion scales like \\(dt\\) - Stochastic motion scales like \\(\\sqrt{dt}\\)</p> <p>They live on different scales. This difference is crucial\u2014it's why noise doesn't vanish or explode.</p>"},{"location":"diffusion/brownian_motion_tutorial/#6-from-random-walk-to-brownian-motion","title":"6. From Random Walk to Brownian Motion","text":"<p>Now let's make the connection rigorous. How exactly does a random walk become Brownian motion?</p>"},{"location":"diffusion/brownian_motion_tutorial/#the-scaling-limit","title":"The Scaling Limit","text":"<p>Setup: Take a random walk with: - Step size: \\(\\Delta x\\) - Time step: \\(\\Delta t\\) - Steps: \\(\\varepsilon_1, \\varepsilon_2, \\ldots\\) with \\(\\mathbb{E}[\\varepsilon_i] = 0\\), \\(\\text{Var}(\\varepsilon_i) = 1\\)</p> <p>Discrete random walk:</p> \\[ S_n = \\sum_{i=1}^n \\varepsilon_i, \\quad S_0 = 0 \\] <p>Rescaling: Define a continuous-time process:</p> \\[ W_n(t) = \\frac{1}{\\sqrt{n}} S_{\\lfloor nt \\rfloor} \\] <p>Interpretation:</p> <ul> <li>Speed up time by factor \\(n\\) (so \\(nt\\) steps happen by time \\(t\\))</li> <li>Shrink space by \\(\\sqrt{n}\\)</li> </ul> <p>Variance check:</p> \\[ \\text{Var}(W_n(t)) = \\text{Var}\\left(\\frac{1}{\\sqrt{n}} S_{\\lfloor nt \\rfloor}\\right) = \\frac{1}{n} \\cdot \\lfloor nt \\rfloor \\approx t \\] <p>Perfect!</p>"},{"location":"diffusion/brownian_motion_tutorial/#donskers-invariance-principle","title":"Donsker's Invariance Principle","text":"<p>Theorem (Donsker, 1951): As \\(n \\to \\infty\\),</p> \\[ W_n(\\cdot) \\Rightarrow B(\\cdot) \\] <p>as processes (convergence in distribution in the space of continuous functions).</p> <p>What this means:</p> <ul> <li>The rescaled random walk converges to Brownian motion</li> <li>Not just at individual times, but as entire random curves</li> <li>This is a functional Central Limit Theorem</li> </ul> <p>Interpretation:</p> <ul> <li>Random walk = microscopic description</li> <li>Brownian motion = macroscopic, continuum description</li> </ul> <p>Just like: - Individual molecules \u2192 thermodynamics - Coin flips \u2192 Gaussian noise - Discrete Markov chains \u2192 SDEs</p>"},{"location":"diffusion/brownian_motion_tutorial/#7-why-this-matters-for-diffusion-models","title":"7. Why This Matters for Diffusion Models","text":"<p>Now we can connect everything to diffusion models.</p>"},{"location":"diffusion/brownian_motion_tutorial/#the-forward-process","title":"The Forward Process","text":"<p>In diffusion models, the forward noising process gradually corrupts clean data into noise.</p> <p>Discrete view (DDPM):</p> \\[ x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1-\\bar{\\alpha}_t} \\varepsilon \\] <p>This is a discrete random walk in noise space.</p> <p>Continuous view (SDE):</p> \\[ dx = f(x,t)\\,dt + g(t)\\,dw(t) \\] <p>This treats the same process as Brownian motion with drift.</p> <p>Key insight: DDPM's discrete noise exactly matches the \\(\\sqrt{dt}\\) scaling of Brownian motion.</p>"},{"location":"diffusion/brownian_motion_tutorial/#why-brownian-motion-is-perfect-for-diffusion","title":"Why Brownian Motion is Perfect for Diffusion","text":"<ol> <li>Continuous paths: Data never jumps discontinuously</li> <li>Gaussian increments: Noise is well-behaved, tractable</li> <li>Independent increments: Markov property enables efficient training</li> <li>Stationary increments: Noise schedule is time-translation invariant</li> <li>Universal limit: Any reasonable discrete noise schedule converges to it</li> </ol>"},{"location":"diffusion/brownian_motion_tutorial/#the-sde-formulation","title":"The SDE Formulation","text":"<p>The forward SDE:</p> \\[ dx = f(x,t)\\,dt + g(t)\\,dw(t) \\] <p>Components:</p> <ul> <li>\\(f(x,t)\\): Drift (deterministic shrinkage toward origin)</li> <li>\\(g(t)\\): Diffusion coefficient (noise strength)</li> <li>\\(dw(t) = \\sqrt{dt} \\cdot \\varepsilon\\): Brownian motion</li> </ul> <p>Why this works:</p> <ul> <li>Drift scales as \\(dt\\) (slow, deterministic)</li> <li>Diffusion scales as \\(\\sqrt{dt}\\) (fast, stochastic)</li> <li>Together they balance to create smooth corruption</li> </ul>"},{"location":"diffusion/brownian_motion_tutorial/#the-reverse-process","title":"The Reverse Process","text":"<p>The reverse SDE:</p> \\[ dx = \\left[f(x,t) - g(t)^2 \\nabla_x \\log p_t(x)\\right]dt + g(t)\\,d\\bar{w}(t) \\] <p>Key point: The reverse process also uses Brownian motion (with time running backwards).</p> <p>Why this is remarkable:</p> <ul> <li>The same stochastic process that corrupts data</li> <li>Can be reversed to generate data</li> <li>All thanks to Anderson's theorem (1982)</li> </ul>"},{"location":"diffusion/brownian_motion_tutorial/#connection-to-score-matching","title":"Connection to Score Matching","text":"<p>The score function \\(\\nabla_x \\log p_t(x)\\) tells you: - Which direction increases probability - How to \"undo\" the Brownian noise</p> <p>Training: Learn to predict the score at all noise levels.</p> <p>Sampling: Use the learned score in the reverse SDE to denoise.</p> <p>The bridge: Brownian motion is what makes this connection clean and tractable.</p>"},{"location":"diffusion/brownian_motion_tutorial/#summary-the-big-picture","title":"Summary: The Big Picture","text":"<p>Let's synthesize everything:</p>"},{"location":"diffusion/brownian_motion_tutorial/#what-brownian-motion-is","title":"What Brownian Motion Is","text":"<p>The universal macroscopic description of motion driven by countless microscopic, memoryless, unbiased fluctuations.</p>"},{"location":"diffusion/brownian_motion_tutorial/#why-it-appears-everywhere","title":"Why It Appears Everywhere","text":"<ul> <li>Pollen grains in water</li> <li>Heat diffusion</li> <li>Stock prices (with caveats)</li> <li>Noise in neural models</li> <li>Forward processes in diffusion models</li> <li>SDE limits of random walks</li> </ul> <p>It is not \"the truth of motion\"\u2014it is the inevitable limit when you refuse to track microscopic detail.</p>"},{"location":"diffusion/brownian_motion_tutorial/#the-key-properties-recap","title":"The Key Properties (Recap)","text":"<ol> <li>\\(B_0 = 0\\): Start at origin (choice of reference frame)</li> <li>Independent increments: No memory (Markov property)</li> <li>Gaussian increments with variance \\(\\propto\\) time: Diffusive scaling</li> <li>Continuous but nowhere differentiable: Infinitely rough</li> </ol>"},{"location":"diffusion/brownian_motion_tutorial/#the-sqrtdt-scaling-one-line-takeaway","title":"The \\(\\sqrt{dt}\\) Scaling (One-Line Takeaway)","text":"<p>Noise scales as \\(\\sqrt{dt}\\) because variance must grow linearly in time for a continuous-time random process to exist; this is the only scaling that yields finite, nontrivial stochastic behavior.</p>"},{"location":"diffusion/brownian_motion_tutorial/#for-diffusion-models","title":"For Diffusion Models","text":"<ul> <li>DDPM: Discrete random walk in noise space</li> <li>SDE formulation: Continuous Brownian motion with drift</li> <li>Same process: Different descriptions, unified by Donsker's theorem</li> <li>Why it works: Brownian motion provides the mathematical foundation for adding and removing noise in a principled way</li> </ul>"},{"location":"diffusion/brownian_motion_tutorial/#further-reading","title":"Further Reading","text":""},{"location":"diffusion/brownian_motion_tutorial/#primary-papers","title":"Primary Papers","text":"<ul> <li>Einstein (1905): Investigations on the Theory of the Brownian Movement</li> <li> <p>Original explanation of Brownian motion via molecular collisions</p> </li> <li> <p>Donsker (1951): An invariance principle for certain probability limit theorems</p> </li> <li> <p>Proves random walk \u2192 Brownian motion convergence</p> </li> <li> <p>Song et al. (2021): Score-Based Generative Modeling through SDEs</p> </li> <li>Applies Brownian motion to diffusion models</li> </ul>"},{"location":"diffusion/brownian_motion_tutorial/#textbooks","title":"Textbooks","text":"<ul> <li>\u00d8ksendal (2003): Stochastic Differential Equations: An Introduction with Applications</li> <li> <p>Comprehensive SDE theory with Brownian motion foundations</p> </li> <li> <p>Karatzas &amp; Shreve (1991): Brownian Motion and Stochastic Calculus</p> </li> <li> <p>Advanced reference with rigorous proofs</p> </li> <li> <p>M\u00f6rters &amp; Peres (2010): Brownian Motion</p> </li> <li>Modern treatment with applications</li> </ul>"},{"location":"diffusion/brownian_motion_tutorial/#related-topics","title":"Related Topics","text":"<ul> <li>It\u00f4 calculus: How to do calculus with Brownian motion</li> <li>Langevin dynamics: Brownian motion + gradient flow</li> <li>Fokker-Planck equation: PDE describing probability evolution</li> <li>Wiener measure: Probability measure on path space</li> </ul>"},{"location":"diffusion/brownian_motion_tutorial/#next-steps","title":"Next Steps","text":"<p>Now that you understand Brownian motion:</p> <ol> <li>See it in action: Run the SDE tutorial notebook to visualize Brownian paths</li> <li>Study the forward SDE: Learn how \\(f(x,t)\\) and \\(g(t)\\) are chosen</li> <li>Understand score matching: See how the score function reverses diffusion</li> <li>Implement DDPM: Connect discrete and continuous views</li> </ol> <p>The mathematical machinery is now in place. Time to build diffusion models!</p>"},{"location":"diffusion/classifier_free_guidance/","title":"Classifier-Free Guidance: Conditional Generation Without Classifiers","text":""},{"location":"diffusion/classifier_free_guidance/#overview","title":"Overview","text":"<p>Classifier-free guidance is a technique for conditional generation in diffusion models that: - Enables high-quality conditional sampling (e.g., text-to-image, class-conditional) - Does not require a separate classifier network - Uses a single model trained for both conditional and unconditional generation - Provides a guidance scale to control the trade-off between diversity and fidelity</p> <p>This technique has become the standard approach for conditional diffusion models, powering systems like Stable Diffusion, DALL-E 2, and Imagen.</p>"},{"location":"diffusion/classifier_free_guidance/#the-problem-conditional-generation","title":"The Problem: Conditional Generation","text":""},{"location":"diffusion/classifier_free_guidance/#goal","title":"Goal","text":"<p>Generate samples from \\(p(x|c)\\) where: - \\(x\\) is the data (e.g., image) - \\(c\\) is the condition (e.g., text prompt, class label, image features)</p>"},{"location":"diffusion/classifier_free_guidance/#naive-approach-doesnt-work","title":"Naive Approach Doesn't Work","text":"<p>Simply training \\(s_\\theta(x_t, t, c) \\approx \\nabla_x \\log p_t(x|c)\\) and sampling doesn't give high-quality results because: - The model learns to average over all modes of \\(p(x|c)\\) - Generated samples are generic and lack detail - No control over how strongly to follow the condition</p> <p>Example: For prompt \"a red car\", the model might generate a blurry, generic car that's somewhat reddish.</p>"},{"location":"diffusion/classifier_free_guidance/#prior-art-classifier-guidance","title":"Prior Art: Classifier Guidance","text":""},{"location":"diffusion/classifier_free_guidance/#the-classifier-guidance-approach-dhariwal-nichol-2021","title":"The Classifier Guidance Approach (Dhariwal &amp; Nichol, 2021)","text":"<p>Idea: Use Bayes' rule to incorporate a classifier into the score:</p> \\[ \\nabla_x \\log p_t(x|c) = \\nabla_x \\log p_t(x) + \\nabla_x \\log p_t(c|x) \\] <p>Implementation: 1. Train an unconditional diffusion model: \\(s_\\theta(x_t, t) \\approx \\nabla_x \\log p_t(x)\\) 2. Train a separate time-dependent classifier: \\(p_\\phi(c|x_t, t)\\) 3. At sampling, use the guided score:</p> \\[ \\tilde{s}(x_t, t, c) = s_\\theta(x_t, t) + w \\cdot \\nabla_{x_t} \\log p_\\phi(c|x_t, t) \\] <p>where \\(w\\) is the guidance scale.</p> <p>With guidance scale \\(w &gt; 1\\):</p> \\[ \\tilde{s}(x_t, t, c) = s_\\theta(x_t, t) + w \\cdot \\nabla_{x_t} \\log p_\\phi(c|x_t, t) = \\nabla_x \\log p_t(x) + w \\cdot \\nabla_x \\log p_t(c|x) \\] <p>This can be rewritten as sampling from:</p> \\[ p_t(x|c)^w \\cdot p_t(x)^{1-w} \\propto p_t(c|x)^w \\cdot p_t(x) \\] <p>Effect: Amplifies the classifier gradient, making samples more strongly conditioned but less diverse.</p>"},{"location":"diffusion/classifier_free_guidance/#problems-with-classifier-guidance","title":"Problems with Classifier Guidance","text":"<ol> <li>Need to train a separate classifier on noisy images \\(x_t\\) at all timesteps</li> <li>Classifier can be adversarially fooled: Model may generate images that fool the classifier rather than truly match the condition</li> <li>Inefficient: Two networks instead of one</li> <li>Limited to differentiable conditions: Can't easily use discrete or structured conditions</li> </ol>"},{"location":"diffusion/classifier_free_guidance/#classifier-free-guidance-the-elegant-solution","title":"Classifier-Free Guidance: The Elegant Solution","text":""},{"location":"diffusion/classifier_free_guidance/#core-idea-ho-salimans-2021","title":"Core Idea (Ho &amp; Salimans, 2021)","text":"<p>Train a single model that learns both: - Conditional score: \\(\\nabla_x \\log p_t(x|c)\\) - Unconditional score: \\(\\nabla_x \\log p_t(x)\\)</p> <p>Key trick: Use the same network but randomly drop the condition during training.</p>"},{"location":"diffusion/classifier_free_guidance/#training-procedure","title":"Training Procedure","text":"<p>Dataset: Pairs \\((x, c)\\) where \\(c\\) is the condition (text, class, etc.)</p> <p>Training: 1. With probability \\(p_{\\text{uncond}}\\) (typically 10-20%), replace \\(c\\) with a null token \\(\\varnothing\\) 2. Train the score network \\(s_\\theta(x_t, t, c)\\) on both:    - Conditional examples: \\((x_t, t, c)\\)    - Unconditional examples: \\((x_t, t, \\varnothing)\\)</p> <p>Loss (standard score matching):</p> \\[ \\mathbb{E}_{t, x_0, \\epsilon, c}\\left[\\left\\| s_\\theta(x_t, t, c) - \\nabla_{x_t} \\log p_t(x_t|x_0) \\right\\|^2\\right] \\] <p>where we sample \\(c \\sim p_{\\text{data}}\\) with probability \\((1-p_{\\text{uncond}})\\) and \\(c = \\varnothing\\) with probability \\(p_{\\text{uncond}}\\).</p>"},{"location":"diffusion/classifier_free_guidance/#sampling-with-guidance","title":"Sampling with Guidance","text":"<p>At generation time, we use implicit classifier guidance by combining conditional and unconditional scores:</p> \\[ \\boxed{\\tilde{s}_\\theta(x_t, t, c) = (1-w) \\cdot s_\\theta(x_t, t, \\varnothing) + w \\cdot s_\\theta(x_t, t, c)} \\] <p>where \\(w\\) is the guidance scale.</p> <p>Alternative formulation (more common in practice):</p> \\[ \\boxed{\\tilde{s}_\\theta(x_t, t, c) = s_\\theta(x_t, t, \\varnothing) + w \\cdot \\left[s_\\theta(x_t, t, c) - s_\\theta(x_t, t, \\varnothing)\\right]} \\] <p>This makes it clearer that we're amplifying the difference between conditional and unconditional scores.</p>"},{"location":"diffusion/classifier_free_guidance/#why-this-works","title":"Why This Works","text":"<p>Mathematical justification:</p> <p>Recall that the conditional score can be decomposed:</p> \\[ \\nabla_x \\log p_t(x|c) = \\nabla_x \\log p_t(x) + \\nabla_x \\log p_t(c|x) \\] <p>Rearranging:</p> \\[ \\nabla_x \\log p_t(c|x) = \\nabla_x \\log p_t(x|c) - \\nabla_x \\log p_t(x) \\] <p>So:</p> \\[ \\nabla_x \\log p_t(x|c) + w \\cdot \\nabla_x \\log p_t(c|x) = \\nabla_x \\log p_t(x|c) + w \\cdot [\\nabla_x \\log p_t(x|c) - \\nabla_x \\log p_t(x)] \\] \\[ = (1+w) \\cdot \\nabla_x \\log p_t(x|c) - w \\cdot \\nabla_x \\log p_t(x) \\] <p>In classifier-free guidance, we directly learn both terms, so:</p> \\[ \\tilde{s}_\\theta(x_t, t, c) = s_\\theta(x_t, t, \\varnothing) + w \\cdot [s_\\theta(x_t, t, c) - s_\\theta(x_t, t, \\varnothing)] \\] <p>is equivalent to classifier guidance without needing the classifier!</p>"},{"location":"diffusion/classifier_free_guidance/#intuition-what-does-guidance-do","title":"Intuition: What Does Guidance Do?","text":""},{"location":"diffusion/classifier_free_guidance/#the-guidance-scale-w","title":"The Guidance Scale \\(w\\)","text":"\\(w\\) Effect Samples \\(w = 0\\) Pure unconditional: \\(\\tilde{s} = s_\\theta(x_t, t, \\varnothing)\\) Diverse, generic \\(w = 1\\) Pure conditional: \\(\\tilde{s} = s_\\theta(x_t, t, c)\\) Balanced \\(w &gt; 1\\) Amplified conditioning: Over-emphasize \\(c\\) High fidelity to \\(c\\), less diverse \\(w &lt; 0\\) Negative guidance: Push away from \\(c\\) Opposite of \\(c\\)"},{"location":"diffusion/classifier_free_guidance/#visual-intuition","title":"Visual Intuition","text":"<p>Think of the score function as a vector field pointing toward higher probability regions:</p> <pre><code>Unconditional score s(x_t, \u2205):\n  Points toward \"generic realistic images\"\n\nConditional score s(x_t, c):\n  Points toward \"realistic images matching c\"\n\nGuided score (w=2):\n  Amplifies the difference, points toward \"images that STRONGLY match c\"\n</code></pre> <p>Effect of \\(w &gt; 1\\):</p> <ul> <li>Samples become more aligned with the condition</li> <li>But also less diverse (mode-seeking behavior)</li> <li>Trade-off: fidelity vs. diversity</li> </ul>"},{"location":"diffusion/classifier_free_guidance/#example-text-to-image","title":"Example: Text-to-Image","text":"<p>Prompt: \"A cat wearing a top hat\"</p> <ul> <li>\\(w = 1\\): Might generate a cat with a hat, somewhat detailed</li> <li>\\(w = 5\\): Cat clearly wearing a fancy top hat, high detail, but less variety across samples</li> <li>\\(w = 10\\): Very specific top-hat-wearing cat, but might lose overall image quality (over-saturated, artifacts)</li> </ul> <p>Typical values: \\(w \\in [1, 10]\\) for text-to-image, with \\(w = 7-8\\) being common.</p>"},{"location":"diffusion/classifier_free_guidance/#implementation-details","title":"Implementation Details","text":""},{"location":"diffusion/classifier_free_guidance/#training-code-structure","title":"Training Code Structure","text":"<pre><code>class ConditionalScoreNetwork(nn.Module):\n    def __init__(self, data_dim, hidden_dim, cond_dim, time_dim):\n        super().__init__()\n        # Network that takes data, time, and condition\n        self.net = UNet(data_dim, hidden_dim, cond_dim, time_dim)\n        # Learnable null token for unconditional generation\n        self.null_embedding = nn.Parameter(torch.zeros(cond_dim))\n\n    def forward(self, x_t, t, c, is_conditional):\n        \"\"\"\n        Args:\n            x_t: Noisy data [batch, data_dim]\n            t: Time [batch]\n            c: Condition [batch, cond_dim]\n            is_conditional: Whether to use condition [batch] (bool)\n        \"\"\"\n        # Replace condition with null token where is_conditional is False\n        c_masked = torch.where(\n            is_conditional[:, None], \n            c, \n            self.null_embedding[None, :]\n        )\n        return self.net(x_t, t, c_masked)\n\n\ndef train_step(model, x_0, c, p_uncond=0.1):\n    \"\"\"Single training step with classifier-free guidance.\"\"\"\n    batch_size = x_0.shape[0]\n\n    # Sample timesteps\n    t = torch.rand(batch_size) * T_max\n\n    # Sample noise\n    eps = torch.randn_like(x_0)\n\n    # Forward diffusion\n    x_t = alpha_bar_t.sqrt() * x_0 + (1 - alpha_bar_t).sqrt() * eps\n\n    # Randomly drop conditions\n    is_conditional = torch.rand(batch_size) &gt; p_uncond\n\n    # Predict score\n    score_pred = model(x_t, t, c, is_conditional)\n\n    # True score (negative scaled noise for VP-SDE)\n    score_true = -eps / (1 - alpha_bar_t).sqrt()\n\n    # Loss\n    loss = ((score_pred - score_true) ** 2).mean()\n\n    return loss\n</code></pre>"},{"location":"diffusion/classifier_free_guidance/#sampling-with-guidance_1","title":"Sampling with Guidance","text":"<pre><code>def sample_with_cfg(model, condition, guidance_scale, num_steps=1000):\n    \"\"\"Sample using classifier-free guidance.\"\"\"\n    x_t = torch.randn(batch_size, data_dim)  # Start from noise\n\n    dt = T_max / num_steps\n\n    for i in reversed(range(num_steps)):\n        t = torch.full((batch_size,), i * dt)\n\n        # Get conditional and unconditional scores\n        with torch.no_grad():\n            # Conditional score\n            s_cond = model(x_t, t, condition, is_conditional=True)\n\n            # Unconditional score\n            s_uncond = model(x_t, t, condition, is_conditional=False)\n\n            # Guided score\n            s_guided = s_uncond + guidance_scale * (s_cond - s_uncond)\n\n        # Reverse SDE step (Euler-Maruyama)\n        drift = -0.5 * beta(t) * (x_t + s_guided)\n        diffusion = beta(t).sqrt()\n\n        x_t = x_t + drift * dt + diffusion * torch.randn_like(x_t) * dt.sqrt()\n\n    return x_t\n</code></pre>"},{"location":"diffusion/classifier_free_guidance/#practical-tips","title":"Practical Tips","text":"<ol> <li>Null token: </li> <li>For text: Use a special <code>&lt;UNCOND&gt;</code> token or zero embedding</li> <li>For class labels: Use a special \"no class\" index</li> <li> <p>For continuous conditions: Use zeros or learnable null embedding</p> </li> <li> <p>Unconditional probability:</p> </li> <li>Typical: \\(p_{\\text{uncond}} = 0.1\\) (10% of training samples)</li> <li>Too low: Poor unconditional generation, weak guidance</li> <li> <p>Too high: Poor conditional generation</p> </li> <li> <p>Guidance scale:</p> </li> <li>Start with \\(w = 1\\) (pure conditional)</li> <li>Increase to \\(w = 5-10\\) for stronger conditioning</li> <li> <p>Monitor diversity: Too high \\(w\\) can collapse to a few modes</p> </li> <li> <p>Computational cost:</p> </li> <li>Need two forward passes per sampling step (conditional + unconditional)</li> <li>Can be mitigated with caching or distillation</li> </ol>"},{"location":"diffusion/classifier_free_guidance/#variants-and-extensions","title":"Variants and Extensions","text":""},{"location":"diffusion/classifier_free_guidance/#1-dynamic-guidance","title":"1. Dynamic Guidance","text":"<p>Instead of fixed \\(w\\), use time-dependent guidance:</p> \\[ \\tilde{s}_\\theta(x_t, t, c) = s_\\theta(x_t, t, \\varnothing) + w(t) \\cdot [s_\\theta(x_t, t, c) - s_\\theta(x_t, t, \\varnothing)] \\] <p>Intuition: Early steps (high noise) benefit from less guidance; later steps (low noise) benefit from stronger guidance.</p>"},{"location":"diffusion/classifier_free_guidance/#2-multi-conditional-guidance","title":"2. Multi-Conditional Guidance","text":"<p>For multiple conditions \\(c_1, c_2, \\ldots\\):</p> \\[ \\tilde{s} = s(x_t, t, \\varnothing) + \\sum_i w_i \\cdot [s(x_t, t, c_i) - s(x_t, t, \\varnothing)] \\] <p>Example: Combine text prompt + style reference + color palette.</p>"},{"location":"diffusion/classifier_free_guidance/#3-negative-prompting","title":"3. Negative Prompting","text":"<p>Use \\(w &lt; 0\\) for certain conditions to avoid them:</p> \\[ \\tilde{s} = s(x_t, t, c_{\\text{positive}}) + w_{\\text{neg}} \\cdot [s(x_t, t, \\varnothing) - s(x_t, t, c_{\\text{negative}})] \\] <p>Example: \"A beautiful landscape\" (positive) + avoid \"blurry, low quality\" (negative).</p>"},{"location":"diffusion/classifier_free_guidance/#4-guidance-distillation","title":"4. Guidance Distillation","text":"<p>Train a separate model to directly predict the guided score, eliminating the two-forward-pass cost:</p> \\[ \\tilde{s}_{\\text{student}}(x_t, t, c) \\approx s_\\theta(x_t, t, \\varnothing) + w \\cdot [s_\\theta(x_t, t, c) - s_\\theta(x_t, t, \\varnothing)] \\]"},{"location":"diffusion/classifier_free_guidance/#connection-to-both-ddpm-and-sde-views","title":"Connection to Both DDPM and SDE Views","text":""},{"location":"diffusion/classifier_free_guidance/#ddpm-view","title":"DDPM View","text":"<p>In the discrete-time DDPM formulation, classifier-free guidance modifies the denoising step:</p> \\[ \\tilde{\\epsilon}_\\theta(x_t, t, c) = \\epsilon_\\theta(x_t, t, \\varnothing) + w \\cdot [\\epsilon_\\theta(x_t, t, c) - \\epsilon_\\theta(x_t, t, \\varnothing)] \\] <p>where \\(\\epsilon_\\theta\\) predicts the noise instead of the score.</p> <p>Sampling:</p> \\[ x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\tilde{\\epsilon}_\\theta(x_t, t, c)\\right) + \\sigma_t z \\]"},{"location":"diffusion/classifier_free_guidance/#sde-view","title":"SDE View","text":"<p>In the continuous-time SDE formulation, guidance modifies the reverse-time SDE:</p> <p>Original reverse SDE:</p> \\[ dx = \\left[-f(x, t) + g(t)^2 \\nabla_x \\log p_t(x|c)\\right]dt + g(t)\\,d\\bar{w} \\] <p>With classifier-free guidance:</p> \\[ dx = \\left[-f(x, t) + g(t)^2 \\tilde{s}_\\theta(x_t, t, c)\\right]dt + g(t)\\,d\\bar{w} \\] <p>where \\(\\tilde{s}_\\theta\\) is the guided score.</p> <p>Key insight: Both views are modifying the drift term by using an amplified score/noise prediction.</p>"},{"location":"diffusion/classifier_free_guidance/#empirical-results","title":"Empirical Results","text":""},{"location":"diffusion/classifier_free_guidance/#from-original-paper-ho-salimans-2021","title":"From Original Paper (Ho &amp; Salimans, 2021)","text":"<p>ImageNet 128\u00d7128:</p> <ul> <li>\\(w = 1\\): FID = 12.6, Diversity = high</li> <li>\\(w = 2\\): FID = 7.0, Diversity = medium</li> <li>\\(w = 4\\): FID = 4.6, Diversity = low</li> </ul> <p>Trade-off:</p> <ul> <li>Higher \\(w\\) \u2192 Better FID (sample quality)</li> <li>Higher \\(w\\) \u2192 Worse diversity (mode collapse)</li> </ul>"},{"location":"diffusion/classifier_free_guidance/#modern-applications","title":"Modern Applications","text":"<p>Stable Diffusion (text-to-image): - Default: \\(w = 7.5\\) - Users can adjust from 1 to 20</p> <p>DALL-E 2, Imagen:</p> <ul> <li>Heavy use of classifier-free guidance</li> <li>Enables high-fidelity text-to-image generation</li> </ul>"},{"location":"diffusion/classifier_free_guidance/#summary","title":"Summary","text":""},{"location":"diffusion/classifier_free_guidance/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Classifier-free guidance enables conditional generation without a separate classifier</li> <li>Train one model for both conditional and unconditional generation</li> <li> <p>Randomly drop conditions during training</p> </li> <li> <p>Guidance scale \\(w\\) controls fidelity vs. diversity</p> </li> <li>\\(w = 1\\): Balanced conditional generation</li> <li>\\(w &gt; 1\\): Stronger conditioning, less diversity</li> <li> <p>Typical: \\(w \\in [5, 10]\\) for text-to-image</p> </li> <li> <p>Works in both DDPM and SDE views</p> </li> <li>DDPM: Modify noise prediction</li> <li> <p>SDE: Modify score/drift term</p> </li> <li> <p>Has become the standard approach</p> </li> <li>Simpler than classifier guidance</li> <li>More flexible</li> <li> <p>Better results in practice</p> </li> <li> <p>Main cost: Two forward passes per step</p> </li> <li>Can be mitigated with distillation</li> <li>Worth it for generation quality</li> </ol>"},{"location":"diffusion/classifier_free_guidance/#when-to-use","title":"When to Use","text":"<p>Use classifier-free guidance when:</p> <ul> <li>You want high-quality conditional generation</li> <li>You have paired data \\((x, c)\\)</li> <li>You want control over conditioning strength</li> <li>You don't want to train a separate classifier</li> </ul> <p>Consider alternatives when:</p> <ul> <li>Two forward passes are too expensive (use distillation)</li> <li>You have very limited conditional data (might not learn unconditional well)</li> <li>Conditions are complex or structured (might need specialized architectures)</li> </ul>"},{"location":"diffusion/classifier_free_guidance/#references","title":"References","text":""},{"location":"diffusion/classifier_free_guidance/#original-papers","title":"Original Papers","text":"<ul> <li>Ho &amp; Salimans (2021): \"Classifier-Free Diffusion Guidance\" (NeurIPS 2021 Workshop)</li> <li>Dhariwal &amp; Nichol (2021): \"Diffusion Models Beat GANs on Image Synthesis\" (introduced classifier guidance)</li> </ul>"},{"location":"diffusion/classifier_free_guidance/#applications","title":"Applications","text":"<ul> <li>Rombach et al. (2022): \"High-Resolution Image Synthesis with Latent Diffusion Models\" (Stable Diffusion)</li> <li>Ramesh et al. (2022): \"Hierarchical Text-Conditional Image Generation with CLIP Latents\" (DALL-E 2)</li> <li>Saharia et al. (2022): \"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding\" (Imagen)</li> </ul>"},{"location":"diffusion/classifier_free_guidance/#related-documents","title":"Related Documents","text":"<ul> <li>DDPM View: See <code>docs/DDPM/</code> for discrete-time formulation</li> <li>SDE View: See <code>docs/SDE/01_diffusion_sde_view.md</code> for continuous-time perspective</li> <li>Conditional Generation: See <code>docs/DDPM/04_ddpm_extensions.md</code> (when available)</li> <li>Score Networks: See <code>docs/diffusion/score_network/</code> for architecture details</li> </ul>"},{"location":"diffusion/integrating_factor/","title":"Understanding Integrating Factors","text":""},{"location":"diffusion/integrating_factor/#what-is-an-integrating-factor","title":"What is an Integrating Factor?","text":"<p>An integrating factor is a function that, when multiplied to a differential equation, makes it easier to solve. It's a powerful technique for solving linear differential equations (both ODEs and SDEs).</p>"},{"location":"diffusion/integrating_factor/#the-core-idea","title":"The Core Idea","text":"<p>Given a differential equation that's hard to solve directly, we multiply both sides by a cleverly chosen function \\(\\mu(t)\\) that transforms it into a form we can integrate easily.</p>"},{"location":"diffusion/integrating_factor/#motivating-example-first-order-linear-ode","title":"Motivating Example: First-Order Linear ODE","text":"<p>Consider the ODE:</p> \\[ \\frac{dy}{dt} + a(t)y = b(t) \\] <p>This is hard to solve because \\(y\\) and \\(\\frac{dy}{dt}\\) are mixed together.</p>"},{"location":"diffusion/integrating_factor/#the-trick-multiply-by-an-integrating-factor","title":"The Trick: Multiply by an Integrating Factor","text":"<p>Define:</p> \\[ \\mu(t) = \\exp\\left(\\int a(t)\\,dt\\right) \\] <p>Key property: This \\(\\mu(t)\\) satisfies:</p> \\[ \\frac{d\\mu}{dt} = a(t)\\mu(t) \\]"},{"location":"diffusion/integrating_factor/#why-this-helps","title":"Why This Helps","text":"<p>Multiply the original ODE by \\(\\mu(t)\\):</p> \\[ \\mu(t)\\frac{dy}{dt} + a(t)\\mu(t)y = \\mu(t)b(t) \\] <p>Now notice: The left side is the derivative of \\(\\mu(t)y(t)\\)!</p> \\[ \\frac{d}{dt}(\\mu y) = \\mu\\frac{dy}{dt} + y\\frac{d\\mu}{dt} = \\mu\\frac{dy}{dt} + a(t)\\mu y \\] <p>So the ODE becomes:</p> \\[ \\frac{d}{dt}(\\mu y) = \\mu(t)b(t) \\] <p>This is now easy to solve! Just integrate both sides:</p> \\[ \\mu(t)y(t) = \\int \\mu(s)b(s)\\,ds + C \\] <p>Then solve for \\(y(t)\\).</p>"},{"location":"diffusion/integrating_factor/#deriving-the-key-property-fracdmudt-atmut","title":"Deriving the Key Property: \\(\\frac{d\\mu}{dt} = a(t)\\mu(t)\\)","text":""},{"location":"diffusion/integrating_factor/#step-1-definition","title":"Step 1: Definition","text":"\\[ \\mu(t) = \\exp\\left(\\int_0^t a(s)\\,ds\\right) \\]"},{"location":"diffusion/integrating_factor/#step-2-apply-the-fundamental-theorem-of-calculus","title":"Step 2: Apply the Fundamental Theorem of Calculus","text":"<p>The derivative of an integral with respect to its upper limit is:</p> \\[ \\frac{d}{dt}\\int_0^t a(s)\\,ds = a(t) \\]"},{"location":"diffusion/integrating_factor/#step-3-apply-the-chain-rule","title":"Step 3: Apply the Chain Rule","text":"<p>Since \\(\\mu(t) = \\exp(u(t))\\) where \\(u(t) = \\int_0^t a(s)\\,ds\\):</p> \\[ \\frac{d\\mu}{dt} = \\frac{d}{dt}\\exp(u(t)) = \\exp(u(t)) \\cdot \\frac{du}{dt} \\] \\[ \\frac{d\\mu}{dt} = \\mu(t) \\cdot a(t) = a(t)\\mu(t) \\] <p>Result: \\(\\boxed{\\frac{d\\mu}{dt} = a(t)\\mu(t)}\\)</p> <p>This is exactly what we need!</p>"},{"location":"diffusion/integrating_factor/#why-this-property-is-useful","title":"Why This Property is Useful","text":"<p>The property \\(\\frac{d\\mu}{dt} = a(t)\\mu(t)\\) allows us to recognize that:</p> \\[ \\frac{d}{dt}(\\mu y) = \\mu\\frac{dy}{dt} + y\\frac{d\\mu}{dt} = \\mu\\frac{dy}{dt} + a(t)\\mu y \\] <p>This matches the left side of our ODE after multiplying by \\(\\mu\\):</p> \\[ \\mu\\frac{dy}{dt} + a(t)\\mu y \\] <p>So we can rewrite the ODE as:</p> \\[ \\frac{d}{dt}(\\mu y) = \\mu b(t) \\] <p>which is immediately integrable.</p>"},{"location":"diffusion/integrating_factor/#application-to-linear-sdes","title":"Application to Linear SDEs","text":"<p>For the linear SDE:</p> \\[ dx = a(t)x\\,dt + b(t)\\,dw \\] <p>we use a similar integrating factor:</p> \\[ \\mu(t) = \\exp\\left(-\\int_0^t a(s)\\,ds\\right) \\] <p>Note the negative sign! This is because we want to cancel the drift term.</p>"},{"location":"diffusion/integrating_factor/#why-the-negative-sign","title":"Why the Negative Sign?","text":"<p>In the SDE case, we want \\(\\frac{d\\mu}{dt} = -a(t)\\mu(t)\\) (not \\(+a(t)\\mu(t)\\)).</p> <p>Let's derive it:</p> \\[ \\mu(t) = \\exp\\left(-\\int_0^t a(s)\\,ds\\right) \\] \\[ \\frac{d\\mu}{dt} = \\exp\\left(-\\int_0^t a(s)\\,ds\\right) \\cdot \\frac{d}{dt}\\left(-\\int_0^t a(s)\\,ds\\right) \\] \\[ \\frac{d\\mu}{dt} = \\mu(t) \\cdot (-a(t)) = -a(t)\\mu(t) \\] <p>Result: \\(\\boxed{\\frac{d\\mu}{dt} = -a(t)\\mu(t)}\\)</p>"},{"location":"diffusion/integrating_factor/#applying-itos-lemma","title":"Applying It\u00f4's Lemma","text":"<p>For the product \\(\\mu(t)x(t)\\), It\u00f4's lemma gives:</p> \\[ d(\\mu x) = \\mu\\,dx + x\\,d\\mu + d\\mu \\cdot dx \\] <p>Since \\(d\\mu = -a(t)\\mu\\,dt\\) (deterministic), the cross term \\(d\\mu \\cdot dx\\) is zero (deterministic \u00d7 stochastic has no quadratic variation).</p> <p>Substitute \\(dx = a(t)x\\,dt + b(t)\\,dw\\):</p> \\[ d(\\mu x) = \\mu(a(t)x\\,dt + b(t)\\,dw) + x(-a(t)\\mu\\,dt) \\] \\[ d(\\mu x) = \\mu a(t)x\\,dt + \\mu b(t)\\,dw - a(t)\\mu x\\,dt \\] <p>The drift terms cancel:</p> \\[ d(\\mu x) = \\mu(t)b(t)\\,dw \\] <p>This is the key step! The drift has been eliminated, leaving only the stochastic term.</p>"},{"location":"diffusion/integrating_factor/#summary-the-integrating-factor-method","title":"Summary: The Integrating Factor Method","text":""},{"location":"diffusion/integrating_factor/#for-odes","title":"For ODEs","text":"<p>Given: \\(\\frac{dy}{dt} + a(t)y = b(t)\\)</p> <ol> <li>Define: \\(\\mu(t) = \\exp\\left(\\int a(t)\\,dt\\right)\\)</li> <li>Property: \\(\\frac{d\\mu}{dt} = a(t)\\mu(t)\\)</li> <li>Multiply ODE by \\(\\mu\\): \\(\\frac{d}{dt}(\\mu y) = \\mu b(t)\\)</li> <li>Integrate: \\(\\mu y = \\int \\mu b\\,dt + C\\)</li> <li>Solve for \\(y\\): \\(y = \\frac{1}{\\mu}\\left(\\int \\mu b\\,dt + C\\right)\\)</li> </ol>"},{"location":"diffusion/integrating_factor/#for-sdes","title":"For SDEs","text":"<p>Given: \\(dx = a(t)x\\,dt + b(t)\\,dw\\)</p> <ol> <li>Define: \\(\\mu(t) = \\exp\\left(-\\int_0^t a(s)\\,ds\\right)\\) (note the negative sign)</li> <li>Property: \\(\\frac{d\\mu}{dt} = -a(t)\\mu(t)\\)</li> <li>Apply It\u00f4's lemma: \\(d(\\mu x) = \\mu b(t)\\,dw\\) (drift cancels)</li> <li>Integrate: \\(\\mu(t)x(t) = x(0) + \\int_0^t \\mu(s)b(s)\\,dw(s)\\)</li> <li>Solve for \\(x\\): \\(x(t) = \\frac{1}{\\mu(t)}\\left(x(0) + \\int_0^t \\mu(s)b(s)\\,dw(s)\\right)\\)</li> </ol>"},{"location":"diffusion/integrating_factor/#intuitive-understanding","title":"Intuitive Understanding","text":""},{"location":"diffusion/integrating_factor/#why-does-it-work","title":"Why Does It Work?","text":"<p>The integrating factor \"absorbs\" the problematic term. Think of it as:</p> <ul> <li>Before: \\(y\\) and \\(\\frac{dy}{dt}\\) are entangled</li> <li>After: The derivative of a product (\\(\\mu y\\)) equals something simple</li> </ul> <p>It's like completing the square or substitution\u2014a transformation that simplifies the problem.</p>"},{"location":"diffusion/integrating_factor/#physical-analogy","title":"Physical Analogy","text":"<p>Imagine you're tracking a particle's position while the coordinate system is also moving. The integrating factor is like switching to a coordinate system that moves with the drift, making the equation simpler.</p>"},{"location":"diffusion/integrating_factor/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Integrating factors transform hard equations into easy ones</li> <li>For ODEs: \\(\\mu(t) = \\exp(\\int a(t)\\,dt)\\) gives \\(\\frac{d\\mu}{dt} = a(t)\\mu(t)\\)</li> <li>For SDEs: \\(\\mu(t) = \\exp(-\\int_0^t a(s)\\,ds)\\) gives \\(\\frac{d\\mu}{dt} = -a(t)\\mu(t)\\)</li> <li>The negative sign in SDEs is crucial\u2014it ensures drift cancellation</li> <li>The product rule (or It\u00f4's lemma) makes the transformed equation integrable</li> </ol>"},{"location":"diffusion/integrating_factor/#references","title":"References","text":"<ul> <li>Boyce &amp; DiPrima: Elementary Differential Equations \u2014 Classic ODE textbook</li> <li>\u00d8ksendal (2003): Stochastic Differential Equations \u2014 Chapter 5 on linear SDEs</li> <li>Kloeden &amp; Platen (1992): Numerical Solution of SDEs \u2014 Comprehensive reference</li> </ul>"},{"location":"diffusion/noise_schedules/","title":"Noise Schedules for Diffusion Models","text":""},{"location":"diffusion/noise_schedules/#overview","title":"Overview","text":"<p>The noise schedule \\(\\beta(t)\\) controls how quickly noise is added during the forward diffusion process. It's a crucial design choice that affects: - Training speed and stability - Sample quality - The balance between preserving signal and reaching pure noise</p> <p>This document covers common noise schedule choices, their properties, and when to use each.</p>"},{"location":"diffusion/noise_schedules/#referenced-from","title":"Referenced From","text":"<p>This document is referenced in: - <code>docs/diffusion/forward_process_derivation.md</code> \u2014 Forward SDE derivation</p>"},{"location":"diffusion/noise_schedules/#mathematical-background","title":"Mathematical Background","text":"<p>Before diving into specific schedules, let's clarify the key quantities and their relationships.</p>"},{"location":"diffusion/noise_schedules/#definitions","title":"Definitions","text":"<p>Noise schedule \\(\\beta(t)\\): - Controls the rate of noise addition at time \\(t\\) - This is what you design/choose - Appears in the VP-SDE: \\(dx = -\\frac{1}{2}\\beta(t)x\\,dt + \\sqrt{\\beta(t)}\\,dw\\)</p> <p>Signal coefficient \\(\\alpha_t\\) or \\(\\alpha(t)\\): - Related to how much of the original signal remains - Defined as: \\(\\alpha(t) = \\exp\\left(-\\frac{1}{2}\\int_0^t \\beta(s)\\,ds\\right)\\) - Sometimes written as: \\(\\sqrt{\\bar{\\alpha}_t} = \\alpha(t)\\)</p> <p>Cumulative signal coefficient \\(\\bar{\\alpha}_t\\): - The square of the signal coefficient: \\(\\bar{\\alpha}_t = \\alpha(t)^2\\) - More commonly used in formulas - Defined as: \\(\\bar{\\alpha}_t = \\exp\\left(-\\int_0^t \\beta(s)\\,ds\\right)\\)</p> <p>Why these specific forms? These definitions emerge from solving the VP-SDE using the integrating factor technique. See <code>alpha_definitions_derivation.md</code> for the complete derivation showing how \\(\\alpha(t) = 1/\\mu(t)\\) where \\(\\mu(t)\\) is the integrating factor.</p>"},{"location":"diffusion/noise_schedules/#the-forward-process","title":"The Forward Process","text":"<p>The clean data \\(x_0\\) is corrupted into noisy data \\(x_t\\):</p> \\[ x_t = \\underbrace{\\sqrt{\\bar{\\alpha}_t}}_{\\text{signal scale}} x_0 + \\underbrace{\\sqrt{1-\\bar{\\alpha}_t}}_{\\text{noise scale}} \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, I) \\] <p>Interpretation:</p> <ul> <li>When \\(\\bar{\\alpha}_t = 1\\): Pure signal (\\(x_t = x_0\\))</li> <li>When \\(\\bar{\\alpha}_t = 0\\): Pure noise (\\(x_t = \\varepsilon\\))</li> <li>The schedule \\(\\beta(t)\\) determines how \\(\\bar{\\alpha}_t\\) decays from 1 to 0</li> </ul>"},{"location":"diffusion/noise_schedules/#relationship-summary","title":"Relationship Summary","text":"\\[ \\beta(t) \\quad \\xrightarrow{\\text{integrate}} \\quad \\bar{\\alpha}_t = \\exp\\left(-\\int_0^t \\beta(s)\\,ds\\right) \\] <p>In practice:</p> <ul> <li>You choose \\(\\beta(t)\\) (the noise schedule)</li> <li>This determines \\(\\bar{\\alpha}_t\\) via integration</li> <li>Alternatively, you can choose \\(\\bar{\\alpha}_t\\) directly and derive \\(\\beta(t)\\) from it</li> </ul> <p>Example: For linear schedule \\(\\beta(t) = \\beta_{\\min} + (\\beta_{\\max} - \\beta_{\\min})t\\):</p> \\[ \\bar{\\alpha}_t = \\exp\\left(-\\int_0^t [\\beta_{\\min} + (\\beta_{\\max} - \\beta_{\\min})s]\\,ds\\right) = \\exp\\left(-\\beta_{\\min}t - \\frac{1}{2}(\\beta_{\\max} - \\beta_{\\min})t^2\\right) \\]"},{"location":"diffusion/noise_schedules/#common-noise-schedule-choices","title":"Common Noise Schedule Choices","text":""},{"location":"diffusion/noise_schedules/#1-linear-schedule","title":"1. Linear Schedule","text":"<p>Formula:</p> <p>$$</p> <p>\\beta(t) = \\beta_{\\min} + (\\beta_{\\max} - \\beta_{\\min}) \\cdot t $$</p> <p>Properties:</p> <ul> <li>Simple and interpretable</li> <li>Noise increases linearly from \\(\\beta_{\\min}\\) to \\(\\beta_{\\max}\\)</li> <li>Used in early DDPM papers (Ho et al., 2020)</li> </ul> <p>Typical values: \\(\\beta_{\\min} = 0.0001\\), \\(\\beta_{\\max} = 0.02\\)</p> <p>Cumulative:</p> <p>$$</p> <p>\\bar{\\alpha}t = \\exp\\left(-\\frac{1}{2}(\\beta) t^2)\\right) $$} t + \\frac{1}{2}(\\beta_{\\max} - \\beta_{\\min</p> <p>When to use: Good default for initial experiments and simple datasets.</p>"},{"location":"diffusion/noise_schedules/#2-cosine-schedule","title":"2. Cosine Schedule","text":"<p>Formula:</p> <p>$$</p> <p>\\beta(t) = 1 - \\cos\\left(\\frac{\\pi t}{2}\\right) $$</p> <p>Or in terms of \\(\\bar{\\alpha}_t\\) directly:</p> <p>$$</p> <p>\\bar{\\alpha}_t = \\cos\\left(\\frac{\\pi t}{2}\\right)^2 $$</p> <p>Properties:</p> <ul> <li>Noise increases slowly at first, then accelerates</li> <li>Better preserves signal at early timesteps</li> <li>Often produces higher quality samples</li> <li>Popular in modern diffusion models (Nichol &amp; Dhariwal, 2021)</li> </ul> <p>Intuition: The cosine function starts flat (slow noise addition) and becomes steeper (faster noise addition) as \\(t \\to 1\\).</p> <p>When to use: Preferred for high-quality image generation and when training stability is important.</p>"},{"location":"diffusion/noise_schedules/#3-polynomial-schedule","title":"3. Polynomial Schedule","text":"<p>Formula:</p> <p>$$</p> <p>\\beta(t) = t^n, \\quad n &gt; 0 $$</p> <p>Properties:</p> <ul> <li>\\(n &lt; 1\\): Noise added faster at the beginning</li> <li>\\(n = 1\\): Linear schedule</li> <li>\\(n &gt; 1\\): Noise added faster at the end</li> </ul> <p>Cumulative:</p> <p>$$</p> <p>\\bar{\\alpha}_t = \\exp\\left(-\\frac{t^{n+1}}{2(n+1)}\\right) $$</p> <p>When to use: When you want to experiment with different temporal profiles. Useful for ablation studies.</p>"},{"location":"diffusion/noise_schedules/#4-sigmoid-schedule","title":"4. Sigmoid Schedule","text":"<p>Formula:</p> <p>$$</p> <p>\\beta(t) = \\frac{\\beta_{\\max}}{1 + \\exp(-k(t - t_0))} $$</p> <p>Properties:</p> <ul> <li>S-shaped curve</li> <li>Slow at beginning and end, fast in the middle</li> <li>Less commonly used</li> </ul> <p>Parameters:</p> <ul> <li>\\(k\\): Controls steepness of transition</li> <li>\\(t_0\\): Center point of transition</li> </ul> <p>When to use: When you want a specific transition region where noise is added most rapidly.</p>"},{"location":"diffusion/noise_schedules/#5-learned-schedule","title":"5. Learned Schedule","text":"<p>Some recent work learns \\(\\beta(t)\\) as a neural network parameter, but this is still experimental.</p> <p>Advantages:</p> <ul> <li>Potentially optimal for specific datasets</li> <li>Can adapt to data characteristics</li> </ul> <p>Disadvantages:</p> <ul> <li>Adds complexity to training</li> <li>May overfit</li> <li>Less interpretable</li> </ul>"},{"location":"diffusion/noise_schedules/#comparison-of-common-schedules","title":"Comparison of Common Schedules","text":"Schedule Early Noise Late Noise Quality Complexity Best For Linear Moderate Moderate Good Simple Initial experiments Cosine Slow Fast Better Simple High-quality generation Polynomial Varies Varies Good Moderate Ablation studies Sigmoid Slow Slow Good Moderate Specific use cases"},{"location":"diffusion/noise_schedules/#visual-comparison","title":"Visual Comparison","text":"<p>For \\(t \\in [0, 1]\\):</p>"},{"location":"diffusion/noise_schedules/#betat-profiles","title":"\\(\\beta(t)\\) Profiles","text":"<ul> <li>Linear: Increases steadily from \\(\\beta_{\\min}\\) to \\(\\beta_{\\max}\\)</li> <li>Cosine: Starts near 0, increases slowly, then accelerates</li> <li>Polynomial (\\(n=2\\)): Starts slow, accelerates quadratically</li> </ul>"},{"location":"diffusion/noise_schedules/#baralpha_t-decay","title":"\\(\\bar{\\alpha}_t\\) Decay","text":"<p>All schedules aim to drive \\(\\bar{\\alpha}_t\\) (signal retention) from 1 to near 0:</p> <ul> <li>Linear: Exponential decay with constant rate</li> <li>Cosine: Slower decay initially, faster later</li> <li>Polynomial: Decay rate depends on \\(n\\)</li> </ul>"},{"location":"diffusion/noise_schedules/#signal-to-noise-ratio-over-time","title":"Signal-to-Noise Ratio Over Time","text":"<p>The SNR is \\(\\frac{\\bar{\\alpha}_t}{1-\\bar{\\alpha}_t}\\):</p> <ul> <li>Linear: Decreases exponentially</li> <li>Cosine: Maintains higher SNR longer at the start</li> <li>This affects which timesteps contribute most to training</li> </ul>"},{"location":"diffusion/noise_schedules/#why-cosine-often-works-better","title":"Why Cosine Often Works Better","text":""},{"location":"diffusion/noise_schedules/#1-preserves-signal-early","title":"1. Preserves Signal Early","text":"<p>Problem with linear: Too much noise added early can destroy fine details.</p> <p>Cosine solution: Slow noise addition at \\(t \\approx 0\\) keeps more information, helping the network learn to denoise subtle features.</p>"},{"location":"diffusion/noise_schedules/#2-efficient-corruption","title":"2. Efficient Corruption","text":"<p>Problem: Need to reach pure noise by \\(t = T\\).</p> <p>Cosine solution: Fast noise addition at \\(t \\approx 1\\) quickly reaches \\(\\mathcal{N}(0, I)\\), ensuring the reverse process starts from a well-defined distribution.</p>"},{"location":"diffusion/noise_schedules/#3-better-training-dynamics","title":"3. Better Training Dynamics","text":"<p>Problem with linear: Some timesteps may be over-represented or under-represented in training.</p> <p>Cosine solution: The network sees more diverse noise levels during training because: - More training samples at moderate noise levels - Better gradient signal across all timesteps</p>"},{"location":"diffusion/noise_schedules/#4-empirical-results","title":"4. Empirical Results","text":"<p>Nichol &amp; Dhariwal (2021) showed that cosine schedules improve: - FID scores on ImageNet - Sample quality on various datasets - Training stability</p>"},{"location":"diffusion/noise_schedules/#discrete-time-equivalents","title":"Discrete-Time Equivalents","text":"<p>In discrete-time DDPM, the noise schedule is typically:</p> \\[ \\beta_t = \\text{linear or cosine interpolation between } \\beta_1 \\text{ and } \\beta_T \\] <p>The continuous-time \\(\\beta(t)\\) is the limit as the number of steps \\(T \\to \\infty\\).</p>"},{"location":"diffusion/noise_schedules/#example-ddpm-linear-schedule","title":"Example: DDPM Linear Schedule","text":"<pre><code>beta_min = 0.0001\nbeta_max = 0.02\nnum_steps = 1000\n\n# Linear interpolation\nbeta = np.linspace(beta_min, beta_max, num_steps)\n\n# Compute alpha_bar\nalpha = 1 - beta\nalpha_bar = np.cumprod(alpha)\n</code></pre>"},{"location":"diffusion/noise_schedules/#example-cosine-schedule-nichol-dhariwal","title":"Example: Cosine Schedule (Nichol &amp; Dhariwal)","text":"<pre><code>def cosine_beta_schedule(num_steps, s=0.008):\n    \"\"\"\n    Cosine schedule as proposed in Nichol &amp; Dhariwal (2021).\n    \"\"\"\n    steps = num_steps + 1\n    t = np.linspace(0, num_steps, steps)\n\n    # Alpha bar from cosine\n    alpha_bar = np.cos((t / num_steps + s) / (1 + s) * np.pi / 2) ** 2\n    alpha_bar = alpha_bar / alpha_bar[0]  # Normalize\n\n    # Derive beta from alpha_bar\n    alpha = alpha_bar[1:] / alpha_bar[:-1]\n    beta = 1 - alpha\n\n    # Clip to reasonable range\n    return np.clip(beta, 0, 0.999)\n</code></pre>"},{"location":"diffusion/noise_schedules/#choosing-a-schedule","title":"Choosing a Schedule","text":""},{"location":"diffusion/noise_schedules/#guidelines","title":"Guidelines","text":"<ol> <li>Start simple: Use linear schedule for initial experiments</li> <li>For better quality: Try cosine schedule</li> <li>For specific needs: Adjust based on your data distribution</li> <li>Monitor: Check that \\(\\bar{\\alpha}_T \\approx 0\\) (data becomes pure noise)</li> </ol>"},{"location":"diffusion/noise_schedules/#key-principle","title":"Key Principle","text":"<p>The schedule should ensure that: - Early timesteps: Preserve enough structure for the network to learn meaningful features - Final timestep: Data is corrupted to approximately pure Gaussian noise \\(\\mathcal{N}(0, I)\\) - Middle timesteps: Smooth transition with good gradient signal</p>"},{"location":"diffusion/noise_schedules/#validation","title":"Validation","text":"<p>Plot \\(\\bar{\\alpha}_t\\) for your schedule and check: - Does it start near 1? (\u2713) - Does it end near 0? (\u2713) - Is the transition smooth? (\u2713) - Are there any abrupt changes? (\u2717)</p>"},{"location":"diffusion/noise_schedules/#advanced-topics","title":"Advanced Topics","text":""},{"location":"diffusion/noise_schedules/#adaptive-schedules","title":"Adaptive Schedules","text":"<p>Some recent work adjusts the schedule during training based on: - Current loss values - Dataset statistics - Per-sample difficulty</p>"},{"location":"diffusion/noise_schedules/#schedule-optimization","title":"Schedule Optimization","text":"<p>Treating \\(\\beta(t)\\) as a hyperparameter that can be optimized: - Grid search over schedule parameters - Bayesian optimization - Neural architecture search</p>"},{"location":"diffusion/noise_schedules/#data-dependent-schedules","title":"Data-Dependent Schedules","text":"<p>Adjusting the schedule based on data properties: - Image resolution - Complexity of structure - Presence of fine details</p>"},{"location":"diffusion/noise_schedules/#summary","title":"Summary","text":"Aspect Recommendation Default choice Cosine schedule Simplest Linear schedule Most flexible Polynomial schedule Best quality Cosine schedule (empirically) Experimental Learned schedule <p>Key takeaway: The cosine schedule is preferred for most modern diffusion models due to its better signal preservation early and efficient corruption late, leading to improved sample quality.</p>"},{"location":"diffusion/noise_schedules/#references","title":"References","text":"<ul> <li>Ho et al. (2020): \"Denoising Diffusion Probabilistic Models\" \u2014 Original DDPM with linear schedule</li> <li>Nichol &amp; Dhariwal (2021): \"Improved Denoising Diffusion Probabilistic Models\" \u2014 Introduced cosine schedule</li> <li>Song et al. (2021): \"Score-Based Generative Modeling through SDEs\" \u2014 Continuous-time perspective</li> <li>Karras et al. (2022): \"Elucidating the Design Space of Diffusion-Based Generative Models\" \u2014 Systematic analysis of schedules</li> </ul>"},{"location":"diffusion/01_ddpm/01_ddpm_basics/","title":"Notebook","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport scanpy as sc\nfrom tqdm.auto import tqdm\n\n# Set random seeds\nnp.random.seed(42)\ntorch.manual_seed(42)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from pathlib import Path  import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import Dataset, DataLoader  import scanpy as sc from tqdm.auto import tqdm  # Set random seeds np.random.seed(42) torch.manual_seed(42)  device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\" print(f\"Using device: {device}\") In\u00a0[\u00a0]: Copied! <pre># Load PBMC 3k data\ndata_path = Path(\"../data/pbmc3k_raw.h5ad\")\n\nif data_path.exists():\n    adata = sc.read_h5ad(data_path)\n    print(f\"Loaded data: {adata.shape}\")\nelse:\n    # Download if not available\n    adata = sc.datasets.pbmc3k()\n    print(f\"Downloaded PBMC 3k: {adata.shape}\")\n\n# Basic preprocessing\nsc.pp.filter_cells(adata, min_genes=200)\nsc.pp.filter_genes(adata, min_cells=3)\n\n# Normalize and log-transform for diffusion model\n# Note: For count-based models (like NB VAE), we'd use raw counts\n# For diffusion, we typically work with normalized continuous data\nsc.pp.normalize_total(adata, target_sum=1e4)\nsc.pp.log1p(adata)\n\n# Select highly variable genes for faster training\nsc.pp.highly_variable_genes(adata, n_top_genes=500, flavor='seurat_v3')\nadata = adata[:, adata.var.highly_variable].copy()\n\nprint(f\"After preprocessing: {adata.shape}\")\nprint(f\"Gene expression range: [{adata.X.min():.2f}, {adata.X.max():.2f}]\")\n</pre> # Load PBMC 3k data data_path = Path(\"../data/pbmc3k_raw.h5ad\")  if data_path.exists():     adata = sc.read_h5ad(data_path)     print(f\"Loaded data: {adata.shape}\") else:     # Download if not available     adata = sc.datasets.pbmc3k()     print(f\"Downloaded PBMC 3k: {adata.shape}\")  # Basic preprocessing sc.pp.filter_cells(adata, min_genes=200) sc.pp.filter_genes(adata, min_cells=3)  # Normalize and log-transform for diffusion model # Note: For count-based models (like NB VAE), we'd use raw counts # For diffusion, we typically work with normalized continuous data sc.pp.normalize_total(adata, target_sum=1e4) sc.pp.log1p(adata)  # Select highly variable genes for faster training sc.pp.highly_variable_genes(adata, n_top_genes=500, flavor='seurat_v3') adata = adata[:, adata.var.highly_variable].copy()  print(f\"After preprocessing: {adata.shape}\") print(f\"Gene expression range: [{adata.X.min():.2f}, {adata.X.max():.2f}]\") In\u00a0[\u00a0]: Copied! <pre># Annotate cell types for conditional generation\nsc.pp.neighbors(adata, n_neighbors=10)\nsc.tl.leiden(adata, resolution=0.5)\n\n# Store cell type labels\ncell_types = adata.obs['leiden'].values\nn_cell_types = len(np.unique(cell_types))\n\nprint(f\"Found {n_cell_types} cell type clusters\")\nprint(adata.obs['leiden'].value_counts())\n</pre> # Annotate cell types for conditional generation sc.pp.neighbors(adata, n_neighbors=10) sc.tl.leiden(adata, resolution=0.5)  # Store cell type labels cell_types = adata.obs['leiden'].values n_cell_types = len(np.unique(cell_types))  print(f\"Found {n_cell_types} cell type clusters\") print(adata.obs['leiden'].value_counts()) In\u00a0[\u00a0]: Copied! <pre>class GeneExpressionDataset(Dataset):\n    \"\"\"Dataset for gene expression with optional conditioning.\"\"\"\n    \n    def __init__(self, adata, condition_key=None):\n        \"\"\"\n        Args:\n            adata: AnnData object with preprocessed gene expression\n            condition_key: Key in adata.obs for conditioning (e.g., 'leiden', 'treatment')\n        \"\"\"\n        # Convert to dense array if sparse\n        if hasattr(adata.X, 'toarray'):\n            self.X = adata.X.toarray()\n        else:\n            self.X = adata.X\n        \n        self.X = torch.FloatTensor(self.X)\n        \n        # Extract conditions if provided\n        if condition_key is not None:\n            conditions = adata.obs[condition_key].values\n            # Convert to integer labels\n            unique_conditions = np.unique(conditions)\n            condition_to_idx = {c: i for i, c in enumerate(unique_conditions)}\n            self.conditions = torch.LongTensor([condition_to_idx[c] for c in conditions])\n            self.n_conditions = len(unique_conditions)\n        else:\n            self.conditions = None\n            self.n_conditions = 0\n    \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        if self.conditions is not None:\n            return self.X[idx], self.conditions[idx]\n        return self.X[idx]\n\n# Create dataset\ndataset = GeneExpressionDataset(adata, condition_key='leiden')\ndataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n\nprint(f\"Dataset size: {len(dataset)}\")\nprint(f\"Gene dimension: {dataset.X.shape[1]}\")\nprint(f\"Number of conditions: {dataset.n_conditions}\")\n</pre> class GeneExpressionDataset(Dataset):     \"\"\"Dataset for gene expression with optional conditioning.\"\"\"          def __init__(self, adata, condition_key=None):         \"\"\"         Args:             adata: AnnData object with preprocessed gene expression             condition_key: Key in adata.obs for conditioning (e.g., 'leiden', 'treatment')         \"\"\"         # Convert to dense array if sparse         if hasattr(adata.X, 'toarray'):             self.X = adata.X.toarray()         else:             self.X = adata.X                  self.X = torch.FloatTensor(self.X)                  # Extract conditions if provided         if condition_key is not None:             conditions = adata.obs[condition_key].values             # Convert to integer labels             unique_conditions = np.unique(conditions)             condition_to_idx = {c: i for i, c in enumerate(unique_conditions)}             self.conditions = torch.LongTensor([condition_to_idx[c] for c in conditions])             self.n_conditions = len(unique_conditions)         else:             self.conditions = None             self.n_conditions = 0          def __len__(self):         return len(self.X)          def __getitem__(self, idx):         if self.conditions is not None:             return self.X[idx], self.conditions[idx]         return self.X[idx]  # Create dataset dataset = GeneExpressionDataset(adata, condition_key='leiden') dataloader = DataLoader(dataset, batch_size=128, shuffle=True)  print(f\"Dataset size: {len(dataset)}\") print(f\"Gene dimension: {dataset.X.shape[1]}\") print(f\"Number of conditions: {dataset.n_conditions}\") In\u00a0[\u00a0]: Copied! <pre>class NoiseScheduler:\n    \"\"\"Linear noise schedule for DDPM.\"\"\"\n    \n    def __init__(self, num_timesteps=1000, beta_start=1e-4, beta_end=0.02):\n        \"\"\"\n        Args:\n            num_timesteps: Number of diffusion steps (T)\n            beta_start: Starting noise variance\n            beta_end: Ending noise variance\n        \"\"\"\n        self.num_timesteps = num_timesteps\n        \n        # Linear schedule for beta\n        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n        \n        # Precompute useful quantities\n        self.alphas = 1.0 - self.betas\n        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n        \n        # For sampling\n        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n        \n        # For posterior q(x_{t-1} | x_t, x_0)\n        self.posterior_variance = (\n            self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n        )\n    \n    def add_noise(self, x_0, t, noise=None):\n        \"\"\"\n        Forward diffusion: q(x_t | x_0) = N(x_t; sqrt(alpha_bar_t) * x_0, (1 - alpha_bar_t) * I)\n        \n        Args:\n            x_0: Original data [batch_size, dim]\n            t: Timestep [batch_size]\n            noise: Optional noise to add (for reproducibility)\n        \n        Returns:\n            x_t: Noisy data at timestep t\n            noise: The noise that was added\n        \"\"\"\n        if noise is None:\n            noise = torch.randn_like(x_0)\n        \n        # Get coefficients for this timestep\n        sqrt_alpha_prod = self.sqrt_alphas_cumprod[t].reshape(-1, 1)\n        sqrt_one_minus_alpha_prod = self.sqrt_one_minus_alphas_cumprod[t].reshape(-1, 1)\n        \n        # x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * noise\n        x_t = sqrt_alpha_prod * x_0 + sqrt_one_minus_alpha_prod * noise\n        \n        return x_t, noise\n\n# Test the scheduler\nscheduler = NoiseScheduler(num_timesteps=1000)\n\n# Visualize noise schedule\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\naxes[0].plot(scheduler.betas.numpy())\naxes[0].set_title('Beta Schedule')\naxes[0].set_xlabel('Timestep')\naxes[0].set_ylabel('Beta')\n\naxes[1].plot(scheduler.alphas_cumprod.numpy())\naxes[1].set_title('Cumulative Alpha')\naxes[1].set_xlabel('Timestep')\naxes[1].set_ylabel('Alpha_bar')\n\naxes[2].plot(scheduler.sqrt_one_minus_alphas_cumprod.numpy())\naxes[2].set_title('Noise Coefficient')\naxes[2].set_xlabel('Timestep')\naxes[2].set_ylabel('sqrt(1 - alpha_bar)')\n\nplt.tight_layout()\nplt.show()\n</pre> class NoiseScheduler:     \"\"\"Linear noise schedule for DDPM.\"\"\"          def __init__(self, num_timesteps=1000, beta_start=1e-4, beta_end=0.02):         \"\"\"         Args:             num_timesteps: Number of diffusion steps (T)             beta_start: Starting noise variance             beta_end: Ending noise variance         \"\"\"         self.num_timesteps = num_timesteps                  # Linear schedule for beta         self.betas = torch.linspace(beta_start, beta_end, num_timesteps)                  # Precompute useful quantities         self.alphas = 1.0 - self.betas         self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)         self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)                  # For sampling         self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)         self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)                  # For posterior q(x_{t-1} | x_t, x_0)         self.posterior_variance = (             self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)         )          def add_noise(self, x_0, t, noise=None):         \"\"\"         Forward diffusion: q(x_t | x_0) = N(x_t; sqrt(alpha_bar_t) * x_0, (1 - alpha_bar_t) * I)                  Args:             x_0: Original data [batch_size, dim]             t: Timestep [batch_size]             noise: Optional noise to add (for reproducibility)                  Returns:             x_t: Noisy data at timestep t             noise: The noise that was added         \"\"\"         if noise is None:             noise = torch.randn_like(x_0)                  # Get coefficients for this timestep         sqrt_alpha_prod = self.sqrt_alphas_cumprod[t].reshape(-1, 1)         sqrt_one_minus_alpha_prod = self.sqrt_one_minus_alphas_cumprod[t].reshape(-1, 1)                  # x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * noise         x_t = sqrt_alpha_prod * x_0 + sqrt_one_minus_alpha_prod * noise                  return x_t, noise  # Test the scheduler scheduler = NoiseScheduler(num_timesteps=1000)  # Visualize noise schedule fig, axes = plt.subplots(1, 3, figsize=(15, 4))  axes[0].plot(scheduler.betas.numpy()) axes[0].set_title('Beta Schedule') axes[0].set_xlabel('Timestep') axes[0].set_ylabel('Beta')  axes[1].plot(scheduler.alphas_cumprod.numpy()) axes[1].set_title('Cumulative Alpha') axes[1].set_xlabel('Timestep') axes[1].set_ylabel('Alpha_bar')  axes[2].plot(scheduler.sqrt_one_minus_alphas_cumprod.numpy()) axes[2].set_title('Noise Coefficient') axes[2].set_xlabel('Timestep') axes[2].set_ylabel('sqrt(1 - alpha_bar)')  plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Take a sample gene expression vector\nx_0 = dataset.X[0:1]  # Shape: [1, n_genes]\n\n# Add noise at different timesteps\ntimesteps = [0, 100, 250, 500, 750, 999]\nnoisy_samples = []\n\nfor t in timesteps:\n    t_tensor = torch.tensor([t])\n    x_t, _ = scheduler.add_noise(x_0, t_tensor)\n    noisy_samples.append(x_t[0].numpy())\n\n# Visualize\nfig, axes = plt.subplots(2, 3, figsize=(15, 8))\naxes = axes.flatten()\n\nfor i, (t, x_t) in enumerate(zip(timesteps, noisy_samples)):\n    axes[i].hist(x_t, bins=50, alpha=0.7)\n    axes[i].set_title(f'Timestep t={t}')\n    axes[i].set_xlabel('Expression value')\n    axes[i].set_ylabel('Frequency')\n    axes[i].axvline(x=0, color='r', linestyle='--', alpha=0.5)\n\nplt.suptitle('Forward Diffusion: Gene Expression \u2192 Noise', fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Original data mean: {x_0.mean():.3f}, std: {x_0.std():.3f}\")\nprint(f\"Final noise mean: {noisy_samples[-1].mean():.3f}, std: {noisy_samples[-1].std():.3f}\")\n</pre> # Take a sample gene expression vector x_0 = dataset.X[0:1]  # Shape: [1, n_genes]  # Add noise at different timesteps timesteps = [0, 100, 250, 500, 750, 999] noisy_samples = []  for t in timesteps:     t_tensor = torch.tensor([t])     x_t, _ = scheduler.add_noise(x_0, t_tensor)     noisy_samples.append(x_t[0].numpy())  # Visualize fig, axes = plt.subplots(2, 3, figsize=(15, 8)) axes = axes.flatten()  for i, (t, x_t) in enumerate(zip(timesteps, noisy_samples)):     axes[i].hist(x_t, bins=50, alpha=0.7)     axes[i].set_title(f'Timestep t={t}')     axes[i].set_xlabel('Expression value')     axes[i].set_ylabel('Frequency')     axes[i].axvline(x=0, color='r', linestyle='--', alpha=0.5)  plt.suptitle('Forward Diffusion: Gene Expression \u2192 Noise', fontsize=14, y=1.02) plt.tight_layout() plt.show()  print(f\"Original data mean: {x_0.mean():.3f}, std: {x_0.std():.3f}\") print(f\"Final noise mean: {noisy_samples[-1].mean():.3f}, std: {noisy_samples[-1].std():.3f}\") In\u00a0[\u00a0]: Copied! <pre>class SinusoidalPositionEmbeddings(nn.Module):\n    \"\"\"Sinusoidal embeddings for timesteps (like in Transformers).\"\"\"\n    \n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n    \n    def forward(self, time):\n        device = time.device\n        half_dim = self.dim // 2\n        embeddings = np.log(10000) / (half_dim - 1)\n        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n        embeddings = time[:, None] * embeddings[None, :]\n        embeddings = torch.cat([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)\n        return embeddings\n\n\nclass MLPBlock(nn.Module):\n    \"\"\"MLP block with residual connection.\"\"\"\n    \n    def __init__(self, dim, dropout=0.1):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, dim * 4),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim * 4, dim),\n            nn.Dropout(dropout),\n        )\n        self.norm = nn.LayerNorm(dim)\n    \n    def forward(self, x):\n        return self.norm(x + self.net(x))\n\n\nclass ConditionalScoreNetwork(nn.Module):\n    \"\"\"Time and condition-conditional score network for gene expression.\"\"\"\n    \n    def __init__(\n        self,\n        input_dim,\n        hidden_dim=256,\n        time_dim=64,\n        n_conditions=0,\n        condition_dim=32,\n        n_layers=4,\n        dropout=0.1,\n    ):\n        \"\"\"\n        Args:\n            input_dim: Gene expression dimension\n            hidden_dim: Hidden layer dimension\n            time_dim: Time embedding dimension\n            n_conditions: Number of condition classes (0 for unconditional)\n            condition_dim: Condition embedding dimension\n            n_layers: Number of MLP blocks\n            dropout: Dropout rate\n        \"\"\"\n        super().__init__()\n        \n        self.input_dim = input_dim\n        self.n_conditions = n_conditions\n        \n        # Time embedding\n        self.time_mlp = nn.Sequential(\n            SinusoidalPositionEmbeddings(time_dim),\n            nn.Linear(time_dim, time_dim * 2),\n            nn.GELU(),\n            nn.Linear(time_dim * 2, time_dim),\n        )\n        \n        # Condition embedding (if conditional)\n        if n_conditions &gt; 0:\n            self.condition_embed = nn.Embedding(n_conditions, condition_dim)\n            total_input_dim = input_dim + time_dim + condition_dim\n        else:\n            self.condition_embed = None\n            total_input_dim = input_dim + time_dim\n        \n        # Input projection\n        self.input_proj = nn.Linear(total_input_dim, hidden_dim)\n        \n        # MLP blocks\n        self.blocks = nn.ModuleList([\n            MLPBlock(hidden_dim, dropout) for _ in range(n_layers)\n        ])\n        \n        # Output projection (predict noise)\n        self.output_proj = nn.Linear(hidden_dim, input_dim)\n    \n    def forward(self, x, t, condition=None):\n        \"\"\"\n        Args:\n            x: Noisy gene expression [batch_size, input_dim]\n            t: Timestep [batch_size]\n            condition: Condition labels [batch_size] (optional)\n        \n        Returns:\n            Predicted noise [batch_size, input_dim]\n        \"\"\"\n        # Time embedding\n        t_emb = self.time_mlp(t)\n        \n        # Concatenate inputs\n        if self.condition_embed is not None and condition is not None:\n            c_emb = self.condition_embed(condition)\n            h = torch.cat([x, t_emb, c_emb], dim=-1)\n        else:\n            h = torch.cat([x, t_emb], dim=-1)\n        \n        # Project to hidden dimension\n        h = self.input_proj(h)\n        \n        # Apply MLP blocks\n        for block in self.blocks:\n            h = block(h)\n        \n        # Predict noise\n        noise_pred = self.output_proj(h)\n        \n        return noise_pred\n\n# Test the network\nmodel = ConditionalScoreNetwork(\n    input_dim=dataset.X.shape[1],\n    hidden_dim=256,\n    n_conditions=dataset.n_conditions,\n    n_layers=4,\n).to(device)\n\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# Test forward pass\nx_test, c_test = next(iter(dataloader))\nx_test, c_test = x_test.to(device), c_test.to(device)\nt_test = torch.randint(0, 1000, (x_test.shape[0],), device=device)\n\nnoise_pred = model(x_test, t_test, c_test)\nprint(f\"Input shape: {x_test.shape}\")\nprint(f\"Output shape: {noise_pred.shape}\")\n</pre> class SinusoidalPositionEmbeddings(nn.Module):     \"\"\"Sinusoidal embeddings for timesteps (like in Transformers).\"\"\"          def __init__(self, dim):         super().__init__()         self.dim = dim          def forward(self, time):         device = time.device         half_dim = self.dim // 2         embeddings = np.log(10000) / (half_dim - 1)         embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)         embeddings = time[:, None] * embeddings[None, :]         embeddings = torch.cat([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)         return embeddings   class MLPBlock(nn.Module):     \"\"\"MLP block with residual connection.\"\"\"          def __init__(self, dim, dropout=0.1):         super().__init__()         self.net = nn.Sequential(             nn.Linear(dim, dim * 4),             nn.GELU(),             nn.Dropout(dropout),             nn.Linear(dim * 4, dim),             nn.Dropout(dropout),         )         self.norm = nn.LayerNorm(dim)          def forward(self, x):         return self.norm(x + self.net(x))   class ConditionalScoreNetwork(nn.Module):     \"\"\"Time and condition-conditional score network for gene expression.\"\"\"          def __init__(         self,         input_dim,         hidden_dim=256,         time_dim=64,         n_conditions=0,         condition_dim=32,         n_layers=4,         dropout=0.1,     ):         \"\"\"         Args:             input_dim: Gene expression dimension             hidden_dim: Hidden layer dimension             time_dim: Time embedding dimension             n_conditions: Number of condition classes (0 for unconditional)             condition_dim: Condition embedding dimension             n_layers: Number of MLP blocks             dropout: Dropout rate         \"\"\"         super().__init__()                  self.input_dim = input_dim         self.n_conditions = n_conditions                  # Time embedding         self.time_mlp = nn.Sequential(             SinusoidalPositionEmbeddings(time_dim),             nn.Linear(time_dim, time_dim * 2),             nn.GELU(),             nn.Linear(time_dim * 2, time_dim),         )                  # Condition embedding (if conditional)         if n_conditions &gt; 0:             self.condition_embed = nn.Embedding(n_conditions, condition_dim)             total_input_dim = input_dim + time_dim + condition_dim         else:             self.condition_embed = None             total_input_dim = input_dim + time_dim                  # Input projection         self.input_proj = nn.Linear(total_input_dim, hidden_dim)                  # MLP blocks         self.blocks = nn.ModuleList([             MLPBlock(hidden_dim, dropout) for _ in range(n_layers)         ])                  # Output projection (predict noise)         self.output_proj = nn.Linear(hidden_dim, input_dim)          def forward(self, x, t, condition=None):         \"\"\"         Args:             x: Noisy gene expression [batch_size, input_dim]             t: Timestep [batch_size]             condition: Condition labels [batch_size] (optional)                  Returns:             Predicted noise [batch_size, input_dim]         \"\"\"         # Time embedding         t_emb = self.time_mlp(t)                  # Concatenate inputs         if self.condition_embed is not None and condition is not None:             c_emb = self.condition_embed(condition)             h = torch.cat([x, t_emb, c_emb], dim=-1)         else:             h = torch.cat([x, t_emb], dim=-1)                  # Project to hidden dimension         h = self.input_proj(h)                  # Apply MLP blocks         for block in self.blocks:             h = block(h)                  # Predict noise         noise_pred = self.output_proj(h)                  return noise_pred  # Test the network model = ConditionalScoreNetwork(     input_dim=dataset.X.shape[1],     hidden_dim=256,     n_conditions=dataset.n_conditions,     n_layers=4, ).to(device)  print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")  # Test forward pass x_test, c_test = next(iter(dataloader)) x_test, c_test = x_test.to(device), c_test.to(device) t_test = torch.randint(0, 1000, (x_test.shape[0],), device=device)  noise_pred = model(x_test, t_test, c_test) print(f\"Input shape: {x_test.shape}\") print(f\"Output shape: {noise_pred.shape}\") In\u00a0[\u00a0]: Copied! <pre>def train_ddpm(model, dataloader, scheduler, num_epochs=100, lr=1e-4):\n    \"\"\"Train DDPM model.\"\"\"\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    model.train()\n    \n    losses = []\n    \n    for epoch in range(num_epochs):\n        epoch_loss = 0.0\n        \n        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False):\n            if len(batch) == 2:\n                x_0, condition = batch\n                x_0 = x_0.to(device)\n                condition = condition.to(device)\n            else:\n                x_0 = batch.to(device)\n                condition = None\n            \n            batch_size = x_0.shape[0]\n            \n            # Sample random timesteps\n            t = torch.randint(0, scheduler.num_timesteps, (batch_size,), device=device)\n            \n            # Add noise\n            noise = torch.randn_like(x_0)\n            x_t, _ = scheduler.add_noise(x_0, t, noise)\n            \n            # Predict noise\n            noise_pred = model(x_t, t, condition)\n            \n            # Compute loss\n            loss = F.mse_loss(noise_pred, noise)\n            \n            # Backprop\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            \n            epoch_loss += loss.item()\n        \n        avg_loss = epoch_loss / len(dataloader)\n        losses.append(avg_loss)\n        \n        if (epoch + 1) % 10 == 0:\n            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n    \n    return losses\n</pre> def train_ddpm(model, dataloader, scheduler, num_epochs=100, lr=1e-4):     \"\"\"Train DDPM model.\"\"\"     optimizer = torch.optim.AdamW(model.parameters(), lr=lr)     model.train()          losses = []          for epoch in range(num_epochs):         epoch_loss = 0.0                  for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False):             if len(batch) == 2:                 x_0, condition = batch                 x_0 = x_0.to(device)                 condition = condition.to(device)             else:                 x_0 = batch.to(device)                 condition = None                          batch_size = x_0.shape[0]                          # Sample random timesteps             t = torch.randint(0, scheduler.num_timesteps, (batch_size,), device=device)                          # Add noise             noise = torch.randn_like(x_0)             x_t, _ = scheduler.add_noise(x_0, t, noise)                          # Predict noise             noise_pred = model(x_t, t, condition)                          # Compute loss             loss = F.mse_loss(noise_pred, noise)                          # Backprop             optimizer.zero_grad()             loss.backward()             torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)             optimizer.step()                          epoch_loss += loss.item()                  avg_loss = epoch_loss / len(dataloader)         losses.append(avg_loss)                  if (epoch + 1) % 10 == 0:             print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")          return losses In\u00a0[\u00a0]: Copied! <pre># Train the model (start with fewer epochs for testing)\nlosses = train_ddpm(\n    model=model,\n    dataloader=dataloader,\n    scheduler=scheduler,\n    num_epochs=50,  # Increase to 200-500 for better results\n    lr=1e-4,\n)\n\n# Plot training curve\nplt.figure(figsize=(10, 5))\nplt.plot(losses)\nplt.xlabel('Epoch')\nplt.ylabel('MSE Loss')\nplt.title('DDPM Training Loss')\nplt.grid(True, alpha=0.3)\nplt.show()\n</pre> # Train the model (start with fewer epochs for testing) losses = train_ddpm(     model=model,     dataloader=dataloader,     scheduler=scheduler,     num_epochs=50,  # Increase to 200-500 for better results     lr=1e-4, )  # Plot training curve plt.figure(figsize=(10, 5)) plt.plot(losses) plt.xlabel('Epoch') plt.ylabel('MSE Loss') plt.title('DDPM Training Loss') plt.grid(True, alpha=0.3) plt.show() In\u00a0[\u00a0]: Copied! <pre>@torch.no_grad()\ndef sample_ddpm(model, scheduler, n_samples, condition=None, device='cpu'):\n    \"\"\"Sample from DDPM model.\n    \n    Args:\n        model: Trained score network\n        scheduler: Noise scheduler\n        n_samples: Number of samples to generate\n        condition: Condition labels [n_samples] (optional)\n        device: Device to run on\n    \n    Returns:\n        Generated samples [n_samples, input_dim]\n    \"\"\"\n    model.eval()\n    \n    # Start from pure noise\n    x = torch.randn(n_samples, model.input_dim, device=device)\n    \n    if condition is not None:\n        condition = condition.to(device)\n    \n    # Reverse diffusion\n    for t in tqdm(reversed(range(scheduler.num_timesteps)), desc=\"Sampling\", total=scheduler.num_timesteps):\n        t_batch = torch.full((n_samples,), t, device=device, dtype=torch.long)\n        \n        # Predict noise\n        noise_pred = model(x, t_batch, condition)\n        \n        # Get scheduler coefficients\n        alpha_t = scheduler.alphas[t]\n        alpha_bar_t = scheduler.alphas_cumprod[t]\n        beta_t = scheduler.betas[t]\n        \n        # Compute mean\n        mean = (1 / torch.sqrt(alpha_t)) * (\n            x - (beta_t / torch.sqrt(1 - alpha_bar_t)) * noise_pred\n        )\n        \n        # Add noise (except at t=0)\n        if t &gt; 0:\n            noise = torch.randn_like(x)\n            sigma_t = torch.sqrt(scheduler.posterior_variance[t])\n            x = mean + sigma_t * noise\n        else:\n            x = mean\n    \n    return x\n</pre> @torch.no_grad() def sample_ddpm(model, scheduler, n_samples, condition=None, device='cpu'):     \"\"\"Sample from DDPM model.          Args:         model: Trained score network         scheduler: Noise scheduler         n_samples: Number of samples to generate         condition: Condition labels [n_samples] (optional)         device: Device to run on          Returns:         Generated samples [n_samples, input_dim]     \"\"\"     model.eval()          # Start from pure noise     x = torch.randn(n_samples, model.input_dim, device=device)          if condition is not None:         condition = condition.to(device)          # Reverse diffusion     for t in tqdm(reversed(range(scheduler.num_timesteps)), desc=\"Sampling\", total=scheduler.num_timesteps):         t_batch = torch.full((n_samples,), t, device=device, dtype=torch.long)                  # Predict noise         noise_pred = model(x, t_batch, condition)                  # Get scheduler coefficients         alpha_t = scheduler.alphas[t]         alpha_bar_t = scheduler.alphas_cumprod[t]         beta_t = scheduler.betas[t]                  # Compute mean         mean = (1 / torch.sqrt(alpha_t)) * (             x - (beta_t / torch.sqrt(1 - alpha_bar_t)) * noise_pred         )                  # Add noise (except at t=0)         if t &gt; 0:             noise = torch.randn_like(x)             sigma_t = torch.sqrt(scheduler.posterior_variance[t])             x = mean + sigma_t * noise         else:             x = mean          return x In\u00a0[\u00a0]: Copied! <pre># Generate samples for each cell type\nn_samples_per_type = 50\ngenerated_samples = []\ngenerated_labels = []\n\nfor cell_type_idx in range(dataset.n_conditions):\n    condition = torch.full((n_samples_per_type,), cell_type_idx, dtype=torch.long)\n    samples = sample_ddpm(model, scheduler, n_samples_per_type, condition, device=device)\n    generated_samples.append(samples.cpu())\n    generated_labels.extend([cell_type_idx] * n_samples_per_type)\n\ngenerated_samples = torch.cat(generated_samples, dim=0).numpy()\ngenerated_labels = np.array(generated_labels)\n\nprint(f\"Generated {generated_samples.shape[0]} samples\")\nprint(f\"Sample shape: {generated_samples.shape}\")\n</pre> # Generate samples for each cell type n_samples_per_type = 50 generated_samples = [] generated_labels = []  for cell_type_idx in range(dataset.n_conditions):     condition = torch.full((n_samples_per_type,), cell_type_idx, dtype=torch.long)     samples = sample_ddpm(model, scheduler, n_samples_per_type, condition, device=device)     generated_samples.append(samples.cpu())     generated_labels.extend([cell_type_idx] * n_samples_per_type)  generated_samples = torch.cat(generated_samples, dim=0).numpy() generated_labels = np.array(generated_labels)  print(f\"Generated {generated_samples.shape[0]} samples\") print(f\"Sample shape: {generated_samples.shape}\") In\u00a0[\u00a0]: Copied! <pre># Compare distributions\nreal_data = dataset.X.numpy()\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 8))\naxes = axes.flatten()\n\n# Overall distribution\naxes[0].hist(real_data.flatten(), bins=50, alpha=0.5, label='Real', density=True)\naxes[0].hist(generated_samples.flatten(), bins=50, alpha=0.5, label='Generated', density=True)\naxes[0].set_title('Overall Distribution')\naxes[0].legend()\n\n# Mean expression per gene\naxes[1].scatter(real_data.mean(axis=0), generated_samples.mean(axis=0), alpha=0.3)\naxes[1].plot([real_data.mean(axis=0).min(), real_data.mean(axis=0).max()],\n             [real_data.mean(axis=0).min(), real_data.mean(axis=0).max()],\n             'r--', alpha=0.5)\naxes[1].set_xlabel('Real mean expression')\naxes[1].set_ylabel('Generated mean expression')\naxes[1].set_title('Mean Expression per Gene')\n\n# Std expression per gene\naxes[2].scatter(real_data.std(axis=0), generated_samples.std(axis=0), alpha=0.3)\naxes[2].plot([real_data.std(axis=0).min(), real_data.std(axis=0).max()],\n             [real_data.std(axis=0).min(), real_data.std(axis=0).max()],\n             'r--', alpha=0.5)\naxes[2].set_xlabel('Real std expression')\naxes[2].set_ylabel('Generated std expression')\naxes[2].set_title('Std Expression per Gene')\n\n# Sample a few genes and compare distributions\nfor i, gene_idx in enumerate([0, 10, 50]):\n    axes[3 + i].hist(real_data[:, gene_idx], bins=30, alpha=0.5, label='Real', density=True)\n    axes[3 + i].hist(generated_samples[:, gene_idx], bins=30, alpha=0.5, label='Generated', density=True)\n    axes[3 + i].set_title(f'Gene {gene_idx}')\n    axes[3 + i].legend()\n\nplt.tight_layout()\nplt.show()\n</pre> # Compare distributions real_data = dataset.X.numpy()  fig, axes = plt.subplots(2, 3, figsize=(15, 8)) axes = axes.flatten()  # Overall distribution axes[0].hist(real_data.flatten(), bins=50, alpha=0.5, label='Real', density=True) axes[0].hist(generated_samples.flatten(), bins=50, alpha=0.5, label='Generated', density=True) axes[0].set_title('Overall Distribution') axes[0].legend()  # Mean expression per gene axes[1].scatter(real_data.mean(axis=0), generated_samples.mean(axis=0), alpha=0.3) axes[1].plot([real_data.mean(axis=0).min(), real_data.mean(axis=0).max()],              [real_data.mean(axis=0).min(), real_data.mean(axis=0).max()],              'r--', alpha=0.5) axes[1].set_xlabel('Real mean expression') axes[1].set_ylabel('Generated mean expression') axes[1].set_title('Mean Expression per Gene')  # Std expression per gene axes[2].scatter(real_data.std(axis=0), generated_samples.std(axis=0), alpha=0.3) axes[2].plot([real_data.std(axis=0).min(), real_data.std(axis=0).max()],              [real_data.std(axis=0).min(), real_data.std(axis=0).max()],              'r--', alpha=0.5) axes[2].set_xlabel('Real std expression') axes[2].set_ylabel('Generated std expression') axes[2].set_title('Std Expression per Gene')  # Sample a few genes and compare distributions for i, gene_idx in enumerate([0, 10, 50]):     axes[3 + i].hist(real_data[:, gene_idx], bins=30, alpha=0.5, label='Real', density=True)     axes[3 + i].hist(generated_samples[:, gene_idx], bins=30, alpha=0.5, label='Generated', density=True)     axes[3 + i].set_title(f'Gene {gene_idx}')     axes[3 + i].legend()  plt.tight_layout() plt.show()"},{"location":"diffusion/01_ddpm/01_ddpm_basics/#diffusion-models-for-gene-expression-ddpm-tutorial","title":"Diffusion Models for Gene Expression: DDPM Tutorial\u00b6","text":"<p>Goal: Implement a denoising diffusion probabilistic model (DDPM) for generating gene expression profiles.</p> <p>This notebook demonstrates:</p> <ol> <li>Core DDPM mechanics (forward/reverse diffusion)</li> <li>Training a time-conditional score network on gene expression data</li> <li>Conditional generation (cell type \u2192 gene expression)</li> <li>Foundation for drug-response prediction (scPPDM approach)</li> </ol> <p>Dataset: PBMC 3k (small subset for fast iteration)</p> <p>Next steps: Extend to perturbation response (baseline + drug \u2192 perturbed expression)</p>"},{"location":"diffusion/01_ddpm/01_ddpm_basics/#prerequisites","title":"Prerequisites\u00b6","text":"<p>Environment: Make sure you're in the <code>genailab</code> conda environment:</p> <pre>mamba activate genailab\n</pre> <p>Required packages: torch, scanpy, numpy, matplotlib, tqdm</p>"},{"location":"diffusion/01_ddpm/01_ddpm_basics/#setup","title":"Setup\u00b6","text":""},{"location":"diffusion/01_ddpm/01_ddpm_basics/#1-load-and-prepare-gene-expression-data","title":"1. Load and Prepare Gene Expression Data\u00b6","text":"<p>We'll use a small subset of PBMC 3k for fast iteration. For production, you'd use the full dataset.</p>"},{"location":"diffusion/01_ddpm/01_ddpm_basics/#2-create-pytorch-dataset","title":"2. Create PyTorch Dataset\u00b6","text":"<p>We'll create a dataset that returns:</p> <ul> <li>Gene expression vector (x)</li> <li>Cell type label (condition)</li> </ul>"},{"location":"diffusion/01_ddpm/01_ddpm_basics/#3-implement-ddpm-components","title":"3. Implement DDPM Components\u00b6","text":""},{"location":"diffusion/01_ddpm/01_ddpm_basics/#31-noise-scheduler","title":"3.1 Noise Scheduler\u00b6","text":"<p>The noise scheduler defines how we add noise in the forward process:</p> <ul> <li>$\\beta_t$: variance schedule (how much noise to add at each step)</li> <li>$\\alpha_t = 1 - \\beta_t$</li> <li>$\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i$ (cumulative product)</li> </ul>"},{"location":"diffusion/01_ddpm/01_ddpm_basics/#32-visualize-forward-diffusion-process","title":"3.2 Visualize Forward Diffusion Process\u00b6","text":"<p>Let's see how a gene expression vector gets progressively noisier.</p>"},{"location":"diffusion/01_ddpm/01_ddpm_basics/#33-time-conditional-score-network","title":"3.3 Time-Conditional Score Network\u00b6","text":"<p>The core of DDPM: a neural network that predicts the noise $\\epsilon_\\theta(x_t, t, c)$ given:</p> <ul> <li>Noisy data $x_t$</li> <li>Timestep $t$</li> <li>Condition $c$ (e.g., cell type, drug)</li> </ul> <p>For gene expression (tabular data), we use an MLP with:</p> <ul> <li>Sinusoidal time embeddings</li> <li>Conditional embeddings</li> <li>Residual connections</li> </ul>"},{"location":"diffusion/01_ddpm/01_ddpm_basics/#4-training-loop","title":"4. Training Loop\u00b6","text":"<p>DDPM training is simple:</p> <ol> <li>Sample a batch of data $x_0$</li> <li>Sample random timesteps $t$</li> <li>Add noise: $x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon$</li> <li>Predict noise: $\\epsilon_\\theta(x_t, t, c)$</li> <li>Compute MSE loss: $\\|\\epsilon - \\epsilon_\\theta(x_t, t, c)\\|^2$</li> </ol>"},{"location":"diffusion/01_ddpm/01_ddpm_basics/#5-sampling-reverse-diffusion","title":"5. Sampling (Reverse Diffusion)\u00b6","text":"<p>Generate new gene expression profiles by:</p> <ol> <li>Start with pure noise $x_T \\sim \\mathcal{N}(0, I)$</li> <li>Iteratively denoise: $x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t, c) \\right) + \\sigma_t z$</li> </ol>"},{"location":"diffusion/01_ddpm/01_ddpm_basics/#6-evaluation","title":"6. Evaluation\u00b6","text":"<p>Compare generated vs real gene expression distributions.</p>"},{"location":"diffusion/01_ddpm/01_ddpm_basics/#7-next-steps-extending-to-drug-response-prediction","title":"7. Next Steps: Extending to Drug-Response Prediction\u00b6","text":"<p>To implement the scPPDM approach for perturbation response:</p>"},{"location":"diffusion/01_ddpm/01_ddpm_basics/#architecture-changes","title":"Architecture Changes:\u00b6","text":"<ol> <li>Input: Concatenate baseline expression + drug embedding</li> <li>Output: Predict perturbed expression (not noise)</li> <li>Conditioning: Drug type + dose</li> </ol>"},{"location":"diffusion/01_ddpm/01_ddpm_basics/#data-requirements","title":"Data Requirements:\u00b6","text":"<ul> <li>Paired samples: (baseline, drug, dose, perturbed_expression)</li> <li>Examples: Sci-Plex, LINCS L1000, Replogle et al. Perturb-seq</li> </ul>"},{"location":"diffusion/01_ddpm/01_ddpm_basics/#modified-forward-process","title":"Modified Forward Process:\u00b6","text":"<pre># Instead of: x_t = sqrt(alpha_bar) * x_0 + sqrt(1 - alpha_bar) * noise\n# Use: x_t = sqrt(alpha_bar) * x_perturbed + sqrt(1 - alpha_bar) * noise\n# Condition on: [x_baseline, drug_embedding, dose]\n</pre>"},{"location":"diffusion/01_ddpm/01_ddpm_basics/#training","title":"Training:\u00b6","text":"<pre># Predict perturbed expression from baseline + drug\ndef forward(x_baseline, drug, dose, t):\n    # Encode drug\n    drug_emb = drug_encoder(drug, dose)\n    \n    # Concatenate baseline + drug info\n    condition = torch.cat([x_baseline, drug_emb], dim=-1)\n    \n    # Predict noise for perturbed expression\n    noise_pred = score_network(x_t, t, condition)\n    \n    return noise_pred\n</pre>"},{"location":"diffusion/01_ddpm/01_ddpm_basics/#sampling","title":"Sampling:\u00b6","text":"<pre># Generate counterfactual response\nx_perturbed = sample_ddpm(\n    model,\n    scheduler,\n    condition={'baseline': x_baseline, 'drug': drug_id, 'dose': dose_value}\n)\n</pre>"},{"location":"diffusion/01_ddpm/01_ddpm_basics/#summary","title":"Summary\u00b6","text":"<p>We've implemented:</p> <ol> <li>\u2705 Noise scheduler (linear beta schedule)</li> <li>\u2705 Forward diffusion (adding noise)</li> <li>\u2705 Time-conditional score network (MLP for tabular data)</li> <li>\u2705 Training loop (simple MSE loss)</li> <li>\u2705 Sampling (reverse diffusion)</li> <li>\u2705 Conditional generation (cell type \u2192 expression)</li> </ol> <p>Next notebook: Implement full scPPDM for drug-response prediction with perturbation datasets.</p>"},{"location":"diffusion/02_sde_formulation/","title":"SDE Formulation for Diffusion Models","text":"<p>A comprehensive tutorial on understanding diffusion models through the lens of Stochastic Differential Equations (SDEs).</p>"},{"location":"diffusion/02_sde_formulation/#overview","title":"Overview","text":"<p>This directory contains a complete learning path for understanding the SDE formulation of diffusion models, from basic concepts to advanced theory. The materials progress from interactive code tutorials to theoretical deep-dives.</p> <p>Why SDEs? The SDE perspective unifies discrete-time DDPM, score-based models, and DDIM into a single continuous-time framework. It provides: - Mathematical clarity: Clean separation between design choices and learning - Flexibility: Easy to design custom diffusion processes - Generality: Discrete DDPM is a special case - Interpretability: Clear connection to probability theory and stochastic processes</p>"},{"location":"diffusion/02_sde_formulation/#learning-path","title":"Learning Path","text":""},{"location":"diffusion/02_sde_formulation/#1-start-here-core-materials","title":"1. Start Here: Core Materials","text":"<p>Read/work through these in order:</p>"},{"location":"diffusion/02_sde_formulation/#sde_formulationmd-comprehensive-theory","title":"sde_formulation.md \u2014 Comprehensive Theory","text":"<ul> <li>What is an SDE? (ODEs vs SDEs)</li> <li>Understanding each symbol: \\(x(t)\\), \\(f(x,t)\\), \\(g(t)\\), \\(w(t)\\)</li> <li>What is chosen vs what is learned</li> <li>Training workflow step-by-step</li> <li>Sampling workflow</li> <li>Connection to DDPM</li> <li>Concrete example: VP-SDE</li> </ul> <p>Start here if you want a complete theoretical foundation.</p>"},{"location":"diffusion/02_sde_formulation/#02_sde_formulationipynb-interactive-tutorial","title":"02_sde_formulation.ipynb \u2014 Interactive Tutorial","text":"<ul> <li>Visualize Brownian motion</li> <li>Simulate forward SDEs (data \u2192 noise)</li> <li>Implement score matching training</li> <li>Sample from reverse SDEs (noise \u2192 data)</li> <li>Compare VP-SDE and probability flow ODE</li> </ul> <p>Start here if you prefer learning by coding and visualization.</p>"},{"location":"diffusion/02_sde_formulation/#2-common-questions","title":"2. Common Questions","text":""},{"location":"diffusion/02_sde_formulation/#sde_qamd-faq-and-conceptual-clarifications","title":"sde_QA.md \u2014 FAQ and Conceptual Clarifications","text":"<p>Addresses frequently asked questions: - How is an SDE system solved? - What models are learned in the SDE formulation? - What do we use the learned score for? - Is Brownian motion the only way to model randomness? - Why don't diffusion models use jump processes, stochastic volatility, etc.?</p> <p>Read this after going through the core materials to solidify understanding.</p>"},{"location":"diffusion/02_sde_formulation/#3-deep-dives-supplementary-materials","title":"3. Deep Dives: Supplementary Materials","text":"<p>These documents provide focused deep-dives into specific topics. Read them in order for systematic understanding, or jump to specific topics as needed.</p>"},{"location":"diffusion/02_sde_formulation/#01-forward-sde-design-choices-new","title":"01. Forward SDE Design Choices \u2b50 NEW","text":"<p>Topic: Understanding \\(f(x,t)\\) and \\(g(t)\\) \u2014 what they are and how to choose them</p> <p>Key insights:</p> <ul> <li>Core principle: \\(f(x,t)\\) and \\(g(t)\\) are design choices, not learned</li> <li>Three standard SDEs: VP-SDE, VE-SDE, sub-VP-SDE</li> <li>Why these specific functions? (mathematical tractability, variance behavior)</li> <li>Design considerations: closed-form marginals, SNR decay, connection to DDPM/NCSN</li> <li>Practical recommendations for choosing your forward SDE</li> </ul> <p>When to read: Start here \u2014 This is foundational for understanding training and sampling</p>"},{"location":"diffusion/02_sde_formulation/#02-brownian-motion-dimensionality","title":"02. Brownian Motion Dimensionality","text":"<p>Topic: Why \\(w(t)\\) and \\(x(t)\\) have the same dimension</p> <p>Key insights:</p> <ul> <li>Brownian motion is \\(d\\)-dimensional, not scalar</li> <li>Each pixel/feature has its own independent Brownian path</li> <li>Noise term \\(g(t)dw(t)\\) must match \\(x(t)\\) dimensionality</li> </ul> <p>When to read: After understanding basic SDE notation (clarifies a common confusion)</p>"},{"location":"diffusion/02_sde_formulation/#03-equivalent-parameterizations","title":"03. Equivalent Parameterizations","text":"<p>Topic: Score vs noise vs clean data prediction</p> <p>Key insights:</p> <ul> <li>Three ways to parameterize the neural network output</li> <li>Mathematical equivalence: \\(s_\\theta \\leftrightarrow \\varepsilon_\\theta \\leftrightarrow \\hat{x}_0\\)</li> <li>Conversion formulas between parameterizations</li> <li>Why DDPM predicts noise but score-based models predict score</li> </ul> <p>When to read: When understanding what the neural network learns</p>"},{"location":"diffusion/02_sde_formulation/#04-training-loss-and-denoising","title":"04. Training Loss and Denoising","text":"<p>Topic: Why predicting score = predicting noise = learning to denoise</p> <p>Key insights:</p> <ul> <li>Derivation of the score matching loss</li> <li>Why \\(-\\varepsilon/\\sigma_t\\) is the target score</li> <li>Connection between denoising and score estimation</li> <li>How \\(g(t)\\) from the forward SDE appears in the loss</li> <li>Intuition: score points toward cleaner versions of data</li> </ul> <p>When to read: When understanding the training objective (requires supplement 01)</p>"},{"location":"diffusion/02_sde_formulation/#05-reverse-sde-and-probability-flow-ode","title":"05. Reverse SDE and Probability Flow ODE","text":"<p>Topic: Sampling mechanics and stochastic vs deterministic generation</p> <p>Key insights:</p> <ul> <li>Term-by-term interpretation of reverse SDE</li> <li>How \\(f(x,t)\\) and \\(g(t)\\) from forward SDE appear in reverse SDE</li> <li>Why the score term reverses diffusion</li> <li>Probability flow ODE: deterministic alternative</li> <li>Trade-offs: SDE (diverse) vs ODE (fast, deterministic)</li> <li>Connection to DDIM</li> </ul> <p>When to read: When understanding sampling/generation (requires supplement 01)</p>"},{"location":"diffusion/02_sde_formulation/#06-fokker-planck-equation-and-effective-drift","title":"06. Fokker-Planck Equation and Effective Drift","text":"<p>Topic: Advanced theory connecting SDEs to PDEs</p> <p>Key insights:</p> <ul> <li>Fokker-Planck equation: from particle trajectories to probability density evolution</li> <li>Why probability flow ODE has the same marginals as the SDE</li> <li>Effective drift interpretation</li> <li>Connection to transport theory</li> </ul> <p>When to read: For advanced theoretical understanding (optional for practitioners)</p>"},{"location":"diffusion/02_sde_formulation/#quick-reference","title":"Quick Reference","text":""},{"location":"diffusion/02_sde_formulation/#file-organization","title":"File Organization","text":"<pre><code>02_sde_formulation/\n\u251c\u2500\u2500 README.md                          # This file (index)\n\u251c\u2500\u2500 sde_formulation.md                 # Core theory document\n\u251c\u2500\u2500 sde_QA.md                          # Common questions\n\u251c\u2500\u2500 02_sde_formulation.ipynb          # Interactive code tutorial\n\u2502\n\u2514\u2500\u2500 supplements/                       # Deep-dive documents\n    \u251c\u2500\u2500 01_forward_sde_design_choices.md\n    \u251c\u2500\u2500 02_brownian_motion_dimensionality.md\n    \u251c\u2500\u2500 03_equivalent_parameterizations.md\n    \u251c\u2500\u2500 04_training_loss_and_denoising.md\n    \u251c\u2500\u2500 05_reverse_sde_and_probability_flow_ode.md\n    \u251c\u2500\u2500 06_fokker_planck_and_effective_drift.md\n    \u251c\u2500\u2500 07_fokker_planck_equation.md            \u2b50 NEW\n    \u2514\u2500\u2500 08_dimensional_analysis.md              \u2b50 NEW\n</code></pre>"},{"location":"diffusion/02_sde_formulation/#suggested-reading-orders","title":"Suggested Reading Orders","text":""},{"location":"diffusion/02_sde_formulation/#for-practitioners-focus-on-implementation","title":"For Practitioners (focus on implementation):","text":"<ol> <li><code>02_sde_formulation.ipynb</code> (code first)</li> <li><code>sde_formulation.md</code> (theory)</li> <li><code>supplements/01_forward_sde_design_choices.md</code> (understand \\(f\\) and \\(g\\))</li> <li><code>supplements/03_equivalent_parameterizations.md</code></li> <li><code>supplements/04_training_loss_and_denoising.md</code></li> <li><code>supplements/05_reverse_sde_and_probability_flow_ode.md</code></li> <li><code>supplements/07_fokker_planck_equation.md</code> (optional: deeper PDE connection)</li> </ol>"},{"location":"diffusion/02_sde_formulation/#for-theorists-focus-on-mathematics","title":"For Theorists (focus on mathematics):","text":"<ol> <li><code>sde_formulation.md</code> (theory first)</li> <li><code>sde_QA.md</code> (clarifications)</li> <li>All supplements in order (01 \u2192 08)</li> <li><code>02_sde_formulation.ipynb</code> (see theory in action)</li> </ol>"},{"location":"diffusion/02_sde_formulation/#for-building-intuition","title":"For Building Intuition:","text":"<ul> <li><code>supplements/08_dimensional_analysis.md</code> (anytime - powerful sanity checks)</li> <li><code>supplements/07_fokker_planck_equation.md</code> (understand probability evolution)</li> <li><code>supplements/02_brownian_motion_dimensionality.md</code> (clarify vector dimensions)</li> </ul>"},{"location":"diffusion/02_sde_formulation/#for-quick-reference","title":"For Quick Reference:","text":"<ul> <li>Jump to <code>sde_QA.md</code> for specific questions</li> <li>Use supplements as needed for deep-dives</li> <li>Start with supplement 01 if confused about what's fixed vs learned</li> <li>Use supplement 08 for dimensional sanity checks</li> </ul>"},{"location":"diffusion/02_sde_formulation/#key-concepts-summary","title":"Key Concepts Summary","text":""},{"location":"diffusion/02_sde_formulation/#what-youll-learn","title":"What You'll Learn","text":"<ol> <li>SDEs describe continuous-time random processes</li> <li>\\(dx = f(x,t)dt + g(t)dw(t)\\)</li> <li> <p>Drift \\(f\\) (deterministic) + diffusion \\(g\\) (random)</p> </li> <li> <p>Only the score function is learned</p> </li> <li>\\(s_\\theta(x,t) \\approx \\nabla_x \\log p_t(x)\\)</li> <li> <p>Everything else (\\(f\\), \\(g\\), forward process) is fixed</p> </li> <li> <p>Training = denoising score matching</p> </li> <li>Learn to predict noise (or equivalently, the score)</li> <li> <p>No SDE solving during training (use closed-form marginals)</p> </li> <li> <p>Sampling = solving reverse SDE</p> </li> <li>Numerically integrate from noise to data</li> <li> <p>Can use stochastic (SDE) or deterministic (ODE) sampling</p> </li> <li> <p>Brownian motion enables tractability</p> </li> <li>Exact reverse-time equations (Anderson's theorem)</li> <li>Closed-form marginals for many SDEs</li> <li>Stable training and sampling</li> </ol>"},{"location":"diffusion/02_sde_formulation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Mathematics: Calculus, probability (Gaussian distributions), basic differential equations</li> <li>Programming: Python, PyTorch/NumPy basics</li> <li>Machine Learning: Neural networks, gradient descent, loss functions</li> <li>Diffusion Models: Helpful to know DDPM basics (see <code>../01_ddpm_basics.ipynb</code>)</li> </ul>"},{"location":"diffusion/02_sde_formulation/#next-steps","title":"Next Steps","text":"<p>After mastering the SDE formulation:</p> <ol> <li>Apply to real problems: See <code>../03_scPPDM_tutorial.ipynb</code> for drug-response prediction</li> <li>Implement custom SDEs: Design diffusion processes for your domain</li> <li>Explore variants: VE-SDE, sub-VP-SDE, conditional generation</li> <li>Read research papers: Song et al. (2021), Ho et al. (2020), Karras et al. (2022)</li> </ol>"},{"location":"diffusion/02_sde_formulation/#references","title":"References","text":""},{"location":"diffusion/02_sde_formulation/#primary-papers","title":"Primary Papers","text":"<ul> <li>Song et al. (2021): Score-Based Generative Modeling through Stochastic Differential Equations</li> <li>The definitive paper on SDE formulation</li> <li> <p>Introduces VP-SDE, VE-SDE, and probability flow ODE</p> </li> <li> <p>Ho et al. (2020): Denoising Diffusion Probabilistic Models (DDPM)</p> </li> <li>Original discrete-time formulation</li> <li> <p>Shows connection to score matching</p> </li> <li> <p>Anderson (1982): Reverse-time diffusion equation models</p> </li> <li>Original theorem on reverse-time SDEs</li> <li>Foundation for all modern diffusion models</li> </ul>"},{"location":"diffusion/02_sde_formulation/#textbooks","title":"Textbooks","text":"<ul> <li>\u00d8ksendal (2003): Stochastic Differential Equations: An Introduction with Applications</li> <li>Comprehensive SDE theory</li> <li> <p>Rigorous mathematical treatment</p> </li> <li> <p>Karatzas &amp; Shreve (1991): Brownian Motion and Stochastic Calculus</p> </li> <li>Advanced reference</li> <li>Detailed proofs and theory</li> </ul>"},{"location":"diffusion/02_sde_formulation/#related-topics","title":"Related Topics","text":"<ul> <li>Score matching: Hyv\u00e4rinen (2005)</li> <li>Langevin dynamics: Neal (2011)</li> <li>Flow matching: Lipman et al. (2023)</li> <li>Rectified flows: Liu et al. (2022)</li> </ul>"},{"location":"diffusion/02_sde_formulation/#contributing","title":"Contributing","text":"<p>These materials are part of the <code>genai-lab</code> project. For questions or suggestions: - See main project: <code>../../README.md</code> - Theory documents: <code>../../../docs/</code> - Production examples: <code>../../../examples/</code></p> <p>Happy learning! \ud83c\udf93</p>"},{"location":"diffusion/02_sde_formulation/02_sde_formulation/","title":"Notebook","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import integrate\nfrom pathlib import Path\nimport sys\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm.auto import tqdm\n\n# Add src to path for importing genailab\nsys.path.insert(0, str(Path('../../../src').resolve()))\n\n# Set style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 4)\n\n# Random seeds\nnp.random.seed(42)\ntorch.manual_seed(42)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\nprint(f'Using device: {device}')\n</pre> import numpy as np import matplotlib.pyplot as plt import seaborn as sns from scipy import integrate from pathlib import Path import sys  import torch import torch.nn as nn import torch.nn.functional as F from tqdm.auto import tqdm  # Add src to path for importing genailab sys.path.insert(0, str(Path('../../../src').resolve()))  # Set style sns.set_style('whitegrid') plt.rcParams['figure.figsize'] = (12, 4)  # Random seeds np.random.seed(42) torch.manual_seed(42)  device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu' print(f'Using device: {device}') <pre>Using device: mps\n</pre> In\u00a0[2]: Copied! <pre>def simulate_brownian_motion(T=1.0, num_steps=1000, num_paths=5):\n    \"\"\"Simulate Brownian motion paths.\n    \n    Args:\n        T: Total time\n        num_steps: Number of discrete steps\n        num_paths: Number of paths to simulate\n    \n    Returns:\n        t: Time points\n        w: Brownian motion paths [num_paths, num_steps]\n    \"\"\"\n    dt = T / num_steps\n    t = np.linspace(0, T, num_steps)\n    \n    # Generate increments: dw ~ N(0, dt)\n    dw = np.random.randn(num_paths, num_steps) * np.sqrt(dt)\n    \n    # Cumulative sum to get w(t)\n    w = np.cumsum(dw, axis=1)\n    \n    # Ensure w(0) = 0\n    w = np.concatenate([np.zeros((num_paths, 1)), w], axis=1)\n    t = np.concatenate([[0], t])\n    \n    return t, w\n\n# Simulate and visualize\nt, w = simulate_brownian_motion(T=1.0, num_steps=1000, num_paths=10)\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Plot 1: Multiple paths\nfor i in range(10):\n    axes[0].plot(t, w[i], alpha=0.6, linewidth=0.8)\naxes[0].set_xlabel('Time t')\naxes[0].set_ylabel('w(t)')\naxes[0].set_title('Brownian Motion Paths')\naxes[0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n\n# Plot 2: Distribution at different times\ntimes = [0.25, 0.5, 0.75, 1.0]\nfor time in times:\n    idx = int(time * 1000)\n    axes[1].hist(w[:, idx], bins=30, alpha=0.5, label=f't={time}', density=True)\naxes[1].set_xlabel('w(t)')\naxes[1].set_ylabel('Density')\naxes[1].set_title('Distribution at Different Times')\naxes[1].legend()\n\n# Plot 3: Variance over time (should be linear)\nvariance = np.var(w, axis=0)\naxes[2].plot(t, variance, label='Empirical variance')\naxes[2].plot(t, t, 'r--', label='Theoretical: Var[w(t)] = t')\naxes[2].set_xlabel('Time t')\naxes[2].set_ylabel('Variance')\naxes[2].set_title('Variance Growth (Linear in Time)')\naxes[2].legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Key property: Variance at t=1.0 is {variance[-1]:.3f} (should be \u2248 1.0)\")\nprint(f\"Standard deviation scales as \u221at: std(w(0.25)) = {np.std(w[:, 250]):.3f} \u2248 {np.sqrt(0.25):.3f}\")\n</pre> def simulate_brownian_motion(T=1.0, num_steps=1000, num_paths=5):     \"\"\"Simulate Brownian motion paths.          Args:         T: Total time         num_steps: Number of discrete steps         num_paths: Number of paths to simulate          Returns:         t: Time points         w: Brownian motion paths [num_paths, num_steps]     \"\"\"     dt = T / num_steps     t = np.linspace(0, T, num_steps)          # Generate increments: dw ~ N(0, dt)     dw = np.random.randn(num_paths, num_steps) * np.sqrt(dt)          # Cumulative sum to get w(t)     w = np.cumsum(dw, axis=1)          # Ensure w(0) = 0     w = np.concatenate([np.zeros((num_paths, 1)), w], axis=1)     t = np.concatenate([[0], t])          return t, w  # Simulate and visualize t, w = simulate_brownian_motion(T=1.0, num_steps=1000, num_paths=10)  fig, axes = plt.subplots(1, 3, figsize=(15, 4))  # Plot 1: Multiple paths for i in range(10):     axes[0].plot(t, w[i], alpha=0.6, linewidth=0.8) axes[0].set_xlabel('Time t') axes[0].set_ylabel('w(t)') axes[0].set_title('Brownian Motion Paths') axes[0].axhline(y=0, color='k', linestyle='--', alpha=0.3)  # Plot 2: Distribution at different times times = [0.25, 0.5, 0.75, 1.0] for time in times:     idx = int(time * 1000)     axes[1].hist(w[:, idx], bins=30, alpha=0.5, label=f't={time}', density=True) axes[1].set_xlabel('w(t)') axes[1].set_ylabel('Density') axes[1].set_title('Distribution at Different Times') axes[1].legend()  # Plot 3: Variance over time (should be linear) variance = np.var(w, axis=0) axes[2].plot(t, variance, label='Empirical variance') axes[2].plot(t, t, 'r--', label='Theoretical: Var[w(t)] = t') axes[2].set_xlabel('Time t') axes[2].set_ylabel('Variance') axes[2].set_title('Variance Growth (Linear in Time)') axes[2].legend()  plt.tight_layout() plt.show()  print(f\"Key property: Variance at t=1.0 is {variance[-1]:.3f} (should be \u2248 1.0)\") print(f\"Standard deviation scales as \u221at: std(w(0.25)) = {np.std(w[:, 250]):.3f} \u2248 {np.sqrt(0.25):.3f}\") <pre>Key property: Variance at t=1.0 is 1.199 (should be \u2248 1.0)\nStandard deviation scales as \u221at: std(w(0.25)) = 0.454 \u2248 0.500\n</pre> In\u00a0[3]: Copied! <pre>from genailab.diffusion import VPSDE\n\n# Test the SDE with different schedules\n# Linear (original DDPM)\n# sde = VPSDE(beta_min=0.1, beta_max=20.0, T=1.0, schedule='linear')\n\n# Cosine (recommended for better quality)\nsde = VPSDE(schedule='cosine')\n\n# Sigmoid with custom steepness\n# sde = VPSDE(schedule='sigmoid', k=12.0)\n\n# Quadratic\n# sde = VPSDE(schedule='quadratic', beta_min=0.1, beta_max=20.0)\n\n# Visualize beta(t) and alpha_bar(t)\nt_vals = np.linspace(0, 1, 100)\nbeta_vals = [sde.beta(t) for t in t_vals]\nalpha_bar_vals = [sde.schedule.alpha_bar(t) for t in t_vals]\n\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.plot(t_vals, beta_vals)\nplt.xlabel('Time t')\nplt.ylabel('\u03b2(t)')\nplt.title(f'Noise Schedule ({sde.schedule_name})')\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\nplt.plot(t_vals, alpha_bar_vals)\nplt.xlabel('Time t')\nplt.ylabel('\u1fb1(t)')\nplt.title('Signal Decay')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</pre> from genailab.diffusion import VPSDE  # Test the SDE with different schedules # Linear (original DDPM) # sde = VPSDE(beta_min=0.1, beta_max=20.0, T=1.0, schedule='linear')  # Cosine (recommended for better quality) sde = VPSDE(schedule='cosine')  # Sigmoid with custom steepness # sde = VPSDE(schedule='sigmoid', k=12.0)  # Quadratic # sde = VPSDE(schedule='quadratic', beta_min=0.1, beta_max=20.0)  # Visualize beta(t) and alpha_bar(t) t_vals = np.linspace(0, 1, 100) beta_vals = [sde.beta(t) for t in t_vals] alpha_bar_vals = [sde.schedule.alpha_bar(t) for t in t_vals]  plt.figure(figsize=(10, 4)) plt.subplot(1, 2, 1) plt.plot(t_vals, beta_vals) plt.xlabel('Time t') plt.ylabel('\u03b2(t)') plt.title(f'Noise Schedule ({sde.schedule_name})') plt.grid(True, alpha=0.3)  plt.subplot(1, 2, 2) plt.plot(t_vals, alpha_bar_vals) plt.xlabel('Time t') plt.ylabel('\u1fb1(t)') plt.title('Signal Decay') plt.grid(True, alpha=0.3)  plt.tight_layout() plt.show() In\u00a0[4]: Copied! <pre>def generate_swiss_roll(n_samples=1000):\n    \"\"\"Generate 2D Swiss roll dataset.\"\"\"\n    theta = np.sqrt(np.random.rand(n_samples)) * 3 * np.pi\n    x = theta * np.cos(theta)\n    y = theta * np.sin(theta)\n    data = np.stack([x, y], axis=1) / 10.0  # Scale down\n    return data\n\n# Generate data\nx0 = generate_swiss_roll(n_samples=2000)\n\n# Apply forward diffusion at different times\ntimes = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\n\nfor i, t in enumerate(times):\n    if t == 0:\n        xt = x0\n    else:\n        xt, _ = sde.sample_from_marginal(x0, t)\n    \n    axes[i].scatter(xt[:, 0], xt[:, 1], alpha=0.3, s=1)\n    axes[i].set_xlim(-3, 3)\n    axes[i].set_ylim(-3, 3)\n    axes[i].set_title(f't = {t:.1f}')\n    axes[i].set_aspect('equal')\n    \n    # Show mean and std\n    mean, std = sde.marginal_prob(x0, t)\n    axes[i].text(0.05, 0.95, f'std={std:.3f}', \n                transform=axes[i].transAxes, \n                verticalalignment='top',\n                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\nplt.suptitle('Forward Diffusion: Swiss Roll \u2192 Gaussian Noise', fontsize=14, y=1.00)\nplt.tight_layout()\nplt.show()\n\nprint(\"Observation: As t increases, the structure dissolves into isotropic Gaussian noise.\")\n</pre> def generate_swiss_roll(n_samples=1000):     \"\"\"Generate 2D Swiss roll dataset.\"\"\"     theta = np.sqrt(np.random.rand(n_samples)) * 3 * np.pi     x = theta * np.cos(theta)     y = theta * np.sin(theta)     data = np.stack([x, y], axis=1) / 10.0  # Scale down     return data  # Generate data x0 = generate_swiss_roll(n_samples=2000)  # Apply forward diffusion at different times times = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]  fig, axes = plt.subplots(2, 3, figsize=(15, 10)) axes = axes.flatten()  for i, t in enumerate(times):     if t == 0:         xt = x0     else:         xt, _ = sde.sample_from_marginal(x0, t)          axes[i].scatter(xt[:, 0], xt[:, 1], alpha=0.3, s=1)     axes[i].set_xlim(-3, 3)     axes[i].set_ylim(-3, 3)     axes[i].set_title(f't = {t:.1f}')     axes[i].set_aspect('equal')          # Show mean and std     mean, std = sde.marginal_prob(x0, t)     axes[i].text(0.05, 0.95, f'std={std:.3f}',                  transform=axes[i].transAxes,                  verticalalignment='top',                 bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))  plt.suptitle('Forward Diffusion: Swiss Roll \u2192 Gaussian Noise', fontsize=14, y=1.00) plt.tight_layout() plt.show()  print(\"Observation: As t increases, the structure dissolves into isotropic Gaussian noise.\") <pre>Observation: As t increases, the structure dissolves into isotropic Gaussian noise.\n</pre> In\u00a0[5]: Copied! <pre>class SimpleScoreNetwork(nn.Module):\n    \"\"\"Simple MLP score network for 2D data.\"\"\"\n    \n    def __init__(self, data_dim=2, hidden_dim=128, time_dim=32):\n        super().__init__()\n        \n        # Time embedding (sinusoidal)\n        self.time_dim = time_dim\n        \n        # Network\n        self.net = nn.Sequential(\n            nn.Linear(data_dim + time_dim, hidden_dim),\n            nn.SiLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.SiLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.SiLU(),\n            nn.Linear(hidden_dim, data_dim),\n        )\n    \n    def time_embedding(self, t):\n        \"\"\"Sinusoidal time embedding.\"\"\"\n        half_dim = self.time_dim // 2\n        emb = np.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n        emb = t[:, None] * emb[None, :]\n        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n        return emb\n    \n    def forward(self, x, t):\n        \"\"\"Predict score: s(x, t) \u2248 \u2207_x log p_t(x).\n        \n        Args:\n            x: Data [batch_size, data_dim]\n            t: Time [batch_size]\n        \n        Returns:\n            score: [batch_size, data_dim]\n        \"\"\"\n        t_emb = self.time_embedding(t)\n        h = torch.cat([x, t_emb], dim=-1)\n        return self.net(h)\n\n# Test the network\nmodel = SimpleScoreNetwork(data_dim=2, hidden_dim=128).to(device)\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# Test forward pass\nx_test = torch.randn(16, 2).to(device)\nt_test = torch.rand(16).to(device)\nscore_test = model(x_test, t_test)\nprint(f\"Input shape: {x_test.shape}\")\nprint(f\"Output shape: {score_test.shape}\")\n</pre> class SimpleScoreNetwork(nn.Module):     \"\"\"Simple MLP score network for 2D data.\"\"\"          def __init__(self, data_dim=2, hidden_dim=128, time_dim=32):         super().__init__()                  # Time embedding (sinusoidal)         self.time_dim = time_dim                  # Network         self.net = nn.Sequential(             nn.Linear(data_dim + time_dim, hidden_dim),             nn.SiLU(),             nn.Linear(hidden_dim, hidden_dim),             nn.SiLU(),             nn.Linear(hidden_dim, hidden_dim),             nn.SiLU(),             nn.Linear(hidden_dim, data_dim),         )          def time_embedding(self, t):         \"\"\"Sinusoidal time embedding.\"\"\"         half_dim = self.time_dim // 2         emb = np.log(10000) / (half_dim - 1)         emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)         emb = t[:, None] * emb[None, :]         emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)         return emb          def forward(self, x, t):         \"\"\"Predict score: s(x, t) \u2248 \u2207_x log p_t(x).                  Args:             x: Data [batch_size, data_dim]             t: Time [batch_size]                  Returns:             score: [batch_size, data_dim]         \"\"\"         t_emb = self.time_embedding(t)         h = torch.cat([x, t_emb], dim=-1)         return self.net(h)  # Test the network model = SimpleScoreNetwork(data_dim=2, hidden_dim=128).to(device) print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")  # Test forward pass x_test = torch.randn(16, 2).to(device) t_test = torch.rand(16).to(device) score_test = model(x_test, t_test) print(f\"Input shape: {x_test.shape}\") print(f\"Output shape: {score_test.shape}\") <pre>Model parameters: 37,762\nInput shape: torch.Size([16, 2])\nOutput shape: torch.Size([16, 2])\n</pre> In\u00a0[6]: Copied! <pre># Use the improved implementation from src/genailab/diffusion\nfrom genailab.diffusion import VPSDE as VPSDE_Module\nfrom genailab.diffusion import SimpleScoreNetwork as ScoreNet\nfrom genailab.diffusion import train_score_network, sample_reverse_sde\n\n# Automatically match the schedule from Cell 5\n# This ensures training uses the same noise schedule as the data\nprint(f\"Detected schedule from Cell 5: {sde.schedule_name}\")\nprint(f\"Schedule parameters: T={sde.T}\")\n\n# Re-create SDE with the same schedule for training\nif sde.schedule_name == 'linear' or sde.schedule_name == 'LinearSchedule':\n    sde_train = VPSDE_Module(beta_min=sde.beta_min, beta_max=sde.beta_max, T=sde.T, schedule='linear')\nelif sde.schedule_name == 'cosine' or sde.schedule_name == 'CosineSchedule':\n    sde_train = VPSDE_Module(schedule='cosine', T=sde.T)\nelif sde.schedule_name == 'sigmoid' or sde.schedule_name == 'SigmoidSchedule':\n    # Extract k parameter if available\n    k = getattr(sde.schedule, 'k', 10.0)\n    sde_train = VPSDE_Module(schedule='sigmoid', k=k, T=sde.T)\nelif sde.schedule_name == 'quadratic' or sde.schedule_name == 'QuadraticSchedule':\n    sde_train = VPSDE_Module(schedule='quadratic', beta_min=sde.beta_min, beta_max=sde.beta_max, T=sde.T)\nelse:\n    # Fallback: use the same schedule object\n    sde_train = sde\n    print(f\"Warning: Unknown schedule type, using existing SDE instance\")\n\n# Use a larger, more powerful network for better score estimation\nmodel = ScoreNet(data_dim=2, hidden_dim=512, num_layers=6).to(device)\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# Train the model with improved hyperparameters\nlosses = train_score_network(\n    model=model,\n    data=x0,\n    sde=sde_train,\n    num_epochs=10000,  # More epochs for better convergence\n    batch_size=256,    # Larger batch for more stable gradients\n    lr=2e-4,           # Lower learning rate for stability\n    device=device,\n)\n\n# Update the global sde variable to use the trained one\nsde = sde_train\n\n# Plot training curve\nplt.figure(figsize=(10, 4))\nplt.plot(losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title(f'Score Matching Training Loss ({sde.schedule_name})')\nplt.yscale('log')\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(f\"Final loss: {losses[-1]:.6f}\")\n</pre> # Use the improved implementation from src/genailab/diffusion from genailab.diffusion import VPSDE as VPSDE_Module from genailab.diffusion import SimpleScoreNetwork as ScoreNet from genailab.diffusion import train_score_network, sample_reverse_sde  # Automatically match the schedule from Cell 5 # This ensures training uses the same noise schedule as the data print(f\"Detected schedule from Cell 5: {sde.schedule_name}\") print(f\"Schedule parameters: T={sde.T}\")  # Re-create SDE with the same schedule for training if sde.schedule_name == 'linear' or sde.schedule_name == 'LinearSchedule':     sde_train = VPSDE_Module(beta_min=sde.beta_min, beta_max=sde.beta_max, T=sde.T, schedule='linear') elif sde.schedule_name == 'cosine' or sde.schedule_name == 'CosineSchedule':     sde_train = VPSDE_Module(schedule='cosine', T=sde.T) elif sde.schedule_name == 'sigmoid' or sde.schedule_name == 'SigmoidSchedule':     # Extract k parameter if available     k = getattr(sde.schedule, 'k', 10.0)     sde_train = VPSDE_Module(schedule='sigmoid', k=k, T=sde.T) elif sde.schedule_name == 'quadratic' or sde.schedule_name == 'QuadraticSchedule':     sde_train = VPSDE_Module(schedule='quadratic', beta_min=sde.beta_min, beta_max=sde.beta_max, T=sde.T) else:     # Fallback: use the same schedule object     sde_train = sde     print(f\"Warning: Unknown schedule type, using existing SDE instance\")  # Use a larger, more powerful network for better score estimation model = ScoreNet(data_dim=2, hidden_dim=512, num_layers=6).to(device) print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")  # Train the model with improved hyperparameters losses = train_score_network(     model=model,     data=x0,     sde=sde_train,     num_epochs=10000,  # More epochs for better convergence     batch_size=256,    # Larger batch for more stable gradients     lr=2e-4,           # Lower learning rate for stability     device=device, )  # Update the global sde variable to use the trained one sde = sde_train  # Plot training curve plt.figure(figsize=(10, 4)) plt.plot(losses) plt.xlabel('Epoch') plt.ylabel('Loss') plt.title(f'Score Matching Training Loss ({sde.schedule_name})') plt.yscale('log') plt.grid(True, alpha=0.3) plt.show()  print(f\"Final loss: {losses[-1]:.6f}\") <pre>Detected schedule from Cell 5: cosine\nSchedule parameters: T=1.0\nModel parameters: 3,457,538\n</pre> <pre>Training:   0%|          | 0/10000 [00:00&lt;?, ?it/s]</pre> <pre>Epoch 500/10000, Loss: 0.2727\nEpoch 1000/10000, Loss: 0.2374\nEpoch 1500/10000, Loss: 0.3083\nEpoch 2000/10000, Loss: 0.2790\nEpoch 2500/10000, Loss: 0.2384\nEpoch 3000/10000, Loss: 0.2435\nEpoch 3500/10000, Loss: 0.2503\nEpoch 4000/10000, Loss: 0.2289\nEpoch 4500/10000, Loss: 0.2470\nEpoch 5000/10000, Loss: 0.2598\nEpoch 5500/10000, Loss: 0.2373\nEpoch 6000/10000, Loss: 0.2577\nEpoch 6500/10000, Loss: 0.2157\nEpoch 7000/10000, Loss: 0.2241\nEpoch 7500/10000, Loss: 0.2350\nEpoch 8000/10000, Loss: 0.2625\nEpoch 8500/10000, Loss: 0.2374\nEpoch 9000/10000, Loss: 0.2499\nEpoch 9500/10000, Loss: 0.2388\nEpoch 10000/10000, Loss: 0.2579\n</pre> <pre>Final loss: 0.257887\n</pre> In\u00a0[\u00a0]: Copied! <pre># Generate samples using the module's sampling function\n# Using more steps for better quality (reduces discretization error)\nsamples, trajectory = sample_reverse_sde(\n    model=model,\n    sde=sde,\n    n_samples=2000,\n    num_steps=1000,  # Increased from 500 for smoother sampling\n    data_dim=2,\n    device=device,\n)\n\nprint(f\"Generated {samples.shape[0]} samples\")\nprint(f\"Trajectory has {len(trajectory)} snapshots\")\nprint(f\"Using {1000} sampling steps for reduced discretization error\")\n</pre> # Generate samples using the module's sampling function # Using more steps for better quality (reduces discretization error) samples, trajectory = sample_reverse_sde(     model=model,     sde=sde,     n_samples=2000,     num_steps=1000,  # Increased from 500 for smoother sampling     data_dim=2,     device=device, )  print(f\"Generated {samples.shape[0]} samples\") print(f\"Trajectory has {len(trajectory)} snapshots\") print(f\"Using {1000} sampling steps for reduced discretization error\") <pre>Sampling:   0%|          | 0/500 [00:00&lt;?, ?it/s]</pre> <pre>Generated 2000 samples\nTrajectory has 11 snapshots\n</pre> In\u00a0[8]: Copied! <pre># Compare real vs generated\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\naxes[0].scatter(x0[:, 0], x0[:, 1], alpha=0.3, s=1, label='Real data')\naxes[0].set_xlim(-3, 3)\naxes[0].set_ylim(-3, 3)\naxes[0].set_title('Real Data (Swiss Roll)')\naxes[0].set_aspect('equal')\naxes[0].legend()\n\naxes[1].scatter(samples[:, 0], samples[:, 1], alpha=0.3, s=1, label='Generated', color='orange')\naxes[1].set_xlim(-3, 3)\naxes[1].set_ylim(-3, 3)\naxes[1].set_title('Generated Samples (via Reverse SDE)')\naxes[1].set_aspect('equal')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n</pre> # Compare real vs generated fig, axes = plt.subplots(1, 2, figsize=(12, 5))  axes[0].scatter(x0[:, 0], x0[:, 1], alpha=0.3, s=1, label='Real data') axes[0].set_xlim(-3, 3) axes[0].set_ylim(-3, 3) axes[0].set_title('Real Data (Swiss Roll)') axes[0].set_aspect('equal') axes[0].legend()  axes[1].scatter(samples[:, 0], samples[:, 1], alpha=0.3, s=1, label='Generated', color='orange') axes[1].set_xlim(-3, 3) axes[1].set_ylim(-3, 3) axes[1].set_title('Generated Samples (via Reverse SDE)') axes[1].set_aspect('equal') axes[1].legend()  plt.tight_layout() plt.show() In\u00a0[9]: Copied! <pre># Show snapshots of reverse process\nnum_snapshots = min(6, len(trajectory))\nindices = np.linspace(0, len(trajectory)-1, num_snapshots, dtype=int)\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\n\nfor i, idx in enumerate(indices):\n    xt = trajectory[idx]\n    t_val = sde.T * (1 - idx / len(trajectory))\n    \n    axes[i].scatter(xt[:, 0], xt[:, 1], alpha=0.3, s=1)\n    axes[i].set_xlim(-3, 3)\n    axes[i].set_ylim(-3, 3)\n    axes[i].set_title(f't = {t_val:.3f}')\n    axes[i].set_aspect('equal')\n\nplt.suptitle('Reverse Diffusion: Noise \u2192 Data', fontsize=14, y=1.00)\nplt.tight_layout()\nplt.show()\n</pre> # Show snapshots of reverse process num_snapshots = min(6, len(trajectory)) indices = np.linspace(0, len(trajectory)-1, num_snapshots, dtype=int)  fig, axes = plt.subplots(2, 3, figsize=(15, 10)) axes = axes.flatten()  for i, idx in enumerate(indices):     xt = trajectory[idx]     t_val = sde.T * (1 - idx / len(trajectory))          axes[i].scatter(xt[:, 0], xt[:, 1], alpha=0.3, s=1)     axes[i].set_xlim(-3, 3)     axes[i].set_ylim(-3, 3)     axes[i].set_title(f't = {t_val:.3f}')     axes[i].set_aspect('equal')  plt.suptitle('Reverse Diffusion: Noise \u2192 Data', fontsize=14, y=1.00) plt.tight_layout() plt.show() In\u00a0[10]: Copied! <pre>from genailab.diffusion import sample_probability_flow_ode\n\n# Generate samples with ODE\nsamples_ode = sample_probability_flow_ode(\n    model=model,\n    sde=sde,\n    n_samples=2000,\n    num_steps=100,\n    data_dim=2,\n    device=device,\n)\n\n# Compare SDE vs ODE sampling\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\naxes[0].scatter(x0[:, 0], x0[:, 1], alpha=0.3, s=1)\naxes[0].set_title('Real Data')\naxes[0].set_aspect('equal')\n\naxes[1].scatter(samples[:, 0], samples[:, 1], alpha=0.3, s=1, color='orange')\naxes[1].set_title('SDE Sampling (Stochastic)')\naxes[1].set_aspect('equal')\n\naxes[2].scatter(samples_ode[:, 0], samples_ode[:, 1], alpha=0.3, s=1, color='green')\naxes[2].set_title('ODE Sampling (Deterministic)')\naxes[2].set_aspect('equal')\n\nfor ax in axes:\n    ax.set_xlim(-3, 3)\n    ax.set_ylim(-3, 3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Note: ODE sampling is faster (fewer steps) but may be less diverse.\")\n</pre> from genailab.diffusion import sample_probability_flow_ode  # Generate samples with ODE samples_ode = sample_probability_flow_ode(     model=model,     sde=sde,     n_samples=2000,     num_steps=100,     data_dim=2,     device=device, )  # Compare SDE vs ODE sampling fig, axes = plt.subplots(1, 3, figsize=(15, 5))  axes[0].scatter(x0[:, 0], x0[:, 1], alpha=0.3, s=1) axes[0].set_title('Real Data') axes[0].set_aspect('equal')  axes[1].scatter(samples[:, 0], samples[:, 1], alpha=0.3, s=1, color='orange') axes[1].set_title('SDE Sampling (Stochastic)') axes[1].set_aspect('equal')  axes[2].scatter(samples_ode[:, 0], samples_ode[:, 1], alpha=0.3, s=1, color='green') axes[2].set_title('ODE Sampling (Deterministic)') axes[2].set_aspect('equal')  for ax in axes:     ax.set_xlim(-3, 3)     ax.set_ylim(-3, 3)  plt.tight_layout() plt.show()  print(\"Note: ODE sampling is faster (fewer steps) but may be less diverse.\") <pre>ODE Sampling:   0%|          | 0/100 [00:00&lt;?, ?it/s]</pre> <pre>Note: ODE sampling is faster (fewer steps) but may be less diverse.\n</pre>"},{"location":"diffusion/02_sde_formulation/02_sde_formulation/#sde-formulation-of-diffusion-models-interactive-tutorial","title":"SDE Formulation of Diffusion Models: Interactive Tutorial\u00b6","text":"<p>This notebook implements the concepts from the companion theory document <code>README.md</code>.</p> <p>Learning objectives:</p> <ol> <li>Visualize Brownian motion and understand its properties</li> <li>Simulate forward SDEs (data \u2192 noise)</li> <li>Implement score matching training</li> <li>Sample from reverse SDEs (noise \u2192 data)</li> <li>Compare VP-SDE and VE-SDE</li> </ol> <p>Prerequisites:</p> <ul> <li>Basic probability (Gaussian distributions)</li> <li>PyTorch fundamentals</li> <li>Understanding of DDPM (see <code>01_ddpm_basics.ipynb</code>)</li> </ul>"},{"location":"diffusion/02_sde_formulation/02_sde_formulation/#setup","title":"Setup\u00b6","text":""},{"location":"diffusion/02_sde_formulation/02_sde_formulation/#1-understanding-brownian-motion","title":"1. Understanding Brownian Motion\u00b6","text":"<p>Before diving into SDEs, let's visualize Brownian motion $w(t)$ and understand its key properties:</p> <ol> <li>$w(0) = 0$</li> <li>Independent increments</li> <li>$w(t+\\Delta t) - w(t) \\sim \\mathcal{N}(0, \\Delta t)$</li> <li>Continuous but nowhere differentiable</li> </ol>"},{"location":"diffusion/02_sde_formulation/02_sde_formulation/#2-forward-sde-data-noise","title":"2. Forward SDE: Data \u2192 Noise\u00b6","text":"<p>Let's implement the forward SDE that corrupts data:</p> <p>$$ dx = f(x,t)\\,dt + g(t)\\,dw(t) $$</p> <p>We'll focus on VP-SDE (Variance-Preserving):</p> <p>$$ dx = -\\frac{1}{2}\\beta(t) x\\,dt + \\sqrt{\\beta(t)}\\,dw(t) $$</p>"},{"location":"diffusion/02_sde_formulation/02_sde_formulation/#visualize-forward-diffusion-on-2d-data","title":"Visualize Forward Diffusion on 2D Data\u00b6","text":"<p>Let's see how the SDE corrupts a simple 2D distribution (Swiss roll).</p>"},{"location":"diffusion/02_sde_formulation/02_sde_formulation/#3-score-function-and-training","title":"3. Score Function and Training\u00b6","text":"<p>The score function is the gradient of the log-density:</p> <p>$$ s(x, t) = \\nabla_x \\log p_t(x) $$</p> <p>For training, we use denoising score matching. The conditional score has a closed form:</p> <p>$$ \\nabla_x \\log p_t(x_t | x_0) = -\\frac{x_t - \\text{mean}(t)}{\\text{std}^2(t)} = -\\frac{\\varepsilon}{\\text{std}(t)} $$</p> <p>where $\\varepsilon$ is the noise we added.</p>"},{"location":"diffusion/02_sde_formulation/02_sde_formulation/#training-loop","title":"Training Loop\u00b6","text":"<p>Train the score network using denoising score matching:</p> <p>$$ \\mathcal{L}(\\theta) = \\mathbb{E}_{t, x_0, \\varepsilon} \\left[ \\lambda(t) \\left\\| s_\\theta(x_t, t) + \\frac{\\varepsilon}{\\sigma(t)} \\right\\|^2 \\right] $$</p>"},{"location":"diffusion/02_sde_formulation/02_sde_formulation/#4-sampling-reverse-sde","title":"4. Sampling: Reverse SDE\u00b6","text":"<p>Generate samples by solving the reverse-time SDE:</p> <p>$$ dx = \\left[f(x,t) - g(t)^2 s_\\theta(x,t)\\right] dt + g(t)\\,d\\bar{w}(t) $$</p> <p>We'll use the Euler-Maruyama method to discretize this.</p>"},{"location":"diffusion/02_sde_formulation/02_sde_formulation/#visualize-generated-samples","title":"Visualize Generated Samples\u00b6","text":""},{"location":"diffusion/02_sde_formulation/02_sde_formulation/#visualize-reverse-diffusion-process","title":"Visualize Reverse Diffusion Process\u00b6","text":""},{"location":"diffusion/02_sde_formulation/02_sde_formulation/#5-probability-flow-ode-deterministic-sampling","title":"5. Probability Flow ODE (Deterministic Sampling)\u00b6","text":"<p>Instead of the stochastic reverse SDE, we can use the probability flow ODE:</p> <p>$$ \\frac{dx}{dt} = f(x,t) - \\frac{1}{2}g(t)^2 s_\\theta(x,t) $$</p> <p>This generates samples deterministically (like DDIM).</p>"},{"location":"diffusion/02_sde_formulation/02_sde_formulation/#summary","title":"Summary\u00b6","text":"<p>We've implemented the SDE framework for diffusion models:</p> <ol> <li>\u2705 Brownian motion: Visualized continuous random walks</li> <li>\u2705 Forward SDE: Corrupted data with VP-SDE</li> <li>\u2705 Score matching: Trained neural network to predict scores</li> <li>\u2705 Reverse SDE: Generated samples by solving reverse-time SDE</li> <li>\u2705 Probability flow ODE: Deterministic sampling alternative</li> </ol> <p>Key takeaways:</p> <ul> <li>SDEs provide a continuous-time view of diffusion</li> <li>Only the score function needs to be learned</li> <li>Reverse SDE (stochastic) vs ODE (deterministic) sampling</li> <li>VP-SDE is the continuous version of DDPM</li> </ul> <p>Next steps:</p> <ul> <li>Apply to high-dimensional data (images, gene expression)</li> <li>Implement conditional generation</li> <li>Study scPPDM (latent-space VP-SDE for drug response)</li> </ul> <p>See <code>README.md</code> for detailed theory and mathematical derivations.</p>"},{"location":"diffusion/02_sde_formulation/sde_QA/","title":"SDE Formulation: Common Questions Answered","text":"<p>This document addresses frequently asked questions about the SDE (Stochastic Differential Equation) formulation of diffusion models. It complements the main tutorial by diving deeper into practical and conceptual questions.</p> <p>Prerequisites: Basic understanding of SDEs. See <code>sde_formulation.md</code> for foundations.</p>"},{"location":"diffusion/02_sde_formulation/sde_QA/#quick-recap-what-is-an-sde","title":"Quick Recap: What is an SDE?","text":"<p>A stochastic differential equation (SDE) adds continuous random noise to deterministic motion:</p> \\[ dx(t) = f(x(t), t)\\,dt + g(t)\\,dw(t) \\] <p>This describes a random process evolving in time, not a single deterministic trajectory.</p> <p>Components:</p> <ul> <li>\\(f(x,t)\\): Drift (deterministic flow)</li> <li>\\(g(t)\\): Diffusion coefficient (noise strength)</li> <li>\\(dw(t)\\): Brownian motion increment</li> </ul>"},{"location":"diffusion/02_sde_formulation/sde_QA/#1-how-is-an-sde-system-solved","title":"1. How is an SDE System Solved?","text":""},{"location":"diffusion/02_sde_formulation/sde_QA/#short-answer","title":"Short Answer","text":"<p>Numerically. Always.</p> <p>There are essentially no closed-form solutions for the SDEs used in diffusion models. Unlike simple ODEs where you might write \\(x(t) = x_0 e^{-\\lambda t}\\), SDEs require numerical simulation.</p>"},{"location":"diffusion/02_sde_formulation/sde_QA/#what-solving-an-sde-actually-means","title":"What \"Solving an SDE\" Actually Means","text":"<p>When we say \"solve an SDE,\" we mean simulating sample paths of the random process \\(x(t)\\).</p> <p>Conceptually: Starting from an initial state \\(x_0\\), we step forward (or backward) in tiny time increments, adding both: 1. Deterministic drift: Where the system \"wants\" to go 2. Random diffusion: Noise that perturbs the path</p> <p>Each simulation produces one random trajectory. Run it 1000 times, get 1000 different paths\u2014all following the same SDE.</p>"},{"location":"diffusion/02_sde_formulation/sde_QA/#the-eulermaruyama-method-basic-solver","title":"The Euler\u2013Maruyama Method (Basic Solver)","text":"<p>For the SDE:</p> \\[ dx(t) = f(x(t),t)\\,dt + g(t)\\,dw(t) \\] <p>Euler\u2013Maruyama discretizes time into steps of size \\(\\Delta t\\):</p> \\[ x_{k+1} = x_k + f(x_k,t_k)\\,\\Delta t + g(t_k)\\sqrt{\\Delta t}\\,\\varepsilon_k, \\quad \\varepsilon_k \\sim \\mathcal{N}(0,I) \\] <p>Interpretation:</p> <ul> <li>Deterministic motion: \\(f(x_k,t_k)\\Delta t\\) \u2014 where drift pushes you</li> <li>Stochastic motion: \\(g(t_k)\\sqrt{\\Delta t}\\,\\varepsilon_k\\) \u2014 random kick from noise</li> </ul> <p>Key insight: Noise scales as \\(\\sqrt{\\Delta t}\\), not \\(\\Delta t\\). This is fundamental to Brownian motion.</p> <p>This is the SDE analogue of Euler's method for ODEs, but with added randomness at each step.</p>"},{"location":"diffusion/02_sde_formulation/sde_QA/#in-diffusion-models-specifically","title":"In Diffusion Models Specifically","text":""},{"location":"diffusion/02_sde_formulation/sde_QA/#forward-process-data-noise","title":"Forward Process (Data \u2192 Noise)","text":"<p>The forward corruption process can be handled in two ways:</p> <ol> <li>Analytically (preferred): Use closed-form marginal distribution    $$    x_t = \\sqrt{\\bar{\\alpha}_t}\\, x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\, \\varepsilon    $$</li> </ol> <p>This is exact and fast\u2014no need to simulate step-by-step.</p> <ol> <li>Numerically (rare): Simulate via Euler\u2013Maruyama</li> <li>Only needed for exotic SDEs without closed forms</li> <li>Slower and less accurate</li> </ol> <p>In practice: We almost always use the closed-form marginal during training.</p>"},{"location":"diffusion/02_sde_formulation/sde_QA/#reverse-process-noise-data","title":"Reverse Process (Noise \u2192 Data)","text":"<p>The reverse process for generation is always solved numerically:</p> <p>Common methods:</p> <ul> <li>Euler\u2013Maruyama: Simple, first-order</li> <li>Predictor\u2013corrector: Alternate between drift step and Langevin correction</li> <li>Higher-order solvers: Heun, Runge-Kutta (better accuracy, fewer steps)</li> <li>ODE solvers: For deterministic sampling (see below)</li> </ul> <p>Why numerical? The reverse SDE depends on the learned score function \\(s_\\theta(x,t)\\), which is a neural network\u2014no closed form exists.</p>"},{"location":"diffusion/02_sde_formulation/sde_QA/#probability-flow-ode-important-special-case","title":"Probability Flow ODE (Important Special Case)","text":"<p>Here's a remarkable fact: The reverse SDE:</p> \\[ dx = \\left[f(x,t) - g(t)^2 \\nabla_x \\log p_t(x)\\right]dt + g(t)\\,dw \\] <p>has an ODE cousin with the same marginal distributions:</p> \\[ dx = \\left[f(x,t) - \\tfrac{1}{2}g(t)^2 \\nabla_x \\log p_t(x)\\right]dt \\] <p>Key differences:</p> Property SDE ODE Randomness Stochastic (\\(+g(t)dw\\)) Deterministic (no noise) Paths Different each run Same path every time Speed Slower (needs small steps) Faster (larger steps OK) Diversity Higher sample diversity Lower diversity <p>Practical implications:</p> <ul> <li>ODE sampling underlies DDIM and fast samplers</li> <li>SDE sampling gives more diverse outputs</li> <li>Both generate from the same distribution \\(p_0(x)\\)</li> </ul> <p>So diffusion models can be sampled stochastically (SDE) or deterministically (ODE)\u2014your choice!</p>"},{"location":"diffusion/02_sde_formulation/sde_QA/#2-what-models-are-learned-in-the-sde-formulation","title":"2. What Models Are Learned in the SDE Formulation?","text":"<p>This is the most important conceptual question. Let's be crystal clear about what's fixed versus what's learned.</p>"},{"location":"diffusion/02_sde_formulation/sde_QA/#what-is-not-learned","title":"What is NOT Learned","text":"<p>You do not learn:</p> <ul> <li>The SDE itself</li> <li>The drift function \\(f(x,t)\\)</li> <li>The diffusion coefficient \\(g(t)\\)</li> <li>The Wiener process \\(w(t)\\)</li> </ul> <p>These are all design choices you make upfront. They define the corruption process but contain no learnable parameters.</p> <p>Why this matters: Many people mistakenly think the neural network learns \"how to add noise.\" It doesn't. The noise schedule is fixed. The network learns something else entirely.</p>"},{"location":"diffusion/02_sde_formulation/sde_QA/#what-is-learned-the-only-thing","title":"What IS Learned (The Only Thing)","text":"\\[ \\boxed{ s_\\theta(x,t) \\approx \\nabla_x \\log p_t(x) } \\] <p>This is called the score function.</p> <p>Interpretation:</p> <ul> <li>Geometrically: Direction of steepest increase in log probability</li> <li>Intuitively: Vector field pointing toward \"more data-like\" regions</li> <li>Practically: Tells you which way to move to denoise the data</li> </ul> <p>Dimensionality: If your data is \\(x \\in \\mathbb{R}^d\\), the score is also a vector in \\(\\mathbb{R}^d\\). For images, that's millions of dimensions\u2014one gradient component per pixel.</p>"},{"location":"diffusion/02_sde_formulation/sde_QA/#what-do-we-use-the-learned-score-for","title":"What Do We Use the Learned Score For?","text":"<p>This is crucial to understand. The score function \\(s_\\theta(x,t)\\) is used for sampling (generation).</p> <p>During sampling, we solve the reverse-time SDE:</p> \\[ dx = \\left[f(x,t) - g(t)^2 s_\\theta(x,t)\\right]dt + g(t)\\,dw \\] <p>At each step: 1. Evaluate the score: \\(s_\\theta(x_t, t)\\) tells us which direction increases probability 2. Drift toward data: The term \\(-g(t)^2 s_\\theta(x,t)\\) pulls us toward high-probability regions 3. Add noise: The term \\(g(t)dw\\) maintains diversity</p> <p>The score is the bridge between noise and data. Without it, we couldn't reverse the diffusion process.</p> <p>Analogy: Imagine you're lost in fog (noise). The score function is like a compass that always points toward civilization (data). By following it and taking small steps, you gradually emerge from the fog.</p>"},{"location":"diffusion/02_sde_formulation/sde_QA/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>Input:</p> <ul> <li>Noisy data: \\(x_t \\in \\mathbb{R}^d\\)</li> <li>Time/noise level: \\(t \\in [0,T]\\) (usually embedded as sinusoidal features)</li> </ul> <p>Output:</p> <p>A vector in \\(\\mathbb{R}^d\\) representing one of these equivalent parameterizations:</p> <ol> <li>Score: \\(s_\\theta(x_t,t) \\approx \\nabla_x \\log p_t(x_t)\\)</li> <li>Noise: \\(\\varepsilon_\\theta(x_t,t) \\approx \\varepsilon\\) (the noise that was added)</li> <li>Clean data: \\(\\hat{x}_0 \\approx x_0\\) (denoised prediction)</li> </ol> <p>These are mathematically equivalent\u2014you can convert between them using the forward process equations.</p>"},{"location":"diffusion/02_sde_formulation/sde_QA/#why-predicting-noise-works","title":"Why Predicting Noise Works","text":"<p>For Gaussian corruption with forward process:</p> \\[ x_t = \\sqrt{\\bar{\\alpha}_t}\\, x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\, \\varepsilon \\] <p>The conditional score has a closed form:</p> \\[ \\nabla_x \\log p_t(x_t \\mid x_0) = -\\frac{\\varepsilon}{\\sqrt{1-\\bar{\\alpha}_t}} \\] <p>Key insight: The score is just the noise, scaled by \\(-1/\\sigma_t\\).</p> <p>So:</p> <ul> <li>Predicting noise \\(\\varepsilon_\\theta\\)</li> <li>Predicting score \\(s_\\theta\\)</li> <li>Predicting clean data \\(\\hat{x}_0\\)</li> </ul> <p>are all the same signal, just scaled/shifted differently. DDPM predicts noise, score-based models predict the score, but they're equivalent.</p>"},{"location":"diffusion/02_sde_formulation/sde_QA/#training-workflow-sde-view","title":"Training Workflow (SDE View)","text":"<p>Here's the complete training loop:</p> <ol> <li>Sample clean data: \\(x_0 \\sim p_{\\text{data}}\\)</li> <li>Sample time: \\(t \\sim \\text{Uniform}(0,T)\\)</li> <li>Sample noise: \\(\\varepsilon \\sim \\mathcal{N}(0,I)\\)</li> <li>Generate noisy data: \\(x_t = \\sqrt{\\bar{\\alpha}_t}\\, x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\, \\varepsilon\\) (closed-form marginal)</li> <li>Predict score/noise: \\(s_\\theta(x_t, t)\\) or \\(\\varepsilon_\\theta(x_t, t)\\)</li> <li>Compute loss: \\(\\mathcal{L} = \\|s_\\theta(x_t,t) - (-\\varepsilon/\\sigma_t)\\|^2\\) (or equivalent)</li> <li>Backpropagate: Update \\(\\theta\\)</li> </ol> <p>Crucial observation: No SDE solving during training! We use the closed-form marginal to generate noisy samples directly. SDE solving only happens during sampling/generation.</p>"},{"location":"diffusion/02_sde_formulation/sde_QA/#3-is-brownian-motion-the-only-way-to-model-randomness","title":"3. Is Brownian Motion the Only Way to Model Randomness?","text":""},{"location":"diffusion/02_sde_formulation/sde_QA/#short-answer_1","title":"Short Answer","text":"<p>No. But it's the only one used in diffusion models (so far).</p> <p>Let's separate mathematical theory from practical machine learning.</p>"},{"location":"diffusion/02_sde_formulation/sde_QA/#why-brownian-motion-is-used-in-diffusion-models","title":"Why Brownian Motion is Used in Diffusion Models","text":"<p>Brownian motion (Wiener process) has unique mathematical properties that make diffusion models tractable:</p> <p>Mathematical properties:</p> <ul> <li>Continuous paths: No sudden jumps, smooth evolution</li> <li>Gaussian increments: \\(w(t+\\Delta t) - w(t) \\sim \\mathcal{N}(0, \\Delta t)\\)</li> <li>Markov property: Future depends only on present, not past</li> <li>Independent increments: Non-overlapping intervals are independent</li> </ul> <p>Why these matter for diffusion models:</p> <ul> <li>Exact reverse SDE: Anderson (1982) proved that Brownian SDEs have tractable reverse-time equations</li> <li>Clean score formulation: The score \\(\\nabla_x \\log p_t(x)\\) has a well-defined meaning</li> <li>Stable training: Gaussian noise is well-behaved, no heavy tails or pathological cases</li> <li>Closed-form marginals: For many SDEs (like VP-SDE), we can compute \\(p_t(x|x_0)\\) analytically</li> </ul> <p>Bottom line: Brownian motion gives us mathematical control. We can derive, train, and sample reliably.</p>"},{"location":"diffusion/02_sde_formulation/sde_QA/#other-stochastic-processes-in-sdes-finance-physics","title":"Other Stochastic Processes in SDEs (Finance, Physics)","text":"<p>You're absolutely right that algorithmic trading and quantitative finance use many other stochastic processes. Here are the main alternatives:</p>"},{"location":"diffusion/02_sde_formulation/sde_QA/#1-jump-processes-levy-processes","title":"1. Jump Processes (L\u00e9vy Processes)","text":"<p>SDE form:</p> \\[ dx = f(x,t)\\,dt + \\sigma\\,dW_t + dJ_t \\] <p>where \\(J_t\\) is a jump process (e.g., compound Poisson).</p> <p>Characteristics:</p> <ul> <li>Sudden jumps: Discontinuous paths</li> <li>Heavy tails: Captures extreme events</li> <li>Market crashes: Models rare but large moves</li> </ul> <p>Examples:</p> <ul> <li>Poisson jumps: Fixed-size jumps at random times</li> <li>Variance Gamma: Infinite activity, finite variation</li> <li>CGMY models: Captures both small and large jumps</li> </ul> <p>Why not in diffusion models? Reverse-time equations for jump processes are much more complex. Score matching becomes ill-defined at jump points.</p>"},{"location":"diffusion/02_sde_formulation/sde_QA/#2-stochastic-volatility-models","title":"2. Stochastic Volatility Models","text":"<p>Example (Heston model):</p> \\[ \\begin{aligned} dS_t &amp;= \\mu S_t\\,dt + \\sqrt{v_t} S_t\\,dW_t \\\\ dv_t &amp;= \\kappa(\\theta - v_t)\\,dt + \\xi \\sqrt{v_t}\\,dB_t \\end{aligned} \\] <p>Characteristics:</p> <ul> <li>Randomness in randomness: Volatility itself is stochastic</li> <li>Two coupled SDEs: State and volatility evolve together</li> <li>Volatility clustering: Periods of high/low volatility persist</li> </ul> <p>Why not in diffusion models? Would require learning a time-varying diffusion coefficient \\(g(x,t)\\), significantly complicating the model.</p>"},{"location":"diffusion/02_sde_formulation/sde_QA/#3-fractional-brownian-motion-fbm","title":"3. Fractional Brownian Motion (fBm)","text":"<p>Characteristics:</p> <ul> <li>Long-range dependence: Past affects future over long horizons</li> <li>Non-Markovian: Violates the Markov property</li> <li>Hurst exponent: \\(H \\in (0,1)\\) controls roughness</li> <li>\\(H = 0.5\\): Standard Brownian motion</li> <li>\\(H &lt; 0.5\\): Rough, mean-reverting</li> <li>\\(H &gt; 0.5\\): Smooth, trending</li> </ul> <p>Applications: Rough volatility models in finance, network traffic</p> <p>Why not in diffusion models? Non-Markovian processes don't have simple reverse-time SDEs. The score function would need to depend on the entire history, not just current state.</p>"},{"location":"diffusion/02_sde_formulation/sde_QA/#4-colored-noise","title":"4. Colored Noise","text":"<p>Characteristics:</p> <ul> <li>Correlated increments: \\(\\text{Cov}(dw_t, dw_s) \\neq 0\\) for \\(t \\neq s\\)</li> <li>Violates white-noise assumption: Brownian motion has \"white\" spectrum</li> <li>Frequency-dependent: Different noise at different timescales</li> </ul> <p>Applications: Physical systems with memory, environmental noise</p> <p>Why not in diffusion models? Breaks the mathematical framework. Anderson's reverse-time theorem assumes white noise.</p>"},{"location":"diffusion/02_sde_formulation/sde_QA/#why-diffusion-models-dont-use-these-yet","title":"Why Diffusion Models Don't Use These (Yet)","text":"<p>The fundamental issue is tractability of reverse-time dynamics.</p> <p>Problems with non-Brownian noise:</p> <ol> <li>Reverse-time equations become messy or unknown: No clean formula like Anderson's theorem</li> <li>Score matching may be ill-defined: What is \\(\\nabla_x \\log p_t(x)\\) at a jump?</li> <li>Sampling becomes unstable: Numerical solvers for exotic SDEs are less reliable</li> <li>No closed-form marginals: Can't efficiently generate training samples</li> </ol> <p>The trade-off: Diffusion models sacrifice realism of noise for mathematical control. Brownian motion is \"boring\" but tractable.</p> <p>Future research: Some recent work explores: - L\u00e9vy diffusion models: Incorporating small jumps - Adaptive noise schedules: Learning \\(g(t)\\) instead of fixing it - Non-Markovian extensions: Using neural ODEs with memory</p> <p>But these are still experimental and not widely adopted.</p>"},{"location":"diffusion/02_sde_formulation/sde_QA/#summary-the-big-picture","title":"Summary: The Big Picture","text":"<p>Let's synthesize everything into a coherent view:</p> <p>Core principles:</p> <ol> <li>An SDE defines how probability mass flows over time: From data to noise (forward) and back (reverse)</li> <li>The forward SDE is fixed and simple: You choose \\(f(x,t)\\) and \\(g(t)\\) upfront</li> <li>The only learned object is the score: \\(s_\\theta(x,t) \\approx \\nabla_x \\log p_t(x)\\), a time-dependent vector field</li> <li>Sampling is numerical integration: Solve the reverse SDE using Euler-Maruyama or ODE solvers</li> <li>Brownian motion enables tractability: Reverse-time theory, score matching, and stable training</li> </ol> <p>Why this framework is powerful:</p> <ul> <li>Continuous-time: More general than discrete DDPM</li> <li>Unified: Score-based models, DDPM, and DDIM are all special cases</li> <li>Flexible: Can design custom SDEs for specific applications</li> <li>Interpretable: Clear separation between design choices and learning</li> </ul> <p>Next steps for deeper understanding:</p> <ol> <li>Take a concrete SDE (e.g., VP-SDE)</li> <li>Write down \\(f(x,t)\\) and \\(g(t)\\) explicitly</li> <li>Derive the closed-form marginal \\(p_t(x|x_0)\\)</li> <li>Discretize the reverse SDE into update rules</li> <li>Implement it in code (see <code>02_sde_formulation.ipynb</code>)</li> </ol> <p>That's where everything clicks and stops being abstract. The math becomes concrete, and you can see exactly how DDPM emerges from the SDE formulation.</p>"},{"location":"diffusion/02_sde_formulation/sde_QA/#further-reading","title":"Further Reading","text":"<ul> <li>Song et al. (2021): Score-Based Generative Modeling through SDEs \u2014 The definitive paper</li> <li>Anderson (1982): Reverse-time diffusion equation models \u2014 Original reverse-time theorem</li> <li>\u00d8ksendal (2003): Stochastic Differential Equations \u2014 Comprehensive textbook</li> <li>Karatzas &amp; Shreve (1991): Brownian Motion and Stochastic Calculus \u2014 Advanced reference</li> </ul>"},{"location":"diffusion/02_sde_formulation/sde_formulation/","title":"Understanding Diffusion Models Through Stochastic Differential Equations","text":"<p>A ground-up introduction to the SDE perspective on diffusion models</p> <p>This guide builds intuition for how diffusion models work by starting with stochastic differential equations (SDEs) as a mathematical framework, before connecting to DDPM or score matching. If you're coming from discrete-time DDPM, this will show you the continuous-time view that unifies all diffusion approaches.</p>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#table-of-contents","title":"Table of Contents","text":"<ol> <li>What is an SDE?</li> <li>Understanding Each Symbol</li> <li>What is Chosen vs What is Learned</li> <li>Training Workflow Step-by-Step</li> <li>Sampling Workflow</li> <li>Connection to DDPM</li> <li>Concrete Example: VP-SDE</li> </ol> <p>Companion notebook: <code>02_sde_formulation.ipynb</code> implements these concepts with code and visualizations.</p>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#1-what-is-an-sde","title":"1. What is an SDE?","text":""},{"location":"diffusion/02_sde_formulation/sde_formulation/#starting-with-odes","title":"Starting with ODEs","text":"<p>An ordinary differential equation (ODE) describes deterministic motion:</p> \\[ \\frac{dx(t)}{dt} = f(x(t), t) \\] <p>Given a starting point \\(x(0)\\), the future trajectory is completely determined. Think of a ball rolling down a hill\u2014if you know the initial position and velocity, you can predict exactly where it will be at any time.</p> <p>Example: Exponential decay $$</p> <p>\\frac{dx}{dt} = -\\lambda x \\quad \\Rightarrow \\quad x(t) = x(0) e^{-\\lambda t} $$</p>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#adding-randomness-sdes","title":"Adding Randomness: SDEs","text":"<p>A stochastic differential equation (SDE) adds continuous random noise to this deterministic motion:</p> \\[ dx(t) = f(x(t), t)\\,dt + g(t)\\,dw(t) \\] <p>This describes a random process evolving in time, not a single deterministic trajectory.</p> <p>Key difference:</p> <ul> <li>ODE: One starting point \u2192 one path</li> <li>SDE: One starting point \u2192 distribution over paths</li> </ul> <p>Physical intuition: A particle in a fluid experiences both: - Drift \\(f(x,t)\\): systematic force (gravity, electric field) - Diffusion \\(g(t)\\,dw(t)\\): random collisions with molecules (Brownian motion)</p>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#2-understanding-each-symbol","title":"2. Understanding Each Symbol","text":"<p>Let's break down the SDE equation term by term:</p> \\[ dx(t) = \\underbrace{f(x(t), t)}_{\\text{drift}}\\,dt + \\underbrace{g(t)}_{\\text{diffusion coefficient}}\\,\\underbrace{dw(t)}_{\\text{Brownian motion}} \\]"},{"location":"diffusion/02_sde_formulation/sde_formulation/#21-state-xt","title":"2.1 State: \\(x(t)\\)","text":"<p>What it is:</p> <ul> <li>\\(x(t) \\in \\mathbb{R}^d\\) is the system's state at time \\(t\\)</li> <li>In diffusion models: a noisy image, gene expression vector, or any data</li> <li>Dimension \\(d\\) can be huge (millions for images)</li> </ul> <p>Crucial point: \\(x(t)\\) is a random variable, not a parameter. At each time \\(t\\), there's a probability distribution \\(p_t(x)\\) over possible states.</p> <p>Example: For an image diffusion model: - \\(t=0\\): \\(x(0)\\) is the clean image - \\(t=0.5\\): \\(x(0.5)\\) is partially noisy - \\(t=1\\): \\(x(1)\\) is pure noise</p>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#22-time-t","title":"2.2 Time: \\(t\\)","text":"<p>What it is:</p> <ul> <li>Continuous time variable: \\(t \\in [0, T]\\)</li> <li>Replaces discrete timesteps \\(t = 0, 1, 2, \\ldots, T\\) from DDPM</li> </ul> <p>Important: This is not physical time. It's a continuous index for noise level: - \\(t=0\\): Clean data (no noise) - \\(t=T\\): Pure noise (data destroyed)</p> <p>Think of it as a \"corruption level\" that smoothly varies from 0 to 100%.</p>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#23-brownian-motion-wt","title":"2.3 Brownian Motion: \\(w(t)\\)","text":"<p>What it is: The source of all randomness in the SDE. Also called a Wiener process.</p> <p>Mathematical properties: 1. \\(w(0) = 0\\) (starts at origin) 2. Independent increments: \\(w(t_2) - w(t_1)\\) is independent of \\(w(t_1) - w(t_0)\\) 3. Gaussian increments: \\(w(t + \\Delta t) - w(t) \\sim \\mathcal{N}(0, \\Delta t)\\) 4. Continuous but nowhere differentiable: Infinitely jagged path</p> <p>Intuition: Imagine a drunk person walking\u2014each step is random, independent of previous steps, and the path gets more erratic over time.</p> <p>Key insight: The differential \\(dw(t)\\) behaves like:</p> \\[ dw(t) \\sim \\sqrt{dt} \\cdot \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, I) \\] <p>This is why noise scales with \\(\\sqrt{\\text{time}}\\), not linearly with time.</p> <p>Visualization: If you plot \\(w(t)\\) over \\([0,1]\\): - It starts at 0 - Wanders randomly - Typical displacement after time \\(t\\) is \\(\\mathcal{O}(\\sqrt{t})\\)</p>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#24-diffusion-coefficient-gt","title":"2.4 Diffusion Coefficient: \\(g(t)\\)","text":"<p>What it is: A scalar (or matrix) function that controls how much randomness is injected at time \\(t\\).</p> <p>In the SDE: $$</p> <p>dx = f(x,t)\\,dt + g(t)\\,dw(t) $$</p> <ul> <li>If \\(g(t) = 0\\): Pure ODE (deterministic)</li> <li>Larger \\(g(t)\\): More noise added per unit time</li> </ul> <p>In diffusion models:</p> <ul> <li>\\(g(t)\\) is chosen by you (not learned)</li> <li>It defines the noise schedule</li> <li>Common choices:</li> <li>Constant: \\(g(t) = \\sigma\\) (uniform noise)</li> <li>Increasing: \\(g(t) = \\beta(t)\\) (more noise over time)</li> <li>Variance-preserving: \\(g(t) = \\sqrt{2\\beta(t)}\\)</li> </ul> <p>Connection to DDPM: The discrete noise schedule \\(\\{\\beta_t\\}\\) is the discretized version of \\(g(t)^2\\).</p>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#25-drift-fx-t","title":"2.5 Drift: \\(f(x, t)\\)","text":"<p>What it is: The deterministic component of motion. It's the average direction and speed that \\(x(t)\\) would move if noise were turned off.</p> <p>Mathematical definition: $$ f(x,t) = \\mathbb{E}\\left[\\frac{dx}{dt} \\,\\Big|\\, x(t) = x\\right] $$</p> <p>The expected rate of change at state \\(x\\) and time \\(t\\).</p> <p>Physical intuition:</p> <ul> <li>Gravity: Pulls objects down</li> <li>Friction: Slows motion proportional to velocity</li> <li>Spring force: Pulls toward equilibrium</li> <li>Chemical gradient: Drives diffusion toward lower concentration</li> </ul> <p>In diffusion models (forward process):</p> <p>Common choices: 1. Zero drift: \\(f(x,t) = 0\\) (pure noise, VE-SDE) 2. Linear drift: \\(f(x,t) = -\\frac{1}{2}\\beta(t) x\\) (variance-preserving, VP-SDE)</p> <p>Key point: Like \\(g(t)\\), the drift \\(f(x,t)\\) is chosen, not learned.</p>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#3-what-is-chosen-vs-what-is-learned","title":"3. What is Chosen vs What is Learned?","text":"<p>This is where most confusion arises. Let's be crystal clear.</p>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#forward-sde-always-fixed","title":"Forward SDE (Always Fixed)","text":"<p>You choose:</p> <ul> <li>Drift function: \\(f(x,t)\\)</li> <li>Diffusion coefficient: \\(g(t)\\)</li> </ul> <p>These define a known corruption process that gradually destroys data structure.</p> \\[ dx = f(x,t)\\,dt + g(t)\\,dw(t) \\] <p>No learning happens here. This is your design choice.</p> <p>Example choices:</p> SDE Type Drift \\(f(x,t)\\) Diffusion \\(g(t)\\) Name VP-SDE \\(-\\frac{1}{2}\\beta(t) x\\) \\(\\sqrt{\\beta(t)}\\) Variance-Preserving VE-SDE \\(0\\) \\(\\sqrt{\\frac{d\\sigma^2(t)}{dt}}\\) Variance-Exploding sub-VP \\(-\\frac{1}{2}\\beta(t) x\\) \\(\\sqrt{\\beta(t)(1-e^{-2\\int_0^t \\beta(s)ds})}\\) Sub-VP"},{"location":"diffusion/02_sde_formulation/sde_formulation/#reverse-sde-contains-learning","title":"Reverse SDE (Contains Learning)","text":"<p>The reverse-time SDE (going from noise back to data) is:</p> \\[ dx = \\left[f(x,t) - g(t)^2 \\nabla_x \\log p_t(x)\\right] dt + g(t)\\,d\\bar{w}(t) \\] <p>where \\(\\bar{w}(t)\\) is a reverse-time Brownian motion.</p> <p>Breaking this down:</p> <ul> <li>\\(f(x,t)\\): Known (same as forward)</li> <li>\\(g(t)\\): Known (same as forward)</li> <li>\\(\\nabla_x \\log p_t(x)\\): Unknown \u2014 this is the score function</li> </ul> <p>The only thing we need to learn:</p> \\[ s_\\theta(x, t) \\approx \\nabla_x \\log p_t(x) \\] <p>A neural network \\(s_\\theta\\) that predicts the score (gradient of log-density) at any noise level \\(t\\).</p> <p>Everything else is fixed mathematics.</p>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#4-training-workflow-step-by-step","title":"4. Training Workflow Step-by-Step","text":"<p>Here's exactly how you train a diffusion model in the SDE framework.</p>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#setup-what-you-have-initially","title":"Setup: What You Have Initially","text":"<ol> <li>Dataset: \\(\\{x_0^{(i)}\\}_{i=1}^N\\) sampled from \\(p_{\\text{data}}(x)\\)</li> <li>Chosen forward SDE: \\(dx = f(x,t)\\,dt + g(t)\\,dw(t)\\)</li> </ol> <p>This forward SDE defines: - A family of distributions \\(\\{p_t(x)\\}_{t \\in [0,T]}\\) - One distribution per noise level</p> <p>Important: You don't know \\(p_t(x)\\) analytically (except at \\(t=0\\) and \\(t=T\\)).</p>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#training-loop","title":"Training Loop","text":"<p>For each training iteration:</p>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#step-1-sample-clean-data","title":"Step 1: Sample Clean Data","text":"\\[ x_0 \\sim \\text{training dataset} \\] <p>Pick a random data point from your dataset.</p>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#step-2-sample-a-timestep","title":"Step 2: Sample a Timestep","text":"\\[ t \\sim \\text{Uniform}(0, T) \\] <p>Randomly choose a noise level. This ensures the network learns the score at all noise levels.</p>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#step-3-corrupt-the-data-simulate-forward-sde","title":"Step 3: Corrupt the Data (Simulate Forward SDE)","text":"<p>Generate noisy data by sampling from the conditional distribution:</p> \\[ x_t \\sim p_t(x \\mid x_0) \\] <p>How to do this in practice:</p> <p>For many SDEs (like VP-SDE), the marginal distribution \\(p_t(x \\mid x_0)\\) has a closed form:</p> \\[ x_t = \\alpha(t) x_0 + \\sigma(t) \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, I) \\] <p>where \\(\\alpha(t)\\) and \\(\\sigma(t)\\) are determined by the SDE coefficients \\(f\\) and \\(g\\).</p> <p>Example (VP-SDE): $$</p> <p>\\alpha(t) = e<sup>{-\\frac{1}{2}\\int_0</sup>t \\beta(s)ds}, \\quad \\sigma^2(t) = 1 - \\alpha^2(t) $$</p>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#step-4-compute-target-score","title":"Step 4: Compute Target Score","text":"<p>For Gaussian corruption, the conditional score is:</p> \\[ \\nabla_x \\log p_t(x_t \\mid x_0) = -\\frac{x_t - \\alpha(t) x_0}{\\sigma^2(t)} \\] <p>This is analytically available because we chose the forward process.</p> <p>Connection to noise prediction: $$</p> <p>\\nabla_x \\log p_t(x_t \\mid x_0) = -\\frac{\\varepsilon}{\\sigma(t)} $$</p> <p>where \\(\\varepsilon\\) is the noise we added. So predicting the score is equivalent to predicting the noise!</p>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#step-5-train-neural-network","title":"Step 5: Train Neural Network","text":"<p>Train a score network \\(s_\\theta(x_t, t)\\) to match the target score using denoising score matching:</p> \\[ \\mathcal{L}(\\theta) = \\mathbb{E}_{t, x_0, x_t} \\left[\\lambda(t) \\left\\| s_\\theta(x_t, t) - \\nabla_x \\log p_t(x_t \\mid x_0) \\right\\|^2\\right] \\] <p>where \\(\\lambda(t)\\) is a weighting function (often \\(\\lambda(t) = \\sigma^2(t)\\)).</p> <p>In practice: <pre><code># Pseudocode\nx_0 = sample_from_dataset()\nt = random_uniform(0, T)\nepsilon = random_normal(shape=x_0.shape)\nx_t = alpha(t) * x_0 + sigma(t) * epsilon\n\n# Predict score\nscore_pred = score_network(x_t, t)\n\n# Target score\nscore_target = -epsilon / sigma(t)\n\n# Loss\nloss = mse_loss(score_pred, score_target)\n</code></pre></p> <p>That's it. This is the entire training procedure.</p>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#5-sampling-workflow","title":"5. Sampling Workflow","text":"<p>After training, generate new samples by solving the reverse SDE.</p>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#step-1-start-from-noise","title":"Step 1: Start from Noise","text":"\\[ x_T \\sim \\mathcal{N}(0, I) \\] <p>Sample pure Gaussian noise.</p>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#step-2-solve-reverse-sde","title":"Step 2: Solve Reverse SDE","text":"<p>Numerically integrate the reverse-time SDE from \\(t=T\\) to \\(t=0\\):</p> \\[ dx = \\left[f(x,t) - g(t)^2 s_\\theta(x,t)\\right] dt + g(t)\\,d\\bar{w}(t) \\] <p>Discretization (Euler-Maruyama method):</p> <pre><code>x = sample_noise()\ndt = -T / num_steps\n\nfor i in range(num_steps):\n    t = T - i * dt\n\n    # Drift term\n    drift = f(x, t) - g(t)**2 * score_network(x, t)\n\n    # Diffusion term\n    diffusion = g(t) * random_normal(shape=x.shape)\n\n    # Update\n    x = x + drift * dt + diffusion * sqrt(-dt)\n\nreturn x  # This is x_0 (generated sample)\n</code></pre>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#alternative-probability-flow-ode","title":"Alternative: Probability Flow ODE","text":"<p>You can also use the deterministic probability flow ODE (no stochasticity):</p> \\[ \\frac{dx}{dt} = f(x,t) - \\frac{1}{2}g(t)^2 \\nabla_x \\log p_t(x) \\] <p>This generates samples via a deterministic path (like DDIM).</p>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#6-connection-to-ddpm","title":"6. Connection to DDPM","text":"<p>How does this relate to the discrete-time DDPM you already know?</p> DDPM Concept SDE View Noise schedule \\(\\{\\beta_t\\}\\) Diffusion coefficient \\(g(t)\\) Forward noising \\(q(x_t \\mid x_{t-1})\\) Forward SDE Reverse denoising \\(p_\\theta(x_{t-1} \\mid x_t)\\) Reverse SDE Predicting noise \\(\\varepsilon_\\theta\\) Predicting score \\(s_\\theta\\) DDPM sampling steps Euler-Maruyama discretization DDIM (deterministic) Probability flow ODE <p>Key insight: DDPM is the discretized version of the VP-SDE with specific choices of \\(f\\) and \\(g\\).</p> <p>Specifically:</p> <ul> <li>DDPM uses \\(\\beta_t\\) schedule</li> <li>This corresponds to VP-SDE with:</li> <li>\\(f(x,t) = -\\frac{1}{2}\\beta(t) x\\)</li> <li>\\(g(t) = \\sqrt{\\beta(t)}\\)</li> </ul> <p>Nothing fundamentally new\u2014just a cleaner, more general lens.</p>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#7-concrete-example-vp-sde","title":"7. Concrete Example: VP-SDE","text":"<p>Let's make everything explicit with the Variance-Preserving SDE.</p>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#forward-process","title":"Forward Process","text":"\\[ dx = -\\frac{1}{2}\\beta(t) x\\,dt + \\sqrt{\\beta(t)}\\,dw(t) \\] <p>Why \"variance-preserving\"? The drift term \\(-\\frac{1}{2}\\beta(t) x\\) exactly balances the diffusion to keep \\(\\mathbb{E}[\\|x_t\\|^2]\\) constant.</p>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#marginal-distribution","title":"Marginal Distribution","text":"<p>The transition distribution has closed form:</p> \\[ p_t(x_t \\mid x_0) = \\mathcal{N}\\left(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1-\\bar{\\alpha}_t) I\\right) \\] <p>where:</p> <p>$$</p> <p>\\bar{\\alpha}_t = \\exp\\left(-\\int_0^t \\beta(s)\\,ds\\right) $$</p> <p>Sampling \\(x_t\\): $$</p> <p>x_t = \\sqrt{\\bar{\\alpha}_t}\\, x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\, \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0,I) $$</p>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#score-function","title":"Score Function","text":"<p>The conditional score is:</p> \\[ \\nabla_x \\log p_t(x_t \\mid x_0) = -\\frac{x_t - \\sqrt{\\bar{\\alpha}_t}\\, x_0}{1 - \\bar{\\alpha}_t} = -\\frac{\\varepsilon}{\\sqrt{1-\\bar{\\alpha}_t}} \\]"},{"location":"diffusion/02_sde_formulation/sde_formulation/#reverse-sde","title":"Reverse SDE","text":"\\[ dx = \\left[-\\frac{1}{2}\\beta(t) x - \\beta(t) s_\\theta(x,t)\\right] dt + \\sqrt{\\beta(t)}\\,d\\bar{w}(t) \\]"},{"location":"diffusion/02_sde_formulation/sde_formulation/#training-loss","title":"Training Loss","text":"\\[ \\mathcal{L}(\\theta) = \\mathbb{E}_{t, x_0, \\varepsilon}\\left[(1-\\bar{\\alpha}_t) \\left\\| s_\\theta(x_t, t) + \\frac{\\varepsilon}{\\sqrt{1-\\bar{\\alpha}_t}} \\right\\|^2\\right] \\] <p>This is exactly the DDPM loss (up to weighting).</p>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#summary","title":"Summary","text":"<p>Core concepts: 1. An SDE describes continuous-time random evolution: \\(dx = f(x,t)\\,dt + g(t)\\,dw(t)\\) 2. Drift \\(f(x,t)\\): Deterministic flow (chosen by you) 3. Diffusion \\(g(t)\\): Noise strength (chosen by you) 4. Brownian motion \\(w(t)\\): Source of randomness 5. Score \\(\\nabla_x \\log p_t(x)\\): The only learned object</p> <p>Training = Learning the score at all noise levels</p> <p>Sampling = Solving the reverse SDE numerically</p> <p>Next steps:</p> <ul> <li>See <code>02_sde_formulation.ipynb</code> for code implementation</li> <li>Study specific SDEs: VP-SDE, VE-SDE, sub-VP-SDE</li> <li>Learn about probability flow ODEs and fast sampling</li> <li>Apply to scPPDM (latent-space VP-SDE for drug response)</li> </ul>"},{"location":"diffusion/02_sde_formulation/sde_formulation/#references","title":"References","text":"<ul> <li>Song et al. (2021): Score-Based Generative Modeling through Stochastic Differential Equations</li> <li>Ho et al. (2020): Denoising Diffusion Probabilistic Models</li> <li>Anderson (1982): Reverse-time diffusion equation models</li> <li>\u00d8ksendal (2003): Stochastic Differential Equations: An Introduction with Applications</li> </ul>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/","title":"Forward SDE Design Choices: Understanding \\(f(x,t)\\) and \\(g(t)\\)","text":""},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#core-principle","title":"Core Principle","text":"<p>\\(f(x,t)\\) and \\(g(t)\\) are design choices, not learned parameters.</p> <p>In the forward SDE:</p> \\[ dx = f(x,t)\\,dt + g(t)\\,dw(t) \\] <ul> <li>\\(f(x,t)\\): Drift function \u2014 You choose this</li> <li>\\(g(t)\\): Diffusion coefficient \u2014 You choose this</li> <li>\\(s_\\theta(x,t)\\): Score function \u2014 The only thing the neural network learns</li> </ul> <p>This document explains the common choices for \\(f\\) and \\(g\\), why they work, and how to select them for your application.</p>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#why-do-we-need-to-choose-f-and-g","title":"Why Do We Need to Choose \\(f\\) and \\(g\\)?","text":"<p>The forward SDE defines how clean data is corrupted into noise. Different choices of \\(f\\) and \\(g\\) lead to:</p> <ol> <li>Different noise schedules: How fast data becomes noise</li> <li>Different variance behaviors: Whether variance grows, shrinks, or stays constant</li> <li>Different mathematical properties: Closed-form marginals, ease of sampling, etc.</li> </ol> <p>Key insight: The choice of forward SDE determines: - What the training objective looks like - How the reverse SDE behaves during sampling - Whether we can compute things analytically</p>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#the-three-standard-sdes","title":"The Three Standard SDEs","text":""},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#overview-table","title":"Overview Table","text":"SDE Type \\(f(x,t)\\) \\(g(t)\\) Variance Behavior Used In VP-SDE \\(-\\frac{1}{2}\\beta(t) x\\) \\(\\sqrt{\\beta(t)}\\) Constant (preserved) DDPM, most models VE-SDE \\(0\\) \\(\\sqrt{\\frac{d\\sigma^2(t)}{dt}}\\) Exploding Score-based models sub-VP-SDE \\(-\\frac{1}{2}\\beta(t) x\\) \\(\\sqrt{\\beta(t)(1-e^{-2\\int_0^t \\beta(s)ds})}\\) Decreasing Compromise"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#1-vp-sde-variance-preserving","title":"1. VP-SDE (Variance-Preserving)","text":""},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#definition","title":"Definition","text":"\\[ dx = -\\frac{1}{2}\\beta(t) x\\,dt + \\sqrt{\\beta(t)}\\,dw(t) \\]"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#why-these-specific-functions","title":"Why These Specific Functions?","text":"<p>Drift: \\(f(x,t) = -\\frac{1}{2}\\beta(t) x\\)</p> <ul> <li>Linear in \\(x\\): Pulls state toward origin (shrinks signal)</li> <li>Time-dependent strength: \\(\\beta(t)\\) controls how fast signal decays</li> <li>Factor of \\(\\frac{1}{2}\\): Carefully chosen to balance the diffusion term</li> </ul> <p>Diffusion: \\(g(t) = \\sqrt{\\beta(t)}\\)</p> <ul> <li>Adds noise proportional to \\(\\sqrt{\\beta(t)}\\)</li> <li>Balances drift: The \\(\\frac{1}{2}\\) in drift exactly compensates diffusion growth</li> </ul>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#key-property-variance-preservation","title":"Key Property: Variance Preservation","text":"<p>The drift and diffusion are perfectly balanced so that:</p> \\[ \\mathbb{E}[\\|x_t\\|^2] = \\mathbb{E}[\\|x_0\\|^2] \\quad \\text{for all } t \\] <p>Why this matters:</p> <ul> <li>Data doesn't \"blow up\" or \"collapse\"</li> <li>Stable numerical behavior</li> <li>Clean mathematical analysis</li> </ul>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#closed-form-marginal","title":"Closed-Form Marginal","text":"<p>VP-SDE has a tractable marginal distribution:</p> \\[ p_t(x_t \\mid x_0) = \\mathcal{N}\\left(x_t; \\sqrt{\\bar{\\alpha}_t}\\, x_0, (1-\\bar{\\alpha}_t) I\\right) \\] <p>where:</p> \\[ \\bar{\\alpha}_t = \\exp\\left(-\\int_0^t \\beta(s)\\,ds\\right) \\] <p>Sampling formula:</p> \\[ x_t = \\sqrt{\\bar{\\alpha}_t}\\, x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\, \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0,I) \\] <p>This is exactly the DDPM forward process!</p>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#common-betat-schedules","title":"Common \\(\\beta(t)\\) Schedules","text":"<p>Linear schedule (DDPM):</p> \\[ \\beta(t) = \\beta_{\\min} + (\\beta_{\\max} - \\beta_{\\min}) \\cdot \\frac{t}{T} \\] <p>Typical values: \\(\\beta_{\\min} = 0.1\\), \\(\\beta_{\\max} = 20\\), \\(T = 1\\)</p> <p>Cosine schedule (Improved DDPM):</p> \\[ \\bar{\\alpha}_t = \\frac{f(t)}{f(0)}, \\quad f(t) = \\cos^2\\left(\\frac{t/T + s}{1+s} \\cdot \\frac{\\pi}{2}\\right) \\] <p>where \\(s = 0.008\\) is a small offset.</p>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#when-to-use-vp-sde","title":"When to Use VP-SDE","text":"<p>\u2705 Use VP-SDE when: - You want stable, well-understood behavior - You're implementing DDPM-style models - You need closed-form marginals for efficient training - You want variance to stay bounded</p> <p>\u274c Consider alternatives when: - You need very high noise levels (variance exploding might be better) - You're working with score-based models from scratch</p>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#2-ve-sde-variance-exploding","title":"2. VE-SDE (Variance-Exploding)","text":""},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#definition_1","title":"Definition","text":"\\[ dx = \\sqrt{\\frac{d\\sigma^2(t)}{dt}}\\,dw(t) \\] <p>Key difference: \\(f(x,t) = 0\\) \u2014 No drift!</p>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#why-these-specific-functions_1","title":"Why These Specific Functions?","text":"<p>Drift: \\(f(x,t) = 0\\)</p> <ul> <li>Pure diffusion: No deterministic flow</li> <li>Simpler: Noise is just added, signal isn't shrunk</li> </ul> <p>Diffusion: \\(g(t) = \\sqrt{\\frac{d\\sigma^2(t)}{dt}}\\)</p> <ul> <li>Chosen so that \\(\\text{Var}(x_t) = \\sigma^2(t)\\)</li> <li>Variance grows (explodes) over time</li> </ul>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#key-property-variance-explosion","title":"Key Property: Variance Explosion","text":"\\[ \\mathbb{E}[\\|x_t\\|^2] = \\mathbb{E}[\\|x_0\\|^2] + \\sigma^2(t) \\] <p>Variance grows without bound as \\(t \\to \\infty\\).</p>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#closed-form-marginal_1","title":"Closed-Form Marginal","text":"\\[ p_t(x_t \\mid x_0) = \\mathcal{N}(x_t; x_0, \\sigma^2(t) I) \\] <p>Sampling formula:</p> \\[ x_t = x_0 + \\sigma(t)\\, \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0,I) \\] <p>Simpler than VP-SDE: Signal isn't scaled, just noise is added.</p>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#common-sigmat-schedules","title":"Common \\(\\sigma(t)\\) Schedules","text":"<p>Geometric schedule:</p> \\[ \\sigma(t) = \\sigma_{\\min} \\left(\\frac{\\sigma_{\\max}}{\\sigma_{\\min}}\\right)^{t/T} \\] <p>Typical values: \\(\\sigma_{\\min} = 0.01\\), \\(\\sigma_{\\max} = 50\\), \\(T = 1\\)</p>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#when-to-use-ve-sde","title":"When to Use VE-SDE","text":"<p>\u2705 Use VE-SDE when: - You want very high noise levels at the end - You're implementing score-based models (NCSN) - You prefer simpler forward process (no signal scaling) - You don't mind unbounded variance</p> <p>\u274c Consider alternatives when: - You want bounded variance (use VP-SDE) - You need to match DDPM exactly - Numerical stability is a concern at high noise</p>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#3-sub-vp-sde-sub-variance-preserving","title":"3. sub-VP-SDE (Sub-Variance-Preserving)","text":""},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#definition_2","title":"Definition","text":"\\[ dx = -\\frac{1}{2}\\beta(t) x\\,dt + \\sqrt{\\beta(t)(1-e^{-2\\int_0^t \\beta(s)ds})}\\,dw(t) \\]"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#why-this-specific-form","title":"Why This Specific Form?","text":"<p>Drift: Same as VP-SDE (\\(-\\frac{1}{2}\\beta(t) x\\))</p> <p>Diffusion: Modified to make variance decrease over time</p> <ul> <li>At \\(t=0\\): \\(g(0) = 0\\) (no noise initially)</li> <li>As \\(t \\to \\infty\\): \\(g(t) \\to \\sqrt{\\beta(t)}\\) (approaches VP-SDE)</li> </ul>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#key-property-variance-decreasing","title":"Key Property: Variance Decreasing","text":"\\[ \\mathbb{E}[\\|x_t\\|^2] &lt; \\mathbb{E}[\\|x_0\\|^2] \\quad \\text{for } t &gt; 0 \\] <p>Variance shrinks over time, unlike VP (constant) or VE (exploding).</p>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#when-to-use-sub-vp-sde","title":"When to Use sub-VP-SDE","text":"<p>\u2705 Use sub-VP-SDE when: - You want a compromise between VP and VE - You need variance to decrease - You're experimenting with novel schedules</p> <p>\u274c Consider alternatives when: - You want standard, well-tested behavior (use VP-SDE) - You need maximum simplicity (use VE-SDE)</p> <p>Note: sub-VP-SDE is less commonly used in practice. Most models stick with VP or VE.</p>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#design-considerations-how-to-choose","title":"Design Considerations: How to Choose","text":""},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#1-mathematical-tractability","title":"1. Mathematical Tractability","text":"<p>Question: Do you need closed-form marginals?</p> <ul> <li>Yes \u2192 VP-SDE or VE-SDE (both have closed forms)</li> <li>No \u2192 Can use custom SDEs, but training is harder</li> </ul> <p>Why it matters: Closed-form marginals let you sample \\(x_t\\) directly from \\(x_0\\) during training, avoiding expensive SDE simulation.</p>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#2-variance-behavior","title":"2. Variance Behavior","text":"<p>Question: How should variance evolve?</p> <ul> <li>Constant \u2192 VP-SDE (most stable)</li> <li>Growing \u2192 VE-SDE (higher noise levels)</li> <li>Shrinking \u2192 sub-VP-SDE (less common)</li> </ul> <p>Why it matters: Affects numerical stability and the range of noise levels explored.</p>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#3-connection-to-existing-methods","title":"3. Connection to Existing Methods","text":"<p>Question: Do you want to match a specific algorithm?</p> <ul> <li>DDPM \u2192 VP-SDE with linear \\(\\beta(t)\\)</li> <li>Score-based models (NCSN) \u2192 VE-SDE</li> <li>DDIM \u2192 Probability flow ODE from VP-SDE</li> </ul> <p>Why it matters: Easier to compare results and leverage existing hyperparameters.</p>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#4-signal-to-noise-ratio-snr","title":"4. Signal-to-Noise Ratio (SNR)","text":"<p>The SNR at time \\(t\\) measures how much signal remains:</p> <p>VP-SDE:</p> \\[ \\text{SNR}(t) = \\frac{\\bar{\\alpha}_t}{1-\\bar{\\alpha}_t} \\] <p>VE-SDE:</p> \\[ \\text{SNR}(t) = \\frac{\\mathbb{E}[\\|x_0\\|^2]}{\\sigma^2(t)} \\] <p>Design goal: SNR should decay smoothly from high (clean) to low (noisy).</p>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#practical-recommendations","title":"Practical Recommendations","text":""},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#for-most-applications-use-vp-sde","title":"For Most Applications: Use VP-SDE","text":"<p>Why:</p> <ul> <li>\u2705 Well-understood and widely used</li> <li>\u2705 Stable variance behavior</li> <li>\u2705 Matches DDPM (easy to find good hyperparameters)</li> <li>\u2705 Closed-form marginals (efficient training)</li> </ul> <p>Start with:</p> <ul> <li>Linear \\(\\beta(t)\\): \\(\\beta_{\\min} = 0.1\\), \\(\\beta_{\\max} = 20\\)</li> <li>Or cosine schedule for better performance</li> </ul>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#for-score-based-models-use-ve-sde","title":"For Score-Based Models: Use VE-SDE","text":"<p>Why:</p> <ul> <li>\u2705 Simpler forward process (no signal scaling)</li> <li>\u2705 Very high noise levels (good for score matching)</li> <li>\u2705 Matches NCSN framework</li> </ul> <p>Start with:</p> <ul> <li>Geometric \\(\\sigma(t)\\): \\(\\sigma_{\\min} = 0.01\\), \\(\\sigma_{\\max} = 50\\)</li> </ul>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#for-custom-applications-experiment-carefully","title":"For Custom Applications: Experiment Carefully","text":"<p>Guidelines: 1. Start with VP-SDE as a baseline 2. Ensure closed-form marginals if possible 3. Check SNR decay: Should be smooth and monotonic 4. Verify numerical stability: Test with different step sizes 5. Compare to baselines: Make sure your custom SDE actually helps</p>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#connection-to-the-reverse-sde","title":"Connection to the Reverse SDE","text":"<p>Once you choose \\(f(x,t)\\) and \\(g(t)\\), the reverse SDE is determined by Anderson's theorem:</p> \\[ dx = \\left[f(x,t) - g(t)^2 \\nabla_x \\log p_t(x)\\right]dt + g(t)\\,d\\bar{w}(t) \\] <p>Key point: The reverse SDE inherits \\(f\\) and \\(g\\) from your forward choice. You only need to learn the score \\(\\nabla_x \\log p_t(x)\\).</p>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#example-vp-sde-reverse-process","title":"Example: VP-SDE Reverse Process","text":"<p>Forward:</p> \\[ dx = -\\frac{1}{2}\\beta(t) x\\,dt + \\sqrt{\\beta(t)}\\,dw(t) \\] <p>Reverse:</p> \\[ dx = \\left[-\\frac{1}{2}\\beta(t) x - \\beta(t) s_\\theta(x,t)\\right]dt + \\sqrt{\\beta(t)}\\,d\\bar{w}(t) \\] <p>Notice: - Original drift \\(-\\frac{1}{2}\\beta(t) x\\) appears - Score correction \\(-\\beta(t) s_\\theta(x,t)\\) added (note: \\(g(t)^2 = \\beta(t)\\)) - Same diffusion coefficient \\(\\sqrt{\\beta(t)}\\)</p>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#summary","title":"Summary","text":""},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>\\(f(x,t)\\) and \\(g(t)\\) are design choices, not learned</li> <li>Three standard SDEs:</li> <li>VP-SDE: Variance-preserving, most common, matches DDPM</li> <li>VE-SDE: Variance-exploding, simpler, used in score-based models</li> <li>sub-VP-SDE: Variance-decreasing, less common</li> <li>Choose based on:</li> <li>Mathematical tractability (closed-form marginals?)</li> <li>Variance behavior (constant, growing, shrinking?)</li> <li>Connection to existing methods (DDPM, NCSN?)</li> <li>Signal-to-noise ratio decay</li> <li>Default recommendation: Start with VP-SDE (linear or cosine schedule)</li> </ol>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#whats-fixed-vs-whats-learned","title":"What's Fixed vs What's Learned","text":"<p>Fixed (your design choices):</p> <ul> <li>Forward SDE: \\(f(x,t)\\), \\(g(t)\\)</li> <li>Noise schedule: \\(\\beta(t)\\) or \\(\\sigma(t)\\)</li> <li>Time range: \\([0, T]\\)</li> </ul> <p>Learned (neural network):</p> <ul> <li>Score function: \\(s_\\theta(x,t) \\approx \\nabla_x \\log p_t(x)\\)</li> </ul> <p>Everything else follows: Once you choose the forward SDE and train the score, the reverse SDE is fully determined.</p>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#further-reading","title":"Further Reading","text":"<ul> <li>Song et al. (2021): Score-Based Generative Modeling through SDEs \u2014 Introduces VP-SDE, VE-SDE, and sub-VP-SDE</li> <li>Ho et al. (2020): DDPM \u2014 Original discrete-time formulation (corresponds to VP-SDE)</li> <li>Song &amp; Ermon (2019): NCSN \u2014 Score-based models (corresponds to VE-SDE)</li> <li>Nichol &amp; Dhariwal (2021): Improved DDPM \u2014 Cosine noise schedule</li> </ul>"},{"location":"diffusion/02_sde_formulation/supplements/01_forward_sde_design_choices/#next-steps","title":"Next Steps","text":"<p>Now that you understand the forward SDE design choices, you can:</p> <ol> <li>See them in action: <code>../02_sde_formulation.ipynb</code> implements VP-SDE</li> <li>Understand training: <code>04_training_loss_and_denoising.md</code> shows how the loss depends on \\(g(t)\\)</li> <li>Understand sampling: <code>05_reverse_sde_and_probability_flow_ode.md</code> shows how \\(f\\) and \\(g\\) appear in the reverse SDE</li> </ol>"},{"location":"diffusion/02_sde_formulation/supplements/02_brownian_motion_dimensionality/","title":"Clarification: Dimensionality of Brownian Motion \\(w(t)\\)","text":""},{"location":"diffusion/02_sde_formulation/supplements/02_brownian_motion_dimensionality/#key-point","title":"Key Point","text":"<p>\\(w(t)\\) and \\(x(t)\\) live in the same space: \\(\\mathbb{R}^d\\)</p>"},{"location":"diffusion/02_sde_formulation/supplements/02_brownian_motion_dimensionality/#details","title":"Details","text":"<p>In the SDE formulation:</p> \\[ dx(t) = f(x(t), t)\\,dt + g(t)\\,dw(t) \\] <ul> <li>\\(x(t) \\in \\mathbb{R}^d\\): The state vector (e.g., flattened image, gene expression vector)</li> <li>\\(w(t) \\in \\mathbb{R}^d\\): A d-dimensional Wiener process (Brownian motion)</li> </ul>"},{"location":"diffusion/02_sde_formulation/supplements/02_brownian_motion_dimensionality/#why-this-matters","title":"Why This Matters","text":"<p>The Brownian motion \\(w(t)\\) must be d-dimensional because:</p> <ol> <li>The differential \\(dw(t)\\) behaves like:</li> </ol> <p>$$</p> <p>dw(t) \\sim \\sqrt{dt} \\cdot \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, I_d)    $$</p> <p>where \\(I_d\\) is the \\(d \\times d\\) identity matrix, so \\(\\varepsilon \\in \\mathbb{R}^d\\).</p> <ol> <li>The noise term \\(g(t) \\, dw(t)\\) is added directly to \\(x(t)\\):</li> <li>Both must have the same dimension for the addition to be valid</li> <li> <p>The actual noise added to the image is \\(g(t) \\, dw(t)\\), not just \\(dw(t)\\)</p> </li> <li> <p>For an image diffusion model:</p> </li> <li>If the image is \\(H \\times W \\times C\\) pixels, then \\(d = H \\times W \\times C\\)</li> <li>\\(w(t)\\) is a \\(d\\)-dimensional random walk</li> <li>Each component of \\(w(t)\\) is an independent 1D Brownian motion</li> </ol>"},{"location":"diffusion/02_sde_formulation/supplements/02_brownian_motion_dimensionality/#intuition","title":"Intuition","text":"<ul> <li>\\(w(t)\\): The underlying d-dimensional random walk (source of randomness)</li> <li>\\(g(t)\\): A scalar (or matrix) that scales the noise magnitude</li> <li>\\(g(t) \\, dw(t)\\): The actual noise increment added to the image at time \\(t\\)</li> </ul> <p>Think of it as: \\(w(t)\\) provides the \"direction and magnitude\" of randomness in d-dimensional space, and \\(g(t)\\) controls how much of that randomness gets injected into the image.</p>"},{"location":"diffusion/02_sde_formulation/supplements/02_brownian_motion_dimensionality/#connection-to-the-document","title":"Connection to the Document","text":"<p>This clarification relates to section 2.3 (Brownian Motion) in <code>sde_formulation.md</code>. The document could be more explicit that \\(w(t)\\) is d-dimensional to match \\(x(t)\\).</p>"},{"location":"diffusion/02_sde_formulation/supplements/03_equivalent_parameterizations/","title":"Equivalence of Score, Noise, and Clean Data Parameterizations","text":""},{"location":"diffusion/02_sde_formulation/supplements/03_equivalent_parameterizations/#the-three-parameterizations","title":"The Three Parameterizations","text":"<p>A neural network in a diffusion model can predict any of these:</p> <ol> <li>Score: \\(s_\\theta(x_t, t) \\approx \\nabla_x \\log p_t(x_t)\\)</li> <li>Noise: \\(\\varepsilon_\\theta(x_t, t) \\approx \\varepsilon\\)</li> <li>Clean data: \\(\\hat{x}_0(x_t, t) \\approx x_0\\)</li> </ol> <p>Claim: These are mathematically equivalent\u2014you can convert between them.</p>"},{"location":"diffusion/02_sde_formulation/supplements/03_equivalent_parameterizations/#the-forward-process-starting-point","title":"The Forward Process (Starting Point)","text":"<p>The forward process corrupts clean data \\(x_0\\) into noisy data \\(x_t\\):</p> \\[ x_t = \\sqrt{\\bar{\\alpha}_t}\\, x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\, \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, I) \\] <p>where:</p> <ul> <li>\\(\\bar{\\alpha}_t = \\exp\\left(-\\int_0^t \\beta(s)\\,ds\\right)\\) (cumulative signal retention)</li> <li>\\(\\sqrt{\\bar{\\alpha}_t}\\) scales the signal</li> <li>\\(\\sqrt{1-\\bar{\\alpha}_t}\\) scales the noise</li> </ul> <p>This is the closed-form marginal of the VP-SDE.</p>"},{"location":"diffusion/02_sde_formulation/supplements/03_equivalent_parameterizations/#deriving-the-conditional-score","title":"Deriving the Conditional Score","text":"<p>The conditional distribution is:</p> \\[ p_t(x_t \\mid x_0) = \\mathcal{N}\\left(x_t; \\sqrt{\\bar{\\alpha}_t}\\, x_0, (1-\\bar{\\alpha}_t)\\, I\\right) \\] <p>For a Gaussian \\(\\mathcal{N}(\\mu, \\Sigma)\\), the score is:</p> \\[ \\nabla_x \\log p(x) = -\\Sigma^{-1}(x - \\mu) \\] <p>Applying this:</p> \\[ \\nabla_x \\log p_t(x_t \\mid x_0) = -\\frac{x_t - \\sqrt{\\bar{\\alpha}_t}\\, x_0}{1 - \\bar{\\alpha}_t} \\]"},{"location":"diffusion/02_sde_formulation/supplements/03_equivalent_parameterizations/#the-key-relationship","title":"The Key Relationship","text":"<p>From the forward process:</p> <p>$$</p> <p>x_t - \\sqrt{\\bar{\\alpha}_t}\\, x_0 = \\sqrt{1-\\bar{\\alpha}_t}\\, \\varepsilon $$</p> <p>Substitute into the score:</p> \\[ \\nabla_x \\log p_t(x_t \\mid x_0) = -\\frac{\\sqrt{1-\\bar{\\alpha}_t}\\, \\varepsilon}{1 - \\bar{\\alpha}_t} = \\boxed{-\\frac{\\varepsilon}{\\sqrt{1-\\bar{\\alpha}_t}}} \\] <p>This is the fundamental relationship: Score = scaled noise.</p>"},{"location":"diffusion/02_sde_formulation/supplements/03_equivalent_parameterizations/#conversion-formulas","title":"Conversion Formulas","text":"<p>Let \\(\\sigma_t = \\sqrt{1-\\bar{\\alpha}_t}\\) (noise standard deviation) and \\(\\alpha_t = \\sqrt{\\bar{\\alpha}_t}\\) (signal scale).</p>"},{"location":"diffusion/02_sde_formulation/supplements/03_equivalent_parameterizations/#score-noise","title":"Score \u2194 Noise","text":"\\[ \\boxed{s_\\theta(x_t, t) = -\\frac{\\varepsilon_\\theta(x_t, t)}{\\sigma_t}} \\] \\[ \\boxed{\\varepsilon_\\theta(x_t, t) = -\\sigma_t \\cdot s_\\theta(x_t, t)} \\]"},{"location":"diffusion/02_sde_formulation/supplements/03_equivalent_parameterizations/#noise-clean-data","title":"Noise \u2194 Clean Data","text":"<p>From \\(x_t = \\alpha_t x_0 + \\sigma_t \\varepsilon\\), solve for \\(x_0\\):</p> \\[ \\boxed{\\hat{x}_0 = \\frac{x_t - \\sigma_t \\varepsilon_\\theta(x_t, t)}{\\alpha_t}} \\] \\[ \\boxed{\\varepsilon_\\theta(x_t, t) = \\frac{x_t - \\alpha_t \\hat{x}_0}{\\sigma_t}} \\]"},{"location":"diffusion/02_sde_formulation/supplements/03_equivalent_parameterizations/#score-clean-data","title":"Score \u2194 Clean Data","text":"<p>Combine the above:</p> \\[ \\boxed{\\hat{x}_0 = \\frac{x_t + \\sigma_t^2 s_\\theta(x_t, t)}{\\alpha_t}} \\] \\[ \\boxed{s_\\theta(x_t, t) = \\frac{\\alpha_t \\hat{x}_0 - x_t}{\\sigma_t^2}} \\]"},{"location":"diffusion/02_sde_formulation/supplements/03_equivalent_parameterizations/#summary-table","title":"Summary Table","text":"If you have... To get Score To get Noise To get Clean Data Score \\(s\\) \u2014 \\(\\varepsilon = -\\sigma_t s\\) \\(\\hat{x}_0 = \\frac{x_t + \\sigma_t^2 s}{\\alpha_t}\\) Noise \\(\\varepsilon\\) \\(s = -\\varepsilon/\\sigma_t\\) \u2014 \\(\\hat{x}_0 = \\frac{x_t - \\sigma_t \\varepsilon}{\\alpha_t}\\) Clean Data \\(\\hat{x}_0\\) \\(s = \\frac{\\alpha_t \\hat{x}_0 - x_t}{\\sigma_t^2}\\) \\(\\varepsilon = \\frac{x_t - \\alpha_t \\hat{x}_0}{\\sigma_t}\\) \u2014 <p>Where: \\(\\alpha_t = \\sqrt{\\bar{\\alpha}_t}\\), \\(\\sigma_t = \\sqrt{1-\\bar{\\alpha}_t}\\)</p>"},{"location":"diffusion/02_sde_formulation/supplements/03_equivalent_parameterizations/#why-different-frameworks-use-different-parameterizations","title":"Why Different Frameworks Use Different Parameterizations","text":""},{"location":"diffusion/02_sde_formulation/supplements/03_equivalent_parameterizations/#ddpm-ho-et-al-2020-predicts-noise-varepsilon","title":"DDPM (Ho et al. 2020): Predicts Noise \\(\\varepsilon\\)","text":"<p>Reason: Empirically more stable training. The noise \\(\\varepsilon \\sim \\mathcal{N}(0, I)\\) has a consistent scale across all timesteps, whereas the score magnitude varies with \\(t\\).</p> <p>Loss:</p> \\[ \\mathcal{L}_{\\text{simple}} = \\mathbb{E}_{t, x_0, \\varepsilon}\\left[\\|\\varepsilon - \\varepsilon_\\theta(x_t, t)\\|^2\\right] \\]"},{"location":"diffusion/02_sde_formulation/supplements/03_equivalent_parameterizations/#score-based-models-song-et-al-2019-predicts-score-s","title":"Score-Based Models (Song et al. 2019): Predicts Score \\(s\\)","text":"<p>Reason: Directly motivated by score matching theory. The score has a clear interpretation as the gradient of log-density.</p> <p>Loss (denoising score matching):</p> <p>$$</p> <p>\\mathcal{L}{\\text{DSM}} = \\mathbb{E}\\left[|s_\\theta(x_t, t) - \\nabla_x \\log p_t(x_t \\mid x_0)|^2\\right] $$</p>"},{"location":"diffusion/02_sde_formulation/supplements/03_equivalent_parameterizations/#v-prediction-salimans-ho-2022-predicts-a-combination","title":"v-prediction (Salimans &amp; Ho 2022): Predicts a Combination","text":"<p>Why? For some noise schedules, predicting a linear combination of noise and data works better:</p> <p>$$</p> <p>v_t = \\alpha_t \\varepsilon - \\sigma_t x_0 $$</p> <p>This balances the learning signal across timesteps.</p>"},{"location":"diffusion/02_sde_formulation/supplements/03_equivalent_parameterizations/#intuitive-understanding","title":"Intuitive Understanding","text":"<p>All three quantities answer the same question from different angles:</p> Parameterization Question Answered Score \\(\\nabla_x \\log p_t(x)\\) \"Which direction increases probability?\" Noise \\(\\varepsilon\\) \"What random noise was added to the clean data?\" Clean data \\(x_0\\) \"What was the original data before corruption?\" <p>Given the forward process, knowing any one of these determines the other two.</p>"},{"location":"diffusion/02_sde_formulation/supplements/03_equivalent_parameterizations/#practical-example","title":"Practical Example","text":"<p>Suppose at timestep \\(t\\): - \\(\\bar{\\alpha}_t = 0.5\\) (so \\(\\alpha_t = \\sqrt{0.5} \\approx 0.707\\), \\(\\sigma_t = \\sqrt{0.5} \\approx 0.707\\)) - Current noisy state: \\(x_t = [1.0, 2.0]\\) - Network predicts noise: \\(\\varepsilon_\\theta = [0.5, 1.0]\\)</p> <p>Then: - Score: \\(s = -\\varepsilon/\\sigma_t = -[0.5, 1.0]/0.707 \\approx [-0.707, -1.414]\\) - Clean data: \\(\\hat{x}_0 = (x_t - \\sigma_t \\varepsilon)/\\alpha_t = ([1.0, 2.0] - 0.707 \\cdot [0.5, 1.0])/0.707 \\approx [0.914, 1.828]\\)</p> <p>All three representations contain the same information, just expressed differently.</p>"},{"location":"diffusion/02_sde_formulation/supplements/03_equivalent_parameterizations/#key-takeaway","title":"Key Takeaway","text":"<p>The forward process equation:</p> \\[ x_t = \\sqrt{\\bar{\\alpha}_t}\\, x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\, \\varepsilon \\] <p>is the Rosetta Stone that connects all three parameterizations. Given any two of \\((x_t, x_0, \\varepsilon)\\) and the noise schedule \\((\\alpha_t, \\sigma_t)\\), you can compute the third\u2014and from there, derive the score.</p>"},{"location":"diffusion/02_sde_formulation/supplements/03_equivalent_parameterizations/#references","title":"References","text":"<ul> <li>Ho et al. (2020): DDPM \u2014 Uses noise prediction</li> <li>Song &amp; Ermon (2019): Score-based models \u2014 Uses score prediction</li> <li>Salimans &amp; Ho (2022): Progressive Distillation \u2014 Introduces v-prediction</li> <li>Karras et al. (2022): EDM \u2014 Analyzes different parameterizations systematically</li> </ul>"},{"location":"diffusion/02_sde_formulation/supplements/04_training_loss_and_denoising/","title":"Understanding the Training Loss: How Learning to Predict Score = Learning to Denoise","text":""},{"location":"diffusion/02_sde_formulation/supplements/04_training_loss_and_denoising/#the-loss-function","title":"The Loss Function","text":"\\[ \\mathcal{L} = \\|s_\\theta(x_t, t) - (-\\varepsilon/\\sigma_t)\\|^2 \\]"},{"location":"diffusion/02_sde_formulation/supplements/04_training_loss_and_denoising/#what-each-term-means","title":"What Each Term Means","text":"<ul> <li>\\(s_\\theta(x_t, t)\\): The neural network's prediction of the score at noisy state \\(x_t\\) and time \\(t\\)</li> <li>\\(-\\varepsilon/\\sigma_t\\): The true target score (what we want the network to predict)</li> <li>\\(\\varepsilon\\): The noise that was added to create \\(x_t\\) from \\(x_0\\)</li> <li>\\(\\sigma_t = \\sqrt{1-\\bar{\\alpha}_t}\\): The noise standard deviation at time \\(t\\)</li> </ul>"},{"location":"diffusion/02_sde_formulation/supplements/04_training_loss_and_denoising/#what-the-loss-measures","title":"What the Loss Measures","text":"<p>The loss measures: How well does the network predict the true score?</p> <p>When the loss is small, \\(s_\\theta(x_t, t) \\approx -\\varepsilon/\\sigma_t\\), meaning the network has learned to identify: - Which direction points toward higher probability (the score) - Which is equivalent to identifying the noise that was added</p>"},{"location":"diffusion/02_sde_formulation/supplements/04_training_loss_and_denoising/#why-varepsilonsigma_t-is-the-target-score","title":"Why \\(-\\varepsilon/\\sigma_t\\) Is the Target Score","text":""},{"location":"diffusion/02_sde_formulation/supplements/04_training_loss_and_denoising/#step-1-the-forward-process","title":"Step 1: The Forward Process","text":"<p>We corrupt clean data \\(x_0\\) into noisy data \\(x_t\\):</p> \\[ x_t = \\alpha_t x_0 + \\sigma_t \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, I) \\] <p>where \\(\\alpha_t = \\sqrt{\\bar{\\alpha}_t}\\) and \\(\\sigma_t = \\sqrt{1-\\bar{\\alpha}_t}\\).</p>"},{"location":"diffusion/02_sde_formulation/supplements/04_training_loss_and_denoising/#step-2-the-conditional-distribution","title":"Step 2: The Conditional Distribution","text":"<p>Given \\(x_0\\), the noisy state \\(x_t\\) follows:</p> \\[ p_t(x_t \\mid x_0) = \\mathcal{N}(x_t; \\alpha_t x_0, \\sigma_t^2 I) \\]"},{"location":"diffusion/02_sde_formulation/supplements/04_training_loss_and_denoising/#step-3-computing-the-score","title":"Step 3: Computing the Score","text":"<p>For a Gaussian \\(\\mathcal{N}(\\mu, \\Sigma)\\), the score is:</p> \\[ \\nabla_x \\log p(x) = -\\Sigma^{-1}(x - \\mu) \\] <p>Applying this:</p> \\[ \\nabla_x \\log p_t(x_t \\mid x_0) = -\\frac{x_t - \\alpha_t x_0}{\\sigma_t^2} \\]"},{"location":"diffusion/02_sde_formulation/supplements/04_training_loss_and_denoising/#step-4-expressing-in-terms-of-noise","title":"Step 4: Expressing in Terms of Noise","text":"<p>From the forward process: \\(x_t - \\alpha_t x_0 = \\sigma_t \\varepsilon\\)</p> <p>Substitute:</p> \\[ \\nabla_x \\log p_t(x_t \\mid x_0) = -\\frac{\\sigma_t \\varepsilon}{\\sigma_t^2} = \\boxed{-\\frac{\\varepsilon}{\\sigma_t}} \\] <p>This is why the target is \\(-\\varepsilon/\\sigma_t\\): It's the analytical score for the conditional distribution \\(p_t(x_t \\mid x_0)\\).</p>"},{"location":"diffusion/02_sde_formulation/supplements/04_training_loss_and_denoising/#why-minimizing-this-loss-teaches-denoising","title":"Why Minimizing This Loss Teaches Denoising","text":""},{"location":"diffusion/02_sde_formulation/supplements/04_training_loss_and_denoising/#the-key-insight-score-denoising-direction","title":"The Key Insight: Score = Denoising Direction","text":"<p>The score function \\(\\nabla_x \\log p_t(x)\\) points in the direction of steepest increase in log probability. In the context of diffusion:</p> <ul> <li>Higher probability regions = regions with more data-like structure</li> <li>Lower probability regions = regions with more noise</li> <li>Following the score = moving from noise toward data = denoising</li> </ul>"},{"location":"diffusion/02_sde_formulation/supplements/04_training_loss_and_denoising/#the-training-process","title":"The Training Process","text":"<ol> <li>We know the noise \\(\\varepsilon\\) (we added it!)</li> <li>We compute the true score \\(-\\varepsilon/\\sigma_t\\) (analytically)</li> <li>We train the network to predict this score</li> <li>The network learns to identify the denoising direction</li> </ol>"},{"location":"diffusion/02_sde_formulation/supplements/04_training_loss_and_denoising/#what-happens-during-training","title":"What Happens During Training","text":"<p>At each iteration:</p> <pre><code>1. Start with clean data: x_0\n2. Add noise: x_t = \u03b1_t x_0 + \u03c3_t \u03b5\n3. Network sees: (x_t, t)\n4. Network predicts: s_\u03b8(x_t, t) \u2248 -\u03b5/\u03c3_t\n5. Loss measures: ||s_\u03b8(x_t, t) - (-\u03b5/\u03c3_t)||\u00b2\n6. Backprop updates \u03b8 to reduce loss\n</code></pre> <p>After many iterations, the network learns: - Given any noisy \\(x_t\\) at any time \\(t\\) - Predict the score \\(s_\\theta(x_t, t)\\) that points toward cleaner data</p>"},{"location":"diffusion/02_sde_formulation/supplements/04_training_loss_and_denoising/#why-this-works-the-reverse-sde-connection","title":"Why This Works: The Reverse SDE Connection","text":"<p>During generation, we solve the reverse SDE:</p> \\[ dx = \\left[f(x,t) - g(t)^2 s_\\theta(x,t)\\right]dt + g(t)\\,dw \\] <p>Important reminder: \\(f(x,t)\\) and \\(g(t)\\) are design choices (not learned). See <code>01_forward_sde_design_choices.md</code> for details on how to choose them.</p> <p>The term \\(-g(t)^2 s_\\theta(x,t)\\) is the denoising force: - \\(s_\\theta(x,t)\\) points toward higher probability (less noise) \u2014 This is learned - \\(g(t)^2\\) scales it appropriately \u2014 This is fixed (from your forward SDE choice) - This term pulls the sample from noise toward data</p> <p>The network learned to denoise during training because: - Training: Learn to predict the score (which points toward \\(x_0\\)) - Generation: Use the score in the reverse SDE to actually denoise</p> <p>Key insight: The reverse SDE inherits \\(f(x,t)\\) and \\(g(t)\\) from your forward SDE design. You only train the score \\(s_\\theta(x,t)\\).</p>"},{"location":"diffusion/02_sde_formulation/supplements/04_training_loss_and_denoising/#the-learning-objective-denoising-score-matching","title":"The Learning Objective: Denoising Score Matching","text":"<p>The loss function implements denoising score matching (Vincent, 2011):</p>"},{"location":"diffusion/02_sde_formulation/supplements/04_training_loss_and_denoising/#the-general-principle","title":"The General Principle","text":"<p>Instead of learning the score of the marginal \\(p_t(x)\\) (hard\u2014we don't have samples), we learn the score of the conditional \\(p_t(x \\mid x_0)\\) (easy\u2014we know \\(x_0\\)).</p>"},{"location":"diffusion/02_sde_formulation/supplements/04_training_loss_and_denoising/#why-this-is-equivalent","title":"Why This Is Equivalent","text":"<p>Under mild conditions, learning the conditional score at all \\((x_0, t)\\) pairs is equivalent to learning the marginal score:</p> \\[ \\mathbb{E}_{x_0 \\sim p_{\\text{data}}} \\left[\\nabla_x \\log p_t(x_t \\mid x_0)\\right] = \\nabla_x \\log p_t(x_t) \\] <p>Intuition: The marginal score is the average of conditional scores over all possible \\(x_0\\).</p>"},{"location":"diffusion/02_sde_formulation/supplements/04_training_loss_and_denoising/#the-training-objective","title":"The Training Objective","text":"\\[ \\mathcal{L}(\\theta) = \\mathbb{E}_{t, x_0, \\varepsilon} \\left[\\lambda(t) \\left\\| s_\\theta(x_t, t) - \\nabla_x \\log p_t(x_t \\mid x_0) \\right\\|^2\\right] \\] <p>where:</p> <ul> <li>\\(t \\sim \\text{Uniform}(0, T)\\): Random timestep</li> <li>\\(x_0 \\sim p_{\\text{data}}\\): Clean data sample</li> <li>\\(\\varepsilon \\sim \\mathcal{N}(0, I)\\): Random noise</li> <li>\\(x_t = \\alpha_t x_0 + \\sigma_t \\varepsilon\\): Noisy data</li> <li>\\(\\lambda(t)\\): Weighting function (often \\(\\lambda(t) = \\sigma_t^2\\))</li> </ul>"},{"location":"diffusion/02_sde_formulation/supplements/04_training_loss_and_denoising/#why-weight-by-lambdat-sigma_t2","title":"Why Weight by \\(\\lambda(t) = \\sigma_t^2\\)?","text":"<p>The weighting compensates for the fact that: - At high noise (\\(\\sigma_t\\) large), the score magnitude is smaller (\\(\\propto 1/\\sigma_t\\)) - Without weighting, the loss would be dominated by low-noise timesteps - Weighting by \\(\\sigma_t^2\\) balances learning across all noise levels</p>"},{"location":"diffusion/02_sde_formulation/supplements/04_training_loss_and_denoising/#the-complete-picture-training-generation","title":"The Complete Picture: Training \u2192 Generation","text":""},{"location":"diffusion/02_sde_formulation/supplements/04_training_loss_and_denoising/#training-phase","title":"Training Phase","text":"<pre><code>Goal: Learn s_\u03b8(x_t, t) \u2248 \u2207_x log p_t(x_t)\n\nMethod:\n1. Sample (x_0, t, \u03b5)\n2. Create x_t = \u03b1_t x_0 + \u03c3_t \u03b5\n3. Compute target: -\u03b5/\u03c3_t\n4. Predict: s_\u03b8(x_t, t)\n5. Minimize: ||s_\u03b8(x_t, t) - (-\u03b5/\u03c3_t)||\u00b2\n</code></pre> <p>Result: Network learns to identify denoising direction at all noise levels.</p>"},{"location":"diffusion/02_sde_formulation/supplements/04_training_loss_and_denoising/#generation-phase","title":"Generation Phase","text":"<pre><code>Goal: Generate x_0 from noise x_T\n\nMethod:\n1. Start: x_T ~ N(0, I)\n2. Solve reverse SDE:\n   dx = [f(x,t) - g(t)\u00b2 s_\u03b8(x,t)] dt + g(t) dw\n3. The term -g(t)\u00b2 s_\u03b8(x,t) denoises by following the score\n4. End: x_0 (generated sample)\n</code></pre> <p>Result: Network's learned score guides the denoising process.</p>"},{"location":"diffusion/02_sde_formulation/supplements/04_training_loss_and_denoising/#intuitive-analogy","title":"Intuitive Analogy","text":"<p>Think of training a compass:</p> <ol> <li>Training: </li> <li>You're in a foggy forest (noisy \\(x_t\\))</li> <li>You know where you started (\\(x_0\\))</li> <li>You know which way is \"home\" (the score \\(-\\varepsilon/\\sigma_t\\))</li> <li> <p>You train a compass (network) to point home</p> </li> <li> <p>Generation:</p> </li> <li>You're lost in fog (pure noise \\(x_T\\))</li> <li>You use the compass (learned score \\(s_\\theta\\))</li> <li>You follow it step by step (solve reverse SDE)</li> <li>You reach home (clean data \\(x_0\\))</li> </ol> <p>The loss function is teaching the compass to always point toward home, regardless of where you are in the fog.</p>"},{"location":"diffusion/02_sde_formulation/supplements/04_training_loss_and_denoising/#why-this-is-better-than-direct-denoising","title":"Why This Is Better Than Direct Denoising","text":"<p>You might wonder: \"Why not just train a network to predict \\(x_0\\) from \\(x_t\\) directly?\"</p>"},{"location":"diffusion/02_sde_formulation/supplements/04_training_loss_and_denoising/#problems-with-direct-prediction","title":"Problems with Direct Prediction","text":"<ol> <li>Mode collapse: The network might average over multiple possible \\(x_0\\) values</li> <li>Blurry outputs: Averaging creates smooth but unrealistic images</li> <li>No diversity: Deterministic mapping \\(x_t \\to x_0\\) gives same output every time</li> </ol>"},{"location":"diffusion/02_sde_formulation/supplements/04_training_loss_and_denoising/#advantages-of-score-matching","title":"Advantages of Score Matching","text":"<ol> <li>Learns the gradient field: Captures the structure of the data manifold</li> <li>Stochastic generation: The reverse SDE adds noise, creating diverse samples</li> <li>Better mode coverage: The score field guides toward all modes, not just averages</li> </ol>"},{"location":"diffusion/02_sde_formulation/supplements/04_training_loss_and_denoising/#summary","title":"Summary","text":"Question Answer What does the loss measure? How well the network predicts the true score Why is the target \\(-\\varepsilon/\\sigma_t\\)? It's the analytical score of \\(p_t(x_t \\mid x_0)\\) How does this teach denoising? The score points toward data; learning the score = learning denoising direction Why backpropagation works? Minimizing loss makes \\(s_\\theta\\) approximate the true score at all \\((x_t, t)\\) How is this used in generation? The learned score guides the reverse SDE to denoise from \\(x_T\\) to \\(x_0\\) <p>The magic: By learning to predict the score (which we can compute analytically during training), the network implicitly learns to denoise, even though it never sees a denoising task during training!</p>"},{"location":"diffusion/02_sde_formulation/supplements/04_training_loss_and_denoising/#references","title":"References","text":"<ul> <li>Vincent (2011): \"A Connection Between Score Matching and Denoising Autoencoders\" \u2014 Original denoising score matching</li> <li>Song &amp; Ermon (2019): \"Generative Modeling by Estimating Gradients of the Data Distribution\" \u2014 Score-based generative models</li> <li>Ho et al. (2020): \"Denoising Diffusion Probabilistic Models\" \u2014 DDPM (equivalent to score matching)</li> <li>Song et al. (2021): \"Score-Based Generative Modeling through SDEs\" \u2014 SDE formulation</li> </ul>"},{"location":"diffusion/02_sde_formulation/supplements/05_reverse_sde_and_probability_flow_ode/","title":"Understanding the Reverse SDE and Probability Flow ODE","text":""},{"location":"diffusion/02_sde_formulation/supplements/05_reverse_sde_and_probability_flow_ode/#the-reverse-time-sde","title":"The Reverse-Time SDE","text":""},{"location":"diffusion/02_sde_formulation/supplements/05_reverse_sde_and_probability_flow_ode/#the-formula","title":"The Formula","text":"\\[ dx = \\left[f(x,t) - g(t)^2 \\nabla_x \\log p_t(x)\\right]dt + g(t)\\,dw \\]"},{"location":"diffusion/02_sde_formulation/supplements/05_reverse_sde_and_probability_flow_ode/#term-by-term-interpretation","title":"Term-by-Term Interpretation","text":"<p>The infinitesimal change in \\(x\\) (the data) is the sum of:</p> <ol> <li>\\(f(x,t)\\,dt\\) \u2014 The original drift from the forward process</li> <li>This term \"continues\" the deterministic flow from the forward SDE</li> <li>In VP-SDE: \\(f(x,t) = -\\frac{1}{2}\\beta(t)x\\) (shrinks toward origin)</li> <li> <p>In VE-SDE: \\(f(x,t) = 0\\) (no drift)</p> </li> <li> <p>\\(-g(t)^2 \\nabla_x \\log p_t(x)\\,dt\\) \u2014 The score correction (the key term!)</p> </li> <li>Points toward regions of higher probability density</li> <li>\\(\\nabla_x \\log p_t(x)\\) is the score function: \"which direction makes \\(x\\) more likely?\"</li> <li>Scaled by \\(g(t)^2\\): stronger correction when noise is high</li> <li> <p>This term reverses the diffusion: it \"undoes\" the noise</p> </li> <li> <p>\\(g(t)\\,dw\\) \u2014 Stochastic noise (Brownian motion)</p> </li> <li>Same diffusion coefficient as the forward process</li> <li>Maintains diversity in generated samples</li> <li>\\(dw\\) is a reverse-time Brownian increment</li> </ol>"},{"location":"diffusion/02_sde_formulation/supplements/05_reverse_sde_and_probability_flow_ode/#intuition","title":"Intuition","text":"<p>Think of it as:</p> \\[ dx = \\underbrace{f(x,t)\\,dt}_{\\text{continue forward drift}} + \\underbrace{(-g(t)^2 \\nabla_x \\log p_t(x))\\,dt}_{\\text{denoise: move toward data}} + \\underbrace{g(t)\\,dw}_{\\text{add randomness}} \\] <p>The score term is what makes generation possible. Without it, we'd just be running the forward process in reverse time, which wouldn't recover data from noise.</p>"},{"location":"diffusion/02_sde_formulation/supplements/05_reverse_sde_and_probability_flow_ode/#where-does-the-probability-flow-ode-come-from","title":"Where Does the Probability Flow ODE Come From?","text":""},{"location":"diffusion/02_sde_formulation/supplements/05_reverse_sde_and_probability_flow_ode/#the-formula_1","title":"The Formula","text":"\\[ dx = \\left[f(x,t) - \\tfrac{1}{2}g(t)^2 \\nabla_x \\log p_t(x)\\right]dt \\] <p>Note: No \\(dw\\) term \u2014 this is deterministic!</p>"},{"location":"diffusion/02_sde_formulation/supplements/05_reverse_sde_and_probability_flow_ode/#the-origin-andersons-result-1982","title":"The Origin: Anderson's Result (1982)","text":"<p>The probability flow ODE comes from a deep result in stochastic calculus:</p> <p>Key theorem (Anderson 1982): For any SDE, there exists a corresponding ODE that produces the same marginal distributions \\(p_t(x)\\) at each time \\(t\\).</p> <p>This is not obvious! The SDE has randomness; the ODE has none. Yet they produce the same distribution over states.</p>"},{"location":"diffusion/02_sde_formulation/supplements/05_reverse_sde_and_probability_flow_ode/#the-derivation-high-level","title":"The Derivation (High-Level)","text":"<ol> <li>Start with the reverse SDE:</li> </ol> <p>$$</p> <p>dx = \\left[f(x,t) - g(t)^2 \\nabla_x \\log p_t(x)\\right]dt + g(t)\\,dw    $$</p> <ol> <li>The Fokker-Planck equation describes how probability density evolves:</li> </ol> <p>$$</p> <p>\\frac{\\partial p_t}{\\partial t} = -\\nabla \\cdot (f \\cdot p_t) + \\frac{1}{2}g(t)^2 \\nabla^2 p_t    $$</p> <ol> <li>Key insight: The diffusion term \\(\\frac{1}{2}g(t)^2 \\nabla^2 p_t\\) can be rewritten as a drift term involving the score:</li> </ol> <p>$$</p> <p>\\frac{1}{2}g(t)^2 \\nabla^2 p_t = \\nabla \\cdot \\left(\\frac{1}{2}g(t)^2 \\nabla p_t\\right) = \\nabla \\cdot \\left(\\frac{1}{2}g(t)^2 p_t \\nabla \\log p_t\\right)    $$</p> <ol> <li>Absorb diffusion into drift: Combine the original drift with this \"effective drift\" from diffusion:</li> </ol> <p>$$</p> <p>\\text{Effective drift} = f(x,t) - \\frac{1}{2}g(t)^2 \\nabla_x \\log p_t(x)    $$</p> <ol> <li>Result: An ODE with this effective drift produces the same Fokker-Planck equation, hence the same \\(p_t(x)\\).</li> </ol>"},{"location":"diffusion/02_sde_formulation/supplements/05_reverse_sde_and_probability_flow_ode/#why-factor-of-12","title":"Why Factor of \u00bd?","text":"<p>Compare the two:</p> Equation Score coefficient Has noise? Reverse SDE \\(g(t)^2\\) Yes (\\(+g(t)dw\\)) Probability Flow ODE \\(\\frac{1}{2}g(t)^2\\) No <p>The factor of \\(\\frac{1}{2}\\) compensates for the missing stochastic term. Roughly: - In the SDE, noise contributes to spreading probability mass - In the ODE, the score term must do all the work of shaping the distribution - But without noise, you need less \"pull\" from the score (hence \\(\\frac{1}{2}\\))</p>"},{"location":"diffusion/02_sde_formulation/supplements/05_reverse_sde_and_probability_flow_ode/#connection-between-sde-and-ode","title":"Connection Between SDE and ODE","text":""},{"location":"diffusion/02_sde_formulation/supplements/05_reverse_sde_and_probability_flow_ode/#same-marginals-different-paths","title":"Same Marginals, Different Paths","text":"<p>Both equations produce samples from the same distribution \\(p_0(x)\\) (data distribution), but:</p> Property Reverse SDE Probability Flow ODE Paths Random (different each run) Deterministic (same every time) Diversity High (noise adds variety) Lower (same input \u2192 same output) Speed Slower (needs small \\(dt\\)) Faster (can use larger steps) Interpolation Not meaningful Latent space is well-defined"},{"location":"diffusion/02_sde_formulation/supplements/05_reverse_sde_and_probability_flow_ode/#why-have-both","title":"Why Have Both?","text":"<p>SDE sampling:</p> <ul> <li>More diverse outputs</li> <li>Better for creative generation</li> <li>Theoretically \"correct\" reverse process</li> </ul> <p>ODE sampling:</p> <ul> <li>Faster (DDIM is based on this)</li> <li>Deterministic: good for reproducibility</li> <li>Enables latent space manipulation</li> <li>Can use advanced ODE solvers (Runge-Kutta, etc.)</li> </ul>"},{"location":"diffusion/02_sde_formulation/supplements/05_reverse_sde_and_probability_flow_ode/#practical-implications","title":"Practical Implications","text":""},{"location":"diffusion/02_sde_formulation/supplements/05_reverse_sde_and_probability_flow_ode/#for-ddpmddim","title":"For DDPM/DDIM","text":"<ul> <li>DDPM: Discretized reverse SDE (with noise)</li> <li>DDIM: Discretized probability flow ODE (deterministic)</li> <li>Both use the same trained model \\(s_\\theta(x,t)\\)</li> </ul>"},{"location":"diffusion/02_sde_formulation/supplements/05_reverse_sde_and_probability_flow_ode/#for-fast-sampling","title":"For Fast Sampling","text":"<p>ODE formulation enables: - Larger time steps (less sensitive to discretization) - Adaptive step-size solvers - Fewer neural network evaluations (faster generation)</p>"},{"location":"diffusion/02_sde_formulation/supplements/05_reverse_sde_and_probability_flow_ode/#the-score-is-everything","title":"The Score Is Everything","text":"<p>In both formulations, the score function \\(\\nabla_x \\log p_t(x)\\) is the only learned quantity. Once you have it, you can choose either: - Stochastic sampling (SDE) - Deterministic sampling (ODE)</p>"},{"location":"diffusion/02_sde_formulation/supplements/05_reverse_sde_and_probability_flow_ode/#summary","title":"Summary","text":"Question Answer What does the reverse SDE compute? \\(dx =\\) (continue drift) + (denoise via score) + (add noise) Where does the ODE come from? Fokker-Planck: absorb diffusion into an effective drift Why factor of \u00bd? Compensates for missing stochastic term Which to use? SDE for diversity, ODE for speed/determinism"},{"location":"diffusion/02_sde_formulation/supplements/05_reverse_sde_and_probability_flow_ode/#references","title":"References","text":"<ul> <li>Anderson (1982): \"Reverse-time diffusion equation models\" \u2014 Original derivation of reverse-time dynamics</li> <li>Song et al. (2021): Score-Based Generative Modeling through SDEs \u2014 Modern treatment with probability flow ODE</li> <li>Ho et al. (2020): Denoising Diffusion Probabilistic Models \u2014 DDPM as discretized SDE</li> </ul>"},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/","title":"The Fokker-Planck Equation and the Effective Drift","text":""},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#overview","title":"Overview","text":"<p>The Fokker-Planck equation (FPE) is the bridge between: - SDEs: Describe individual particle/sample trajectories - PDEs: Describe how the probability density of all particles evolves</p> <p>This is the key to understanding why the probability flow ODE exists and why its effective drift has a specific form.</p>"},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#the-fokker-planck-equation","title":"The Fokker-Planck Equation","text":""},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#the-formula","title":"The Formula","text":"<p>For the SDE:</p> \\[ dx = f(x,t)\\,dt + g(t)\\,dw \\] <p>The corresponding Fokker-Planck equation is:</p> \\[ \\frac{\\partial p_t(x)}{\\partial t} = -\\nabla \\cdot \\left(f(x,t) \\cdot p_t(x)\\right) + \\frac{1}{2}g(t)^2 \\nabla^2 p_t(x) \\]"},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#what-this-equation-describes","title":"What This Equation Describes","text":"<p>The FPE tells us how the probability density \\(p_t(x)\\) changes over time.</p> <ul> <li>Left side: Rate of change of probability density at location \\(x\\)</li> <li>Right side: Two effects that cause probability to flow</li> </ul>"},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#term-by-term-interpretation","title":"Term-by-Term Interpretation","text":""},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#term-1-drift-advection","title":"Term 1: Drift (Advection)","text":"\\[ -\\nabla \\cdot \\left(f(x,t) \\cdot p_t(x)\\right) \\] <p>What it means: Probability is carried along by the drift \\(f(x,t)\\).</p> <p>Physical analogy: Imagine probability as a fluid. The drift \\(f\\) is like a current that pushes the fluid. Where the current converges, probability accumulates; where it diverges, probability spreads out.</p> <p>Mathematical structure:</p> <ul> <li>\\(f(x,t) \\cdot p_t(x)\\) is the probability flux (probability per unit time per unit area flowing due to drift)</li> <li>\\(\\nabla \\cdot (\\cdot)\\) is the divergence operator</li> <li>The negative sign means: if flux flows out of a region (positive divergence), probability decreases there</li> </ul> <p>Expansion (for scalar case):</p> \\[ -\\nabla \\cdot (f \\cdot p) = -\\frac{\\partial}{\\partial x}(f \\cdot p) = -f \\frac{\\partial p}{\\partial x} - p \\frac{\\partial f}{\\partial x} \\] <p>The first term is advection (probability carried by flow), the second accounts for varying drift.</p>"},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#clarification-the-drift-fxt","title":"Clarification: The Drift \\(f(x,t)\\)","text":""},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#notation-is-f-cdot-p-a-dot-product","title":"Notation: Is \\(f \\cdot p\\) a Dot Product?","text":"<p>No. The notation \\(f(x,t) \\cdot p_t(x)\\) is scalar multiplication, not a dot product:</p> <ul> <li>\\(f(x,t) \\in \\mathbb{R}^d\\) is a vector (the drift field)</li> <li>\\(p_t(x) \\in \\mathbb{R}\\) is a scalar (probability density)</li> <li>\\(f \\cdot p\\) means: multiply each component of the vector \\(f\\) by the scalar \\(p\\)</li> </ul> <p>The result is a vector in \\(\\mathbb{R}^d\\), which is then acted on by the divergence operator \\(\\nabla \\cdot\\).</p>"},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#units-of-drift","title":"Units of Drift","text":"\\[ \\text{Units of } f(x,t) = \\frac{[\\text{position}]}{[\\text{time}]} = \\text{velocity} \\] <p>In the SDE \\(dx = f(x,t)\\,dt + g(t)\\,dw\\): - \\(dx\\) has units of position - \\(dt\\) has units of time - Therefore \\(f(x,t)\\) must have units of position/time for dimensional consistency</p>"},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#physical-meaning-of-drift","title":"Physical Meaning of Drift","text":"<p>The drift \\(f(x,t)\\) is the deterministic velocity field. It answers:</p> <p>\"If there were no noise, which direction and how fast would \\(x\\) move?\"</p> <p>Examples:</p> System Drift \\(f(x,t)\\) Meaning Particle in gravity \\(f = -g\\) (constant) Falls at constant rate Spring (Hooke's law) \\(f = -kx\\) Pulled toward origin VP-SDE (diffusion models) \\(f = -\\frac{1}{2}\\beta(t)x\\) Shrinks toward origin VE-SDE \\(f = 0\\) No deterministic motion <p>In diffusion models (VP-SDE): The drift \\(f(x,t) = -\\frac{1}{2}\\beta(t)x\\) means the state \\(x\\) (e.g., an image) is deterministically pulled toward the origin at a rate proportional to \\(\\beta(t)\\). This \"shrinking\" combines with the noise term to gradually destroy the signal while keeping variance bounded.</p>"},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#units-of-probability-flux","title":"Units of Probability Flux","text":"<p>The probability flux due to drift is:</p> \\[ J_{\\text{drift}} = f(x,t) \\cdot p_t(x) \\] <p>Units:</p> <p>$$</p> <p>\\frac{[\\text{position}]}{[\\text{time}]} \\times \\frac{[\\text{probability}]}{[\\text{position}]^d} = \\frac{[\\text{probability}]}{[\\text{time}] \\cdot [\\text{position}]^{d-1}} $$</p> <p>Meaning: Rate of probability flowing per unit (hyper)surface area due to deterministic motion. Think of it as: \"How much probability passes through a surface per unit time because of the drift?\"</p>"},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#term-2-diffusion","title":"Term 2: Diffusion","text":"\\[ \\frac{1}{2}g(t)^2 \\nabla^2 p_t(x) \\] <p>What it means: Probability spreads out due to random noise.</p> <p>Physical analogy: Think of dropping ink in water. Even without currents, the ink spreads due to molecular collisions. This is diffusion\u2014probability smooths out, moving from high-concentration to low-concentration regions.</p> <p>Mathematical structure:</p> <ul> <li>\\(\\nabla^2 = \\sum_i \\frac{\\partial^2}{\\partial x_i^2}\\) is the Laplacian</li> <li>\\(g(t)^2\\) controls the diffusion strength</li> <li>The factor \\(\\frac{1}{2}\\) comes from It\u00f4 calculus conventions</li> </ul> <p>Key property: Diffusion always smooths. Peaks get lower, valleys get filled. This is why pure diffusion eventually leads to a uniform distribution (in the absence of boundaries).</p>"},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#the-fpe-as-a-conservation-law","title":"The FPE as a Conservation Law","text":"<p>The FPE can be written as a continuity equation:</p> \\[ \\frac{\\partial p_t}{\\partial t} + \\nabla \\cdot J = 0 \\] <p>where \\(J\\) is the probability current (flux):</p> \\[ J = f \\cdot p_t - \\frac{1}{2}g(t)^2 \\nabla p_t \\] <p>Interpretation: Probability is conserved\u2014it doesn't appear or disappear, only flows from one location to another.</p> <p>The total probability current has two parts: 1. Drift current: \\(f \\cdot p_t\\) (probability carried by deterministic flow) 2. Diffusion current: \\(-\\frac{1}{2}g(t)^2 \\nabla p_t\\) (probability flowing from high to low density)</p>"},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#interpreting-the-fpe-for-diffusion-models","title":"Interpreting the FPE for Diffusion Models","text":""},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#forward-process","title":"Forward Process","text":"<p>For the VP-SDE: \\(dx = -\\frac{1}{2}\\beta(t)x\\,dt + \\sqrt{\\beta(t)}\\,dw\\)</p> <p>The FPE becomes:</p> <p>$$</p> <p>\\frac{\\partial p_t}{\\partial t} = \\frac{1}{2}\\beta(t) \\nabla \\cdot (x \\cdot p_t) + \\frac{1}{2}\\beta(t) \\nabla^2 p_t $$</p> <p>What happens:</p> <ul> <li>Drift term: Pushes probability toward the origin (\\(x \\to 0\\))</li> <li>Diffusion term: Spreads probability out</li> <li>Combined effect: Probability converges to \\(\\mathcal{N}(0, I)\\) as \\(t \\to T\\)</li> </ul>"},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#reverse-process","title":"Reverse Process","text":"<p>For the reverse-time SDE, the FPE runs backward, describing how probability flows from noise back to data.</p>"},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#why-the-effective-drift-takes-this-form","title":"Why the Effective Drift Takes This Form","text":"<p>Now we derive why the probability flow ODE has:</p> \\[ \\text{Effective drift} = f(x,t) - \\frac{1}{2}g(t)^2 \\nabla_x \\log p_t(x) \\]"},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#the-goal","title":"The Goal","text":"<p>Find an ODE (no noise) that produces the same probability evolution as the SDE.</p>"},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#step-1-start-with-the-fpe","title":"Step 1: Start with the FPE","text":"<p>The SDE:</p> <p>$$</p> <p>dx = f(x,t)\\,dt + g(t)\\,dw $$</p> <p>has FPE:</p> <p>$$</p> <p>\\frac{\\partial p_t}{\\partial t} = -\\nabla \\cdot (f \\cdot p_t) + \\frac{1}{2}g(t)^2 \\nabla^2 p_t $$</p>"},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#step-2-rewrite-the-diffusion-term","title":"Step 2: Rewrite the Diffusion Term","text":"<p>The key trick is to express \\(\\nabla^2 p\\) using the score function.</p> <p>Identity: For any smooth density \\(p\\):</p> <p>$$</p> <p>\\nabla^2 p = \\nabla \\cdot (\\nabla p) $$</p> <p>Now use the score: Since \\(\\nabla \\log p = \\frac{\\nabla p}{p}\\), we have \\(\\nabla p = p \\cdot \\nabla \\log p\\).</p> <p>Therefore:</p> <p>$$</p> <p>\\nabla^2 p = \\nabla \\cdot (\\nabla p) = \\nabla \\cdot (p \\cdot \\nabla \\log p) $$</p>"},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#step-3-substitute-into-the-fpe","title":"Step 3: Substitute into the FPE","text":"\\[ \\frac{\\partial p_t}{\\partial t} = -\\nabla \\cdot (f \\cdot p_t) + \\frac{1}{2}g(t)^2 \\nabla \\cdot (p_t \\cdot \\nabla \\log p_t) \\]"},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#step-4-combine-the-divergence-terms","title":"Step 4: Combine the Divergence Terms","text":"<p>Factor out the divergence:</p> <p>$$</p> <p>\\frac{\\partial p_t}{\\partial t} = -\\nabla \\cdot \\left(f \\cdot p_t - \\frac{1}{2}g(t)^2 p_t \\cdot \\nabla \\log p_t\\right) $$</p> \\[ \\frac{\\partial p_t}{\\partial t} = -\\nabla \\cdot \\left(\\left[f - \\frac{1}{2}g(t)^2 \\nabla \\log p_t\\right] \\cdot p_t\\right) \\]"},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#step-5-recognize-this-as-a-pure-advection-equation","title":"Step 5: Recognize This as a Pure Advection Equation","text":"<p>The equation now has the form:</p> <p>$$</p> <p>\\frac{\\partial p_t}{\\partial t} = -\\nabla \\cdot (\\tilde{f} \\cdot p_t) $$</p> <p>where:</p> <p>$$</p> <p>\\boxed{\\tilde{f}(x,t) = f(x,t) - \\frac{1}{2}g(t)^2 \\nabla_x \\log p_t(x)} $$</p> <p>This is the FPE of an ODE (no diffusion term). The ODE:</p> <p>$$</p> <p>dx = \\tilde{f}(x,t)\\,dt = \\left[f(x,t) - \\frac{1}{2}g(t)^2 \\nabla_x \\log p_t(x)\\right]dt $$</p> <p>produces the exact same probability evolution as the original SDE!</p>"},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#intuitive-understanding-of-the-effective-drift","title":"Intuitive Understanding of the Effective Drift","text":""},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#why-frac12gt2-nabla-log-p_t","title":"Why \\(-\\frac{1}{2}g(t)^2 \\nabla \\log p_t\\)?","text":"<p>The diffusion term in the SDE (\\(g(t)dw\\)) causes probability to spread out. In the absence of this noise, we need something to compensate.</p> <p>The score term does exactly this:</p> <ul> <li>\\(\\nabla \\log p_t\\) points toward higher density</li> <li>The term \\(-\\frac{1}{2}g(t)^2 \\nabla \\log p_t\\) pulls samples toward high-density regions</li> <li>This \"anti-diffusion\" counteracts the spreading that would have happened from noise</li> </ul>"},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#physical-analogy","title":"Physical Analogy","text":"<p>Imagine two ways to move particles from point A to point B:</p> <ol> <li> <p>SDE way: Random walk with a drift. Particles take wiggly paths, but on average they move from A to B.</p> </li> <li> <p>ODE way: Deterministic flow with enhanced drift. Particles take smooth paths, but the drift is adjusted to account for the \"missing\" randomness.</p> </li> </ol> <p>Both methods move the same amount of probability mass from A to B, but via different particle trajectories.</p>"},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#why-factor-of-12","title":"Why Factor of \u00bd?","text":"<p>In the SDE, the diffusion term contributes to probability spreading with coefficient \\(\\frac{1}{2}g(t)^2\\) (from It\u00f4 calculus). To compensate for this exactly, the effective drift correction must also have coefficient \\(\\frac{1}{2}g(t)^2\\).</p>"},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#summary-table","title":"Summary Table","text":"Concept Formula Interpretation Fokker-Planck equation \\(\\frac{\\partial p_t}{\\partial t} = -\\nabla \\cdot (fp_t) + \\frac{1}{2}g^2 \\nabla^2 p_t\\) How probability density evolves Drift term \\(-\\nabla \\cdot (fp_t)\\) Probability carried by deterministic flow Diffusion term \\(\\frac{1}{2}g^2 \\nabla^2 p_t\\) Probability spreading from noise Probability current \\(J = fp_t - \\frac{1}{2}g^2 \\nabla p_t\\) Total probability flow Effective drift \\(f - \\frac{1}{2}g^2 \\nabla \\log p_t\\) ODE drift that matches SDE marginals"},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#connection-to-diffusion-models","title":"Connection to Diffusion Models","text":"<p>In diffusion models:</p> <ol> <li> <p>Training: Learn the score \\(s_\\theta(x,t) \\approx \\nabla_x \\log p_t(x)\\)</p> </li> <li> <p>SDE sampling: Use the reverse SDE with learned score    $$    dx = [f - g^2 s_\\theta]dt + g\\,dw    $$</p> </li> <li> <p>ODE sampling (DDIM): Use the probability flow ODE with learned score    $$</p> </li> </ol> <p>dx = [f - \\tfrac{1}{2}g^2 s_\\theta]dt    $$</p> <p>Both use the same learned score \\(s_\\theta\\), but: - SDE has coefficient \\(g^2\\) on the score (plus noise term) - ODE has coefficient \\(\\frac{1}{2}g^2\\) on the score (no noise)</p> <p>The Fokker-Planck analysis proves they generate from the same distribution.</p>"},{"location":"diffusion/02_sde_formulation/supplements/06_fokker_planck_and_effective_drift/#references","title":"References","text":"<ul> <li>Risken (1989): \"The Fokker-Planck Equation\" \u2014 Comprehensive textbook</li> <li>\u00d8ksendal (2003): \"Stochastic Differential Equations\" \u2014 Mathematical foundations</li> <li>Song et al. (2021): \"Score-Based Generative Modeling through SDEs\" \u2014 Application to diffusion models</li> <li>Maoutsa et al. (2020): \"Interacting Particle Solutions of Fokker-Planck Equations\" \u2014 Numerical methods</li> </ul>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/","title":"The Fokker-Planck Equation: From SDEs to Probability Evolution","text":"<p>Understanding how stochastic processes evolve probability distributions</p> <p>The Fokker-Planck equation (FPE) is the bridge between individual particle motion (described by SDEs) and collective probability evolution (described by PDEs). It's the reason we can train diffusion models by learning score functions and the foundation for understanding reverse-time sampling.</p> <p>This supplement builds intuition from first principles, explains why divergence and the Laplacian must appear, and connects everything to diffusion models.</p>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#table-of-contents","title":"Table of Contents","text":"<ol> <li>The Central Question</li> <li>From Particles to Densities</li> <li>Conservation Laws and Divergence</li> <li>The Laplacian and Diffusion</li> <li>The Complete Fokker-Planck Equation</li> <li>Geometric Intuition</li> <li>Connection to Diffusion Models</li> </ol>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#1-the-central-question","title":"1. The Central Question","text":""},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#the-setup","title":"The Setup","text":"<p>You have a stochastic differential equation:</p> \\[ dX_t = f(X_t, t)\\,dt + g(t)\\,dW_t \\] <p>This describes how a single particle moves randomly through space.</p> <p>But in diffusion models, we don't care about individual trajectories. We care about probability distributions:</p> \\[ p_t(x) = \\text{probability density of } X_t \\text{ at position } x \\] <p>The question: How does \\(p_t(x)\\) evolve over time?</p> <p>The answer: The Fokker-Planck equation.</p>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#2-from-particles-to-densities","title":"2. From Particles to Densities","text":""},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#the-conceptual-shift","title":"The Conceptual Shift","text":"<p>Particle view: Track individual random trajectories \\(X_t(\\omega)\\) for each random outcome \\(\\omega\\).</p> <p>Density view: Track the probability density \\(p_t(x)\\) describing where particles are likely to be.</p> <p>Key insight: Even though individual particles move randomly, the density evolves deterministically according to a PDE.</p>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#why-this-matters","title":"Why This Matters","text":"<p>In diffusion models: - Forward process: We know how to corrupt data (SDE) - Training: We learn the score function \\(\\nabla_x \\log p_t(x)\\) - Sampling: We need to evolve probability backwards</p> <p>The FPE is what connects these three pieces.</p>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#3-conservation-laws-and-divergence","title":"3. Conservation Laws and Divergence","text":""},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#the-physical-principle","title":"The Physical Principle","text":"<p>Imagine a huge swarm of particles. At time \\(t\\), they're spread out with density \\(p(x,t)\\).</p> <p>Take any region \\(V\\) in space. The total probability inside is:</p> \\[ \\mathbb{P}(X_t \\in V) = \\int_V p(x,t)\\,dx \\] <p>Fundamental assumption: Probability doesn't teleport. It can't be created or destroyed. The only way it changes is by flowing across the boundary.</p>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#probability-current","title":"Probability Current","text":"<p>To describe flow, introduce a probability current (or flux) \\(J(x,t)\\):</p> \\[ J(x,t) = \\text{probability flow per unit area per unit time} \\] <p>The total probability leaving \\(V\\) per unit time is:</p> \\[ \\text{outflow} = \\int_{\\partial V} J(x,t) \\cdot n(x)\\,dS \\] <p>where \\(n(x)\\) is the outward unit normal on the boundary \\(\\partial V\\).</p>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#conservation-equation","title":"Conservation Equation","text":"<p>Conservation states:</p> \\[ \\frac{d}{dt}\\int_V p(x,t)\\,dx = -\\int_{\\partial V} J \\cdot n\\,dS \\] <p>The minus sign: outward flow decreases the amount inside.</p>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#enter-divergence","title":"Enter Divergence","text":"<p>By the divergence theorem (Gauss's theorem):</p> \\[ \\int_{\\partial V} J \\cdot n\\,dS = \\int_V \\nabla \\cdot J\\,dx \\] <p>Substituting:</p> \\[ \\int_V \\partial_t p\\,dx = -\\int_V \\nabla \\cdot J\\,dx \\] <p>Since this holds for every region \\(V\\), the integrands must match:</p> \\[ \\boxed{\\partial_t p(x,t) + \\nabla \\cdot J(x,t) = 0} \\] <p>This is the continuity equation.</p>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#why-divergence-appears","title":"Why Divergence Appears","text":"<p>Divergence is not a choice\u2014it's the unique local operator that measures \"net outflow from an infinitesimal volume.\"</p> <p>Physical meaning: \\(\\nabla \\cdot J\\) tells you whether probability is accumulating (negative) or escaping (positive) at a point.</p> <p>Geometric meaning: Divergence measures how a flow expands or contracts volume.</p>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#4-the-laplacian-and-diffusion","title":"4. The Laplacian and Diffusion","text":""},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#what-is-the-laplacian","title":"What Is the Laplacian?","text":"<p>The Laplacian of a scalar field \\(u(x)\\) is:</p> \\[ \\Delta u(x) = \\nabla^2 u(x) = \\sum_{i=1}^d \\frac{\\partial^2 u}{\\partial x_i^2} \\] <p>But what does it mean?</p>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#the-key-intuition","title":"The Key Intuition","text":"<p>At a point \\(x\\), the Laplacian answers:</p> <p>Is the value here higher or lower than the average value nearby?</p> <p>More precisely:</p> \\[ \\Delta u(x) \\propto \\lim_{r \\to 0} \\frac{\\text{Avg}_{|y-x|=r} u(y) - u(x)}{r^2} \\] <p>Interpretation:</p> <ul> <li>\\(\\Delta u(x) &gt; 0\\): \\(u(x)\\) is below its local average (valley)</li> <li>\\(\\Delta u(x) &lt; 0\\): \\(u(x)\\) is above its local average (peak)</li> <li>\\(\\Delta u(x) = 0\\): \\(u(x)\\) matches its local average (locally flat)</li> </ul>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#derivation-via-taylor-expansion","title":"Derivation via Taylor Expansion","text":"<p>Take a sphere of radius \\(r\\) centered at \\(x_0\\). The average value on the sphere is:</p> \\[ \\langle u \\rangle_{S_r} = \\frac{1}{|S_r|} \\int_{S_r} u(x_0 + h)\\,dS \\] <p>Taylor expand \\(u(x_0 + h)\\):</p> \\[ u(x_0 + h) = u(x_0) + \\nabla u(x_0) \\cdot h + \\frac{1}{2} h^\\top H(x_0) h + o(|h|^2) \\] <p>where \\(H\\) is the Hessian matrix.</p> <p>Key observations: 1. Constant term: \\(u(x_0)\\) survives 2. Linear term: \\(\\nabla u(x_0) \\cdot h\\) vanishes by symmetry (sphere is symmetric) 3. Quadratic term: Only diagonal terms survive, giving \\(\\frac{r^2}{2d} \\Delta u(x_0)\\)</p> <p>Result:</p> \\[ \\langle u \\rangle_{S_r} - u(x_0) = \\frac{r^2}{2d} \\Delta u(x_0) + o(r^2) \\] <p>So the Laplacian literally measures the difference between a point and its neighborhood average!</p>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#why-diffusion-equals-the-laplacian","title":"Why Diffusion Equals the Laplacian","text":"<p>Consider pure Brownian motion:</p> \\[ dX_t = \\sigma\\,dW_t \\] <p>Particles spread isotropically. The density evolves as:</p> \\[ \\partial_t p = \\frac{\\sigma^2}{2} \\Delta p \\] <p>This is the heat equation.</p> <p>Why this form?</p> <ol> <li>Local: Depends only on infinitesimal neighborhood</li> <li>Isotropic: Doesn't prefer any direction</li> <li>Mass-conserving: Total probability stays 1</li> <li>Gaussian: Produces exactly the Gaussian spreading of Brownian motion</li> </ol> <p>Physical interpretation: Diffusion is nature's way of eliminating curvature. Peaks decrease, valleys fill.</p>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#5-the-complete-fokker-planck-equation","title":"5. The Complete Fokker-Planck Equation","text":""},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#two-mechanisms-of-motion","title":"Two Mechanisms of Motion","text":"<p>An SDE has two parts:</p> \\[ dX_t = \\underbrace{f(X_t,t)\\,dt}_{\\text{drift}} + \\underbrace{g(t)\\,dW_t}_{\\text{diffusion}} \\] <p>Drift: Deterministic transport along a vector field Diffusion: Random spreading due to noise</p>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#the-probability-current","title":"The Probability Current","text":"<p>The total current has two contributions:</p> \\[ J(x,t) = \\underbrace{f(x,t)\\,p(x,t)}_{\\text{advection}} - \\underbrace{\\frac{g(t)^2}{2}\\nabla p(x,t)}_{\\text{diffusion}} \\] <p>Advection term: \\(J_{\\text{drift}} = f \\cdot p\\) - Density times velocity (like fluid mechanics) - Probability is transported by the drift field</p> <p>Diffusion term: \\(J_{\\text{diff}} = -D \\nabla p\\) (Fick's law) - Flow goes from high to low concentration - \\(D = g(t)^2/2\\) is the diffusion coefficient</p>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#the-full-equation","title":"The Full Equation","text":"<p>Plug into the continuity equation \\(\\partial_t p + \\nabla \\cdot J = 0\\):</p> \\[ \\partial_t p = -\\nabla \\cdot (f p) + \\frac{g(t)^2}{2} \\Delta p \\] <p>Expanding the divergence:</p> \\[ \\boxed{\\partial_t p = -\\nabla \\cdot (f p) + \\frac{g(t)^2}{2} \\Delta p} \\] <p>Or equivalently:</p> \\[ \\boxed{\\partial_t p = -f \\cdot \\nabla p - p\\,\\nabla \\cdot f + \\frac{g(t)^2}{2} \\Delta p} \\] <p>This is the Fokker-Planck equation (also called the forward Kolmogorov equation).</p>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#reading-the-equation","title":"Reading the Equation","text":"<p>Term by term:</p> <ol> <li>\\(-\\nabla \\cdot (f p)\\): Probability transported by drift</li> <li>\\(\\frac{g(t)^2}{2} \\Delta p\\): Probability spread by noise</li> </ol> <p>As a sentence: \"Probability changes because it's pushed by drift and smoothed by diffusion.\"</p>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#6-geometric-intuition","title":"6. Geometric Intuition","text":""},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#divergence-where-does-probability-go","title":"Divergence: Where Does Probability Go?","text":"<p>Question: Is probability accumulating or escaping at this point?</p> <p>Answer: \\(\\nabla \\cdot J\\)</p> <ul> <li>Positive divergence: Net outflow \u2192 density decreases</li> <li>Negative divergence: Net inflow \u2192 density increases</li> <li>Zero divergence: Incompressible flow \u2192 density unchanged by transport</li> </ul> <p>Mental image: Stand at a point with a microscopic balloon. Divergence tells you whether the balloon inflates (source), deflates (sink), or stays constant (incompressible).</p>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#laplacian-how-curved-is-the-density","title":"Laplacian: How Curved Is the Density?","text":"<p>Question: Is this point higher or lower than its surroundings?</p> <p>Answer: \\(\\Delta p\\)</p> <ul> <li>Positive Laplacian: Valley \u2192 fills in</li> <li>Negative Laplacian: Peak \u2192 flattens out</li> <li>Zero Laplacian: Harmonic \u2192 locally balanced</li> </ul> <p>Mental image: Diffusion punishes disagreement with neighbors. The Laplacian measures that disagreement.</p>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#together-flow-smoothing","title":"Together: Flow + Smoothing","text":"<p>The FPE describes two fundamental motions:</p> <ol> <li>Divergence: Rearranges probability (transport)</li> <li>Laplacian: Smooths probability (diffusion)</li> </ol> <p>These are the only two local operations that respect conservation, isotropy, and locality.</p>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#7-connection-to-diffusion-models","title":"7. Connection to Diffusion Models","text":""},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#the-forward-process","title":"The Forward Process","text":"<p>In diffusion models, the forward SDE is:</p> \\[ dx = f(x,t)\\,dt + g(t)\\,dw \\] <p>Common choices: - VP-SDE: \\(f(x,t) = -\\frac{1}{2}\\beta(t) x\\), \\(g(t) = \\sqrt{\\beta(t)}\\) - VE-SDE: \\(f(x,t) = 0\\), \\(g(t) = \\sqrt{\\frac{d\\sigma^2(t)}{dt}}\\)</p> <p>The density \\(p_t(x)\\) evolves via FPE:</p> \\[ \\partial_t p_t = -\\nabla \\cdot (f p_t) + \\frac{g(t)^2}{2} \\Delta p_t \\]"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#the-score-function","title":"The Score Function","text":"<p>Rewrite the FPE using the score function \\(s_t(x) = \\nabla_x \\log p_t(x)\\):</p> \\[ \\nabla p_t = p_t \\cdot s_t \\] <p>Substitute into the Laplacian term:</p> \\[ \\Delta p_t = \\nabla \\cdot (\\nabla p_t) = \\nabla \\cdot (p_t s_t) = p_t |s_t|^2 + p_t \\nabla \\cdot s_t \\] <p>The FPE becomes:</p> \\[ \\partial_t p_t = -\\nabla \\cdot \\left(p_t \\left[f - \\frac{g^2}{2} s_t - \\frac{g^2}{2} \\nabla \\cdot s_t\\right]\\right) \\] <p>This form shows how the score appears in the probability current.</p>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#the-reverse-process","title":"The Reverse Process","text":"<p>The reverse-time SDE (Anderson, 1982) is:</p> \\[ dx = \\left[f(x,t) - g(t)^2 \\nabla_x \\log p_t(x)\\right]dt + g(t)\\,d\\bar{w} \\] <p>Key insight: The score function \\(\\nabla_x \\log p_t(x)\\) is what you need to reverse the diffusion.</p> <p>Training: Learn \\(s_\\theta(x,t) \\approx \\nabla_x \\log p_t(x)\\) using denoising score matching.</p> <p>Sampling: Use the learned score in the reverse SDE to generate data.</p>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#why-the-fpe-matters","title":"Why the FPE Matters","text":"<ol> <li>Connects SDEs to PDEs: Particle motion \u2192 density evolution</li> <li>Justifies score matching: The score appears naturally in the reverse SDE</li> <li>Explains training: We're learning the gradient of the log-density</li> <li>Enables sampling: The reverse SDE uses the learned score</li> </ol>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#summary-the-big-picture","title":"Summary: The Big Picture","text":""},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#what-weve-learned","title":"What We've Learned","text":"<ol> <li>Conservation forces divergence: Probability flow requires \\(\\partial_t p + \\nabla \\cdot J = 0\\)</li> <li>Drift gives advection: \\(J_{\\text{drift}} = f \\cdot p\\) (density times velocity)</li> <li>Noise gives diffusion: \\(J_{\\text{diff}} = -D \\nabla p\\) (Fick's law)</li> <li>FPE combines both: \\(\\partial_t p = -\\nabla \\cdot (f p) + D \\Delta p\\)</li> <li>Score function bridges: \\(\\nabla \\log p\\) connects forward and reverse processes</li> </ol>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#the-operators-are-not-arbitrary","title":"The Operators Are Not Arbitrary","text":"<ul> <li>Divergence: The unique local measure of net outflow</li> <li>Laplacian: The unique isotropic local smoothing operator</li> </ul> <p>They appear because they must\u2014any other choice would violate conservation, locality, or isotropy.</p>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#for-diffusion-models","title":"For Diffusion Models","text":"<p>The FPE explains: - Why we can learn score functions - How reverse-time sampling works - Why the Laplacian appears in generators - How SDEs and PDEs are two views of the same process</p>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#further-reading","title":"Further Reading","text":""},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#primary-sources","title":"Primary Sources","text":"<ul> <li>Fokker (1914): Original work on Brownian motion</li> <li>Planck (1917): Generalization to arbitrary drift</li> <li>Kolmogorov (1931): Forward and backward equations</li> <li>Anderson (1982): Reverse-time SDE</li> </ul>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#modern-treatments","title":"Modern Treatments","text":"<ul> <li>\u00d8ksendal (2003): Stochastic Differential Equations</li> <li> <p>Chapter 7: The Fokker-Planck equation</p> </li> <li> <p>Pavliotis (2014): Stochastic Processes and Applications</p> </li> <li> <p>Comprehensive treatment of FPE and applications</p> </li> <li> <p>Song et al. (2021): Score-Based Generative Modeling through SDEs</p> </li> <li>Application to diffusion models</li> </ul>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#related-supplements","title":"Related Supplements","text":"<ul> <li><code>01_forward_sde_design_choices.md</code>: How to choose \\(f\\) and \\(g\\)</li> <li><code>05_reverse_sde_and_probability_flow_ode.md</code>: Reverse-time sampling</li> <li><code>06_fokker_planck_and_effective_drift.md</code>: Advanced FPE topics</li> </ul>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#exercises","title":"Exercises","text":""},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#conceptual","title":"Conceptual","text":"<ol> <li> <p>Why must divergence appear? Explain in your own words why conservation of probability forces the continuity equation.</p> </li> <li> <p>Laplacian intuition: For a 1D function \\(u(x)\\), explain why \\(u''(x) &gt; 0\\) means \\(u(x)\\) is below its local average.</p> </li> <li> <p>Score connection: Show that \\(\\nabla p = p \\cdot \\nabla \\log p\\) and explain why this is useful.</p> </li> </ol>"},{"location":"diffusion/02_sde_formulation/supplements/07_fokker_planck_equation/#computational","title":"Computational","text":"<ol> <li> <p>Verify FPE for Ornstein-Uhlenbeck: For \\(dx = -\\theta x\\,dt + \\sigma\\,dw\\), verify that the Gaussian density \\(p_t(x) = \\mathcal{N}(x; \\mu_t, \\sigma_t^2)\\) satisfies the FPE.</p> </li> <li> <p>Simulate and compare: Simulate the SDE and solve the FPE numerically. Compare the histogram of particles to the PDE solution.</p> </li> </ol> <p>Next: Now that you understand the FPE, you're ready to see how it connects to the generator of an SDE and how It\u00f4's formula makes everything rigorous. See the main SDE tutorial for the complete picture!</p>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/","title":"Dimensional Analysis in SDEs and Diffusion Models","text":"<p>Why units matter: from physical intuition to diffusion model design</p> <p>Dimensional analysis is one of the most powerful sanity checks in physics and mathematics. In SDEs and diffusion models, tracking units reveals why certain formulas must have specific forms, why the \\(\\sqrt{dt}\\) scaling is inevitable, and why score functions have inverse data units.</p> <p>This supplement builds unit intuition from classical diffusion, translates it to abstract data spaces, and shows how dimensional consistency constrains diffusion model design.</p>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Why Units Matter</li> <li>Units in Classical Diffusion</li> <li>Units in Abstract Data Spaces</li> <li>The Score Function's Units</li> <li>Dimensional Consistency in SDEs</li> <li>Applications to Diffusion Models</li> </ol>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#1-why-units-matter","title":"1. Why Units Matter","text":""},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#the-power-of-dimensional-analysis","title":"The Power of Dimensional Analysis","text":"<p>Dimensional analysis is the practice of tracking physical units through equations. It's powerful because:</p> <ol> <li>Error detection: Dimensionally inconsistent equations are wrong</li> <li>Structure revelation: Units force certain mathematical forms</li> <li>Scaling laws: Dimensional reasoning predicts how systems behave</li> <li>Sanity checks: Quick verification without detailed calculation</li> </ol> <p>Example: If someone claims \\(\\text{velocity} = \\text{acceleration} \\times \\text{time}^2\\), you can immediately reject it:</p> \\[ [L/T] \\neq [L/T^2] \\times [T^2] = [L] \\] <p>The units don't match, so the equation is wrong.</p>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#units-in-diffusion-models","title":"Units in Diffusion Models","text":"<p>Even though diffusion models work in abstract \"data space\" (pixels, latent vectors, gene expression), units still exist and still constrain the mathematics.</p> <p>Tracking units explains: - Why noise variance grows linearly in time - Why score functions have units of inverse data - Why \\(\\sqrt{dt}\\) appears everywhere - Why certain noise schedules are valid</p>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#2-units-in-classical-diffusion","title":"2. Units in Classical Diffusion","text":""},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#the-setup","title":"The Setup","text":"<p>Start with ordinary physical space: - Space variable \\(x\\) has units of length: \\([x] = [L]\\) - Time \\(t\\) has units of time: \\([t] = [T]\\)</p> <p>Everything else follows from these choices.</p>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#units-of-probability-density","title":"Units of Probability Density","text":"<p>A probability density \\(p(x,t)\\) is not dimensionless.</p> <p>The probability of finding a particle in region \\(V\\) is:</p> \\[ \\mathbb{P}(X_t \\in V) = \\int_V p(x,t)\\,dx \\] <p>The left side is dimensionless (it's a probability). The volume element \\(dx\\) has units \\([L^d]\\) in \\(d\\) dimensions.</p> <p>Therefore:</p> \\[ [p] \\cdot [L^d] = 1 \\quad \\Rightarrow \\quad [p] = [L^{-d}] \\] <p>Key insight: Density means \"per unit volume.\"</p>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#units-of-the-drift","title":"Units of the Drift","text":"<p>In an SDE:</p> \\[ dX_t = f(X_t)\\,dt + \\sigma\\,dW_t \\] <p>The term \\(f(X_t)\\,dt\\) must have the same units as \\(dX_t\\), which is length.</p> <p>Since \\([dt] = [T]\\), we must have:</p> \\[ [f] = [L/T] \\] <p>So \\(f\\) is a velocity field\u2014not metaphorically, but dimensionally.</p>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#units-of-the-probability-current","title":"Units of the Probability Current","text":"<p>The probability current is:</p> \\[ J(x,t) = f(x)\\,p(x,t) \\] <p>Multiply the units:</p> \\[ [J] = [f] \\cdot [p] = [L/T] \\cdot [L^{-d}] = [L^{-(d-1)}/T] \\] <p>Interpretation: \\(J\\) measures \"probability crossing a unit area per unit time.\"</p> <p>This is exactly what a flux should measure. The equation \\(J = (\\text{density}) \\times (\\text{velocity})\\) is not an analogy\u2014it's dimensional necessity.</p>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#units-of-divergence","title":"Units of Divergence","text":"<p>The divergence operator \\(\\nabla \\cdot\\) involves spatial derivatives:</p> \\[ \\nabla \\cdot J = \\sum_{i=1}^d \\frac{\\partial J_i}{\\partial x_i} \\] <p>Each derivative contributes a factor of \\([1/L]\\):</p> \\[ [\\nabla \\cdot J] = [J] \\cdot [1/L] = [L^{-(d-1)}/T] \\cdot [1/L] = [L^{-d}/T] \\] <p>Check the continuity equation:</p> \\[ \\partial_t p + \\nabla \\cdot J = 0 \\] <p>Both terms have units \\([L^{-d}/T]\\). \u2713</p> <p>Why divergence? It's the only operator with the right units to convert flux into density change.</p>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#units-of-diffusion","title":"Units of Diffusion","text":"<p>For the diffusion equation:</p> \\[ \\partial_t p = D\\,\\Delta p \\] <p>The Laplacian contributes two spatial derivatives:</p> \\[ [\\Delta] = [1/L^2] \\] <p>So:</p> \\[ [D] \\cdot [p] \\cdot [1/L^2] = [p]/[T] \\] <p>This forces:</p> \\[ [D] = [L^2/T] \\] <p>Physical meaning: Diffusion coefficient measures \"how much area a random walker explores per unit time.\"</p> <p>This is why Brownian motion spreads as variance \\(\\sim Dt\\).</p>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#units-of-the-noise-term","title":"Units of the Noise Term","text":"<p>Brownian motion satisfies:</p> \\[ [dW_t] = [\\sqrt{T}] \\] <p>For \\(\\sigma\\,dW_t\\) to have units of length:</p> \\[ [\\sigma] = [L/\\sqrt{T}] \\] <p>Then:</p> \\[ D = \\frac{\\sigma^2}{2} \\quad \\Rightarrow \\quad [D] = [L^2/T] \\] <p>Everything is consistent. \u2713</p>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#3-units-in-abstract-data-spaces","title":"3. Units in Abstract Data Spaces","text":""},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#the-translation","title":"The Translation","text":"<p>In diffusion models, space is no longer physical space. Instead:</p> <ul> <li>\\(x \\in \\mathbb{R}^d\\) is data space</li> <li>Coordinates are pixel intensities, audio amplitudes, latent features, etc.</li> </ul> <p>There's no meter stick\u2014but units still exist.</p>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#the-abstract-unit","title":"The Abstract Unit","text":"<p>Call the unit of data \\([X]\\). This could be: - Pixel intensity (0-255 or normalized) - Audio amplitude - Gene expression level - Latent coordinate</p> <p>The key is that \\([X]\\) is not dimensionless\u2014it's the unit of your data.</p>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#reinterpreting-all-units","title":"Reinterpreting All Units","text":"<p>Now translate everything from physical space to data space:</p> Quantity Physical Space Data Space State \\([L]\\) \\([X]\\) Density \\([L^{-d}]\\) \\([X^{-d}]\\) Drift \\([L/T]\\) \\([X/T]\\) Current \\([L^{-(d-1)}/T]\\) \\([X^{-(d-1)}/T]\\) Noise scale \\([L/\\sqrt{T}]\\) \\([X/\\sqrt{T}]\\) Diffusion coeff \\([L^2/T]\\) \\([X^2/T]\\) <p>The dimensional structure is identical.</p>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#why-this-matters","title":"Why This Matters","text":"<p>Even though \"meters\" are gone, the relationships between units remain:</p> <ul> <li>Gradients have units \\([1/X]\\)</li> <li>Divergence has units \\([1/X]\\)</li> <li>Laplacians have units \\([1/X^2]\\)</li> </ul> <p>These constraints apply whether \\(X\\) is meters or pixels.</p>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#4-the-score-functions-units","title":"4. The Score Function's Units","text":""},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#definition","title":"Definition","text":"<p>The score function is:</p> \\[ s(x,t) = \\nabla_x \\log p(x,t) \\] <p>What are its units?</p>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#careful-derivation","title":"Careful Derivation","text":"<p>Step 1: Units of \\(p(x,t)\\)</p> \\[ [p] = [X^{-d}] \\] <p>Step 2: The logarithm issue</p> <p>Strictly speaking, \\(\\log\\) requires a dimensionless argument. The proper interpretation is:</p> \\[ \\log p \\equiv \\log(p/p_0) \\] <p>where \\(p_0\\) is a reference density with the same units as \\(p\\). Since \\(p_0\\) is constant, it doesn't affect gradients:</p> \\[ \\nabla_x \\log p = \\nabla_x \\log(p/p_0) = \\nabla_x \\log p \\] <p>Step 3: Apply the gradient</p> <p>Use the identity:</p> \\[ \\nabla_x \\log p = \\frac{\\nabla_x p}{p} \\] <p>The gradient contributes \\([1/X]\\):</p> \\[ [\\nabla_x p] = [\\nabla_x] \\cdot [p] = [1/X] \\cdot [X^{-d}] = [X^{-(d+1)}] \\] <p>Divide by \\(p\\):</p> \\[ \\left[\\frac{\\nabla_x p}{p}\\right] = \\frac{[X^{-(d+1)}]}{[X^{-d}]} = [X^{-1}] \\] <p>Therefore:</p> \\[ \\boxed{[s] = [\\nabla_x \\log p] = [X^{-1}]} \\] <p>The score has units of inverse data.</p>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#sanity-check-1d-gaussian","title":"Sanity Check: 1D Gaussian","text":"<p>For a 1D Gaussian:</p> \\[ p(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) \\] <p>The score is:</p> \\[ \\frac{d}{dx}\\log p(x) = -\\frac{x-\\mu}{\\sigma^2} \\] <p>Units: - Numerator: \\([x-\\mu] = [X]\\) - Denominator: \\([\\sigma^2] = [X^2]\\)</p> <p>So:</p> \\[ \\left[-\\frac{x-\\mu}{\\sigma^2}\\right] = [X]/[X^2] = [X^{-1}] \\] <p>Exactly as predicted. \u2713</p>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#5-dimensional-consistency-in-sdes","title":"5. Dimensional Consistency in SDEs","text":""},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#the-forward-sde","title":"The Forward SDE","text":"\\[ dx = f(x,t)\\,dt + g(t)\\,dw \\] <p>Check units:</p> <p>Left side: \\([dx] = [X]\\)</p> <p>Right side, drift term: \\([f] \\cdot [dt] = [X/T] \\cdot [T] = [X]\\) \u2713</p> <p>Right side, diffusion term: \\([g] \\cdot [dw] = [X/\\sqrt{T}] \\cdot [\\sqrt{T}] = [X]\\) \u2713</p> <p>Everything matches.</p>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#the-reverse-sde","title":"The Reverse SDE","text":"\\[ dx = \\left[f(x,t) - g(t)^2 \\nabla_x \\log p_t(x)\\right]dt + g(t)\\,d\\bar{w} \\] <p>Check the score correction term:</p> \\[ [g^2 \\cdot \\nabla_x \\log p] = [X^2/T] \\cdot [1/X] = [X/T] \\] <p>This matches the drift \\([f] = [X/T]\\). \u2713</p> <p>Key insight: The reverse SDE is dimensionally consistent because the score has units \\([1/X]\\).</p>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#the-fokker-planck-equation","title":"The Fokker-Planck Equation","text":"\\[ \\partial_t p = -\\nabla \\cdot (f p) + \\frac{g^2}{2} \\Delta p \\] <p>Check each term:</p> <p>Left side: \\([\\partial_t p] = [X^{-d}/T]\\)</p> <p>Right side, drift term: \\([\\nabla \\cdot (f p)] = [1/X] \\cdot [X/T] \\cdot [X^{-d}] = [X^{-d}/T]\\) \u2713</p> <p>Right side, diffusion term: \\([g^2 \\Delta p] = [X^2/T] \\cdot [1/X^2] \\cdot [X^{-d}] = [X^{-d}/T]\\) \u2713</p> <p>All terms have the same units.</p>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#6-applications-to-diffusion-models","title":"6. Applications to Diffusion Models","text":""},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#why-noise-variance-grows-linearly","title":"Why Noise Variance Grows Linearly","text":"<p>Variance has units \\([X^2]\\). Time has units \\([T]\\).</p> <p>For dimensional consistency:</p> \\[ \\text{Var}(x_t) \\sim [X^2/T] \\cdot [T] = [X^2] \\] <p>The diffusion coefficient \\(D = [X^2/T]\\) must multiply time to give variance.</p> <p>Conclusion: Variance must grow linearly in time for dimensional consistency.</p>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#why-sqrtdt-appears","title":"Why \\(\\sqrt{dt}\\) Appears","text":"<p>Noise increments must have units \\([X]\\). Time steps have units \\([T]\\).</p> <p>For \\(\\sigma \\sqrt{dt} \\cdot \\varepsilon\\) to have units \\([X]\\):</p> \\[ [\\sigma] \\cdot [\\sqrt{T}] = [X] \\quad \\Rightarrow \\quad [\\sigma] = [X/\\sqrt{T}] \\] <p>Conclusion: Noise must scale as \\(\\sqrt{dt}\\) for dimensional consistency.</p>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#why-score-functions-are-learned","title":"Why Score Functions Are Learned","text":"<p>The reverse SDE requires \\(\\nabla_x \\log p_t(x)\\) with units \\([1/X]\\).</p> <p>A neural network with: - Input: \\(x \\in \\mathbb{R}^d\\) (units \\([X]\\)) - Output: \\(s_\\theta(x,t) \\in \\mathbb{R}^d\\) (units \\([1/X]\\))</p> <p>naturally has the right dimensional structure.</p>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#time-reparameterization","title":"Time Reparameterization","text":"<p>Different noise schedules are valid if they preserve dimensional consistency.</p> <p>For example, reparameterizing time \\(t \\to \\tau(t)\\) requires:</p> \\[ \\frac{d\\tau}{dt} = \\text{dimensionless} \\] <p>So \\(\\tau\\) has the same units as \\(t\\), and all formulas remain dimensionally consistent.</p>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#noise-schedule-constraints","title":"Noise Schedule Constraints","text":"<p>A noise schedule \\(\\beta(t)\\) must have units:</p> \\[ [\\beta(t)] = [1/T] \\] <p>So that \\(\\beta(t)\\,dt\\) is dimensionless (it's integrated to give \\(\\bar{\\alpha}_t\\)).</p> <p>Similarly, \\(g(t) = \\sqrt{\\beta(t)}\\) has units:</p> \\[ [g(t)] = [1/\\sqrt{T}] \\] <p>Wait, this seems wrong! Let me recalculate...</p> <p>Actually, for VP-SDE: \\(g(t) = \\sqrt{\\beta(t)}\\) where \\(\\beta(t)\\) is a rate with units \\([1/T]\\).</p> <p>But we need \\([g] = [X/\\sqrt{T}]\\) for the noise term to work.</p> <p>Resolution: The data is implicitly normalized so that \\([X] = 1\\) (dimensionless in the normalized space). Then:</p> \\[ [g] = [1/\\sqrt{T}] \\] <p>which is consistent with \\(g(t) = \\sqrt{\\beta(t)}\\).</p> <p>Lesson: Normalization affects dimensional analysis, but the structure remains.</p>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#summary-the-power-of-units","title":"Summary: The Power of Units","text":""},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Units constrain structure: Dimensional consistency forces certain mathematical forms</li> <li>Density has units: \\([p] = [X^{-d}]\\) (per unit volume)</li> <li>Drift is velocity: \\([f] = [X/T]\\)</li> <li>Score is inverse data: \\([s] = [1/X]\\)</li> <li>Noise scales as \\(\\sqrt{dt}\\): Only scaling that gives finite variance</li> <li>Divergence and Laplacian have fixed units: \\([1/X]\\) and \\([1/X^2]\\)</li> </ol>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#why-this-matters_1","title":"Why This Matters","text":"<p>Dimensional analysis: - Detects errors: Inconsistent units mean wrong equations - Guides design: Units constrain valid noise schedules - Builds intuition: Physical reasoning transfers to abstract spaces - Enables sanity checks: Quick verification without detailed math</p>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#for-diffusion-models","title":"For Diffusion Models","text":"<p>Even in abstract data spaces: - Probability behaves like a compressible fluid - Density, velocity, and flux have well-defined units - Score functions naturally have inverse data units - Time reparameterization must preserve dimensional structure</p> <p>The units don't let you cheat. They force the mathematics whether the \"space\" is meters or pixels.</p>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#further-reading","title":"Further Reading","text":""},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#dimensional-analysis","title":"Dimensional Analysis","text":"<ul> <li>Barenblatt (1996): Scaling, Self-similarity, and Intermediate Asymptotics</li> <li> <p>Comprehensive treatment of dimensional analysis</p> </li> <li> <p>Bridgman (1922): Dimensional Analysis</p> </li> <li>Classic text on the method</li> </ul>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#applications-to-sdes","title":"Applications to SDEs","text":"<ul> <li>Pavliotis (2014): Stochastic Processes and Applications</li> <li> <p>Section on scaling and dimensional analysis</p> </li> <li> <p>Gardiner (2009): Stochastic Methods</p> </li> <li>Physical intuition for SDEs</li> </ul>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#diffusion-models","title":"Diffusion Models","text":"<ul> <li>Song et al. (2021): Score-Based Generative Modeling through SDEs</li> <li>Implicit dimensional structure in noise schedules</li> </ul>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#exercises","title":"Exercises","text":""},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#conceptual","title":"Conceptual","text":"<ol> <li> <p>Why inverse data? Explain in your own words why the score function must have units \\([1/X]\\).</p> </li> <li> <p>Variance scaling: Show that if noise scaled as \\(dt\\) instead of \\(\\sqrt{dt}\\), variance would have the wrong units.</p> </li> <li> <p>Gradient units: Verify that \\([\\nabla_x] = [1/X]\\) by considering the definition of a derivative.</p> </li> </ol>"},{"location":"diffusion/02_sde_formulation/supplements/08_dimensional_analysis/#computational","title":"Computational","text":"<ol> <li> <p>Check VP-SDE: For VP-SDE with \\(f(x,t) = -\\frac{1}{2}\\beta(t) x\\) and \\(g(t) = \\sqrt{\\beta(t)}\\), verify dimensional consistency assuming normalized data.</p> </li> <li> <p>Score network: Design a neural network architecture that naturally outputs a score function with the correct units.</p> </li> </ol> <p>Next: With dimensional intuition in place, you can now confidently design noise schedules, verify equations, and understand why certain formulas are inevitable rather than arbitrary!</p>"},{"location":"diffusion/03_medical_imaging_diffusion/","title":"Medical Imaging Diffusion Models","text":"<p>This notebook demonstrates diffusion models on realistic medical images, bridging the gap between toy examples (Swiss roll) and high-dimensional applications (gene expression).</p>"},{"location":"diffusion/03_medical_imaging_diffusion/#learning-objectives","title":"Learning Objectives","text":"<ol> <li>Apply U-Net architecture to real medical images</li> <li>Handle grayscale medical imaging data (X-rays, CT slices)</li> <li>Implement data preprocessing for medical images</li> <li>Generate synthetic medical images with diffusion models</li> <li>Evaluate quality with domain-specific metrics</li> </ol>"},{"location":"diffusion/03_medical_imaging_diffusion/#datasets-used","title":"Datasets Used","text":"<p>We use publicly available, realistic medical imaging datasets that are computationally feasible:</p>"},{"location":"diffusion/03_medical_imaging_diffusion/#1-chest-x-ray-images-primary-dataset","title":"1. Chest X-Ray Images (Primary Dataset)","text":"<ul> <li>Source: NIH Chest X-ray Dataset (downsampled)</li> <li>Size: 128\u00d7128 grayscale images (downsampled from 1024\u00d71024)</li> <li>Modality: X-ray (radiography)</li> <li>Use case: Generate synthetic chest X-rays</li> <li>Why: Widely used, clinically relevant, single-channel (memory efficient)</li> <li>Download: Available via Kaggle or NIH Clinical Center</li> </ul>"},{"location":"diffusion/03_medical_imaging_diffusion/#2-brain-mri-slices-alternative","title":"2. Brain MRI Slices (Alternative)","text":"<ul> <li>Source: BraTS or IXI Dataset (2D slices)</li> <li>Size: 128\u00d7128 or 256\u00d7256 grayscale</li> <li>Modality: MRI (T1, T2, FLAIR)</li> <li>Use case: Generate brain MRI slices</li> <li>Why: Important for neuroimaging, good for demonstrating multi-modal generation</li> </ul>"},{"location":"diffusion/03_medical_imaging_diffusion/#3-histopathology-patches-advanced","title":"3. Histopathology Patches (Advanced)","text":"<ul> <li>Source: Camelyon16/17 or PatchCamelyon</li> <li>Size: 96\u00d796 RGB patches</li> <li>Modality: H&amp;E stained tissue</li> <li>Use case: Generate tissue patches for data augmentation</li> <li>Why: Connects to your pathology-ai-lab project</li> </ul>"},{"location":"diffusion/03_medical_imaging_diffusion/#computational-requirements","title":"Computational Requirements","text":""},{"location":"diffusion/03_medical_imaging_diffusion/#memory-efficient-setup-recommended","title":"Memory-Efficient Setup (Recommended)","text":"<ul> <li>Image size: 128\u00d7128 (or 64\u00d764 for faster iteration)</li> <li>Batch size: 16-32</li> <li>Model: UNet2D with base_channels=32 or 64</li> <li>Training time: 2-4 hours on M1/M2 Mac or consumer GPU</li> <li>Memory: ~4-8GB GPU/unified memory</li> </ul>"},{"location":"diffusion/03_medical_imaging_diffusion/#full-resolution-setup-if-resources-available","title":"Full-Resolution Setup (If resources available)","text":"<ul> <li>Image size: 256\u00d7256</li> <li>Batch size: 8-16</li> <li>Model: UNet2D with base_channels=64</li> <li>Training time: 8-12 hours</li> <li>Memory: ~16GB GPU memory</li> </ul>"},{"location":"diffusion/03_medical_imaging_diffusion/#notebook-structure","title":"Notebook Structure","text":"<ol> <li>Setup &amp; Data Loading</li> <li>Download and preprocess medical images</li> <li>Create PyTorch dataset</li> <li> <p>Visualize samples</p> </li> <li> <p>Model Architecture</p> </li> <li>Implement UNet2D for medical images</li> <li>Time conditioning</li> <li> <p>GroupNorm for small batches</p> </li> <li> <p>Training</p> </li> <li>VP-SDE with cosine schedule</li> <li>Score matching loss</li> <li> <p>Training loop with checkpointing</p> </li> <li> <p>Generation &amp; Evaluation</p> </li> <li>Sample synthetic images</li> <li>Visual quality assessment</li> <li>Quantitative metrics (FID, IS)</li> <li> <p>Domain-specific evaluation</p> </li> <li> <p>Applications</p> </li> <li>Data augmentation for downstream tasks</li> <li>Conditional generation (by disease, view angle)</li> <li>Inpainting and super-resolution</li> </ol>"},{"location":"diffusion/03_medical_imaging_diffusion/#key-differences-from-toy-examples","title":"Key Differences from Toy Examples","text":"Aspect Toy (Swiss Roll) Medical Imaging Data 2D points 128\u00d7128 images (16K dims) Architecture Simple MLP U-Net with skip connections Training time Minutes Hours Evaluation Visual FID, clinical metrics Applications Educational Data augmentation, synthesis"},{"location":"diffusion/03_medical_imaging_diffusion/#prerequisites","title":"Prerequisites","text":"<ul> <li>Completed <code>02_sde_formulation.ipynb</code></li> <li>Understanding of convolutional neural networks</li> <li>Familiarity with medical imaging (helpful but not required)</li> </ul>"},{"location":"diffusion/03_medical_imaging_diffusion/#next-steps","title":"Next Steps","text":"<p>After this notebook: - <code>04_gene_expression_diffusion.ipynb</code>: High-dimensional tabular data - Your <code>pathology-ai-lab</code> project: Whole-slide imaging with diffusion models</p>"},{"location":"diffusion/03_medical_imaging_diffusion/#references","title":"References","text":""},{"location":"diffusion/03_medical_imaging_diffusion/#datasets","title":"Datasets","text":"<ul> <li>NIH Chest X-ray: Wang et al. (2017) \"ChestX-ray8: Hospital-scale Chest X-ray Database\"</li> <li>BraTS: Menze et al. (2015) \"The Multimodal Brain Tumor Image Segmentation Benchmark\"</li> <li>Camelyon: Bejnordi et al. (2017) \"Diagnostic Assessment of Deep Learning Algorithms\"</li> </ul>"},{"location":"diffusion/03_medical_imaging_diffusion/#medical-imaging-diffusion","title":"Medical Imaging Diffusion","text":"<ul> <li>MedSegDiff: Wu et al. (2023) \"MedSegDiff: Medical Image Segmentation with Diffusion Models\"</li> <li>DiffMIC: \u00d6zbey et al. (2023) \"Unsupervised Medical Image Translation with Adversarial Diffusion Models\"</li> <li>RoentGen: Chambon et al. (2022) \"RoentGen: Vision-Language Foundation Model for Chest X-ray Generation\"</li> </ul>"},{"location":"diffusion/03_medical_imaging_diffusion/#architecture","title":"Architecture","text":"<ul> <li>U-Net: Ronneberger et al. (2015) \"U-Net: Convolutional Networks for Biomedical Image Segmentation\"</li> <li>DDPM: Ho et al. (2020) \"Denoising Diffusion Probabilistic Models\"</li> </ul>"},{"location":"diffusion/03_medical_imaging_diffusion/03_medical_imaging_diffusion/","title":"Notebook","text":"In\u00a0[5]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom PIL import Image\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\n\n# Import genailab modules (no sys.path manipulation needed!)\nfrom genailab.diffusion import (\n    VPSDE,\n    UNet2D,\n    train_score_network,\n    sample_reverse_sde,\n    sample_probability_flow_ode\n)\nfrom genailab import get_config, get_checkpoint_dir, get_device\n\n# Configuration\nconfig = get_config()\ndevice = get_device()\ncheckpoint_dir = get_checkpoint_dir(\"diffusion/medical_imaging\")\n\n# Set style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 4)\n\n# Random seeds\nnp.random.seed(42)\ntorch.manual_seed(42)\n\nprint(f'Using device: {device}')\nprint(f'Project root: {config.project_root}')\nprint(f'Checkpoints will be saved to: {checkpoint_dir}')\n</pre> import numpy as np import matplotlib.pyplot as plt import seaborn as sns from pathlib import Path from PIL import Image from tqdm.auto import tqdm  import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import Dataset, DataLoader import torchvision.transforms as transforms  # Import genailab modules (no sys.path manipulation needed!) from genailab.diffusion import (     VPSDE,     UNet2D,     train_score_network,     sample_reverse_sde,     sample_probability_flow_ode ) from genailab import get_config, get_checkpoint_dir, get_device  # Configuration config = get_config() device = get_device() checkpoint_dir = get_checkpoint_dir(\"diffusion/medical_imaging\")  # Set style sns.set_style('whitegrid') plt.rcParams['figure.figsize'] = (12, 4)  # Random seeds np.random.seed(42) torch.manual_seed(42)  print(f'Using device: {device}') print(f'Project root: {config.project_root}') print(f'Checkpoints will be saved to: {checkpoint_dir}') <pre>Using device: mps\nProject root: /Users/pleiadian53/work/genai-lab\nCheckpoints will be saved to: /Users/pleiadian53/work/genai-lab/checkpoints/diffusion/medical_imaging\n</pre> In\u00a0[6]: Copied! <pre># ============================================================================\n# Model Preset Configuration\n# ============================================================================\n\n# Choose preset: 'tiny', 'small', 'medium', or 'large'\nPRESET = 'tiny'  # Change this to 'medium' or 'large' for better quality\n\n# Preset configurations\nPRESETS = {\n    \"tiny\": {\n        \"base_channels\": 16,\n        \"channel_multipliers\": (1, 2, 4),\n        \"num_res_blocks\": 1,\n        \"time_emb_dim\": 64,\n        \"img_size\": 64,\n        \"batch_size\": 32,\n        \"epochs\": 100,\n        \"n_samples\": 1000,\n        \"description\": \"~1M params - Quick logic verification (M1 Mac friendly)\",\n    },\n    \"small\": {\n        \"base_channels\": 32,\n        \"channel_multipliers\": (1, 2, 4),\n        \"num_res_blocks\": 1,\n        \"time_emb_dim\": 128,\n        \"img_size\": 64,\n        \"batch_size\": 32,\n        \"epochs\": 500,\n        \"n_samples\": 2000,\n        \"description\": \"~5M params - Local testing with reasonable quality\",\n    },\n    \"medium\": {\n        \"base_channels\": 48,\n        \"channel_multipliers\": (1, 2, 4, 8),\n        \"num_res_blocks\": 2,\n        \"time_emb_dim\": 192,\n        \"img_size\": 128,\n        \"batch_size\": 16,\n        \"epochs\": 2000,\n        \"n_samples\": 2000,\n        \"description\": \"~20M params - Good quality, needs decent GPU\",\n    },\n    \"large\": {\n        \"base_channels\": 64,\n        \"channel_multipliers\": (1, 2, 4, 8),\n        \"num_res_blocks\": 2,\n        \"time_emb_dim\": 256,\n        \"img_size\": 128,\n        \"batch_size\": 32,\n        \"epochs\": 5000,\n        \"n_samples\": 2000,\n        \"description\": \"~50M params - Production quality (A40/A100 recommended)\",\n    },\n}\n\n# Get selected preset\nconfig_preset = PRESETS[PRESET]\n\nprint(f\"Selected preset: {PRESET}\")\nprint(f\"Description: {config_preset['description']}\")\nprint(f\"\\nConfiguration:\")\nprint(f\"  Image size: {config_preset['img_size']}\u00d7{config_preset['img_size']}\")\nprint(f\"  Base channels: {config_preset['base_channels']}\")\nprint(f\"  Channel multipliers: {config_preset['channel_multipliers']}\")\nprint(f\"  Residual blocks: {config_preset['num_res_blocks']}\")\nprint(f\"  Time embedding dim: {config_preset['time_emb_dim']}\")\nprint(f\"  Batch size: {config_preset['batch_size']}\")\nprint(f\"  Training epochs: {config_preset['epochs']}\")\nprint(f\"  Dataset samples: {config_preset['n_samples']}\")\n\n# ============================================================================\n# Dataset Preparation\n# ============================================================================\n\n# Import the improved SyntheticXRayDataset from genailab\nfrom genailab.diffusion import SyntheticXRayDataset\n\n# Create dataset with diverse synthetic X-rays\ndataset = SyntheticXRayDataset(\n    n_samples=config_preset['n_samples'], \n    img_size=config_preset['img_size'], \n    seed=42\n)\nprint(f\"\\nDataset: {len(dataset)} images, shape: {dataset[0].shape}\")\n\n# Create dataloader\ndataloader = DataLoader(\n    dataset, \n    batch_size=config_preset['batch_size'], \n    shuffle=True, \n    num_workers=0\n)\nprint(f\"Batches per epoch: {len(dataloader)}\")\n</pre> # ============================================================================ # Model Preset Configuration # ============================================================================  # Choose preset: 'tiny', 'small', 'medium', or 'large' PRESET = 'tiny'  # Change this to 'medium' or 'large' for better quality  # Preset configurations PRESETS = {     \"tiny\": {         \"base_channels\": 16,         \"channel_multipliers\": (1, 2, 4),         \"num_res_blocks\": 1,         \"time_emb_dim\": 64,         \"img_size\": 64,         \"batch_size\": 32,         \"epochs\": 100,         \"n_samples\": 1000,         \"description\": \"~1M params - Quick logic verification (M1 Mac friendly)\",     },     \"small\": {         \"base_channels\": 32,         \"channel_multipliers\": (1, 2, 4),         \"num_res_blocks\": 1,         \"time_emb_dim\": 128,         \"img_size\": 64,         \"batch_size\": 32,         \"epochs\": 500,         \"n_samples\": 2000,         \"description\": \"~5M params - Local testing with reasonable quality\",     },     \"medium\": {         \"base_channels\": 48,         \"channel_multipliers\": (1, 2, 4, 8),         \"num_res_blocks\": 2,         \"time_emb_dim\": 192,         \"img_size\": 128,         \"batch_size\": 16,         \"epochs\": 2000,         \"n_samples\": 2000,         \"description\": \"~20M params - Good quality, needs decent GPU\",     },     \"large\": {         \"base_channels\": 64,         \"channel_multipliers\": (1, 2, 4, 8),         \"num_res_blocks\": 2,         \"time_emb_dim\": 256,         \"img_size\": 128,         \"batch_size\": 32,         \"epochs\": 5000,         \"n_samples\": 2000,         \"description\": \"~50M params - Production quality (A40/A100 recommended)\",     }, }  # Get selected preset config_preset = PRESETS[PRESET]  print(f\"Selected preset: {PRESET}\") print(f\"Description: {config_preset['description']}\") print(f\"\\nConfiguration:\") print(f\"  Image size: {config_preset['img_size']}\u00d7{config_preset['img_size']}\") print(f\"  Base channels: {config_preset['base_channels']}\") print(f\"  Channel multipliers: {config_preset['channel_multipliers']}\") print(f\"  Residual blocks: {config_preset['num_res_blocks']}\") print(f\"  Time embedding dim: {config_preset['time_emb_dim']}\") print(f\"  Batch size: {config_preset['batch_size']}\") print(f\"  Training epochs: {config_preset['epochs']}\") print(f\"  Dataset samples: {config_preset['n_samples']}\")  # ============================================================================ # Dataset Preparation # ============================================================================  # Import the improved SyntheticXRayDataset from genailab from genailab.diffusion import SyntheticXRayDataset  # Create dataset with diverse synthetic X-rays dataset = SyntheticXRayDataset(     n_samples=config_preset['n_samples'],      img_size=config_preset['img_size'],      seed=42 ) print(f\"\\nDataset: {len(dataset)} images, shape: {dataset[0].shape}\")  # Create dataloader dataloader = DataLoader(     dataset,      batch_size=config_preset['batch_size'],      shuffle=True,      num_workers=0 ) print(f\"Batches per epoch: {len(dataloader)}\") <pre>Selected preset: tiny\nDescription: ~1M params - Quick logic verification (M1 Mac friendly)\n\nConfiguration:\n  Image size: 64\u00d764\n  Base channels: 16\n  Channel multipliers: (1, 2, 4)\n  Residual blocks: 1\n  Time embedding dim: 64\n  Batch size: 32\n  Training epochs: 100\n  Dataset samples: 1000\n\nDataset: 1000 images, shape: torch.Size([1, 64, 64])\nBatches per epoch: 32\n</pre> In\u00a0[7]: Copied! <pre># Visualize samples\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\naxes = axes.flatten()\n\nfor i in range(10):\n    img = dataset[i].squeeze().numpy()\n    # Denormalize from [-1, 1] to [0, 1]\n    img = (img + 1) / 2\n    axes[i].imshow(img, cmap='gray', vmin=0, vmax=1)\n    axes[i].axis('off')\n    axes[i].set_title(f'Sample {i+1}')\n\nplt.suptitle('Synthetic Chest X-ray Images (128\u00d7128)', fontsize=14)\nplt.tight_layout()\nplt.show()\n\nprint(\"Note: These are synthetic images for demonstration.\")\nprint(\"Replace with real medical images for production use.\")\n</pre> # Visualize samples fig, axes = plt.subplots(2, 5, figsize=(15, 6)) axes = axes.flatten()  for i in range(10):     img = dataset[i].squeeze().numpy()     # Denormalize from [-1, 1] to [0, 1]     img = (img + 1) / 2     axes[i].imshow(img, cmap='gray', vmin=0, vmax=1)     axes[i].axis('off')     axes[i].set_title(f'Sample {i+1}')  plt.suptitle('Synthetic Chest X-ray Images (128\u00d7128)', fontsize=14) plt.tight_layout() plt.show()  print(\"Note: These are synthetic images for demonstration.\") print(\"Replace with real medical images for production use.\") <pre>Note: These are synthetic images for demonstration.\nReplace with real medical images for production use.\n</pre> In\u00a0[8]: Copied! <pre># Initialize U-Net model with preset configuration\nmodel = UNet2D(\n    in_channels=1,              # Grayscale X-rays\n    out_channels=1,\n    base_channels=config_preset['base_channels'],\n    channel_multipliers=config_preset['channel_multipliers'],\n    num_res_blocks=config_preset['num_res_blocks'],\n    time_emb_dim=config_preset['time_emb_dim'],\n).to(device)\n\nn_params = sum(p.numel() for p in model.parameters())\nprint(f\"Model parameters: {n_params:,}\")\n\n# Test forward pass\nx_test = torch.randn(4, 1, config_preset['img_size'], config_preset['img_size']).to(device)\nt_test = torch.rand(4).to(device)\nout_test = model(x_test, t_test)\nprint(f\"Input shape: {x_test.shape}\")\nprint(f\"Output shape: {out_test.shape}\")\nprint(\"\u2713 Model initialized successfully\")\n</pre> # Initialize U-Net model with preset configuration model = UNet2D(     in_channels=1,              # Grayscale X-rays     out_channels=1,     base_channels=config_preset['base_channels'],     channel_multipliers=config_preset['channel_multipliers'],     num_res_blocks=config_preset['num_res_blocks'],     time_emb_dim=config_preset['time_emb_dim'], ).to(device)  n_params = sum(p.numel() for p in model.parameters()) print(f\"Model parameters: {n_params:,}\")  # Test forward pass x_test = torch.randn(4, 1, config_preset['img_size'], config_preset['img_size']).to(device) t_test = torch.rand(4).to(device) out_test = model(x_test, t_test) print(f\"Input shape: {x_test.shape}\") print(f\"Output shape: {out_test.shape}\") print(\"\u2713 Model initialized successfully\") <pre>Model parameters: 539,697\nInput shape: torch.Size([4, 1, 64, 64])\nOutput shape: torch.Size([4, 1, 64, 64])\n\u2713 Model initialized successfully\n</pre> In\u00a0[9]: Copied! <pre># Initialize SDE with cosine schedule (recommended for images)\nsde = VPSDE(schedule='cosine', T=1.0)\nprint(f\"Using {sde.schedule_name} noise schedule\")\n\n# Visualize the noise schedule\nt_vals = np.linspace(0, 1, 100)\nbeta_vals = [sde.beta(t) for t in t_vals]\nalpha_bar_vals = [sde.schedule.alpha_bar(t) for t in t_vals]\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\naxes[0].plot(t_vals, beta_vals)\naxes[0].set_xlabel('Time t')\naxes[0].set_ylabel('\u03b2(t)')\naxes[0].set_title('Noise Schedule')\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(t_vals, alpha_bar_vals)\naxes[1].set_xlabel('Time t')\naxes[1].set_ylabel('\u1fb1(t)')\naxes[1].set_title('Signal Retention')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</pre> # Initialize SDE with cosine schedule (recommended for images) sde = VPSDE(schedule='cosine', T=1.0) print(f\"Using {sde.schedule_name} noise schedule\")  # Visualize the noise schedule t_vals = np.linspace(0, 1, 100) beta_vals = [sde.beta(t) for t in t_vals] alpha_bar_vals = [sde.schedule.alpha_bar(t) for t in t_vals]  fig, axes = plt.subplots(1, 2, figsize=(12, 4)) axes[0].plot(t_vals, beta_vals) axes[0].set_xlabel('Time t') axes[0].set_ylabel('\u03b2(t)') axes[0].set_title('Noise Schedule') axes[0].grid(True, alpha=0.3)  axes[1].plot(t_vals, alpha_bar_vals) axes[1].set_xlabel('Time t') axes[1].set_ylabel('\u1fb1(t)') axes[1].set_title('Signal Retention') axes[1].grid(True, alpha=0.3)  plt.tight_layout() plt.show() <pre>Using cosine noise schedule\n</pre> In\u00a0[10]: Copied! <pre># Take one sample image\nx0 = dataset[0].unsqueeze(0).numpy()  # (1, 1, 128, 128)\n\n# Apply forward diffusion at different times\ntimes = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\n\nfor i, t in enumerate(times):\n    if t == 0:\n        xt = x0\n    else:\n        xt, _ = sde.sample_from_marginal(x0, t)\n    \n    # Denormalize and display\n    img = xt[0, 0]  # (128, 128)\n    img = (img + 1) / 2  # [-1, 1] \u2192 [0, 1]\n    \n    axes[i].imshow(img, cmap='gray', vmin=0, vmax=1)\n    axes[i].axis('off')\n    axes[i].set_title(f't = {t:.1f}')\n    \n    # Show std - extract scalar from array\n    _, std = sde.marginal_prob(x0, t)\n    std_val = float(np.mean(std))  # Get scalar value\n    axes[i].text(0.05, 0.95, f'\u03c3={std_val:.3f}', \n                transform=axes[i].transAxes,\n                verticalalignment='top',\n                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.suptitle('Forward Diffusion: X-ray \u2192 Noise', fontsize=14, y=0.98)\nplt.tight_layout()\nplt.show()\n\nprint(\"Observation: The X-ray structure gradually dissolves into Gaussian noise.\")\n</pre> # Take one sample image x0 = dataset[0].unsqueeze(0).numpy()  # (1, 1, 128, 128)  # Apply forward diffusion at different times times = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]  fig, axes = plt.subplots(2, 3, figsize=(15, 10)) axes = axes.flatten()  for i, t in enumerate(times):     if t == 0:         xt = x0     else:         xt, _ = sde.sample_from_marginal(x0, t)          # Denormalize and display     img = xt[0, 0]  # (128, 128)     img = (img + 1) / 2  # [-1, 1] \u2192 [0, 1]          axes[i].imshow(img, cmap='gray', vmin=0, vmax=1)     axes[i].axis('off')     axes[i].set_title(f't = {t:.1f}')          # Show std - extract scalar from array     _, std = sde.marginal_prob(x0, t)     std_val = float(np.mean(std))  # Get scalar value     axes[i].text(0.05, 0.95, f'\u03c3={std_val:.3f}',                  transform=axes[i].transAxes,                 verticalalignment='top',                 bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))  plt.suptitle('Forward Diffusion: X-ray \u2192 Noise', fontsize=14, y=0.98) plt.tight_layout() plt.show()  print(\"Observation: The X-ray structure gradually dissolves into Gaussian noise.\") <pre>Observation: The X-ray structure gradually dissolves into Gaussian noise.\n</pre> In\u00a0[11]: Copied! <pre># Import training function from genailab\nfrom genailab.diffusion import train_image_diffusion\n\n# Train the model with preset configuration\nprint(\"Starting training...\")\nprint(f\"Preset: {PRESET} ({config_preset['description']})\")\nprint(f\"This will take approximately {config_preset['epochs'] * len(dataloader) / 60:.0f} minutes on {device}\")\nprint(f\"Checkpoints will be saved to: {checkpoint_dir}\")\n\nlosses = train_image_diffusion(\n    model=model,\n    dataloader=dataloader,\n    sde=sde,\n    num_epochs=config_preset['epochs'],\n    lr=2e-4,\n    device=device,\n    save_every=max(100, config_preset['epochs'] // 5),  # Save 5 checkpoints\n    checkpoint_dir=checkpoint_dir\n)\n</pre> # Import training function from genailab from genailab.diffusion import train_image_diffusion  # Train the model with preset configuration print(\"Starting training...\") print(f\"Preset: {PRESET} ({config_preset['description']})\") print(f\"This will take approximately {config_preset['epochs'] * len(dataloader) / 60:.0f} minutes on {device}\") print(f\"Checkpoints will be saved to: {checkpoint_dir}\")  losses = train_image_diffusion(     model=model,     dataloader=dataloader,     sde=sde,     num_epochs=config_preset['epochs'],     lr=2e-4,     device=device,     save_every=max(100, config_preset['epochs'] // 5),  # Save 5 checkpoints     checkpoint_dir=checkpoint_dir ) <pre>Starting training...\nPreset: tiny (~1M params - Quick logic verification (M1 Mac friendly))\nThis will take approximately 53 minutes on mps\nCheckpoints will be saved to: /Users/pleiadian53/work/genai-lab/checkpoints/diffusion/medical_imaging\n</pre> <pre>Training:   0%|          | 0/100 [00:00&lt;?, ?it/s]</pre> In\u00a0[12]: Copied! <pre># Plot training curve\nplt.figure(figsize=(10, 4))\nplt.plot(losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.yscale('log')\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(f\"Final loss: {losses[-1]:.6f}\")\nprint(f\"Best loss: {min(losses):.6f} at epoch {np.argmin(losses) + 1}\")\n</pre> # Plot training curve plt.figure(figsize=(10, 4)) plt.plot(losses) plt.xlabel('Epoch') plt.ylabel('Loss') plt.title('Training Loss') plt.yscale('log') plt.grid(True, alpha=0.3) plt.show()  print(f\"Final loss: {losses[-1]:.6f}\") print(f\"Best loss: {min(losses):.6f} at epoch {np.argmin(losses) + 1}\") <pre>Final loss: 0.168302\nBest loss: 0.136660 at epoch 99\n</pre> In\u00a0[13]: Copied! <pre># Sampling function for images\n@torch.no_grad()\ndef sample_images(\n    model,\n    sde,\n    n_samples=16,\n    img_size=None,  # Will use preset img_size if None\n    num_steps=500,\n    device='cuda'\n):\n    \"\"\"Sample images using reverse SDE.\"\"\"\n    \n    if img_size is None:\n        img_size = config_preset['img_size']\n    \n    model.eval()\n    \n    # Start from noise\n    x = torch.randn(n_samples, 1, img_size, img_size, device=device)\n    \n    dt = -sde.T / num_steps\n    trajectory = [x.cpu().numpy()]\n    \n    for i in tqdm(range(num_steps), desc=\"Sampling\", leave=False):\n        t = sde.T - i * (-dt)\n        t_batch = torch.ones(n_samples, device=device) * t\n        \n        # Predict score\n        score = model(x, t_batch)\n        \n        # Drift\n        drift = sde.drift(x.cpu().numpy(), t)\n        drift = torch.FloatTensor(drift).to(device)\n        g_t = sde.diffusion(t)\n        drift = drift - (g_t ** 2) * score\n        \n        # Diffusion\n        noise = torch.randn_like(x)\n        diffusion = g_t * noise * np.sqrt(-dt)\n        \n        # Update\n        x = x + drift * dt + diffusion\n        \n        if i % 50 == 0:\n            trajectory.append(x.cpu().numpy())\n    \n    return x.cpu().numpy(), np.array(trajectory)\n\n\n# Generate samples\nprint(\"Generating synthetic X-rays...\")\nsamples, trajectory = sample_images(\n    model=model,\n    sde=sde,\n    n_samples=16,\n    img_size=config_preset['img_size'],\n    num_steps=500,\n    device=device\n)\n\nprint(f\"Generated {samples.shape[0]} images\")\nprint(f\"Trajectory: {len(trajectory)} snapshots\")\n</pre> # Sampling function for images @torch.no_grad() def sample_images(     model,     sde,     n_samples=16,     img_size=None,  # Will use preset img_size if None     num_steps=500,     device='cuda' ):     \"\"\"Sample images using reverse SDE.\"\"\"          if img_size is None:         img_size = config_preset['img_size']          model.eval()          # Start from noise     x = torch.randn(n_samples, 1, img_size, img_size, device=device)          dt = -sde.T / num_steps     trajectory = [x.cpu().numpy()]          for i in tqdm(range(num_steps), desc=\"Sampling\", leave=False):         t = sde.T - i * (-dt)         t_batch = torch.ones(n_samples, device=device) * t                  # Predict score         score = model(x, t_batch)                  # Drift         drift = sde.drift(x.cpu().numpy(), t)         drift = torch.FloatTensor(drift).to(device)         g_t = sde.diffusion(t)         drift = drift - (g_t ** 2) * score                  # Diffusion         noise = torch.randn_like(x)         diffusion = g_t * noise * np.sqrt(-dt)                  # Update         x = x + drift * dt + diffusion                  if i % 50 == 0:             trajectory.append(x.cpu().numpy())          return x.cpu().numpy(), np.array(trajectory)   # Generate samples print(\"Generating synthetic X-rays...\") samples, trajectory = sample_images(     model=model,     sde=sde,     n_samples=16,     img_size=config_preset['img_size'],     num_steps=500,     device=device )  print(f\"Generated {samples.shape[0]} images\") print(f\"Trajectory: {len(trajectory)} snapshots\") <pre>Generating synthetic X-rays...\n</pre> <pre>Sampling:   0%|          | 0/500 [00:00&lt;?, ?it/s]</pre> <pre>Generated 16 images\nTrajectory: 11 snapshots\n</pre> In\u00a0[14]: Copied! <pre># Display generated images\nfig, axes = plt.subplots(4, 4, figsize=(12, 12))\naxes = axes.flatten()\n\nfor i in range(16):\n    img = samples[i, 0]  # (128, 128)\n    # Denormalize from [-1, 1] to [0, 1]\n    img = (img + 1) / 2\n    img = np.clip(img, 0, 1)\n    \n    axes[i].imshow(img, cmap='gray', vmin=0, vmax=1)\n    axes[i].axis('off')\n\nplt.suptitle('Generated Synthetic X-rays', fontsize=14)\nplt.tight_layout()\nplt.show()\n</pre> # Display generated images fig, axes = plt.subplots(4, 4, figsize=(12, 12)) axes = axes.flatten()  for i in range(16):     img = samples[i, 0]  # (128, 128)     # Denormalize from [-1, 1] to [0, 1]     img = (img + 1) / 2     img = np.clip(img, 0, 1)          axes[i].imshow(img, cmap='gray', vmin=0, vmax=1)     axes[i].axis('off')  plt.suptitle('Generated Synthetic X-rays', fontsize=14) plt.tight_layout() plt.show() In\u00a0[15]: Copied! <pre># Show reverse diffusion trajectory\nnum_snapshots = min(6, len(trajectory))\nindices = np.linspace(0, len(trajectory)-1, num_snapshots, dtype=int)\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\n\nfor i, idx in enumerate(indices):\n    xt = trajectory[idx][0, 0]  # First sample, first channel\n    t_val = sde.T * (1 - idx / (len(trajectory) - 1))\n    \n    # Denormalize\n    img = (xt + 1) / 2\n    img = np.clip(img, 0, 1)\n    \n    axes[i].imshow(img, cmap='gray', vmin=0, vmax=1)\n    axes[i].axis('off')\n    axes[i].set_title(f't = {t_val:.3f}')\n\nplt.suptitle('Reverse Diffusion: Noise \u2192 X-ray', fontsize=14, y=0.98)\nplt.tight_layout()\nplt.show()\n\nprint(\"Observation: The model progressively denoises to reveal X-ray structure.\")\n</pre> # Show reverse diffusion trajectory num_snapshots = min(6, len(trajectory)) indices = np.linspace(0, len(trajectory)-1, num_snapshots, dtype=int)  fig, axes = plt.subplots(2, 3, figsize=(15, 10)) axes = axes.flatten()  for i, idx in enumerate(indices):     xt = trajectory[idx][0, 0]  # First sample, first channel     t_val = sde.T * (1 - idx / (len(trajectory) - 1))          # Denormalize     img = (xt + 1) / 2     img = np.clip(img, 0, 1)          axes[i].imshow(img, cmap='gray', vmin=0, vmax=1)     axes[i].axis('off')     axes[i].set_title(f't = {t_val:.3f}')  plt.suptitle('Reverse Diffusion: Noise \u2192 X-ray', fontsize=14, y=0.98) plt.tight_layout() plt.show()  print(\"Observation: The model progressively denoises to reveal X-ray structure.\") <pre>Observation: The model progressively denoises to reveal X-ray structure.\n</pre> In\u00a0[16]: Copied! <pre># Side-by-side comparison\nfig, axes = plt.subplots(2, 8, figsize=(16, 4))\n\n# Real images (top row)\nfor i in range(8):\n    img = dataset[i].squeeze().numpy()\n    img = (img + 1) / 2\n    axes[0, i].imshow(img, cmap='gray', vmin=0, vmax=1)\n    axes[0, i].axis('off')\n    if i == 0:\n        axes[0, i].set_title('Real', fontsize=12, fontweight='bold')\n\n# Generated images (bottom row)\nfor i in range(8):\n    img = samples[i, 0]\n    img = (img + 1) / 2\n    img = np.clip(img, 0, 1)\n    axes[1, i].imshow(img, cmap='gray', vmin=0, vmax=1)\n    axes[1, i].axis('off')\n    if i == 0:\n        axes[1, i].set_title('Generated', fontsize=12, fontweight='bold')\n\nplt.suptitle('Real vs Generated X-rays', fontsize=14)\nplt.tight_layout()\nplt.show()\n</pre> # Side-by-side comparison fig, axes = plt.subplots(2, 8, figsize=(16, 4))  # Real images (top row) for i in range(8):     img = dataset[i].squeeze().numpy()     img = (img + 1) / 2     axes[0, i].imshow(img, cmap='gray', vmin=0, vmax=1)     axes[0, i].axis('off')     if i == 0:         axes[0, i].set_title('Real', fontsize=12, fontweight='bold')  # Generated images (bottom row) for i in range(8):     img = samples[i, 0]     img = (img + 1) / 2     img = np.clip(img, 0, 1)     axes[1, i].imshow(img, cmap='gray', vmin=0, vmax=1)     axes[1, i].axis('off')     if i == 0:         axes[1, i].set_title('Generated', fontsize=12, fontweight='bold')  plt.suptitle('Real vs Generated X-rays', fontsize=14) plt.tight_layout() plt.show() In\u00a0[18]: Copied! <pre># Simple evaluation: pixel statistics\nreal_images = np.array([dataset[i].squeeze().numpy() for i in range(100)])\ngen_images = samples[:, 0, :, :]\n\nprint(\"Pixel Statistics Comparison:\")\nprint(f\"Real - Mean: {real_images.mean():.3f}, Std: {real_images.std():.3f}\")\nprint(f\"Generated - Mean: {gen_images.mean():.3f}, Std: {gen_images.std():.3f}\")\n\n# Histogram comparison\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\naxes[0].hist(real_images.flatten(), bins=50, alpha=0.7, label='Real', density=True)\naxes[0].hist(gen_images.flatten(), bins=50, alpha=0.7, label='Generated', density=True)\naxes[0].set_xlabel('Pixel Value')\naxes[0].set_ylabel('Density')\naxes[0].set_title('Pixel Value Distribution')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Power spectrum (frequency analysis)\nreal_fft = np.abs(np.fft.fft2(real_images[0]))\ngen_fft = np.abs(np.fft.fft2(gen_images[0]))\n\n# Use center row index based on actual image size (not hardcoded 64)\ncenter_row = real_fft.shape[0] // 2\n\naxes[1].plot(np.log(real_fft[center_row, :]), label='Real', alpha=0.7)\naxes[1].plot(np.log(gen_fft[center_row, :]), label='Generated', alpha=0.7)\naxes[1].set_xlabel('Frequency')\naxes[1].set_ylabel('Log Magnitude')\naxes[1].set_title('Frequency Spectrum (Center Row)')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</pre> # Simple evaluation: pixel statistics real_images = np.array([dataset[i].squeeze().numpy() for i in range(100)]) gen_images = samples[:, 0, :, :]  print(\"Pixel Statistics Comparison:\") print(f\"Real - Mean: {real_images.mean():.3f}, Std: {real_images.std():.3f}\") print(f\"Generated - Mean: {gen_images.mean():.3f}, Std: {gen_images.std():.3f}\")  # Histogram comparison fig, axes = plt.subplots(1, 2, figsize=(12, 4))  axes[0].hist(real_images.flatten(), bins=50, alpha=0.7, label='Real', density=True) axes[0].hist(gen_images.flatten(), bins=50, alpha=0.7, label='Generated', density=True) axes[0].set_xlabel('Pixel Value') axes[0].set_ylabel('Density') axes[0].set_title('Pixel Value Distribution') axes[0].legend() axes[0].grid(True, alpha=0.3)  # Power spectrum (frequency analysis) real_fft = np.abs(np.fft.fft2(real_images[0])) gen_fft = np.abs(np.fft.fft2(gen_images[0]))  # Use center row index based on actual image size (not hardcoded 64) center_row = real_fft.shape[0] // 2  axes[1].plot(np.log(real_fft[center_row, :]), label='Real', alpha=0.7) axes[1].plot(np.log(gen_fft[center_row, :]), label='Generated', alpha=0.7) axes[1].set_xlabel('Frequency') axes[1].set_ylabel('Log Magnitude') axes[1].set_title('Frequency Spectrum (Center Row)') axes[1].legend() axes[1].grid(True, alpha=0.3)  plt.tight_layout() plt.show() <pre>Pixel Statistics Comparison:\nReal - Mean: -0.118, Std: 0.314\nGenerated - Mean: -0.176, Std: 0.578\n</pre> In\u00a0[19]: Copied! <pre>print(\"Application: Data Augmentation\")\nprint(\"=\"*50)\nprint(f\"Original dataset: {len(dataset)} images\")\nprint(f\"Generated images: {len(samples)} images\")\nprint(f\"Augmented dataset: {len(dataset) + len(samples)} images (+{len(samples)/len(dataset)*100:.0f}%)\")\nprint(\"\\nUse case: Train disease classifier with augmented data\")\n</pre> print(\"Application: Data Augmentation\") print(\"=\"*50) print(f\"Original dataset: {len(dataset)} images\") print(f\"Generated images: {len(samples)} images\") print(f\"Augmented dataset: {len(dataset) + len(samples)} images (+{len(samples)/len(dataset)*100:.0f}%)\") print(\"\\nUse case: Train disease classifier with augmented data\") <pre>Application: Data Augmentation\n==================================================\nOriginal dataset: 1000 images\nGenerated images: 16 images\nAugmented dataset: 1016 images (+2%)\n\nUse case: Train disease classifier with augmented data\n</pre>"},{"location":"diffusion/03_medical_imaging_diffusion/03_medical_imaging_diffusion/#diffusion-models-for-medical-imaging","title":"Diffusion Models for Medical Imaging\u00b6","text":"<p>This notebook applies diffusion models to realistic medical images, demonstrating practical applications of the SDE framework.</p> <p>Use case: Generate synthetic chest X-rays for data augmentation and medical AI training.</p> <p>Learning objectives:</p> <ol> <li>Apply U-Net architecture to real medical images</li> <li>Handle grayscale medical imaging data preprocessing</li> <li>Train diffusion models on 128\u00d7128 X-ray images</li> <li>Generate synthetic medical images</li> <li>Evaluate with domain-specific metrics</li> </ol> <p>Dataset: Chest X-ray images (downsampled to 128\u00d7128 for computational efficiency)</p> <p>Prerequisites: <code>02_sde_formulation.ipynb</code></p>"},{"location":"diffusion/03_medical_imaging_diffusion/03_medical_imaging_diffusion/#setup","title":"Setup\u00b6","text":""},{"location":"diffusion/03_medical_imaging_diffusion/03_medical_imaging_diffusion/#1-configuration-model-presets","title":"1. Configuration &amp; Model Presets\u00b6","text":"<p>Choose a model size based on your hardware:</p> Preset Parameters VRAM Image Size Epochs Hardware tiny ~1M &lt;2GB 64\u00d764 100 M1 Mac, CPU small ~5M ~4GB 64\u00d764 500 T4, RTX 3060 medium ~20M ~8GB 128\u00d7128 2000 RTX 3080, A10 large ~50M ~16GB 128\u00d7128 5000 A40, A100 <p>For this demo: We'll use the tiny preset for quick testing. Change to medium or large for better quality.</p>"},{"location":"diffusion/03_medical_imaging_diffusion/03_medical_imaging_diffusion/#visualize-sample-images","title":"Visualize Sample Images\u00b6","text":""},{"location":"diffusion/03_medical_imaging_diffusion/03_medical_imaging_diffusion/#2-model-architecture-u-net-for-medical-images","title":"2. Model Architecture: U-Net for Medical Images\u00b6","text":"<p>We use the UNet2D architecture specifically designed for medical imaging:</p> <ul> <li>Encoder-decoder with skip connections</li> <li>Multi-scale feature extraction</li> <li>GroupNorm (works well with small batches)</li> <li>Time conditioning at each resolution</li> </ul>"},{"location":"diffusion/03_medical_imaging_diffusion/03_medical_imaging_diffusion/#3-training-setup","title":"3. Training Setup\u00b6","text":"<p>We'll use:</p> <ul> <li>VP-SDE with cosine schedule (better for images)</li> <li>Denoising score matching loss</li> <li>Adam optimizer with learning rate 2e-4</li> <li>Gradient clipping for stability</li> </ul>"},{"location":"diffusion/03_medical_imaging_diffusion/03_medical_imaging_diffusion/#visualize-forward-diffusion-process","title":"Visualize Forward Diffusion Process\u00b6","text":"<p>Let's see how the SDE corrupts a real X-ray image over time.</p>"},{"location":"diffusion/03_medical_imaging_diffusion/03_medical_imaging_diffusion/#4-training-the-diffusion-model","title":"4. Training the Diffusion Model\u00b6","text":"<p>Note: Training on medical images takes longer than toy examples.</p> <ul> <li>Quick test: 1,000 epochs (~10 minutes on M1 Mac)</li> <li>Good quality: 10,000 epochs (~1-2 hours)</li> <li>High quality: 50,000+ epochs (~8-12 hours)</li> </ul> <p>For this demo, we'll train for a moderate number of epochs. You can increase for better results.</p>"},{"location":"diffusion/03_medical_imaging_diffusion/03_medical_imaging_diffusion/#5-generate-synthetic-x-rays","title":"5. Generate Synthetic X-rays\u00b6","text":"<p>Now let's use the trained model to generate new synthetic X-ray images!</p>"},{"location":"diffusion/03_medical_imaging_diffusion/03_medical_imaging_diffusion/#visualize-generated-images","title":"Visualize Generated Images\u00b6","text":""},{"location":"diffusion/03_medical_imaging_diffusion/03_medical_imaging_diffusion/#visualize-reverse-diffusion-process","title":"Visualize Reverse Diffusion Process\u00b6","text":""},{"location":"diffusion/03_medical_imaging_diffusion/03_medical_imaging_diffusion/#compare-real-vs-generated","title":"Compare Real vs Generated\u00b6","text":""},{"location":"diffusion/03_medical_imaging_diffusion/03_medical_imaging_diffusion/#6-evaluation-metrics","title":"6. Evaluation Metrics\u00b6","text":"<p>For medical images, we evaluate:</p> <ol> <li>Visual quality: Do images look realistic?</li> <li>FID (Fr\u00e9chet Inception Distance): Measures distribution similarity</li> <li>Structural similarity: SSIM between real and generated</li> <li>Clinical utility: Can radiologists distinguish real from synthetic?</li> </ol>"},{"location":"diffusion/03_medical_imaging_diffusion/03_medical_imaging_diffusion/#7-applications","title":"7. Applications\u00b6","text":""},{"location":"diffusion/03_medical_imaging_diffusion/03_medical_imaging_diffusion/#a-data-augmentation","title":"A. Data Augmentation\u00b6","text":"<p>Use generated images to augment training data for downstream tasks (disease classification, segmentation).</p>"},{"location":"diffusion/03_medical_imaging_diffusion/03_medical_imaging_diffusion/#b-conditional-generation-future-work","title":"B. Conditional Generation (Future Work)\u00b6","text":"<p>Extend the model to generate X-rays conditioned on:</p> <ul> <li>Disease labels (normal, pneumonia, COVID-19)</li> <li>Patient demographics (age, sex)</li> <li>View angle (PA, lateral)</li> </ul> <p>This requires modifying the U-Net to accept condition embeddings.</p>"},{"location":"diffusion/03_medical_imaging_diffusion/03_medical_imaging_diffusion/#c-image-to-image-translation","title":"C. Image-to-Image Translation\u00b6","text":"<p>Use diffusion for:</p> <ul> <li>Super-resolution (low-res \u2192 high-res)</li> <li>Denoising (noisy \u2192 clean)</li> <li>Modality transfer (CT \u2192 MRI)</li> </ul>"},{"location":"diffusion/03_medical_imaging_diffusion/03_medical_imaging_diffusion/#8-next-steps","title":"8. Next Steps\u00b6","text":""},{"location":"diffusion/03_medical_imaging_diffusion/03_medical_imaging_diffusion/#to-improve-this-notebook","title":"To improve this notebook:\u00b6","text":"<ol> <li><p>Use real medical images:</p> <ul> <li>Download NIH Chest X-ray dataset</li> <li>Replace <code>SyntheticXRayDataset</code> with real data loader</li> <li>Preprocess: resize, normalize, augment</li> </ul> </li> <li><p>Train longer:</p> <ul> <li>Increase to 50,000-100,000 epochs</li> <li>Use learning rate scheduling</li> <li>Implement EMA (exponential moving average)</li> </ul> </li> <li><p>Scale up:</p> <ul> <li>Use 256\u00d7256 images</li> <li>Increase base_channels to 128</li> <li>Add attention layers at low resolutions</li> </ul> </li> <li><p>Add conditioning:</p> <ul> <li>Condition on disease labels</li> <li>Implement classifier-free guidance</li> <li>Generate specific pathologies</li> </ul> </li> <li><p>Evaluate rigorously:</p> <ul> <li>Compute FID with pretrained features</li> <li>Clinical evaluation by radiologists</li> <li>Downstream task performance</li> </ul> </li> </ol>"},{"location":"diffusion/03_medical_imaging_diffusion/03_medical_imaging_diffusion/#connect-to-your-pathology-ai-lab","title":"Connect to your pathology-ai-lab:\u00b6","text":"<pre># Adapt this notebook for histopathology\nfrom pathology_ai_lab.data import load_camelyon_patches\n\n# Load H&amp;E patches\npatches = load_camelyon_patches(size=96, n_samples=10000)\n\n# Train diffusion model on tissue patches\nmodel = UNet2D(in_channels=3, out_channels=3, base_channels=64)\n# ... train ...\n\n# Generate synthetic tissue for augmentation\nsynthetic_patches = sample_images(model, sde, n_samples=1000)\n</pre>"},{"location":"diffusion/03_medical_imaging_diffusion/03_medical_imaging_diffusion/#summary","title":"Summary\u00b6","text":"<p>In this notebook, we:</p> <ol> <li>\u2705 Applied U-Net architecture to medical images</li> <li>\u2705 Trained diffusion model on 128\u00d7128 X-rays</li> <li>\u2705 Generated synthetic medical images</li> <li>\u2705 Evaluated with pixel statistics and visual inspection</li> <li>\u2705 Discussed practical applications</li> </ol> <p>Key takeaways:</p> <ul> <li>U-Net is essential for spatial structure preservation</li> <li>Medical images require longer training than toy data</li> <li>Cosine schedule works well for images</li> <li>Generated images can augment limited medical datasets</li> <li>Clinical validation is crucial for medical AI</li> </ul> <p>Next: <code>04_gene_expression_diffusion.ipynb</code> - High-dimensional tabular data</p>"},{"location":"diffusion/04_gene_expression_diffusion/","title":"Diffusion Models for Gene Expression Data","text":"<p>This notebook extends the SDE-based diffusion framework to generate realistic gene expression data \u2014 a key capability for drug discovery and computational biology.</p>"},{"location":"diffusion/04_gene_expression_diffusion/#motivation","title":"Motivation","text":"<p>Companies like Synthesize Bio (GEM-1), Insilico Medicine (Precious3GPT), and scGPT are building generative models for gene expression. Applications include:</p> <ul> <li>Drug target discovery \u2014 Generate expression profiles under hypothetical perturbations</li> <li>Clinical trial acceleration \u2014 In-silico patient simulation</li> <li>Data augmentation \u2014 Generate rare cell types or disease states</li> </ul>"},{"location":"diffusion/04_gene_expression_diffusion/#key-concepts","title":"Key Concepts","text":""},{"location":"diffusion/04_gene_expression_diffusion/#why-latent-diffusion","title":"Why Latent Diffusion?","text":"Challenge Direct Diffusion Latent Diffusion Dimensionality 2,000-20,000 genes 32-128 latent dims Training speed Slow Fast Structure May miss correlations VAE captures structure Conditioning Complex Natural via embeddings"},{"location":"diffusion/04_gene_expression_diffusion/#architecture","title":"Architecture","text":"<pre><code>Gene Expression (n_genes) \n    \u2192 VAE Encoder \n    \u2192 Latent Space (z_dim) \n    \u2192 Diffusion (VP-SDE) \n    \u2192 VAE Decoder \n    \u2192 Gene Expression (n_genes)\n</code></pre>"},{"location":"diffusion/04_gene_expression_diffusion/#learning-objectives","title":"Learning Objectives","text":"<ol> <li>Understand challenges of applying diffusion to gene expression</li> <li>Implement latent diffusion for high-dimensional biological data</li> <li>Add conditional generation (tissue, disease, cell type)</li> <li>Evaluate with biological metrics (gene correlations, PCA overlap)</li> </ol>"},{"location":"diffusion/04_gene_expression_diffusion/#prerequisites","title":"Prerequisites","text":"<ul> <li><code>02_sde_formulation/</code> \u2014 SDE basics and score matching</li> <li>Understanding of VAEs (variational autoencoders)</li> </ul>"},{"location":"diffusion/04_gene_expression_diffusion/#files","title":"Files","text":"<ul> <li><code>03_gene_expression_diffusion.ipynb</code> \u2014 Main tutorial notebook</li> </ul>"},{"location":"diffusion/04_gene_expression_diffusion/#next-steps","title":"Next Steps","text":"<p>After this notebook: 1. Apply to real single-cell data (PBMC3k via <code>SingleCellDataset</code>) 2. Add conditional generation for perturbation prediction 3. Connect to scPPDM framework for drug response modeling</p>"},{"location":"diffusion/04_gene_expression_diffusion/#references","title":"References","text":"<ul> <li>Synthesize Bio \u2014 GEM-1 gene expression model</li> <li>scGPT \u2014 Generative pre-trained transformer for single-cell</li> <li>Stable Diffusion \u2014 Latent diffusion for images</li> </ul>"},{"location":"diffusion/04_gene_expression_diffusion/04_gene_expression_diffusion/","title":"Notebook","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport sys\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\nfrom sklearn.decomposition import PCA\n\nsys.path.insert(0, str(Path('../../../src').resolve()))\n\nfrom genailab.diffusion import VPSDE, train_score_network, sample_reverse_sde\nfrom genailab.data import ToyBulkDataset\n\nsns.set_style('whitegrid')\nnp.random.seed(42)\ntorch.manual_seed(42)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\nprint(f'Using device: {device}')\n</pre> import numpy as np import matplotlib.pyplot as plt import seaborn as sns from pathlib import Path import sys  import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import DataLoader from tqdm.auto import tqdm from sklearn.decomposition import PCA  sys.path.insert(0, str(Path('../../../src').resolve()))  from genailab.diffusion import VPSDE, train_score_network, sample_reverse_sde from genailab.data import ToyBulkDataset  sns.set_style('whitegrid') np.random.seed(42) torch.manual_seed(42)  device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu' print(f'Using device: {device}') In\u00a0[\u00a0]: Copied! <pre>dataset = ToyBulkDataset(n=5000, n_genes=500, n_tissues=5, n_diseases=3, n_batches=4, seed=42)\nprint(f\"Dataset: {len(dataset)} samples, {dataset.n_genes} genes\")\nprint(f\"Conditions: {list(dataset.cond.keys())}\")\n</pre> dataset = ToyBulkDataset(n=5000, n_genes=500, n_tissues=5, n_diseases=3, n_batches=4, seed=42) print(f\"Dataset: {len(dataset)} samples, {dataset.n_genes} genes\") print(f\"Conditions: {list(dataset.cond.keys())}\") In\u00a0[\u00a0]: Copied! <pre># Visualize data\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\npca = PCA(n_components=2)\nx_pca = pca.fit_transform(dataset.x.numpy())\nscatter = axes[0].scatter(x_pca[:, 0], x_pca[:, 1], c=dataset.cond['tissue'].numpy(), cmap='tab10', alpha=0.5, s=5)\naxes[0].set_title('PCA by Tissue')\nplt.colorbar(scatter, ax=axes[0])\naxes[1].hist(dataset.x.numpy().flatten(), bins=50, density=True)\naxes[1].set_title('Expression Distribution')\nplt.tight_layout()\nplt.show()\n</pre> # Visualize data fig, axes = plt.subplots(1, 2, figsize=(12, 4)) pca = PCA(n_components=2) x_pca = pca.fit_transform(dataset.x.numpy()) scatter = axes[0].scatter(x_pca[:, 0], x_pca[:, 1], c=dataset.cond['tissue'].numpy(), cmap='tab10', alpha=0.5, s=5) axes[0].set_title('PCA by Tissue') plt.colorbar(scatter, ax=axes[0]) axes[1].hist(dataset.x.numpy().flatten(), bins=50, density=True) axes[1].set_title('Expression Distribution') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>class GeneVAE(nn.Module):\n    def __init__(self, n_genes, latent_dim=32, hidden_dim=256):\n        super().__init__()\n        self.latent_dim = latent_dim\n        self.encoder = nn.Sequential(nn.Linear(n_genes, hidden_dim), nn.LayerNorm(hidden_dim), nn.SiLU(),\n                                     nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim), nn.SiLU())\n        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n        self.decoder = nn.Sequential(nn.Linear(latent_dim, hidden_dim), nn.LayerNorm(hidden_dim), nn.SiLU(),\n                                     nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim), nn.SiLU(),\n                                     nn.Linear(hidden_dim, n_genes))\n    \n    def encode(self, x):\n        h = self.encoder(x)\n        return self.fc_mu(h), self.fc_logvar(h)\n    \n    def decode(self, z):\n        return self.decoder(z)\n    \n    def forward(self, x):\n        mu, logvar = self.encode(x)\n        z = mu + torch.randn_like(mu) * torch.exp(0.5 * logvar)\n        return self.decode(z), mu, logvar\n</pre> class GeneVAE(nn.Module):     def __init__(self, n_genes, latent_dim=32, hidden_dim=256):         super().__init__()         self.latent_dim = latent_dim         self.encoder = nn.Sequential(nn.Linear(n_genes, hidden_dim), nn.LayerNorm(hidden_dim), nn.SiLU(),                                      nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim), nn.SiLU())         self.fc_mu = nn.Linear(hidden_dim, latent_dim)         self.fc_logvar = nn.Linear(hidden_dim, latent_dim)         self.decoder = nn.Sequential(nn.Linear(latent_dim, hidden_dim), nn.LayerNorm(hidden_dim), nn.SiLU(),                                      nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim), nn.SiLU(),                                      nn.Linear(hidden_dim, n_genes))          def encode(self, x):         h = self.encoder(x)         return self.fc_mu(h), self.fc_logvar(h)          def decode(self, z):         return self.decoder(z)          def forward(self, x):         mu, logvar = self.encode(x)         z = mu + torch.randn_like(mu) * torch.exp(0.5 * logvar)         return self.decode(z), mu, logvar In\u00a0[\u00a0]: Copied! <pre># Train VAE\nlatent_dim = 32\nvae = GeneVAE(n_genes=dataset.n_genes, latent_dim=latent_dim).to(device)\noptimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\ndata = dataset.x.to(device)\n\nfor epoch in tqdm(range(2000), desc=\"Training VAE\"):\n    idx = np.random.choice(len(data), 128)\n    x = data[idx]\n    recon, mu, logvar = vae(x)\n    loss = F.mse_loss(recon, x) + 0.01 * (-0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp()))\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    if (epoch + 1) % 500 == 0:\n        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n</pre> # Train VAE latent_dim = 32 vae = GeneVAE(n_genes=dataset.n_genes, latent_dim=latent_dim).to(device) optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3) data = dataset.x.to(device)  for epoch in tqdm(range(2000), desc=\"Training VAE\"):     idx = np.random.choice(len(data), 128)     x = data[idx]     recon, mu, logvar = vae(x)     loss = F.mse_loss(recon, x) + 0.01 * (-0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp()))     optimizer.zero_grad()     loss.backward()     optimizer.step()     if (epoch + 1) % 500 == 0:         print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\") In\u00a0[\u00a0]: Copied! <pre># Get latent representations\nvae.eval()\nwith torch.no_grad():\n    mu, _ = vae.encode(data)\n    latent_data = mu.cpu().numpy()\nprint(f\"Latent shape: {latent_data.shape} (compression: {dataset.n_genes/latent_dim:.0f}x)\")\n</pre> # Get latent representations vae.eval() with torch.no_grad():     mu, _ = vae.encode(data)     latent_data = mu.cpu().numpy() print(f\"Latent shape: {latent_data.shape} (compression: {dataset.n_genes/latent_dim:.0f}x)\") In\u00a0[\u00a0]: Copied! <pre># Train diffusion in latent space\nfrom genailab.diffusion import SimpleScoreNetwork\n\nscore_net = SimpleScoreNetwork(data_dim=latent_dim, hidden_dim=256, num_layers=4).to(device)\nsde = VPSDE(beta_min=0.1, beta_max=20.0, T=1.0)\n\nlosses = train_score_network(score_net, latent_data, sde, num_epochs=5000, batch_size=128, lr=1e-3, device=device)\n\nplt.figure(figsize=(10, 4))\nplt.plot(losses)\nplt.yscale('log')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Latent Diffusion Training')\nplt.show()\n</pre> # Train diffusion in latent space from genailab.diffusion import SimpleScoreNetwork  score_net = SimpleScoreNetwork(data_dim=latent_dim, hidden_dim=256, num_layers=4).to(device) sde = VPSDE(beta_min=0.1, beta_max=20.0, T=1.0)  losses = train_score_network(score_net, latent_data, sde, num_epochs=5000, batch_size=128, lr=1e-3, device=device)  plt.figure(figsize=(10, 4)) plt.plot(losses) plt.yscale('log') plt.xlabel('Epoch') plt.ylabel('Loss') plt.title('Latent Diffusion Training') plt.show() In\u00a0[\u00a0]: Copied! <pre># Generate samples\nlatent_samples, _ = sample_reverse_sde(score_net, sde, n_samples=1000, num_steps=500, data_dim=latent_dim, device=device)\n\nwith torch.no_grad():\n    gene_samples = vae.decode(torch.FloatTensor(latent_samples).to(device)).cpu().numpy()\n\nprint(f\"Generated {gene_samples.shape[0]} samples with {gene_samples.shape[1]} genes\")\n</pre> # Generate samples latent_samples, _ = sample_reverse_sde(score_net, sde, n_samples=1000, num_steps=500, data_dim=latent_dim, device=device)  with torch.no_grad():     gene_samples = vae.decode(torch.FloatTensor(latent_samples).to(device)).cpu().numpy()  print(f\"Generated {gene_samples.shape[0]} samples with {gene_samples.shape[1]} genes\") In\u00a0[\u00a0]: Copied! <pre># Evaluate\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Distribution\naxes[0].hist(dataset.x.numpy().flatten(), bins=50, density=True, alpha=0.5, label='Real')\naxes[0].hist(gene_samples.flatten(), bins=50, density=True, alpha=0.5, label='Generated')\naxes[0].legend()\naxes[0].set_title('Expression Distribution')\n\n# Gene means\nreal_means = dataset.x.numpy().mean(axis=0)\ngen_means = gene_samples.mean(axis=0)\naxes[1].scatter(real_means, gen_means, alpha=0.3, s=5)\naxes[1].plot([-2, 2], [-2, 2], 'r--')\ncorr = np.corrcoef(real_means, gen_means)[0, 1]\naxes[1].set_title(f'Gene Means (r={corr:.3f})')\n\n# PCA\npca = PCA(n_components=2)\nreal_pca = pca.fit_transform(dataset.x.numpy())\ngen_pca = pca.transform(gene_samples)\naxes[2].scatter(real_pca[:, 0], real_pca[:, 1], alpha=0.3, s=5, label='Real')\naxes[2].scatter(gen_pca[:, 0], gen_pca[:, 1], alpha=0.3, s=5, label='Generated')\naxes[2].legend()\naxes[2].set_title('PCA Overlay')\n\nplt.tight_layout()\nplt.show()\n</pre> # Evaluate fig, axes = plt.subplots(1, 3, figsize=(15, 4))  # Distribution axes[0].hist(dataset.x.numpy().flatten(), bins=50, density=True, alpha=0.5, label='Real') axes[0].hist(gene_samples.flatten(), bins=50, density=True, alpha=0.5, label='Generated') axes[0].legend() axes[0].set_title('Expression Distribution')  # Gene means real_means = dataset.x.numpy().mean(axis=0) gen_means = gene_samples.mean(axis=0) axes[1].scatter(real_means, gen_means, alpha=0.3, s=5) axes[1].plot([-2, 2], [-2, 2], 'r--') corr = np.corrcoef(real_means, gen_means)[0, 1] axes[1].set_title(f'Gene Means (r={corr:.3f})')  # PCA pca = PCA(n_components=2) real_pca = pca.fit_transform(dataset.x.numpy()) gen_pca = pca.transform(gene_samples) axes[2].scatter(real_pca[:, 0], real_pca[:, 1], alpha=0.3, s=5, label='Real') axes[2].scatter(gen_pca[:, 0], gen_pca[:, 1], alpha=0.3, s=5, label='Generated') axes[2].legend() axes[2].set_title('PCA Overlay')  plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Import count-aware decoders and losses from genailab\nfrom genailab.model.decoders import NegativeBinomialDecoder, ZINBDecoder\nfrom genailab.objectives.losses import nb_loss, zinb_loss, elbo_loss_nb, elbo_loss_zinb\n\nprint(\"Available count-aware components:\")\nprint(\"  Decoders: NegativeBinomialDecoder, ZINBDecoder\")\nprint(\"  Losses: nb_loss, zinb_loss, elbo_loss_nb, elbo_loss_zinb\")\n</pre> # Import count-aware decoders and losses from genailab from genailab.model.decoders import NegativeBinomialDecoder, ZINBDecoder from genailab.objectives.losses import nb_loss, zinb_loss, elbo_loss_nb, elbo_loss_zinb  print(\"Available count-aware components:\") print(\"  Decoders: NegativeBinomialDecoder, ZINBDecoder\") print(\"  Losses: nb_loss, zinb_loss, elbo_loss_nb, elbo_loss_zinb\") In\u00a0[\u00a0]: Copied! <pre>class GeneVAE_NB(nn.Module):\n    \"\"\"VAE with Negative Binomial decoder for count data.\n    \n    Architecture:\n        Encoder: expression \u2192 latent (mu, logvar)\n        Decoder: latent \u2192 NB parameters (mu, theta)\n    \"\"\"\n    \n    def __init__(self, n_genes, latent_dim=32, hidden_dim=256):\n        super().__init__()\n        self.n_genes = n_genes\n        self.latent_dim = latent_dim\n        \n        # Encoder (same as before)\n        self.encoder = nn.Sequential(\n            nn.Linear(n_genes, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.SiLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.SiLU()\n        )\n        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n        \n        # NB Decoder: outputs rate parameters\n        self.decoder_net = nn.Sequential(\n            nn.Linear(latent_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.SiLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.SiLU()\n        )\n        self.rho_head = nn.Linear(hidden_dim, n_genes)  # Rate (before library scaling)\n        \n        # Gene-specific dispersion (learned parameter)\n        self.log_theta = nn.Parameter(torch.zeros(n_genes))\n    \n    def encode(self, x):\n        \"\"\"Encode expression to latent distribution.\"\"\"\n        h = self.encoder(x)\n        return self.fc_mu(h), self.fc_logvar(h)\n    \n    def reparameterize(self, mu, logvar):\n        \"\"\"Sample z from q(z|x) using reparameterization trick.\"\"\"\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n    \n    def decode(self, z, library_size=None):\n        \"\"\"Decode latent to NB parameters.\n        \n        Returns:\n            mu: Expected counts (n_samples, n_genes)\n            theta: Dispersion (n_genes,) broadcast to (n_samples, n_genes)\n        \"\"\"\n        h = self.decoder_net(z)\n        \n        # Rate: softmax ensures non-negative and sums to 1\n        rho = F.softmax(self.rho_head(h), dim=-1)\n        \n        # Scale by library size (total counts per sample)\n        if library_size is not None:\n            if library_size.dim() == 1:\n                library_size = library_size.unsqueeze(-1)\n            mu = rho * library_size\n        else:\n            # Default: assume library size = n_genes (normalized)\n            mu = rho * self.n_genes\n        \n        # Dispersion: exp to ensure positive\n        theta = torch.exp(self.log_theta).unsqueeze(0).expand(z.shape[0], -1)\n        \n        return mu, theta\n    \n    def forward(self, x, library_size=None):\n        \"\"\"Full forward pass.\"\"\"\n        # Encode\n        enc_mu, enc_logvar = self.encode(x)\n        z = self.reparameterize(enc_mu, enc_logvar)\n        \n        # Decode to NB parameters\n        dec_mu, dec_theta = self.decode(z, library_size)\n        \n        return dec_mu, dec_theta, enc_mu, enc_logvar\n\nprint(\"GeneVAE_NB defined with Negative Binomial decoder\")\n</pre> class GeneVAE_NB(nn.Module):     \"\"\"VAE with Negative Binomial decoder for count data.          Architecture:         Encoder: expression \u2192 latent (mu, logvar)         Decoder: latent \u2192 NB parameters (mu, theta)     \"\"\"          def __init__(self, n_genes, latent_dim=32, hidden_dim=256):         super().__init__()         self.n_genes = n_genes         self.latent_dim = latent_dim                  # Encoder (same as before)         self.encoder = nn.Sequential(             nn.Linear(n_genes, hidden_dim),             nn.LayerNorm(hidden_dim),             nn.SiLU(),             nn.Linear(hidden_dim, hidden_dim),             nn.LayerNorm(hidden_dim),             nn.SiLU()         )         self.fc_mu = nn.Linear(hidden_dim, latent_dim)         self.fc_logvar = nn.Linear(hidden_dim, latent_dim)                  # NB Decoder: outputs rate parameters         self.decoder_net = nn.Sequential(             nn.Linear(latent_dim, hidden_dim),             nn.LayerNorm(hidden_dim),             nn.SiLU(),             nn.Linear(hidden_dim, hidden_dim),             nn.LayerNorm(hidden_dim),             nn.SiLU()         )         self.rho_head = nn.Linear(hidden_dim, n_genes)  # Rate (before library scaling)                  # Gene-specific dispersion (learned parameter)         self.log_theta = nn.Parameter(torch.zeros(n_genes))          def encode(self, x):         \"\"\"Encode expression to latent distribution.\"\"\"         h = self.encoder(x)         return self.fc_mu(h), self.fc_logvar(h)          def reparameterize(self, mu, logvar):         \"\"\"Sample z from q(z|x) using reparameterization trick.\"\"\"         std = torch.exp(0.5 * logvar)         eps = torch.randn_like(std)         return mu + eps * std          def decode(self, z, library_size=None):         \"\"\"Decode latent to NB parameters.                  Returns:             mu: Expected counts (n_samples, n_genes)             theta: Dispersion (n_genes,) broadcast to (n_samples, n_genes)         \"\"\"         h = self.decoder_net(z)                  # Rate: softmax ensures non-negative and sums to 1         rho = F.softmax(self.rho_head(h), dim=-1)                  # Scale by library size (total counts per sample)         if library_size is not None:             if library_size.dim() == 1:                 library_size = library_size.unsqueeze(-1)             mu = rho * library_size         else:             # Default: assume library size = n_genes (normalized)             mu = rho * self.n_genes                  # Dispersion: exp to ensure positive         theta = torch.exp(self.log_theta).unsqueeze(0).expand(z.shape[0], -1)                  return mu, theta          def forward(self, x, library_size=None):         \"\"\"Full forward pass.\"\"\"         # Encode         enc_mu, enc_logvar = self.encode(x)         z = self.reparameterize(enc_mu, enc_logvar)                  # Decode to NB parameters         dec_mu, dec_theta = self.decode(z, library_size)                  return dec_mu, dec_theta, enc_mu, enc_logvar  print(\"GeneVAE_NB defined with Negative Binomial decoder\") In\u00a0[\u00a0]: Copied! <pre># Generate synthetic count data for demonstration\n# (Our ToyBulkDataset uses log-normalized data; let's create count-like data)\n\ndef generate_synthetic_counts(n_samples=5000, n_genes=500, seed=42):\n    \"\"\"Generate synthetic count data with NB-like properties.\"\"\"\n    np.random.seed(seed)\n    \n    # Base expression rates (log-normal distributed)\n    log_base_rates = np.random.normal(2, 2, n_genes)\n    base_rates = np.exp(log_base_rates)\n    \n    # Sample-specific library sizes (total counts)\n    library_sizes = np.random.lognormal(10, 0.5, n_samples)\n    \n    # Gene-specific dispersion (smaller = more overdispersion)\n    dispersions = np.random.uniform(0.1, 10, n_genes)\n    \n    # Generate counts using Negative Binomial\n    counts = np.zeros((n_samples, n_genes))\n    for i in range(n_samples):\n        # Rates for this sample\n        rates = base_rates * (library_sizes[i] / base_rates.sum())\n        for j in range(n_genes):\n            # NB parameterization: mean=mu, var=mu + mu^2/theta\n            mu = rates[j]\n            theta = dispersions[j]\n            # Convert to scipy's NB parameterization\n            p = theta / (theta + mu)\n            counts[i, j] = np.random.negative_binomial(theta, p) if p &lt; 1 else 0\n    \n    return counts.astype(np.float32), library_sizes.astype(np.float32)\n\n# Generate count data\ncount_data, library_sizes = generate_synthetic_counts(n_samples=5000, n_genes=500)\nprint(f\"Count data shape: {count_data.shape}\")\nprint(f\"Count range: [{count_data.min():.0f}, {count_data.max():.0f}]\")\nprint(f\"Sparsity (zeros): {(count_data == 0).mean()*100:.1f}%\")\nprint(f\"Library sizes: mean={library_sizes.mean():.0f}, std={library_sizes.std():.0f}\")\n\n# Visualize\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\naxes[0].hist(count_data.flatten(), bins=50, density=True, log=True)\naxes[0].set_xlabel('Count')\naxes[0].set_ylabel('Density (log)')\naxes[0].set_title('Count Distribution (heavy-tailed)')\n\naxes[1].hist(np.log1p(count_data).flatten(), bins=50, density=True)\naxes[1].set_xlabel('log1p(Count)')\naxes[1].set_ylabel('Density')\naxes[1].set_title('Log-transformed Distribution')\n\naxes[2].hist(library_sizes, bins=30, density=True)\naxes[2].set_xlabel('Library Size')\naxes[2].set_ylabel('Density')\naxes[2].set_title('Library Size Distribution')\n\nplt.tight_layout()\nplt.show()\n</pre> # Generate synthetic count data for demonstration # (Our ToyBulkDataset uses log-normalized data; let's create count-like data)  def generate_synthetic_counts(n_samples=5000, n_genes=500, seed=42):     \"\"\"Generate synthetic count data with NB-like properties.\"\"\"     np.random.seed(seed)          # Base expression rates (log-normal distributed)     log_base_rates = np.random.normal(2, 2, n_genes)     base_rates = np.exp(log_base_rates)          # Sample-specific library sizes (total counts)     library_sizes = np.random.lognormal(10, 0.5, n_samples)          # Gene-specific dispersion (smaller = more overdispersion)     dispersions = np.random.uniform(0.1, 10, n_genes)          # Generate counts using Negative Binomial     counts = np.zeros((n_samples, n_genes))     for i in range(n_samples):         # Rates for this sample         rates = base_rates * (library_sizes[i] / base_rates.sum())         for j in range(n_genes):             # NB parameterization: mean=mu, var=mu + mu^2/theta             mu = rates[j]             theta = dispersions[j]             # Convert to scipy's NB parameterization             p = theta / (theta + mu)             counts[i, j] = np.random.negative_binomial(theta, p) if p &lt; 1 else 0          return counts.astype(np.float32), library_sizes.astype(np.float32)  # Generate count data count_data, library_sizes = generate_synthetic_counts(n_samples=5000, n_genes=500) print(f\"Count data shape: {count_data.shape}\") print(f\"Count range: [{count_data.min():.0f}, {count_data.max():.0f}]\") print(f\"Sparsity (zeros): {(count_data == 0).mean()*100:.1f}%\") print(f\"Library sizes: mean={library_sizes.mean():.0f}, std={library_sizes.std():.0f}\")  # Visualize fig, axes = plt.subplots(1, 3, figsize=(15, 4))  axes[0].hist(count_data.flatten(), bins=50, density=True, log=True) axes[0].set_xlabel('Count') axes[0].set_ylabel('Density (log)') axes[0].set_title('Count Distribution (heavy-tailed)')  axes[1].hist(np.log1p(count_data).flatten(), bins=50, density=True) axes[1].set_xlabel('log1p(Count)') axes[1].set_ylabel('Density') axes[1].set_title('Log-transformed Distribution')  axes[2].hist(library_sizes, bins=30, density=True) axes[2].set_xlabel('Library Size') axes[2].set_ylabel('Density') axes[2].set_title('Library Size Distribution')  plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Train VAE with NB decoder on count data\n# Note: We log-transform input for encoder stability, but use NB loss on original counts\n\n# Prepare data\ncount_tensor = torch.FloatTensor(count_data).to(device)\nlibrary_tensor = torch.FloatTensor(library_sizes).to(device)\n\n# Log-transform for encoder input (standard practice in scVI, scGen)\nlog_counts = torch.log1p(count_tensor)\n\n# Initialize model\nvae_nb = GeneVAE_NB(n_genes=500, latent_dim=32, hidden_dim=256).to(device)\noptimizer_nb = torch.optim.Adam(vae_nb.parameters(), lr=1e-3)\n\n# Training loop\nlosses_nb = []\nfor epoch in tqdm(range(3000), desc=\"Training VAE-NB\"):\n    idx = np.random.choice(len(count_tensor), 128)\n    x_counts = count_tensor[idx]\n    x_log = log_counts[idx]\n    lib_size = library_tensor[idx]\n    \n    # Forward pass (encode log-transformed, decode to NB params)\n    dec_mu, dec_theta, enc_mu, enc_logvar = vae_nb(x_log, lib_size)\n    \n    # Loss: NB reconstruction + KL divergence\n    loss, loss_dict = elbo_loss_nb(\n        x=x_counts,           # Original counts for NB loss\n        mu=dec_mu,            # Predicted mean\n        theta=dec_theta,      # Predicted dispersion\n        enc_mu=enc_mu,        # Encoder mean\n        enc_logvar=enc_logvar,# Encoder logvar\n        beta=0.01             # KL weight (low to avoid posterior collapse)\n    )\n    \n    optimizer_nb.zero_grad()\n    loss.backward()\n    optimizer_nb.step()\n    \n    losses_nb.append(loss.item())\n    \n    if (epoch + 1) % 1000 == 0:\n        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}, \"\n              f\"Recon: {loss_dict['recon'].item():.4f}, KL: {loss_dict['kl'].item():.4f}\")\n\n# Plot training curve\nplt.figure(figsize=(10, 4))\nplt.plot(losses_nb)\nplt.yscale('log')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('VAE-NB Training (NB Reconstruction Loss)')\nplt.grid(True, alpha=0.3)\nplt.show()\n</pre> # Train VAE with NB decoder on count data # Note: We log-transform input for encoder stability, but use NB loss on original counts  # Prepare data count_tensor = torch.FloatTensor(count_data).to(device) library_tensor = torch.FloatTensor(library_sizes).to(device)  # Log-transform for encoder input (standard practice in scVI, scGen) log_counts = torch.log1p(count_tensor)  # Initialize model vae_nb = GeneVAE_NB(n_genes=500, latent_dim=32, hidden_dim=256).to(device) optimizer_nb = torch.optim.Adam(vae_nb.parameters(), lr=1e-3)  # Training loop losses_nb = [] for epoch in tqdm(range(3000), desc=\"Training VAE-NB\"):     idx = np.random.choice(len(count_tensor), 128)     x_counts = count_tensor[idx]     x_log = log_counts[idx]     lib_size = library_tensor[idx]          # Forward pass (encode log-transformed, decode to NB params)     dec_mu, dec_theta, enc_mu, enc_logvar = vae_nb(x_log, lib_size)          # Loss: NB reconstruction + KL divergence     loss, loss_dict = elbo_loss_nb(         x=x_counts,           # Original counts for NB loss         mu=dec_mu,            # Predicted mean         theta=dec_theta,      # Predicted dispersion         enc_mu=enc_mu,        # Encoder mean         enc_logvar=enc_logvar,# Encoder logvar         beta=0.01             # KL weight (low to avoid posterior collapse)     )          optimizer_nb.zero_grad()     loss.backward()     optimizer_nb.step()          losses_nb.append(loss.item())          if (epoch + 1) % 1000 == 0:         print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}, \"               f\"Recon: {loss_dict['recon'].item():.4f}, KL: {loss_dict['kl'].item():.4f}\")  # Plot training curve plt.figure(figsize=(10, 4)) plt.plot(losses_nb) plt.yscale('log') plt.xlabel('Epoch') plt.ylabel('Loss') plt.title('VAE-NB Training (NB Reconstruction Loss)') plt.grid(True, alpha=0.3) plt.show() In\u00a0[\u00a0]: Copied! <pre># Step 1: Extract latent representations from VAE-NB\nvae_nb.eval()\nwith torch.no_grad():\n    enc_mu, _ = vae_nb.encode(log_counts)\n    latent_data_nb = enc_mu.cpu().numpy()\n\nprint(f\"Latent shape: {latent_data_nb.shape}\")\nprint(f\"Latent range: [{latent_data_nb.min():.2f}, {latent_data_nb.max():.2f}]\")\n\n# Step 2: Train diffusion in latent space\nscore_net_nb = SimpleScoreNetwork(data_dim=32, hidden_dim=256, num_layers=4).to(device)\nsde_nb = VPSDE(beta_min=0.1, beta_max=20.0, T=1.0)\n\nlosses_diff_nb = train_score_network(\n    score_net_nb, latent_data_nb, sde_nb, \n    num_epochs=5000, batch_size=128, lr=1e-3, device=device\n)\n\nplt.figure(figsize=(10, 4))\nplt.plot(losses_diff_nb)\nplt.yscale('log')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Latent Diffusion Training (for NB-VAE)')\nplt.grid(True, alpha=0.3)\nplt.show()\n</pre> # Step 1: Extract latent representations from VAE-NB vae_nb.eval() with torch.no_grad():     enc_mu, _ = vae_nb.encode(log_counts)     latent_data_nb = enc_mu.cpu().numpy()  print(f\"Latent shape: {latent_data_nb.shape}\") print(f\"Latent range: [{latent_data_nb.min():.2f}, {latent_data_nb.max():.2f}]\")  # Step 2: Train diffusion in latent space score_net_nb = SimpleScoreNetwork(data_dim=32, hidden_dim=256, num_layers=4).to(device) sde_nb = VPSDE(beta_min=0.1, beta_max=20.0, T=1.0)  losses_diff_nb = train_score_network(     score_net_nb, latent_data_nb, sde_nb,      num_epochs=5000, batch_size=128, lr=1e-3, device=device )  plt.figure(figsize=(10, 4)) plt.plot(losses_diff_nb) plt.yscale('log') plt.xlabel('Epoch') plt.ylabel('Loss') plt.title('Latent Diffusion Training (for NB-VAE)') plt.grid(True, alpha=0.3) plt.show() In\u00a0[\u00a0]: Copied! <pre># Step 3: Generate samples using latent diffusion + NB decoder\n\n# Sample latent vectors from diffusion\nlatent_samples_nb, _ = sample_reverse_sde(\n    score_net_nb, sde_nb, \n    n_samples=500, num_steps=500, data_dim=32, device=device\n)\n\n# Decode to NB parameters\nvae_nb.eval()\nwith torch.no_grad():\n    z_tensor = torch.FloatTensor(latent_samples_nb).to(device)\n    # Use median library size for generation\n    gen_lib_size = torch.full((500,), library_sizes.mean(), device=device)\n    gen_mu, gen_theta = vae_nb.decode(z_tensor, gen_lib_size)\n\n# Sample from NB distribution to get actual counts\ndef sample_from_nb(mu, theta):\n    \"\"\"Sample counts from Negative Binomial distribution.\"\"\"\n    mu_np = mu.cpu().numpy()\n    theta_np = theta.cpu().numpy()\n    \n    # NB parameterization: p = theta / (theta + mu)\n    p = theta_np / (theta_np + mu_np + 1e-8)\n    p = np.clip(p, 1e-8, 1 - 1e-8)\n    \n    # Sample\n    samples = np.zeros_like(mu_np)\n    for i in range(mu_np.shape[0]):\n        for j in range(mu_np.shape[1]):\n            if p[i, j] &lt; 1:\n                samples[i, j] = np.random.negative_binomial(theta_np[i, j], p[i, j])\n    \n    return samples\n\n# Generate count samples\ngen_counts = sample_from_nb(gen_mu, gen_theta)\nprint(f\"Generated {gen_counts.shape[0]} count samples\")\nprint(f\"Count range: [{gen_counts.min():.0f}, {gen_counts.max():.0f}]\")\nprint(f\"Sparsity (zeros): {(gen_counts == 0).mean()*100:.1f}%\")\n</pre> # Step 3: Generate samples using latent diffusion + NB decoder  # Sample latent vectors from diffusion latent_samples_nb, _ = sample_reverse_sde(     score_net_nb, sde_nb,      n_samples=500, num_steps=500, data_dim=32, device=device )  # Decode to NB parameters vae_nb.eval() with torch.no_grad():     z_tensor = torch.FloatTensor(latent_samples_nb).to(device)     # Use median library size for generation     gen_lib_size = torch.full((500,), library_sizes.mean(), device=device)     gen_mu, gen_theta = vae_nb.decode(z_tensor, gen_lib_size)  # Sample from NB distribution to get actual counts def sample_from_nb(mu, theta):     \"\"\"Sample counts from Negative Binomial distribution.\"\"\"     mu_np = mu.cpu().numpy()     theta_np = theta.cpu().numpy()          # NB parameterization: p = theta / (theta + mu)     p = theta_np / (theta_np + mu_np + 1e-8)     p = np.clip(p, 1e-8, 1 - 1e-8)          # Sample     samples = np.zeros_like(mu_np)     for i in range(mu_np.shape[0]):         for j in range(mu_np.shape[1]):             if p[i, j] &lt; 1:                 samples[i, j] = np.random.negative_binomial(theta_np[i, j], p[i, j])          return samples  # Generate count samples gen_counts = sample_from_nb(gen_mu, gen_theta) print(f\"Generated {gen_counts.shape[0]} count samples\") print(f\"Count range: [{gen_counts.min():.0f}, {gen_counts.max():.0f}]\") print(f\"Sparsity (zeros): {(gen_counts == 0).mean()*100:.1f}%\") In\u00a0[\u00a0]: Copied! <pre># Evaluate: Compare real vs generated count distributions\nfig, axes = plt.subplots(2, 3, figsize=(15, 8))\n\n# Row 1: Count distributions\naxes[0, 0].hist(count_data.flatten(), bins=50, density=True, alpha=0.5, label='Real', log=True)\naxes[0, 0].hist(gen_counts.flatten(), bins=50, density=True, alpha=0.5, label='Generated', log=True)\naxes[0, 0].set_xlabel('Count')\naxes[0, 0].set_ylabel('Density (log)')\naxes[0, 0].set_title('Count Distribution')\naxes[0, 0].legend()\n\n# Log-transformed comparison\naxes[0, 1].hist(np.log1p(count_data).flatten(), bins=50, density=True, alpha=0.5, label='Real')\naxes[0, 1].hist(np.log1p(gen_counts).flatten(), bins=50, density=True, alpha=0.5, label='Generated')\naxes[0, 1].set_xlabel('log1p(Count)')\naxes[0, 1].set_ylabel('Density')\naxes[0, 1].set_title('Log-transformed Distribution')\naxes[0, 1].legend()\n\n# Gene means correlation\nreal_means = count_data.mean(axis=0)\ngen_means = gen_counts.mean(axis=0)\naxes[0, 2].scatter(real_means, gen_means, alpha=0.3, s=5)\nmax_val = max(real_means.max(), gen_means.max())\naxes[0, 2].plot([0, max_val], [0, max_val], 'r--')\ncorr = np.corrcoef(real_means, gen_means)[0, 1]\naxes[0, 2].set_xlabel('Real Gene Mean')\naxes[0, 2].set_ylabel('Generated Gene Mean')\naxes[0, 2].set_title(f'Gene Means (r={corr:.3f})')\n\n# Row 2: More detailed comparisons\n# Gene variances\nreal_vars = count_data.var(axis=0)\ngen_vars = gen_counts.var(axis=0)\naxes[1, 0].scatter(np.log1p(real_vars), np.log1p(gen_vars), alpha=0.3, s=5)\nmax_var = max(np.log1p(real_vars).max(), np.log1p(gen_vars).max())\naxes[1, 0].plot([0, max_var], [0, max_var], 'r--')\ncorr_var = np.corrcoef(real_vars, gen_vars)[0, 1]\naxes[1, 0].set_xlabel('Real Gene Variance (log)')\naxes[1, 0].set_ylabel('Generated Gene Variance (log)')\naxes[1, 0].set_title(f'Gene Variances (r={corr_var:.3f})')\n\n# Sparsity per gene\nreal_sparsity = (count_data == 0).mean(axis=0)\ngen_sparsity = (gen_counts == 0).mean(axis=0)\naxes[1, 1].scatter(real_sparsity, gen_sparsity, alpha=0.3, s=5)\naxes[1, 1].plot([0, 1], [0, 1], 'r--')\ncorr_sparse = np.corrcoef(real_sparsity, gen_sparsity)[0, 1]\naxes[1, 1].set_xlabel('Real Sparsity')\naxes[1, 1].set_ylabel('Generated Sparsity')\naxes[1, 1].set_title(f'Gene Sparsity (r={corr_sparse:.3f})')\n\n# PCA comparison\npca = PCA(n_components=2)\nreal_pca = pca.fit_transform(np.log1p(count_data[:500]))  # Subsample for speed\ngen_pca = pca.transform(np.log1p(gen_counts))\naxes[1, 2].scatter(real_pca[:, 0], real_pca[:, 1], alpha=0.3, s=5, label='Real')\naxes[1, 2].scatter(gen_pca[:, 0], gen_pca[:, 1], alpha=0.3, s=5, label='Generated')\naxes[1, 2].set_xlabel('PC1')\naxes[1, 2].set_ylabel('PC2')\naxes[1, 2].set_title('PCA Overlay (log-transformed)')\naxes[1, 2].legend()\n\nplt.tight_layout()\nplt.suptitle('Latent Diffusion + NB Decoder: Real vs Generated Counts', y=1.02, fontsize=14)\nplt.show()\n\nprint(\"\\nSummary Statistics:\")\nprint(f\"  Real counts - Mean: {count_data.mean():.1f}, Sparsity: {(count_data==0).mean()*100:.1f}%\")\nprint(f\"  Generated   - Mean: {gen_counts.mean():.1f}, Sparsity: {(gen_counts==0).mean()*100:.1f}%\")\n</pre> # Evaluate: Compare real vs generated count distributions fig, axes = plt.subplots(2, 3, figsize=(15, 8))  # Row 1: Count distributions axes[0, 0].hist(count_data.flatten(), bins=50, density=True, alpha=0.5, label='Real', log=True) axes[0, 0].hist(gen_counts.flatten(), bins=50, density=True, alpha=0.5, label='Generated', log=True) axes[0, 0].set_xlabel('Count') axes[0, 0].set_ylabel('Density (log)') axes[0, 0].set_title('Count Distribution') axes[0, 0].legend()  # Log-transformed comparison axes[0, 1].hist(np.log1p(count_data).flatten(), bins=50, density=True, alpha=0.5, label='Real') axes[0, 1].hist(np.log1p(gen_counts).flatten(), bins=50, density=True, alpha=0.5, label='Generated') axes[0, 1].set_xlabel('log1p(Count)') axes[0, 1].set_ylabel('Density') axes[0, 1].set_title('Log-transformed Distribution') axes[0, 1].legend()  # Gene means correlation real_means = count_data.mean(axis=0) gen_means = gen_counts.mean(axis=0) axes[0, 2].scatter(real_means, gen_means, alpha=0.3, s=5) max_val = max(real_means.max(), gen_means.max()) axes[0, 2].plot([0, max_val], [0, max_val], 'r--') corr = np.corrcoef(real_means, gen_means)[0, 1] axes[0, 2].set_xlabel('Real Gene Mean') axes[0, 2].set_ylabel('Generated Gene Mean') axes[0, 2].set_title(f'Gene Means (r={corr:.3f})')  # Row 2: More detailed comparisons # Gene variances real_vars = count_data.var(axis=0) gen_vars = gen_counts.var(axis=0) axes[1, 0].scatter(np.log1p(real_vars), np.log1p(gen_vars), alpha=0.3, s=5) max_var = max(np.log1p(real_vars).max(), np.log1p(gen_vars).max()) axes[1, 0].plot([0, max_var], [0, max_var], 'r--') corr_var = np.corrcoef(real_vars, gen_vars)[0, 1] axes[1, 0].set_xlabel('Real Gene Variance (log)') axes[1, 0].set_ylabel('Generated Gene Variance (log)') axes[1, 0].set_title(f'Gene Variances (r={corr_var:.3f})')  # Sparsity per gene real_sparsity = (count_data == 0).mean(axis=0) gen_sparsity = (gen_counts == 0).mean(axis=0) axes[1, 1].scatter(real_sparsity, gen_sparsity, alpha=0.3, s=5) axes[1, 1].plot([0, 1], [0, 1], 'r--') corr_sparse = np.corrcoef(real_sparsity, gen_sparsity)[0, 1] axes[1, 1].set_xlabel('Real Sparsity') axes[1, 1].set_ylabel('Generated Sparsity') axes[1, 1].set_title(f'Gene Sparsity (r={corr_sparse:.3f})')  # PCA comparison pca = PCA(n_components=2) real_pca = pca.fit_transform(np.log1p(count_data[:500]))  # Subsample for speed gen_pca = pca.transform(np.log1p(gen_counts)) axes[1, 2].scatter(real_pca[:, 0], real_pca[:, 1], alpha=0.3, s=5, label='Real') axes[1, 2].scatter(gen_pca[:, 0], gen_pca[:, 1], alpha=0.3, s=5, label='Generated') axes[1, 2].set_xlabel('PC1') axes[1, 2].set_ylabel('PC2') axes[1, 2].set_title('PCA Overlay (log-transformed)') axes[1, 2].legend()  plt.tight_layout() plt.suptitle('Latent Diffusion + NB Decoder: Real vs Generated Counts', y=1.02, fontsize=14) plt.show()  print(\"\\nSummary Statistics:\") print(f\"  Real counts - Mean: {count_data.mean():.1f}, Sparsity: {(count_data==0).mean()*100:.1f}%\") print(f\"  Generated   - Mean: {gen_counts.mean():.1f}, Sparsity: {(gen_counts==0).mean()*100:.1f}%\")"},{"location":"diffusion/04_gene_expression_diffusion/04_gene_expression_diffusion/#diffusion-models-for-gene-expression-data","title":"Diffusion Models for Gene Expression Data\u00b6","text":"<p>This notebook extends the SDE-based diffusion framework to generate realistic gene expression data.</p> <p>Motivation:</p> <ul> <li>Companies like Synthesize Bio (GEM-1), Insilico Medicine (Precious3GPT), and scGPT are building generative models for gene expression</li> <li>Applications: drug target discovery, clinical trial acceleration, in-silico perturbation experiments</li> </ul> <p>Learning objectives:</p> <ol> <li>Understand challenges of applying diffusion to gene expression data</li> <li>Implement latent diffusion for high-dimensional biological data</li> <li>Add conditional generation (cell type, tissue, disease)</li> <li>Evaluate generated samples with biological metrics</li> </ol> <p>Prerequisites: <code>02_sde_formulation.ipynb</code></p>"},{"location":"diffusion/04_gene_expression_diffusion/04_gene_expression_diffusion/#setup","title":"Setup\u00b6","text":""},{"location":"diffusion/04_gene_expression_diffusion/04_gene_expression_diffusion/#1-load-synthetic-gene-expression-data","title":"1. Load Synthetic Gene Expression Data\u00b6","text":""},{"location":"diffusion/04_gene_expression_diffusion/04_gene_expression_diffusion/#2-latent-diffusion-approach","title":"2. Latent Diffusion Approach\u00b6","text":"<p>Key insight: Run diffusion in a learned latent space (like Stable Diffusion, scPPDM).</p> <pre><code>Genes (500) \u2192 VAE Encoder \u2192 Latent (32) \u2192 Diffusion \u2192 VAE Decoder \u2192 Genes (500)\n</code></pre>"},{"location":"diffusion/04_gene_expression_diffusion/04_gene_expression_diffusion/#3-handling-count-data-the-core-challenge","title":"3. Handling Count Data: The Core Challenge\u00b6","text":"<p>The approach above uses MSE loss and Gaussian outputs, which works for log-normalized expression data. But real gene expression is count data:</p> <ul> <li>UMI counts (scRNA-seq): integers with many zeros</li> <li>TPM/FPKM (bulk RNA-seq): continuous but count-derived</li> <li>Heavy-tailed: few highly expressed genes, many low/zero</li> </ul> <p>Problem: Adding Gaussian noise to counts doesn't have clear biological meaning.</p> <p>Solution: Use count-aware decoders (Negative Binomial, Zero-Inflated NB) that output distribution parameters instead of point estimates.</p>"},{"location":"diffusion/04_gene_expression_diffusion/04_gene_expression_diffusion/#31-vae-with-negative-binomial-decoder","title":"3.1 VAE with Negative Binomial Decoder\u00b6","text":"<p>The key insight: run diffusion in continuous latent space, but decode to count distributions.</p> <pre><code>Counts \u2192 Encoder \u2192 z (continuous) \u2192 Diffusion \u2192 z' \u2192 NB Decoder \u2192 NB(\u03bc, \u03b8) \u2192 Sample counts\n</code></pre> <p>The NB decoder outputs:</p> <ul> <li>\u03bc (mu): Expected count per gene</li> <li>\u03b8 (theta): Dispersion parameter (inverse overdispersion)</li> </ul>"},{"location":"diffusion/04_gene_expression_diffusion/04_gene_expression_diffusion/#32-latent-diffusion-with-nb-decoder","title":"3.2 Latent Diffusion with NB Decoder\u00b6","text":"<p>Now we combine the trained VAE-NB with diffusion in latent space:</p> <ol> <li>Extract latent representations from VAE-NB encoder</li> <li>Train diffusion in the continuous latent space</li> <li>Sample: noise \u2192 diffusion \u2192 latent \u2192 NB decoder \u2192 sample from NB distribution</li> </ol>"},{"location":"diffusion/04_gene_expression_diffusion/04_gene_expression_diffusion/#4-summary-key-takeaways","title":"4. Summary &amp; Key Takeaways\u00b6","text":""},{"location":"diffusion/04_gene_expression_diffusion/04_gene_expression_diffusion/#what-we-learned","title":"What We Learned\u00b6","text":"<p>The Count Data Challenge:</p> <ul> <li>Gene expression is fundamentally count data (UMI counts, TPM)</li> <li>Standard diffusion assumes continuous data with Gaussian noise</li> <li>Adding noise to counts doesn't have clear biological meaning</li> </ul> <p>Solutions Implemented:</p> Approach How It Works Pros Cons Latent Diffusion Diffusion in VAE latent space Well-defined, flexible Requires VAE training NB Decoder Output NB(\u03bc, \u03b8) parameters Proper count model More complex ZINB Decoder NB + dropout probability \u03c0 Handles sparsity Even more complex <p>The Recommended Pipeline:</p> <pre><code>Counts \u2192 log1p \u2192 Encoder \u2192 z (continuous) \u2192 Diffusion \u2192 z' \u2192 NB Decoder \u2192 NB(\u03bc,\u03b8) \u2192 Sample\n</code></pre>"},{"location":"diffusion/04_gene_expression_diffusion/04_gene_expression_diffusion/#implementation-in-genai-lab","title":"Implementation in genai-lab\u00b6","text":"<ul> <li><code>src/genailab/model/decoders.py</code>: <code>NegativeBinomialDecoder</code>, <code>ZINBDecoder</code></li> <li><code>src/genailab/objectives/losses.py</code>: <code>nb_loss()</code>, <code>zinb_loss()</code>, <code>elbo_loss_nb()</code>, <code>elbo_loss_zinb()</code></li> </ul>"},{"location":"diffusion/04_gene_expression_diffusion/04_gene_expression_diffusion/#next-steps","title":"Next Steps\u00b6","text":"<ol> <li>Add conditioning - Condition on tissue, disease, perturbation</li> <li>Apply to real data - PBMC3k, GTEx, scPerturb</li> <li>Implement ZINB - For sparse scRNA-seq with dropout</li> <li>Benchmark - Compare with scVI, scGen on standard tasks</li> <li>Connect to scPPDM - Full perturbation prediction pipeline</li> </ol>"},{"location":"diffusion/04_gene_expression_diffusion/04_gene_expression_diffusion/#references","title":"References\u00b6","text":"<ul> <li>Lopez et al. (2018) - \"Deep generative modeling for single-cell transcriptomics\" (scVI)</li> <li>Lotfollahi et al. (2020) - \"scGen predicts single-cell perturbation responses\"</li> <li>See also: <code>docs/incubation/generative-ai-for-gene-expression-prediction.md</code></li> </ul>"},{"location":"diffusion/DiT/diffusion_transformer/","title":"Diffusion Transformers (DiT): A Tutorial","text":"<p>This tutorial explains Diffusion Transformers (DiT) \u2014 the architectural shift from convolutional U-Nets to Transformers for generative modeling. We cover why this shift happened, how DiT works, and why it generalizes beyond images.</p> <p>Prerequisites: Familiarity with rectified flow or diffusion models (see <code>docs/flow_matching/rectifying_flow.md</code>).</p>"},{"location":"diffusion/DiT/diffusion_transformer/#1-what-is-a-diffusion-transformer","title":"1. What is a Diffusion Transformer?","text":"<p>A Diffusion Transformer (DiT) is not a new diffusion theory \u2014 it's an architectural choice.</p> <p>DiT is simply a Transformer used to parameterize the function learned in diffusion or flow-based models:</p> \\[ v_\\theta(x, t, c) \\quad \\text{(velocity prediction for rectified flow)} \\] \\[ \\varepsilon_\\theta(x, t, c) \\quad \\text{(noise prediction for DDPM)} \\] \\[ s_\\theta(x, t) \\quad \\text{(score prediction for score matching)} \\] <p>The objective (what to learn) and the architecture (how to learn it) are orthogonal design choices.</p>"},{"location":"diffusion/DiT/diffusion_transformer/#2-why-u-nets-dominated-early-diffusion","title":"2. Why U-Nets Dominated Early Diffusion","text":"<p>Historically, diffusion models used U-Net architectures because:</p> Strength Why It Helped Local structure Images have strong spatial correlations Multiscale features Downsampling captures global context Efficient Convolutions are fast and well-optimized Inductive bias Spatial structure is built into the architecture <p>U-Net learns:</p> <ul> <li>Local interactions in early layers</li> <li>Global interactions via progressive downsampling</li> <li>Skip connections preserve fine details</li> </ul> <p>This worked extremely well for images, but came with limitations:</p> <ul> <li>Fixed grid assumptions: Inputs must be regular grids</li> <li>Awkward conditioning: Adding new conditions requires architectural changes</li> <li>Limited flexibility: Hard to apply to non-image data</li> <li>Special handling: Time and modality need custom integration</li> </ul>"},{"location":"diffusion/DiT/diffusion_transformer/#3-the-architectural-shift-grids-tokens","title":"3. The Architectural Shift: Grids \u2192 Tokens","text":"<p>Transformers operate on tokens, not grids. The key conceptual move in DiT:</p> <p>Represent the input \\(x_t\\) as a sequence of tokens.</p> <p>For images:</p> <ol> <li>Split image into patches (e.g., 16\u00d716 pixels)</li> <li>Flatten each patch into a vector</li> <li>Embed into token space</li> </ol> <p>For other domains:</p> <ul> <li>Genes, cells, regions, timepoints \u2192 tokens</li> <li>Patches are a metaphor, not a requirement</li> </ul>"},{"location":"diffusion/DiT/diffusion_transformer/#4-input-representation","title":"4. Input Representation","text":"<p>Let \\(x_t \\in \\mathbb{R}^d\\) be the noisy (or interpolated) input at time \\(t\\).</p> <p>Tokenization:</p> \\[ X_t = [x_t^{(1)}, x_t^{(2)}, \\ldots, x_t^{(N)}] \\] <p>where:</p> <ul> <li>\\(x_t^{(i)} \\in \\mathbb{R}^{d_{\\text{patch}}}\\) is the \\(i\\)-th patch</li> <li>\\(N\\) is the number of tokens</li> </ul> <p>Embedding:</p> \\[ h^{(i)} = W_{\\text{embed}} \\cdot x_t^{(i)} + e^{(i)}_{\\text{pos}} \\] <p>The Transformer input is:</p> \\[ H = [h^{(1)}, \\ldots, h^{(N)}] \\]"},{"location":"diffusion/DiT/diffusion_transformer/#5-time-conditioning-via-adaptive-layernorm","title":"5. Time Conditioning via Adaptive LayerNorm","text":"<p>Diffusion models are time-conditioned. DiT handles this elegantly through modulation, not concatenation.</p> <p>Standard Transformer block:</p> \\[ \\text{Block}(H) = \\text{MLP}(\\text{Attention}(\\text{LN}(H))) \\] <p>DiT with Adaptive LayerNorm (AdaLN):</p> \\[ \\text{AdaLN}(h, t) = \\gamma(t) \\cdot \\text{LN}(h) + \\beta(t) \\] <p>where \\(\\gamma(t)\\) and \\(\\beta(t)\\) are produced from a time embedding:</p> \\[ \\tau = \\text{TimeEmbed}(t) \\quad \\rightarrow \\quad (\\gamma, \\beta) = \\text{MLP}(\\tau) \\] <p>Deep dive: For a detailed explanation of how time embeddings work and why the MLP doesn't \"perturb ordering,\" see time_embeddings_explained.md.</p> <p>Key insight: Time controls the behavior of the network at every layer, not just its input.</p> <p>This is the FiLM (Feature-wise Linear Modulation) pattern, which is much cleaner than concatenating \\(t\\) to inputs.</p>"},{"location":"diffusion/DiT/diffusion_transformer/#6-conditioning-beyond-time","title":"6. Conditioning Beyond Time","text":"<p>The same AdaLN mechanism handles arbitrary conditions:</p> <ul> <li>Class labels</li> <li>Text embeddings</li> <li>Perturbation tokens</li> <li>Experimental conditions</li> </ul> <p>Two approaches:</p> <ol> <li>Modulation: Embed condition \\(c \\mapsto e_c\\), use for AdaLN parameters</li> <li>Cross-attention: Append condition tokens, attend to them</li> </ol> <p>Transformers make adding new conditions trivial \u2014 no architectural surgery required.</p>"},{"location":"diffusion/DiT/diffusion_transformer/#7-what-the-transformer-computes","title":"7. What the Transformer Computes","text":"<p>Inside the Transformer:</p> \\[ H_{\\text{out}} = \\text{Transformer}(H_{\\text{in}}, t, c) \\] <p>Then project back to output space:</p> \\[ v_\\theta(x_t, t, c) = W_{\\text{out}} \\cdot H_{\\text{out}} \\] <p>Conceptually:</p> <ul> <li>Self-attention: Learns global dependencies between all tokens</li> <li>MLPs: Refine local nonlinearities</li> <li>Time modulation: Tells the network where it is along the trajectory</li> </ul> <p>This works regardless of training objective (score matching, noise prediction, or rectified flow).</p>"},{"location":"diffusion/DiT/diffusion_transformer/#8-dit-rectified-flow","title":"8. DiT + Rectified Flow","text":"<p>Combining DiT with rectified flow is particularly elegant.</p> <p>Recall rectified flow target:</p> \\[ \\text{target} = x_1 - x_0 \\] <p>DiT training loss:</p> \\[ \\mathcal{L} = \\mathbb{E}_{x_0, x_1, t} \\left[ \\left\\| v_\\theta(x_t, t) - (x_1 - x_0) \\right\\|^2 \\right] \\] <p>where:</p> <ul> <li>\\(v_\\theta\\) is a Transformer</li> <li>\\(x_t\\) is tokenized</li> <li>\\(t\\) modulates every layer via AdaLN</li> </ul> <p>Why this combination works well:</p> Component Contribution Transformers Model long-range structure via attention Rectified flow Simple, stable regression target AdaLN Clean time/condition integration ODE sampling Fast, deterministic generation"},{"location":"diffusion/DiT/diffusion_transformer/#9-why-dit-scales-better-than-u-net","title":"9. Why DiT Scales Better Than U-Net","text":"<p>Three structural reasons:</p>"},{"location":"diffusion/DiT/diffusion_transformer/#global-context-is-native","title":"Global Context is Native","text":"<p>Self-attention is global by default. No need for deep pyramids to propagate information across the image.</p>"},{"location":"diffusion/DiT/diffusion_transformer/#shape-flexibility","title":"Shape Flexibility","text":"<p>With packing/masking tricks (Patch-n-Pack):</p> <ul> <li>Variable image sizes in same batch</li> <li>Variable video lengths</li> <li>Heterogeneous biological objects</li> </ul> <p>This is impossible to do cleanly with CNNs.</p>"},{"location":"diffusion/DiT/diffusion_transformer/#conditioning-is-first-class","title":"Conditioning is First-Class","text":"<p>Adding a new condition:</p> <ul> <li>Add tokens, or</li> <li>Add modulation parameters</li> </ul> <p>No architectural changes needed.</p>"},{"location":"diffusion/DiT/diffusion_transformer/#10-beyond-images-dit-as-a-general-engine","title":"10. Beyond Images: DiT as a General Engine","text":"<p>Once you think of DiT as:</p> <p>\"A Transformer learning a time-dependent vector field\"</p> <p>It becomes a general-purpose continuous generative engine.</p> <p>Applications:</p> <ul> <li>Images (Stable Diffusion 3, DALL-E 3)</li> <li>Videos (Sora, Goku)</li> <li>Audio (AudioLDM)</li> <li>Molecules (protein structure)</li> <li>Trajectories (robotics)</li> <li>Latent biological states (gene expression)</li> </ul> <p>Key insight:</p> <ul> <li>Rectified flow removes density assumptions</li> <li>Transformers remove grid assumptions</li> <li>Together, they're highly portable</li> </ul>"},{"location":"diffusion/DiT/diffusion_transformer/#11-summary","title":"11. Summary","text":"<p>A Diffusion Transformer is a Transformer trained to predict time-conditioned vector fields, replacing convolutional inductive bias with global token interaction.</p> <p>Key components:</p> Component Purpose Patch embedding Convert input to tokens Positional encoding Preserve spatial/sequential structure AdaLN Time and condition modulation Self-attention Global dependencies Output projection Map back to target space <p>The modern generative stack:</p> <pre><code>Rectified Flow (objective) + DiT (architecture) + AdaLN (conditioning)\n</code></pre>"},{"location":"diffusion/DiT/diffusion_transformer/#references","title":"References","text":"<ul> <li>Peebles &amp; Xie (2023) - \"Scalable Diffusion Models with Transformers\" (DiT paper)</li> <li>Perez et al. (2018) - \"FiLM: Visual Reasoning with a General Conditioning Layer\"</li> <li>Dosovitskiy et al. (2020) - \"An Image is Worth 16x16 Words\" (ViT)</li> </ul>"},{"location":"diffusion/DiT/diffusion_transformer/#advanced-topics-alternative-backbones-for-biology","title":"Advanced Topics: Alternative Backbones for Biology","text":"<p>The following sections explore alternatives to Transformers for biological applications, where tokenization may not be natural.</p>"},{"location":"diffusion/DiT/diffusion_transformer/#12-what-diffusion-actually-requires-from-a-backbone","title":"12. What Diffusion Actually Requires from a Backbone","text":"<p>Strip away the branding. A diffusion or rectified-flow model needs a function:</p> \\[ f_\\theta(x_t, t, c) \\rightarrow \\text{vector field} \\] <p>Requirements:</p> <ul> <li>Accept a state representation</li> <li>Condition on time</li> <li>Optionally condition on context</li> <li>Output a vector of the same dimensionality as the state</li> </ul> <p>The real requirement:</p> <p>A model capable of learning global dependencies and time-conditioned transformations.</p> <p>Transformers satisfy this \u2014 but they are not unique.</p>"},{"location":"diffusion/DiT/diffusion_transformer/#13-state-space-models-as-diffusion-backbones","title":"13. State-Space Models as Diffusion Backbones","text":"<p>Can SSMs (Mamba, S4) or long convolutions (Hyena) be diffusion backbones?</p> <p>Yes. In fact, this is a natural pairing.</p> <p>Why?</p> <ul> <li>Rectified flow defines continuous-time dynamics</li> <li>State-space models are literally designed to model dynamics</li> </ul> <p>Architectures like:</p> <ul> <li>Long convolution models</li> <li>SSMs (S4, Mamba)</li> <li>Hyena-style implicit sequence operators</li> </ul> <p>are philosophically aligned with flow-based generative modeling.</p> <p>Why Transformers won historically:</p> <ul> <li>Easy to scale</li> <li>Clean conditioning via cross-attention</li> <li>Unified modalities early</li> <li>Infrastructure exists</li> </ul> <p>But this is historical inertia, not a fundamental requirement.</p>"},{"location":"diffusion/DiT/diffusion_transformer/#14-the-tokenization-problem-for-gene-expression","title":"14. The Tokenization Problem for Gene Expression","text":"<p>Gene expression vectors:</p> \\[ x \\in \\mathbb{R}^{G} \\] <p>where \\(G\\) is the number of genes.</p> <p>Properties:</p> <ul> <li>Unordered (no natural sequence)</li> <li>Dense (most genes have non-zero expression)</li> <li>Compositional (relative, not absolute)</li> <li>Population-relative</li> </ul> <p>The problem with \"genes as tokens\":</p> <p>Approaches like Geneformer rank genes by expression and treat them as a sequence. This works, but feels ontologically wrong:</p> <p>Ranking genes is not a natural ordering of biological state \u2014 it's an engineering trick.</p>"},{"location":"diffusion/DiT/diffusion_transformer/#15-better-representations-for-gene-expression","title":"15. Better Representations for Gene Expression","text":""},{"location":"diffusion/DiT/diffusion_transformer/#option-a-state-vector-no-tokens","title":"Option A: State Vector (No Tokens)","text":"<p>Treat expression as a single state vector:</p> <ul> <li>\\(x_t \\in \\mathbb{R}^G\\)</li> <li>Backbone: MLP, SSM, or continuous-time operator</li> <li>Time-conditioning via FiLM</li> </ul> <p>This aligns beautifully with rectified flow \u2014 you're learning a velocity in gene-expression space.</p>"},{"location":"diffusion/DiT/diffusion_transformer/#option-b-latent-space-diffusion","title":"Option B: Latent-Space Diffusion","text":"<p>Instead of tokenizing raw expression:</p> <ol> <li>Encode expression into latent state \\(z \\in \\mathbb{R}^d\\)</li> <li>Run diffusion/rectified flow in latent space</li> <li>Decode only if necessary</li> </ol> <p>The backbone sees:</p> <ul> <li>Smooth, lower-dimensional states</li> <li>No artificial ordering</li> <li>No sparsity pathologies</li> </ul> <p>This is where JEPA, VAEs, and diffusion naturally converge.</p>"},{"location":"diffusion/DiT/diffusion_transformer/#option-c-set-based-representations","title":"Option C: Set-Based Representations","text":"<p>If you insist on tokens, do it honestly:</p> <ul> <li>Represent expression as a set (unordered)</li> <li>Genes have embeddings</li> <li>Expression value modulates them</li> <li>Use permutation-invariant operators</li> <li>Attention without positional encoding</li> </ul>"},{"location":"diffusion/DiT/diffusion_transformer/#option-d-dynamics-first-ssm-friendly","title":"Option D: Dynamics-First (SSM-Friendly)","text":"<p>If your data is time-series, perturb-seq, or trajectories:</p> <ul> <li>The sequence is time, not genes</li> <li>Each timestep holds a full expression state or latent</li> <li>Backbone models temporal evolution</li> </ul> <p>This is where SSMs and Hyena-style operators shine.</p>"},{"location":"diffusion/DiT/diffusion_transformer/#16-a-natural-architecture-for-perturb-seq","title":"16. A Natural Architecture for Perturb-Seq","text":"<p>Combining the insights above:</p> <pre><code>Expression \u2192 Encoder \u2192 Latent State\n                          \u2193\n              SSM/Hyena modeling latent dynamics\n                          \u2193\n              Rectified flow in latent space\n                          \u2193\n              Decoder \u2192 Expression (if needed)\n</code></pre> <p>Properties:</p> <ul> <li>No fake tokens</li> <li>No gene ranking</li> <li>Natural temporal modeling</li> <li>Proper count handling via VAE decoder</li> </ul>"},{"location":"diffusion/DiT/diffusion_transformer/#17-the-organizing-principle","title":"17. The Organizing Principle","text":"<p>Tokenization is a convenience for architectures, not a requirement of the data.</p> <p>Once you internalize this:</p> <ul> <li>DiT becomes \"Transformer-as-backbone\"</li> <li>Rectified flow becomes \"state evolution\"</li> <li>Hyena/SSMs become first-class alternatives</li> <li>Gene expression stops being forced into unnatural formats</li> </ul>"},{"location":"diffusion/DiT/diffusion_transformer/#future-directions","title":"Future Directions","text":"<p>See <code>docs/incubation/</code> for explorations of:</p> <ul> <li>Latent rectified-flow + SSM architectures for perturb-seq</li> <li>Transformer vs SSM inductive biases for biological dynamics</li> <li>When tokenization is biologically meaningful (pathways, modules)</li> </ul>"},{"location":"diffusion/DiT/time_embeddings_explained/","title":"Time Embeddings in Diffusion Transformers: Deep Dive","text":"<p>Related: See Adaptive LayerNorm in diffusion_transformer.md for the broader context of how time embeddings are used in DiT.</p>"},{"location":"diffusion/DiT/time_embeddings_explained/#overview","title":"Overview","text":"<p>Time embeddings are a crucial component of diffusion models, allowing the network to adapt its behavior based on the current noise level. This document explains:</p> <ol> <li>What time embeddings are</li> <li>Why they don't \"perturb ordering\" when passed through MLPs</li> <li>How they differ from positional embeddings</li> <li>How they're used in Diffusion Transformers (DiT)</li> </ol>"},{"location":"diffusion/DiT/time_embeddings_explained/#the-core-question","title":"The Core Question","text":"<p>When looking at the AdaLN formulation in DiT:</p> <pre><code># From diffusion_transformer.md, lines 118-124\n\u03c4 = TimeEmbed(t)           # Time embedding\n(\u03b3, \u03b2) = MLP(\u03c4)            # Pass through MLP\nh_modulated = \u03b3 \u00b7 LN(h) + \u03b2  # Modulate features\n</code></pre> <p>Common confusion: \"Doesn't passing the time embedding through an MLP perturb the temporal ordering?\"</p> <p>Answer: No! The ordering is preserved because it's already encoded in the time embedding itself. The MLP learns to transform time information into useful modulation parameters without losing temporal relationships.</p>"},{"location":"diffusion/DiT/time_embeddings_explained/#time-embeddings-vs-positional-embeddings","title":"Time Embeddings vs Positional Embeddings","text":"<p>These are different concepts that are often confused:</p>"},{"location":"diffusion/DiT/time_embeddings_explained/#positional-embeddings-nlp-transformers","title":"Positional Embeddings (NLP Transformers)","text":"<p>Purpose: Tell the model \"where in the sequence am I?\"</p> <pre><code># Sequence position\ntokens = [\"The\", \"cat\", \"sat\"]  # positions 0, 1, 2\n\n# Positional embeddings\npos_embed_0 = sinusoidal_embedding(0)  # \"The\" is at position 0\npos_embed_1 = sinusoidal_embedding(1)  # \"cat\" is at position 1\npos_embed_2 = sinusoidal_embedding(2)  # \"sat\" is at position 2\n\n# Added directly to token embeddings\ntoken_with_pos = token_embed + pos_embed\n</code></pre> <p>Use case: Preserving sequence order in transformers (which otherwise treat input as a set)</p>"},{"location":"diffusion/DiT/time_embeddings_explained/#time-embeddings-diffusion-models","title":"Time Embeddings (Diffusion Models)","text":"<p>Purpose: Tell the model \"how much noise is in the data?\"</p> <pre><code># Diffusion timestep\nt = 500  # Current noise level (0=clean, 1000=pure noise)\n\n# Time embedding\ntime_embed = sinusoidal_embedding(t)  # Encodes noise level\n\n# Used to CONDITION model behavior (not added to tokens!)\n\u03b3, \u03b2 = MLP(time_embed)\nh_modulated = \u03b3 * LayerNorm(h) + \u03b2\n</code></pre> <p>Use case: Adapting model behavior to different noise levels</p>"},{"location":"diffusion/DiT/time_embeddings_explained/#key-differences","title":"Key Differences","text":"Aspect Positional Embedding Time Embedding Encodes Sequence position Noise level Applied to Each token separately Entire image/data Integration Added to token embeddings Used for feature modulation Varies per Token Timestep Example Token 0 vs Token 1 t=100 vs t=500"},{"location":"diffusion/DiT/time_embeddings_explained/#how-time-embeddings-work","title":"How Time Embeddings Work","text":""},{"location":"diffusion/DiT/time_embeddings_explained/#step-1-sinusoidal-encoding","title":"Step 1: Sinusoidal Encoding","text":"<p>Time embeddings convert a scalar timestep into a high-dimensional vector:</p> <pre><code>def sinusoidal_time_embedding(t, dim=256):\n    \"\"\"\n    Converts scalar timestep t into a high-dimensional vector.\n\n    Key property: Similar timesteps \u2192 similar embeddings\n\n    Args:\n        t: Timestep(s), shape (batch_size,)\n        dim: Embedding dimension (default 256)\n\n    Returns:\n        Embedding of shape (batch_size, dim)\n    \"\"\"\n    half_dim = dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim) * -emb)\n    emb = t[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n    return emb\n</code></pre> <p>Why sinusoidal? Uses multiple frequencies to encode the timestep value smoothly:</p> <pre><code># Frequency components\nfreq_1 = sin(t * \u03c9\u2081), cos(t * \u03c9\u2081)  # Low frequency (captures large changes)\nfreq_2 = sin(t * \u03c9\u2082), cos(t * \u03c9\u2082)  # Medium frequency\n...\nfreq_n = sin(t * \u03c9\u2099), cos(t * \u03c9\u2099)  # High frequency (captures fine changes)\n\n# Concatenate all frequencies\ntime_embed = [freq_1, freq_2, ..., freq_n]\n</code></pre>"},{"location":"diffusion/DiT/time_embeddings_explained/#step-2-ordering-preservation","title":"Step 2: Ordering Preservation","text":"<p>The key property: similar timesteps have similar embeddings</p> <pre><code># Example: Close timesteps\nemb_500 = sinusoidal_embedding(500)\nemb_501 = sinusoidal_embedding(501)\n\ncosine_similarity(emb_500, emb_501) \u2248 0.999  # Very similar!\n\n# Example: Distant timesteps\nemb_100 = sinusoidal_embedding(100)\nemb_900 = sinusoidal_embedding(900)\n\ncosine_similarity(emb_100, emb_900) \u2248 0.2   # Very different!\n</code></pre> <p>Visualization:</p> <pre><code>Timestep Space          Embedding Space\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nt=0    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192     emb\u2080   = [0.0,  1.0,  0.0,  1.0, ...]\nt=100  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192     emb\u2081\u2080\u2080 = [0.9,  0.4, -0.3,  0.8, ...]\nt=500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192     emb\u2085\u2080\u2080 = [0.2, -0.8,  0.4, -0.1, ...]\nt=1000 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192     emb\u2081\u2080\u2080\u2080= [-0.5, 0.8,  0.2, -0.9, ...]\n\nDistance relationships preserved:\ndist(0, 100) &lt; dist(0, 500) &lt; dist(0, 1000)\ndist(emb\u2080, emb\u2081\u2080\u2080) &lt; dist(emb\u2080, emb\u2085\u2080\u2080) &lt; dist(emb\u2080, emb\u2081\u2080\u2080\u2080)\n</code></pre>"},{"location":"diffusion/DiT/time_embeddings_explained/#why-mlps-dont-destroy-ordering","title":"Why MLPs Don't \"Destroy\" Ordering","text":""},{"location":"diffusion/DiT/time_embeddings_explained/#the-mlps-role","title":"The MLP's Role","text":"<p>The MLP transforms time embeddings into modulation parameters:</p> <pre><code># Time embedding encodes \"what time is it\"\ntime_embed = sinusoidal_embedding(t)  # (batch, 256)\n\n# MLP learns \"how should I adjust model behavior for this time?\"\nadaLN_params = MLP(time_embed)  # (batch, 1024)\n\u03b3, \u03b2 = adaLN_params.chunk(2, dim=-1)  # (batch, 512), (batch, 512)\n\n# Modulate features based on time\nh_modulated = \u03b3 * LayerNorm(h) + \u03b2\n</code></pre>"},{"location":"diffusion/DiT/time_embeddings_explained/#why-ordering-is-preserved","title":"Why Ordering is Preserved","text":"<p>Reason 1: Input distinguishability</p> <p>Different timesteps have different embeddings:</p> <pre><code>emb_100 = [0.9, 0.1, -0.3, ...]   # t=100 \u2192 unique vector\nemb_500 = [0.2, -0.8, 0.4, ...]   # t=500 \u2192 different vector\nemb_900 = [-0.5, 0.8, 0.2, ...]   # t=900 \u2192 another different vector\n</code></pre> <p>Reason 2: Function preserves distinguishability</p> <p>An MLP maps different inputs to different outputs:</p> <pre><code>\u03b3\u2081\u2080\u2080, \u03b2\u2081\u2080\u2080 = MLP(emb_100)  # Produces one set of parameters\n\u03b3\u2085\u2080\u2080, \u03b2\u2085\u2080\u2080 = MLP(emb_500)  # Produces different parameters\n\u03b3\u2089\u2080\u2080, \u03b2\u2089\u2080\u2080 = MLP(emb_900)  # Produces yet different parameters\n\n# As long as emb_100 \u2260 emb_500 \u2260 emb_900 (which they are!)\n# Then \u03b3\u2081\u2080\u2080 \u2260 \u03b3\u2085\u2080\u2080 \u2260 \u03b3\u2089\u2080\u2080 (assuming the MLP isn't degenerate)\n</code></pre> <p>Reason 3: Smooth functions preserve smoothness</p> <p>MLPs with smooth activations (SiLU, GELU) are continuous:</p> <pre><code># If inputs are similar\nemb_500 \u2248 emb_501  # Close timesteps\n\n# Then outputs are similar (by continuity)\nMLP(emb_500) \u2248 MLP(emb_501)\n\n# Temporal relationships are preserved\n</code></pre>"},{"location":"diffusion/DiT/time_embeddings_explained/#what-the-mlp-actually-learns","title":"What the MLP Actually Learns","text":"<p>The MLP learns time-dependent behavior:</p> <pre><code># Early timesteps (high noise, t\u22481000)\n# Model needs to focus on global structure\n\u03b3_early = [0.1, 0.1, 0.1, ...]  # Small scale \u2192 suppress details\n\u03b2_early = [0, 0, 0, ...]        # No shift\n\n# Middle timesteps (medium noise, t\u2248500)\n# Model balances structure and detail\n\u03b3_mid = [0.5, 0.8, 0.3, ...]    # Mixed scale\n\u03b2_mid = [0.1, -0.2, 0.3, ...]   # Some shift\n\n# Late timesteps (low noise, t\u2248100)\n# Model needs to refine fine details\n\u03b3_late = [1.2, 1.5, 0.8, ...]   # Large scale \u2192 amplify details\n\u03b2_late = [0.5, -0.3, 0.9, ...]  # Large shift \u2192 fine-tune features\n</code></pre> <p>The learned mapping:</p> <pre><code>Time Embedding       \u2192  MLP  \u2192  Modulation Strategy\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500          \u2500\u2500\u2500     \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nemb(t=1000, high noise) \u2192 \u03b3_suppress_details, \u03b2_zero\nemb(t=500, mid noise)   \u2192 \u03b3_mixed, \u03b2_adjust\nemb(t=100, low noise)   \u2192 \u03b3_amplify_details, \u03b2_finetune\n</code></pre>"},{"location":"diffusion/DiT/time_embeddings_explained/#complete-pipeline-in-dit","title":"Complete Pipeline in DiT","text":"<p>Here's exactly what happens in a DiT block:</p> <pre><code>class DiTBlock(nn.Module):\n    def __init__(self, hidden_dim=512, time_embed_dim=256):\n        self.norm = LayerNorm(hidden_dim)\n        self.attention = MultiHeadAttention(hidden_dim)\n\n        # MLP to convert time embedding \u2192 modulation parameters\n        self.adaLN_mlp = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(time_embed_dim, 2 * hidden_dim),\n        )\n\n    def forward(self, h, t):\n        \"\"\"\n        Args:\n            h: Token features, shape (batch, num_tokens, hidden_dim)\n            t: Timesteps, shape (batch,)\n\n        Returns:\n            Output features, shape (batch, num_tokens, hidden_dim)\n        \"\"\"\n        # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        # STEP 1: Create time embedding (preserves ordering!)\n        # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        time_embed = sinusoidal_embedding(t)  # (batch, 256)\n\n        # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        # STEP 2: Transform to modulation parameters\n        # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        adaLN_params = self.adaLN_mlp(time_embed)  # (batch, 1024)\n        \u03b3, \u03b2 = adaLN_params.chunk(2, dim=-1)  # 2x (batch, 512)\n\n        # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        # STEP 3: Apply Adaptive LayerNorm\n        # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        h_norm = self.norm(h)  # (batch, num_tokens, 512)\n\n        # Broadcast \u03b3 and \u03b2 across tokens\n        \u03b3 = \u03b3.unsqueeze(1)  # (batch, 1, 512) \u2192 broadcast to all tokens\n        \u03b2 = \u03b2.unsqueeze(1)  # (batch, 1, 512) \u2192 broadcast to all tokens\n\n        # Modulate features based on time\n        h_modulated = \u03b3 * h_norm + \u03b2  # (batch, num_tokens, 512)\n\n        # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        # STEP 4: Standard attention (now time-conditioned!)\n        # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        h_out = self.attention(h_modulated)\n\n        return h + h_out  # Residual connection\n</code></pre>"},{"location":"diffusion/DiT/time_embeddings_explained/#flow-diagram","title":"Flow Diagram","text":"<pre><code>Input: h (token features), t (scalar timestep)\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nt = 500 (scalar)\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 1: Sinusoidal Embedding                \u2502\n\u2502 (Preserves temporal ordering)               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193\ntime_embed = [0.2, -0.8, 0.4, ..., 0.1]  (256-dim vector)\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 2: MLP Transform                       \u2502\n\u2502 (Learns time-dependent behavior)            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193\n\u03b3 = [1.2, 0.8, 1.5, ...]  (512-dim)\n\u03b2 = [0.1, -0.3, 0.5, ...]  (512-dim)\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 3: Modulate Features                   \u2502\n\u2502 (Condition on time)                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193\nh_modulated = \u03b3 \u2299 LayerNorm(h) + \u03b2\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 4: Attention &amp; Processing              \u2502\n\u2502 (Standard transformer operations)           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193\nOutput\n</code></pre> <p>Key insight: The MLP doesn't operate on your data tokens (h). It operates on the time embedding to produce modulation parameters that affect how the model processes the tokens.</p>"},{"location":"diffusion/DiT/time_embeddings_explained/#concrete-example-with-numbers","title":"Concrete Example with Numbers","text":"<p>Let's trace through with actual values:</p>"},{"location":"diffusion/DiT/time_embeddings_explained/#example-1-high-noise-t900","title":"Example 1: High Noise (t=900)","text":"<pre><code># Input\nt = 900\n\n# Step 1: Time embedding\ntime_embed = sinusoidal_embedding(900)\n# \u2192 [-0.448, 0.894, 0.732, -0.681, ..., 0.123]  (256 values)\n\n# Step 2: MLP \u2192 modulation parameters\n\u03b3, \u03b2 = MLP(time_embed)\n# \u03b3 \u2192 [0.1, 0.1, 0.2, 0.1, ..., 0.1]  # Small values \u2192 suppress details\n# \u03b2 \u2192 [0.0, 0.0, 0.0, 0.0, ..., 0.0]  # Near zero \u2192 no shift\n\n# Step 3: Modulate features\nh = [[0.5, -0.2, 0.8, 0.3, ...],   # Token 1\n     [0.3, 0.1, -0.4, 0.6, ...],   # Token 2\n     ...]\n\nh_modulated = \u03b3 * LayerNorm(h) + \u03b2\n# Result: Features are suppressed (focus on structure, ignore details)\n</code></pre>"},{"location":"diffusion/DiT/time_embeddings_explained/#example-2-low-noise-t100","title":"Example 2: Low Noise (t=100)","text":"<pre><code># Input\nt = 100\n\n# Step 1: Time embedding\ntime_embed = sinusoidal_embedding(100)\n# \u2192 [0.985, 0.174, -0.342, 0.940, ..., -0.766]  (256 values)\n# Note: Very different from t=900 embedding!\n\n# Step 2: MLP \u2192 modulation parameters\n\u03b3, \u03b2 = MLP(time_embed)\n# \u03b3 \u2192 [1.5, 1.2, 1.8, 1.3, ..., 1.6]  # Large values \u2192 amplify details\n# \u03b2 \u2192 [0.3, -0.2, 0.5, -0.4, ..., 0.6]  # Non-zero \u2192 fine-tune\n\n# Step 3: Modulate features\nh_modulated = \u03b3 * LayerNorm(h) + \u03b2\n# Result: Features are amplified and shifted (refine details)\n</code></pre>"},{"location":"diffusion/DiT/time_embeddings_explained/#ordering-is-preserved","title":"Ordering is Preserved","text":"<pre><code># Distance in timestep space\n|t_900 - t_100| = 800  # Large distance\n\n# Distance in embedding space\n||emb_900 - emb_100|| = 5.23  # Large distance (preserves ordering!)\n\n# Distance in modulation space\n||\u03b3_900 - \u03b3_100|| = 12.4  # Large distance (different behaviors!)\n</code></pre> <p>The key: The MLP preserves the distinguishability between different timesteps while learning useful transformations.</p>"},{"location":"diffusion/DiT/time_embeddings_explained/#why-this-design-is-better","title":"Why This Design is Better","text":""},{"location":"diffusion/DiT/time_embeddings_explained/#alternative-1-direct-addition-not-used","title":"Alternative 1: Direct Addition (Not Used)","text":"<pre><code># Add time info directly to tokens (like positional embedding)\nh = h + time_embed.unsqueeze(1)  # Broadcast time to all tokens\n</code></pre> <p>Problems:</p> <ul> <li>\u274c Same time signal added to all features</li> <li>\u274c Can't selectively affect different channels</li> <li>\u274c Less expressive</li> </ul>"},{"location":"diffusion/DiT/time_embeddings_explained/#alternative-2-concatenation-not-used","title":"Alternative 2: Concatenation (Not Used)","text":"<pre><code># Concatenate time info with features\nh = torch.cat([h, time_embed.expand(num_tokens, -1)], dim=-1)\n</code></pre> <p>Problems:</p> <ul> <li>\u274c Increases dimensionality</li> <li>\u274c Wastes computation</li> <li>\u274c Less flexible than modulation</li> </ul>"},{"location":"diffusion/DiT/time_embeddings_explained/#adaln-what-dit-uses","title":"AdaLN (What DiT Uses) \u2713","text":"<pre><code># Time-dependent scaling and shifting\n\u03b3, \u03b2 = MLP(time_embed)\nh = \u03b3 * LayerNorm(h) + \u03b2\n</code></pre> <p>Advantages:</p> <ul> <li>\u2705 Different features can be affected differently</li> <li>\u2705 Learned optimal time-dependent behavior</li> <li>\u2705 Powerful: can amplify/suppress features based on noise level</li> <li>\u2705 Efficient: no extra dimensions</li> </ul>"},{"location":"diffusion/DiT/time_embeddings_explained/#comparison-different-conditioning-methods","title":"Comparison: Different Conditioning Methods","text":"Method How it Works Pros Cons AdaLN (DiT) \u03b3, \u03b2 = MLP(t)h = \u03b3\u00b7LN(h) + \u03b2 \u2705 Flexible\u2705 Powerful\u2705 Efficient Slightly complex FiLM Same as AdaLN \u2705 Same as AdaLN Same as AdaLN Cross-Attention Attend to time token \u2705 Very flexible \u274c Expensive (O(n\u00b2)) Addition h = h + emb(t) \u2705 Simple \u274c Less expressive Concatenation h = [h; emb(t)] \u2705 Simple \u274c Increases dims <p>DiT uses AdaLN/FiLM because it offers the best balance of expressiveness and efficiency.</p>"},{"location":"diffusion/DiT/time_embeddings_explained/#implementation-tips","title":"Implementation Tips","text":""},{"location":"diffusion/DiT/time_embeddings_explained/#1-time-embedding-dimension","title":"1. Time Embedding Dimension","text":"<pre><code># Common choices\ntime_embed_dim = 256  # Standard\ntime_embed_dim = 512  # More capacity\ntime_embed_dim = 128  # Smaller models\n\n# Rule of thumb: ~1/2 to 1x of hidden_dim\n</code></pre>"},{"location":"diffusion/DiT/time_embeddings_explained/#2-mlp-architecture","title":"2. MLP Architecture","text":"<pre><code># Simple (DiT-S)\nMLP = nn.Sequential(\n    nn.SiLU(),\n    nn.Linear(time_embed_dim, 2 * hidden_dim)\n)\n\n# With intermediate layer (more capacity)\nMLP = nn.Sequential(\n    nn.SiLU(),\n    nn.Linear(time_embed_dim, 4 * hidden_dim),\n    nn.SiLU(),\n    nn.Linear(4 * hidden_dim, 2 * hidden_dim)\n)\n</code></pre>"},{"location":"diffusion/DiT/time_embeddings_explained/#3-initialization","title":"3. Initialization","text":"<pre><code># Initialize \u03b3 to 1, \u03b2 to 0 for identity at start\ndef init_adaln_mlp(module):\n    if isinstance(module, nn.Linear):\n        # Last layer: initialize to produce \u03b3\u22481, \u03b2\u22480\n        nn.init.zeros_(module.weight)\n        nn.init.zeros_(module.bias)\n</code></pre>"},{"location":"diffusion/DiT/time_embeddings_explained/#4-numerical-stability","title":"4. Numerical Stability","text":"<pre><code># Clip \u03b3 to prevent extreme scaling\n\u03b3 = \u03b3.clamp(min=0.1, max=10.0)\n\n# Or use softer constraints\n\u03b3 = torch.exp(\u03b3.clamp(min=-2, max=2))  # Ensures \u03b3 \u2208 [0.14, 7.4]\n</code></pre>"},{"location":"diffusion/DiT/time_embeddings_explained/#for-gene-expression-additional-considerations","title":"For Gene Expression: Additional Considerations","text":"<p>When adapting time embeddings for gene expression data:</p>"},{"location":"diffusion/DiT/time_embeddings_explained/#1-timestep-range","title":"1. Timestep Range","text":"<pre><code># Images: Often T=1000\nT = 1000\n\n# Gene expression: May want different range\nT = 100   # Fewer steps (faster sampling)\nT = 1000  # Standard (better quality)\n\n# Continuous time (rectified flow)\nt \u2208 [0, 1]  # Normalized time\n</code></pre>"},{"location":"diffusion/DiT/time_embeddings_explained/#2-domain-specific-modulation","title":"2. Domain-Specific Modulation","text":"<pre><code># Standard: modulate all features equally\nh_modulated = \u03b3 * LN(h) + \u03b2\n\n# Gene-specific: different modulation per gene group\n\u03b3_housekeeping = \u03b3[:, :1000]   # Housekeeping genes\n\u03b3_variable = \u03b3[:, 1000:]       # Variable genes\n\nh_modulated[:, :1000] = \u03b3_housekeeping * LN(h[:, :1000]) + \u03b2[:, :1000]\nh_modulated[:, 1000:] = \u03b3_variable * LN(h[:, 1000:]) + \u03b2[:, 1000:]\n</code></pre>"},{"location":"diffusion/DiT/time_embeddings_explained/#3-biological-constraints","title":"3. Biological Constraints","text":"<pre><code># Enforce positive expression after modulation\nh_modulated = F.softplus(\u03b3 * LN(h) + \u03b2)\n\n# Or use log-space\nlog_h_modulated = \u03b3 * LN(log_h) + \u03b2\nh_modulated = torch.exp(log_h_modulated)\n</code></pre>"},{"location":"diffusion/DiT/time_embeddings_explained/#analogy-thermostat-control","title":"Analogy: Thermostat Control","text":"<p>To build intuition, think of time embeddings like a thermostat:</p> <pre><code>Time Embedding = Temperature Reading\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nSummer (t=900, high noise): thermometer reads 90\u00b0F\n  \u2192 time_embed = [high values]\n\nWinter (t=100, low noise): thermometer reads 30\u00b0F\n  \u2192 time_embed = [low values]\n\nMLP = Control Logic\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n[90\u00b0F reading] \u2192 MLP \u2192 (\u03b3=0.1, \u03b2=0)\n                        \"Turn AC on, suppress heating\"\n\n[30\u00b0F reading] \u2192 MLP \u2192 (\u03b3=1.5, \u03b2=0.5)\n                        \"Turn heater on, boost warmth\"\n\nResult: Adaptive Behavior\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nHigh noise (90\u00b0F): Model focuses on structure (suppress detail features)\nLow noise (30\u00b0F):  Model refines details (amplify detail features)\n</code></pre> <p>The MLP doesn't \"lose\" the temperature information. It learns: \"Given THIS temperature, adjust controls THIS way.\"</p> <p>Similarly, the MLP in DiT learns: \"Given THIS noise level, modulate features THIS way.\"</p>"},{"location":"diffusion/DiT/time_embeddings_explained/#common-misconceptions","title":"Common Misconceptions","text":""},{"location":"diffusion/DiT/time_embeddings_explained/#misconception-1-mlp-destroys-temporal-ordering","title":"\u274c Misconception 1: \"MLP destroys temporal ordering\"","text":"<p>Reality: The time embedding already encodes the ordering. The MLP just transforms it while preserving distinguishability.</p>"},{"location":"diffusion/DiT/time_embeddings_explained/#misconception-2-time-embeddings-are-like-positional-embeddings","title":"\u274c Misconception 2: \"Time embeddings are like positional embeddings\"","text":"<p>Reality: Different purposes: - Positional: Where in sequence (per token) - Time: How much noise (global)</p>"},{"location":"diffusion/DiT/time_embeddings_explained/#misconception-3-should-add-time-embedding-to-features-directly","title":"\u274c Misconception 3: \"Should add time embedding to features directly\"","text":"<p>Reality: Modulation (AdaLN) is more powerful and expressive than addition.</p>"},{"location":"diffusion/DiT/time_embeddings_explained/#misconception-4-time-embedding-needs-to-be-high-dimensional","title":"\u274c Misconception 4: \"Time embedding needs to be high-dimensional\"","text":"<p>Reality: 256-dim is usually sufficient. More isn't always better (can lead to overfitting).</p>"},{"location":"diffusion/DiT/time_embeddings_explained/#further-reading","title":"Further Reading","text":""},{"location":"diffusion/DiT/time_embeddings_explained/#papers","title":"Papers","text":"<ul> <li>DDPM (Ho et al., 2020): Introduced noise prediction with time conditioning</li> <li>Improved DDPM (Nichol &amp; Dhariwal, 2021): Learned noise schedules</li> <li>DiT (Peebles &amp; Xie, 2023): Adaptive LayerNorm conditioning</li> <li>FiLM (Perez et al., 2018): Feature-wise linear modulation</li> </ul>"},{"location":"diffusion/DiT/time_embeddings_explained/#related-topics","title":"Related Topics","text":"<ul> <li>Adaptive LayerNorm in diffusion_transformer.md</li> <li>Time embedding in DDPM training</li> <li>Sinusoidal positional encoding (if exists)</li> </ul>"},{"location":"diffusion/DiT/time_embeddings_explained/#summary","title":"Summary","text":"<p>Time embeddings convert scalar timesteps into high-dimensional vectors that allow diffusion models to adapt their behavior to different noise levels.</p> <p>Key points:</p> <ol> <li>Purpose: Encode noise level, not sequence position</li> <li>Method: Sinusoidal encoding preserves temporal ordering</li> <li>Integration: Used for feature modulation (AdaLN), not addition</li> <li>MLP role: Learns time-dependent behavior without destroying ordering</li> <li>Design choice: AdaLN is more expressive than addition or concatenation</li> </ol> <p>The ordering is preserved because: - Sinusoidal encoding makes similar times \u2192 similar embeddings - MLP is a smooth, continuous function - Different times produce different modulation parameters</p> <p>The result: The model learns to denoise differently at different noise levels, from focusing on global structure (high noise) to refining fine details (low noise).</p> <p>Last Updated: January 12, 2026</p> <p>Back to: DiT Documentation | Diffusion Models Overview</p>"},{"location":"diffusion/forward_process/forward_process_derivation/","title":"Deriving the Forward Process: From SDE to Closed-Form Marginal","text":""},{"location":"diffusion/forward_process/forward_process_derivation/#the-goal","title":"The Goal","text":"<p>We want to derive the closed-form relationship between clean data \\(x_0\\) and noisy data \\(x_t\\):</p> \\[ \\boxed{x_t = \\sqrt{\\bar{\\alpha}_t}\\, x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\, \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, I)} \\] <p>This formula lets us generate noisy samples at any time \\(t\\) directly, without simulating the SDE step-by-step.</p>"},{"location":"diffusion/forward_process/forward_process_derivation/#starting-point-the-vp-sde","title":"Starting Point: The VP-SDE","text":"<p>The Variance-Preserving SDE is:</p> \\[ dx = -\\frac{1}{2}\\beta(t)\\,x\\,dt + \\sqrt{\\beta(t)}\\,dw \\] <p>where:</p> <ul> <li>\\(\\beta(t) &gt; 0\\) is the noise schedule (a design choice)</li> <li>\\(dw\\) is the Brownian motion increment</li> </ul> <p>Goal: Solve this SDE to find \\(x_t\\) in terms of \\(x_0\\).</p>"},{"location":"diffusion/forward_process/forward_process_derivation/#noise-schedule-choices","title":"Noise Schedule Choices","text":"<p>The noise schedule \\(\\beta(t)\\) controls how quickly noise is added over time. It's a crucial design choice that affects training stability and sample quality.</p>"},{"location":"diffusion/forward_process/forward_process_derivation/#common-schedules","title":"Common Schedules","text":"Schedule When to Use Key Property Linear Initial experiments Simple, uniform noise addition Cosine High-quality generation Preserves signal early, efficient corruption Polynomial Ablation studies Flexible temporal profile"},{"location":"diffusion/forward_process/forward_process_derivation/#recommended-cosine-schedule","title":"Recommended: Cosine Schedule","text":"<p>The cosine schedule often produces better results because: 1. Preserves signal at early timesteps (slow noise addition) 2. Efficiently corrupts to pure noise at late timesteps 3. Better training dynamics across all noise levels</p>"},{"location":"diffusion/forward_process/forward_process_derivation/#for-details","title":"For Details","text":"<p>See <code>noise_schedules.md</code> for: - Complete formulas for all schedules - Derivations of cumulative noise \\(\\bar{\\alpha}_t\\) - Visual comparisons and intuitions - Implementation examples - Guidelines for choosing and tuning schedules</p>"},{"location":"diffusion/forward_process/forward_process_derivation/#method-solving-linear-sdes","title":"Method: Solving Linear SDEs","text":"<p>The VP-SDE is a linear SDE (drift is linear in \\(x\\)). For linear SDEs, we can find closed-form solutions using integrating factors.</p>"},{"location":"diffusion/forward_process/forward_process_derivation/#general-linear-sde","title":"General Linear SDE","text":"<p>A linear SDE has the form:</p> \\[ dx = a(t)\\,x\\,dt + b(t)\\,dw \\] <p>For VP-SDE: \\(a(t) = -\\frac{1}{2}\\beta(t)\\) and \\(b(t) = \\sqrt{\\beta(t)}\\).</p>"},{"location":"diffusion/forward_process/forward_process_derivation/#step-1-define-the-integrating-factor","title":"Step 1: Define the Integrating Factor","text":"<p>Define:</p> \\[ \\mu(t) = \\exp\\left(-\\int_0^t a(s)\\,ds\\right) = \\exp\\left(\\frac{1}{2}\\int_0^t \\beta(s)\\,ds\\right) \\] <p>This is chosen so that \\(\\frac{d\\mu}{dt} = -a(t)\\mu(t)\\).</p>"},{"location":"diffusion/forward_process/forward_process_derivation/#step-2-apply-the-product-rule-itos-lemma","title":"Step 2: Apply the Product Rule (It\u00f4's Lemma)","text":"<p>Consider the product \\(\\mu(t) x(t)\\). By It\u00f4's lemma:</p> \\[ d(\\mu x) = \\mu\\,dx + x\\,d\\mu + \\underbrace{d\\mu \\cdot dx}_{=0 \\text{ (since } d\\mu \\text{ is deterministic)}} \\] <p>Since \\(d\\mu = -a(t)\\mu\\,dt\\):</p> \\[ d(\\mu x) = \\mu\\,dx - a(t)\\mu x\\,dt \\] <p>Substitute \\(dx = a(t)x\\,dt + b(t)\\,dw\\):</p> \\[ d(\\mu x) = \\mu(a(t)x\\,dt + b(t)\\,dw) - a(t)\\mu x\\,dt \\] \\[ d(\\mu x) = \\mu a(t)x\\,dt + \\mu b(t)\\,dw - a(t)\\mu x\\,dt \\] <p>The drift terms cancel:</p> \\[ d(\\mu x) = \\mu(t) b(t)\\,dw \\]"},{"location":"diffusion/forward_process/forward_process_derivation/#step-3-integrate-both-sides","title":"Step 3: Integrate Both Sides","text":"<p>Integrate from \\(0\\) to \\(t\\):</p> \\[ \\mu(t) x(t) - \\mu(0) x(0) = \\int_0^t \\mu(s) b(s)\\,dw(s) \\] <p>Since \\(\\mu(0) = 1\\):</p> \\[ \\mu(t) x(t) = x(0) + \\int_0^t \\mu(s) b(s)\\,dw(s) \\]"},{"location":"diffusion/forward_process/forward_process_derivation/#step-4-solve-for-xt","title":"Step 4: Solve for \\(x(t)\\)","text":"\\[ x(t) = \\frac{1}{\\mu(t)} x(0) + \\frac{1}{\\mu(t)} \\int_0^t \\mu(s) b(s)\\,dw(s) \\] <p>Define:</p> \\[ \\alpha(t) = \\frac{1}{\\mu(t)} = \\exp\\left(-\\frac{1}{2}\\int_0^t \\beta(s)\\,ds\\right) \\] <p>Then:</p> \\[ x(t) = \\alpha(t) x(0) + \\alpha(t) \\int_0^t \\mu(s) b(s)\\,dw(s) \\]"},{"location":"diffusion/forward_process/forward_process_derivation/#simplifying-the-stochastic-integral","title":"Simplifying the Stochastic Integral","text":""},{"location":"diffusion/forward_process/forward_process_derivation/#the-integral-is-gaussian","title":"The Integral is Gaussian","text":"<p>The stochastic integral \\(\\int_0^t \\mu(s) b(s)\\,dw(s)\\) is a sum of Gaussian increments, so it's Gaussian with:</p> <ul> <li>Mean: \\(0\\) (It\u00f4 integrals have zero mean)</li> <li>Variance: \\(\\int_0^t \\mu(s)^2 b(s)^2\\,ds\\) (It\u00f4 isometry)</li> </ul>"},{"location":"diffusion/forward_process/forward_process_derivation/#computing-the-variance","title":"Computing the Variance","text":"<p>Recall: - \\(\\mu(s) = \\exp\\left(\\frac{1}{2}\\int_0^s \\beta(u)\\,du\\right)\\) - \\(b(s) = \\sqrt{\\beta(s)}\\)</p> <p>So:</p> \\[ \\mu(s)^2 b(s)^2 = \\exp\\left(\\int_0^s \\beta(u)\\,du\\right) \\cdot \\beta(s) \\] <p>The variance of the stochastic integral is:</p> \\[ \\text{Var} = \\int_0^t \\exp\\left(\\int_0^s \\beta(u)\\,du\\right) \\beta(s)\\,ds \\] <p>Trick: Let \\(\\Phi(s) = \\int_0^s \\beta(u)\\,du\\). Then \\(\\frac{d\\Phi}{ds} = \\beta(s)\\), so:</p> \\[ \\text{Var} = \\int_0^t e^{\\Phi(s)}\\,d\\Phi(s) = e^{\\Phi(t)} - e^{\\Phi(0)} = e^{\\Phi(t)} - 1 \\] <p>Since \\(\\Phi(t) = \\int_0^t \\beta(s)\\,ds\\) and \\(\\mu(t) = e^{\\Phi(t)/2}\\):</p> \\[ \\text{Var} = \\mu(t)^2 - 1 \\]"},{"location":"diffusion/forward_process/forward_process_derivation/#variance-of-the-noise-term-in-xt","title":"Variance of the Noise Term in \\(x(t)\\)","text":"<p>The noise term in \\(x(t)\\) is:</p> \\[ \\alpha(t) \\int_0^t \\mu(s) b(s)\\,dw(s) \\] <p>Its variance is:</p> \\[ \\alpha(t)^2 \\cdot (\\mu(t)^2 - 1) = \\frac{1}{\\mu(t)^2} \\cdot (\\mu(t)^2 - 1) = 1 - \\frac{1}{\\mu(t)^2} = 1 - \\alpha(t)^2 \\]"},{"location":"diffusion/forward_process/forward_process_derivation/#the-final-result","title":"The Final Result","text":""},{"location":"diffusion/forward_process/forward_process_derivation/#distribution-of-x_t","title":"Distribution of \\(x_t\\)","text":"<p>We've shown that \\(x_t\\) given \\(x_0\\) is Gaussian:</p> \\[ x_t \\mid x_0 \\sim \\mathcal{N}\\left(\\alpha(t) x_0, (1 - \\alpha(t)^2) I\\right) \\]"},{"location":"diffusion/forward_process/forward_process_derivation/#reparameterization","title":"Reparameterization","text":"<p>We can write this as:</p> \\[ x_t = \\alpha(t) x_0 + \\sqrt{1 - \\alpha(t)^2}\\, \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, I) \\]"},{"location":"diffusion/forward_process/forward_process_derivation/#standard-notation","title":"Standard Notation","text":"<p>Using \\(\\bar{\\alpha}_t = \\alpha(t)^2 = \\exp\\left(-\\int_0^t \\beta(s)\\,ds\\right)\\):</p> \\[ \\boxed{x_t = \\sqrt{\\bar{\\alpha}_t}\\, x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\, \\varepsilon} \\] <p>where:</p> <ul> <li>\\(\\sqrt{\\bar{\\alpha}_t}\\) = signal coefficient (how much of \\(x_0\\) remains)</li> <li>\\(\\sqrt{1-\\bar{\\alpha}_t}\\) = noise coefficient (how much noise is added)</li> </ul>"},{"location":"diffusion/forward_process/forward_process_derivation/#interpreting-the-formula","title":"Interpreting the Formula","text":""},{"location":"diffusion/forward_process/forward_process_derivation/#at-different-times","title":"At Different Times","text":"Time \\(t\\) \\(\\bar{\\alpha}_t\\) Signal Noise Interpretation \\(t = 0\\) \\(1\\) \\(x_0\\) \\(0\\) Clean data Small \\(t\\) \\(\\approx 1\\) \\(\\approx x_0\\) Small Slightly noisy Large \\(t\\) \\(\\approx 0\\) \\(\\approx 0\\) \\(\\approx \\varepsilon\\) Almost pure noise \\(t \\to \\infty\\) \\(0\\) \\(0\\) \\(\\varepsilon\\) Pure Gaussian noise"},{"location":"diffusion/forward_process/forward_process_derivation/#why-variance-preserving","title":"Why \"Variance-Preserving\"?","text":"<p>The variance of \\(x_t\\) is:</p> \\[ \\text{Var}(x_t) = \\bar{\\alpha}_t \\cdot \\text{Var}(x_0) + (1 - \\bar{\\alpha}_t) \\cdot I \\] <p>If \\(\\text{Var}(x_0) = I\\) (data is pre-normalized), then:</p> \\[ \\text{Var}(x_t) = \\bar{\\alpha}_t \\cdot I + (1 - \\bar{\\alpha}_t) \\cdot I = I \\] <p>The variance is preserved at all times! This is why it's called the Variance-Preserving SDE.</p>"},{"location":"diffusion/forward_process/forward_process_derivation/#connection-to-ddpm","title":"Connection to DDPM","text":"<p>In discrete-time DDPM, the noise schedule is:</p> \\[ \\alpha_t = \\sqrt{1 - \\beta_t}, \\quad \\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s^2 = \\prod_{s=1}^t (1 - \\beta_s) \\] <p>The continuous-time limit (\\(\\Delta t \\to 0\\)) gives:</p> \\[ \\bar{\\alpha}_t = \\exp\\left(-\\int_0^t \\beta(s)\\,ds\\right) \\] <p>which matches our derivation. DDPM is the discretized version of the VP-SDE.</p>"},{"location":"diffusion/forward_process/forward_process_derivation/#summary","title":"Summary","text":"Quantity Formula Meaning VP-SDE \\(dx = -\\frac{1}{2}\\beta(t)x\\,dt + \\sqrt{\\beta(t)}\\,dw\\) Continuous-time forward process Signal coefficient \\(\\sqrt{\\bar{\\alpha}_t} = \\exp\\left(-\\frac{1}{2}\\int_0^t \\beta(s)\\,ds\\right)\\) How much of \\(x_0\\) survives Noise coefficient \\(\\sqrt{1-\\bar{\\alpha}_t}\\) How much noise is added Closed-form marginal \\(x_t = \\sqrt{\\bar{\\alpha}_t}\\,x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\,\\varepsilon\\) Sample noisy data directly"},{"location":"diffusion/forward_process/forward_process_derivation/#why-this-matters-for-training","title":"Why This Matters for Training","text":"<p>During training, we need to sample \\((x_t, t)\\) pairs efficiently. Without the closed-form marginal, we'd have to simulate the SDE step-by-step from \\(t=0\\) to some random \\(t\\)\u2014expensive!</p> <p>With the closed form: 1. Sample \\(x_0\\) from data 2. Sample \\(t\\) uniformly 3. Sample \\(\\varepsilon \\sim \\mathcal{N}(0, I)\\) 4. Compute \\(x_t = \\sqrt{\\bar{\\alpha}_t}\\,x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\,\\varepsilon\\) directly</p> <p>This is fast, exact, and enables efficient training of diffusion models.</p>"},{"location":"diffusion/forward_process/forward_process_derivation/#references","title":"References","text":"<ul> <li>Song et al. (2021): Score-Based Generative Modeling through SDEs</li> <li>Ho et al. (2020): Denoising Diffusion Probabilistic Models</li> <li>\u00d8ksendal (2003): Stochastic Differential Equations \u2014 Chapter 5 on linear SDEs</li> <li>S\u00e4rkk\u00e4 &amp; Solin (2019): Applied Stochastic Differential Equations \u2014 Practical derivations</li> </ul>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/","title":"Fokker-Planck Equation: Derivation and Intuition","text":""},{"location":"diffusion/reverse_process/fokker_planck_derivation/#overview","title":"Overview","text":"<p>The Fokker-Planck equation (also called the forward Kolmogorov equation) describes how the probability distribution of a stochastic process evolves over time. It is the bridge between individual particle dynamics (described by SDEs) and collective probability evolution.</p> <p>This document derives the Fokker-Planck equation from first principles and explains why probability distributions must obey it.</p>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#referenced-from","title":"Referenced From","text":"<ul> <li>Main Document: <code>docs/diffusion/reverse_process/reverse_process_derivation.md</code> \u2014 Uses the Fokker-Planck equation to derive the reverse SDE</li> <li>Related: <code>notebooks/diffusion/02_sde_formulation/supplements/07_fokker_planck_equation.md</code></li> </ul>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#table-of-contents","title":"Table of Contents","text":"<ol> <li>The Setting</li> <li>Intuitive Picture</li> <li>Derivation via Infinitesimal Evolution</li> <li>Kramers-Moyal Expansion (Rigorous)</li> <li>Physical Interpretation</li> <li>Connection to Conservation Laws</li> <li>Example: Simple Diffusion</li> <li>Why This Matters for Reverse SDEs</li> </ol>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#the-setting","title":"The Setting","text":""},{"location":"diffusion/reverse_process/fokker_planck_derivation/#stochastic-differential-equation","title":"Stochastic Differential Equation","text":"<p>Consider a \\(d\\)-dimensional stochastic process \\(x(t) \\in \\mathbb{R}^d\\) governed by:</p> \\[ dx = f(x,t)\\,dt + g(t)\\,dw \\] <p>where:</p> <ul> <li>\\(f(x,t) \\in \\mathbb{R}^d\\) is the drift (deterministic force)</li> <li>\\(g(t) \\in \\mathbb{R}\\) is the diffusion coefficient (noise amplitude)</li> <li>\\(w(t) \\in \\mathbb{R}^d\\) is standard Brownian motion</li> </ul>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#probability-distribution","title":"Probability Distribution","text":"<p>At each time \\(t\\), the random variable \\(x(t)\\) has a probability distribution:</p> \\[ p_t(x) = p(x, t) \\] <p>where \\(\\int p_t(x)\\,dx = 1\\).</p>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#the-question","title":"The Question","text":"<p>How does \\(p_t(x)\\) evolve over time?</p> <p>The Fokker-Planck equation answers this:</p> \\[ \\boxed{\\frac{\\partial p_t}{\\partial t} = -\\nabla \\cdot (f p_t) + \\frac{1}{2}g(t)^2 \\nabla^2 p_t} \\]"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#intuitive-picture","title":"Intuitive Picture","text":"<p>Before diving into the math, let's build intuition.</p>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#what-changes-probability-at-a-point","title":"What Changes Probability at a Point?","text":"<p>Imagine a point \\(x\\) in space. The probability density \\(p_t(x)\\) at this point can change due to:</p> <ol> <li>Drift (advection): Particles drift from nearby points to \\(x\\), or from \\(x\\) to elsewhere</li> <li>Diffusion (spreading): Particles randomly wander in and out of \\(x\\)</li> </ol> <p>These two mechanisms give rise to the two terms in the Fokker-Planck equation.</p>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#analogy-heat-equation","title":"Analogy: Heat Equation","text":"<p>You can think of probability like heat: - Drift term: Like a wind blowing heat from one place to another - Diffusion term: Like heat spreading from hot to cold regions</p> <p>The Fokker-Planck equation is essentially a heat equation with an additional drift/advection term.</p>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#derivation-via-infinitesimal-evolution","title":"Derivation via Infinitesimal Evolution","text":"<p>We'll derive the equation by considering how probability evolves over an infinitesimal time step \\(\\Delta t\\).</p>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#step-1-chapman-kolmogorov-equation","title":"Step 1: Chapman-Kolmogorov Equation","text":"<p>The probability at time \\(t + \\Delta t\\) is related to the probability at time \\(t\\) by:</p> \\[ p_{t+\\Delta t}(x) = \\int p_t(x') \\, p(x, t+\\Delta t \\mid x', t) \\, dx' \\] <p>where \\(p(x, t+\\Delta t \\mid x', t)\\) is the transition probability from \\(x'\\) at time \\(t\\) to \\(x\\) at time \\(t + \\Delta t\\).</p> <p>Interpretation: To find the probability of being at \\(x\\) at time \\(t + \\Delta t\\), we sum over all possible starting points \\(x'\\) at time \\(t\\), weighted by their probability.</p>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#step-2-taylor-expand-the-left-side","title":"Step 2: Taylor Expand the Left Side","text":"\\[ p_{t+\\Delta t}(x) = p_t(x) + \\frac{\\partial p_t}{\\partial t} \\Delta t + O(\\Delta t^2) \\]"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#step-3-understand-the-transition-probability","title":"Step 3: Understand the Transition Probability","text":"<p>For the SDE \\(dx = f(x,t)\\,dt + g(t)\\,dw\\), the change over \\(\\Delta t\\) is:</p> \\[ \\Delta x = x(t + \\Delta t) - x(t) = f(x,t) \\Delta t + g(t) \\sqrt{\\Delta t} \\, \\xi \\] <p>where \\(\\xi \\sim \\mathcal{N}(0, I)\\) is a standard normal random variable.</p> <p>This means: - Mean displacement: \\(\\mathbb{E}[\\Delta x \\mid x] = f(x,t) \\Delta t\\) - Covariance: \\(\\text{Cov}[\\Delta x \\mid x] = g(t)^2 \\Delta t \\, I\\)</p>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#step-4-express-transition-probability","title":"Step 4: Express Transition Probability","text":"<p>For small \\(\\Delta t\\), the transition from \\(x'\\) to \\(x\\) is approximately:</p> \\[ p(x \\mid x', \\Delta t) \\approx \\delta(x - x' - f(x',t)\\Delta t) * \\mathcal{N}(0, g(t)^2 \\Delta t \\, I) \\] <p>where \\(*\\) denotes convolution.</p> <p>More precisely, using Gaussian approximation:</p> \\[ p(x \\mid x', \\Delta t) \\approx \\frac{1}{(2\\pi g^2 \\Delta t)^{d/2}} \\exp\\left(-\\frac{|x - x' - f(x',t)\\Delta t|^2}{2g^2 \\Delta t}\\right) \\]"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#step-5-substitute-into-chapman-kolmogorov","title":"Step 5: Substitute into Chapman-Kolmogorov","text":"\\[ p_{t+\\Delta t}(x) = \\int p_t(x') \\, p(x \\mid x', \\Delta t) \\, dx' \\] <p>Now we use a clever trick: expand around \\(x' = x\\) (nearby points contribute most).</p> <p>Let \\(\\delta x = x - x'\\), so \\(x' = x - \\delta x\\):</p> \\[ p_{t+\\Delta t}(x) = \\int p_t(x - \\delta x) \\, p(x \\mid x - \\delta x, \\Delta t) \\, d(\\delta x) \\]"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#step-6-taylor-expand-p_tx-delta-x","title":"Step 6: Taylor Expand \\(p_t(x - \\delta x)\\)","text":"\\[ p_t(x - \\delta x) = p_t(x) - \\delta x \\cdot \\nabla p_t(x) + \\frac{1}{2} \\sum_{i,j} \\delta x_i \\delta x_j \\frac{\\partial^2 p_t}{\\partial x_i \\partial x_j} + \\ldots \\] <p>Using index notation:</p> \\[ p_t(x - \\delta x) \\approx p_t(x) - \\sum_i \\delta x_i \\frac{\\partial p_t}{\\partial x_i} + \\frac{1}{2} \\sum_{i,j} \\delta x_i \\delta x_j \\frac{\\partial^2 p_t}{\\partial x_i \\partial x_j} \\]"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#step-7-compute-moments-of-delta-x","title":"Step 7: Compute Moments of \\(\\delta x\\)","text":"<p>From the transition probability, conditioned on starting at \\(x' = x - \\delta x \\approx x\\):</p> <p>First moment:</p> <p>$$</p> <p>\\mathbb{E}[\\delta x] = f(x,t) \\Delta t + O(\\Delta t^2) $$</p> <p>Second moment (each component):</p> <p>$$</p> <p>\\mathbb{E}[\\delta x_i \\delta x_j] = \\begin{cases} g(t)^2 \\Delta t + f_i f_j \\Delta t^2 &amp; \\text{if } i = j \\ f_i f_j \\Delta t^2 &amp; \\text{if } i \\neq j \\end{cases} $$</p> <p>For small \\(\\Delta t\\), the \\(\\Delta t^2\\) terms are negligible, so:</p> \\[ \\mathbb{E}[\\delta x_i \\delta x_j] \\approx g(t)^2 \\Delta t \\, \\delta_{ij} \\] <p>where \\(\\delta_{ij}\\) is the Kronecker delta.</p>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#step-8-substitute-moments","title":"Step 8: Substitute Moments","text":"\\[ p_{t+\\Delta t}(x) = \\int \\left[p_t(x) - \\sum_i \\delta x_i \\frac{\\partial p_t}{\\partial x_i} + \\frac{1}{2} \\sum_{i,j} \\delta x_i \\delta x_j \\frac{\\partial^2 p_t}{\\partial x_i \\partial x_j}\\right] p(\\delta x \\mid x, \\Delta t) \\, d(\\delta x) \\] <p>Taking expectations over \\(\\delta x\\):</p> \\[ p_{t+\\Delta t}(x) = p_t(x) - \\sum_i \\mathbb{E}[\\delta x_i] \\frac{\\partial p_t}{\\partial x_i} + \\frac{1}{2} \\sum_{i,j} \\mathbb{E}[\\delta x_i \\delta x_j] \\frac{\\partial^2 p_t}{\\partial x_i \\partial x_j} \\] <p>Substitute the moments:</p> \\[ p_{t+\\Delta t}(x) = p_t(x) - \\sum_i f_i(x,t) \\Delta t \\frac{\\partial p_t}{\\partial x_i} + \\frac{1}{2} \\sum_i g(t)^2 \\Delta t \\frac{\\partial^2 p_t}{\\partial x_i^2} \\]"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#step-9-rearrange-and-take-limit","title":"Step 9: Rearrange and Take Limit","text":"\\[ p_{t+\\Delta t}(x) - p_t(x) = -\\sum_i f_i(x,t) \\frac{\\partial p_t}{\\partial x_i} \\Delta t + \\frac{1}{2} g(t)^2 \\sum_i \\frac{\\partial^2 p_t}{\\partial x_i^2} \\Delta t \\] <p>Divide by \\(\\Delta t\\) and take \\(\\Delta t \\to 0\\):</p> \\[ \\frac{\\partial p_t}{\\partial t} = -\\sum_i \\frac{\\partial}{\\partial x_i} \\left[f_i(x,t) p_t(x)\\right] + \\frac{1}{2} g(t)^2 \\sum_i \\frac{\\partial^2 p_t}{\\partial x_i^2} \\]"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#step-10-vector-notation","title":"Step 10: Vector Notation","text":"<p>Using \\(\\nabla \\cdot\\) for divergence and \\(\\nabla^2\\) for Laplacian:</p> \\[ \\boxed{\\frac{\\partial p_t}{\\partial t} = -\\nabla \\cdot (f p_t) + \\frac{1}{2}g(t)^2 \\nabla^2 p_t} \\] <p>This is the Fokker-Planck equation!</p>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#kramers-moyal-expansion-rigorous","title":"Kramers-Moyal Expansion (Rigorous)","text":"<p>The derivation above can be made rigorous using the Kramers-Moyal expansion.</p>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#jump-moments","title":"Jump Moments","text":"<p>Define the jump moments:</p> \\[ M^{(n)}_i(x, t) = \\lim_{\\Delta t \\to 0} \\frac{1}{\\Delta t} \\mathbb{E}\\left[(\\Delta x_i)^n \\mid x(t) = x\\right] \\]"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#kramers-moyal-theorem","title":"Kramers-Moyal Theorem","text":"<p>The evolution of the probability distribution is:</p> \\[ \\frac{\\partial p_t}{\\partial t} = \\sum_{n=1}^\\infty \\frac{(-1)^n}{n!} \\sum_{i_1, \\ldots, i_n} \\frac{\\partial^n}{\\partial x_{i_1} \\cdots \\partial x_{i_n}} \\left[M^{(n)}_{i_1 \\cdots i_n} p_t\\right] \\]"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#fokker-planck-approximation","title":"Fokker-Planck Approximation","text":"<p>For continuous diffusion processes (like those generated by SDEs with smooth coefficients), the Kramers-Moyal expansion terminates at \\(n=2\\):</p> \\[ M^{(n)} = 0 \\quad \\text{for } n \\geq 3 \\] <p>This gives:</p> \\[ \\frac{\\partial p_t}{\\partial t} = -\\sum_i \\frac{\\partial}{\\partial x_i} [M^{(1)}_i p_t] + \\frac{1}{2} \\sum_{i,j} \\frac{\\partial^2}{\\partial x_i \\partial x_j} [M^{(2)}_{ij} p_t] \\]"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#for-our-sde","title":"For Our SDE","text":"<p>From \\(dx = f(x,t)\\,dt + g(t)\\,dw\\):</p> \\[ M^{(1)}_i = f_i(x,t) \\] \\[ M^{(2)}_{ij} = g(t)^2 \\delta_{ij} \\] <p>Substituting:</p> \\[ \\frac{\\partial p_t}{\\partial t} = -\\sum_i \\frac{\\partial}{\\partial x_i} [f_i p_t] + \\frac{1}{2} g(t)^2 \\sum_i \\frac{\\partial^2 p_t}{\\partial x_i^2} \\] <p>This is the Fokker-Planck equation.</p>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#physical-interpretation","title":"Physical Interpretation","text":"<p>Let's break down each term:</p>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#the-full-equation","title":"The Full Equation","text":"\\[ \\frac{\\partial p_t}{\\partial t} = -\\nabla \\cdot (f p_t) + \\frac{1}{2}g(t)^2 \\nabla^2 p_t \\]"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#term-1-rate-of-change","title":"Term 1: Rate of Change","text":"\\[ \\frac{\\partial p_t}{\\partial t} \\] <p>Meaning: How fast probability density is changing at point \\(x\\) at time \\(t\\).</p>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#term-2-drift-advection","title":"Term 2: Drift (Advection)","text":"\\[ -\\nabla \\cdot (f p_t) = -\\sum_i \\frac{\\partial}{\\partial x_i} [f_i(x,t) p_t(x)] \\] <p>Meaning: Probability flux due to deterministic drift.</p> <p>Expanded form:</p> \\[ -\\nabla \\cdot (f p_t) = -f \\cdot \\nabla p_t - p_t \\nabla \\cdot f \\] <ul> <li>\\(-f \\cdot \\nabla p_t\\): Advection of probability along the drift field</li> <li>\\(-p_t \\nabla \\cdot f\\): Change in probability due to compression/expansion of the drift field</li> </ul> <p>Physical picture: Probability \"flows\" along the drift field \\(f(x,t)\\), like wind blowing particles.</p>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#term-3-diffusion-spreading","title":"Term 3: Diffusion (Spreading)","text":"\\[ \\frac{1}{2}g(t)^2 \\nabla^2 p_t = \\frac{1}{2}g(t)^2 \\sum_i \\frac{\\partial^2 p_t}{\\partial x_i^2} \\] <p>Meaning: Probability spreads from high-density to low-density regions.</p> <p>Sign convention:</p> <ul> <li>\\(\\nabla^2 p_t &gt; 0\\): Concave up \u2192 probability flows in (increases)</li> <li>\\(\\nabla^2 p_t &lt; 0\\): Concave down \u2192 probability flows out (decreases)</li> </ul> <p>Physical picture: Random motion causes probability to diffuse, like heat spreading in a metal rod.</p>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#connection-to-conservation-laws","title":"Connection to Conservation Laws","text":""},{"location":"diffusion/reverse_process/fokker_planck_derivation/#continuity-equation","title":"Continuity Equation","text":"<p>The Fokker-Planck equation can be written as a continuity equation:</p> \\[ \\frac{\\partial p_t}{\\partial t} + \\nabla \\cdot J = 0 \\] <p>where \\(J\\) is the probability current (flux):</p> \\[ J = f p_t - \\frac{1}{2}g(t)^2 \\nabla p_t \\] <p>Components:</p> <ul> <li>\\(f p_t\\): Drift current (probability flowing along drift)</li> <li>\\(-\\frac{1}{2}g(t)^2 \\nabla p_t\\): Diffusion current (Fick's law, probability flowing down gradients)</li> </ul>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#conservation-of-probability","title":"Conservation of Probability","text":"<p>Integrating over all space:</p> \\[ \\frac{d}{dt} \\int p_t(x)\\,dx = \\int \\frac{\\partial p_t}{\\partial t}\\,dx = -\\int \\nabla \\cdot J\\,dx = 0 \\] <p>(using divergence theorem, assuming \\(J \\to 0\\) at infinity).</p> <p>Result: Total probability is conserved, as it must be!</p>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#example-simple-diffusion","title":"Example: Simple Diffusion","text":""},{"location":"diffusion/reverse_process/fokker_planck_derivation/#setup","title":"Setup","text":"<p>Consider pure diffusion with no drift:</p> \\[ dx = \\sigma \\, dw \\] <p>where \\(\\sigma\\) is constant.</p>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#fokker-planck-equation","title":"Fokker-Planck Equation","text":"\\[ \\frac{\\partial p_t}{\\partial t} = \\frac{\\sigma^2}{2} \\nabla^2 p_t \\] <p>This is the heat equation!</p>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#solution","title":"Solution","text":"<p>Starting from a point mass \\(p_0(x) = \\delta(x - x_0)\\), the solution is:</p> \\[ p_t(x) = \\frac{1}{(2\\pi \\sigma^2 t)^{d/2}} \\exp\\left(-\\frac{|x - x_0|^2}{2\\sigma^2 t}\\right) \\] <p>This is a Gaussian with: - Mean: \\(x_0\\) (stays at starting point) - Variance: \\(\\sigma^2 t\\) (spreads linearly with time)</p> <p>Verification: Substitute this solution into the heat equation \u2014 it works!</p>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#example-ornstein-uhlenbeck-process","title":"Example: Ornstein-Uhlenbeck Process","text":""},{"location":"diffusion/reverse_process/fokker_planck_derivation/#setup_1","title":"Setup","text":"<p>Consider the SDE:</p> \\[ dx = -\\theta x \\, dt + \\sigma \\, dw \\] <p>where:</p> <ul> <li>\\(-\\theta x\\): Drift toward origin (like a spring)</li> <li>\\(\\sigma\\): Constant diffusion</li> </ul>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#fokker-planck-equation_1","title":"Fokker-Planck Equation","text":"\\[ \\frac{\\partial p_t}{\\partial t} = \\nabla \\cdot (\\theta x p_t) + \\frac{\\sigma^2}{2} \\nabla^2 p_t \\]"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#stationary-distribution","title":"Stationary Distribution","text":"<p>At equilibrium (\\(\\partial p / \\partial t = 0\\)):</p> \\[ 0 = \\nabla \\cdot (\\theta x p) + \\frac{\\sigma^2}{2} \\nabla^2 p \\] <p>Solving this ODE:</p> \\[ p_\\infty(x) = \\mathcal{N}\\left(0, \\frac{\\sigma^2}{2\\theta}\\right) \\] <p>Interpretation: The drift pulls particles toward the origin, while diffusion spreads them out. The equilibrium is a balance between these forces.</p>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#why-this-matters-for-reverse-sdes","title":"Why This Matters for Reverse SDEs","text":""},{"location":"diffusion/reverse_process/fokker_planck_derivation/#forward-process","title":"Forward Process","text":"<p>The forward SDE:</p> \\[ dx = f(x,t)\\,dt + g(t)\\,dw \\] <p>generates a probability evolution governed by:</p> \\[ \\frac{\\partial p_t}{\\partial t} = -\\nabla \\cdot (f p_t) + \\frac{1}{2}g(t)^2 \\nabla^2 p_t \\]"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#reverse-process","title":"Reverse Process","text":"<p>To reverse the process, we need to find an SDE whose Fokker-Planck equation gives backward evolution:</p> \\[ \\frac{\\partial p_t}{\\partial t} = +\\nabla \\cdot (f p_t) - \\frac{1}{2}g(t)^2 \\nabla^2 p_t \\] <p>(note the sign flips).</p>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#key-insight","title":"Key Insight","text":"<p>The Fokker-Planck equation can be rewritten using the score function \\(\\nabla \\log p_t\\):</p> \\[ \\nabla^2 p_t = \\nabla \\cdot (\\nabla p_t) = \\nabla \\cdot (p_t \\nabla \\log p_t) \\] <p>This allows us to express the diffusion term in terms of the score, leading to the effective drift:</p> \\[ \\tilde{f} = f - \\frac{1}{2}g^2 \\nabla \\log p_t \\] <p>The reverse SDE uses this effective drift to reverse the probability evolution.</p> <p>See: <code>reverse_process_derivation.md</code> for the full derivation.</p>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#summary","title":"Summary","text":""},{"location":"diffusion/reverse_process/fokker_planck_derivation/#what-we-learned","title":"What We Learned","text":"<ol> <li>Fokker-Planck Equation: Describes how probability distributions evolve for stochastic processes</li> <li>Two Mechanisms: Drift (advection) and diffusion (spreading)</li> <li>Derivation: From infinitesimal evolution using Chapman-Kolmogorov and Taylor expansion</li> <li>Physical Meaning: Continuity equation for probability flux</li> <li>Connection to Reverse SDEs: The score function appears when rewriting the diffusion term</li> </ol>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#the-equation","title":"The Equation","text":"\\[ \\boxed{\\frac{\\partial p_t}{\\partial t} = -\\nabla \\cdot (f p_t) + \\frac{1}{2}g(t)^2 \\nabla^2 p_t} \\] <p>Drift term: \\(-\\nabla \\cdot (f p_t)\\) \u2014 Probability flows along \\(f\\)</p> <p>Diffusion term: \\(\\frac{1}{2}g^2 \\nabla^2 p_t\\) \u2014 Probability spreads</p>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#references","title":"References","text":""},{"location":"diffusion/reverse_process/fokker_planck_derivation/#classic-texts","title":"Classic Texts","text":"<ul> <li>Risken (1989): \"The Fokker-Planck Equation: Methods of Solution and Applications\"</li> <li>Gardiner (2009): \"Stochastic Methods: A Handbook for the Natural and Social Sciences\"</li> <li>\u00d8ksendal (2003): \"Stochastic Differential Equations: An Introduction with Applications\"</li> </ul>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#papers","title":"Papers","text":"<ul> <li>Fokker (1914): \"Die mittlere Energie rotierender elektrischer Dipole im Strahlungsfeld\" \u2014 Original work</li> <li>Planck (1917): \"\u00dcber einen Satz der statistischen Dynamik und seine Erweiterung in der Quantentheorie\"</li> <li>Kolmogorov (1931): \"\u00dcber die analytischen Methoden in der Wahrscheinlichkeitsrechnung\" \u2014 Forward equation</li> </ul>"},{"location":"diffusion/reverse_process/fokker_planck_derivation/#related-documents","title":"Related Documents","text":"<ul> <li>Reverse Process: <code>reverse_process_derivation.md</code></li> <li>Forward Process: <code>forward_process_derivation.md</code></li> <li>Supplement in Notebook: <code>notebooks/diffusion/02_sde_formulation/supplements/07_fokker_planck_equation.md</code></li> </ul>"},{"location":"diffusion/reverse_process/reverse_process_derivation/","title":"Deriving the Reverse-Time SDE: From Noise Back to Data","text":"<p>How do we reverse a diffusion process? The mathematical foundation of generative diffusion models</p> <p>This document derives the reverse-time SDE, which is the mathematical key to generating samples from noise in diffusion models.</p>"},{"location":"diffusion/reverse_process/reverse_process_derivation/#table-of-contents","title":"Table of Contents","text":"<ol> <li>The Problem: Reversing Diffusion</li> <li>Anderson's Theorem (1982)</li> <li>Intuitive Understanding</li> <li>Derivation via Fokker-Planck Equation</li> <li>Why the Score Function Appears</li> <li>Connection to Diffusion Models</li> <li>Summary</li> </ol>"},{"location":"diffusion/reverse_process/reverse_process_derivation/#referenced-from","title":"Referenced From","text":"<ul> <li>Notebook: <code>notebooks/diffusion/02_sde_formulation/sde_formulation.md</code> \u2014 Section on Reverse SDE</li> </ul>"},{"location":"diffusion/reverse_process/reverse_process_derivation/#1-the-problem-reversing-diffusion","title":"1. The Problem: Reversing Diffusion","text":""},{"location":"diffusion/reverse_process/reverse_process_derivation/#the-forward-process-easy","title":"The Forward Process (Easy)","text":"<p>We know how to add noise to data. For the VP-SDE:</p> \\[ dx = -\\frac{1}{2}\\beta(t)x\\,dt + \\sqrt{\\beta(t)}\\,dw \\] <p>Starting from clean data \\(x_0\\), we can simulate this forward in time to get noisy \\(x_T \\approx \\mathcal{N}(0, I)\\).</p>"},{"location":"diffusion/reverse_process/reverse_process_derivation/#the-reverse-process-hard","title":"The Reverse Process (Hard)","text":"<p>Question: Can we run this process backwards to go from noise \\(x_T\\) back to data \\(x_0\\)?</p> <p>Naive attempt: Just negate time?</p> \\[ dx = +\\frac{1}{2}\\beta(t)x\\,dt - \\sqrt{\\beta(t)}\\,dw \\quad \\text{\u274c WRONG} \\] <p>Problem: This doesn't work! Simply negating the drift and noise doesn't give you the correct reverse process.</p> <p>Why not? Because the forward process generates a distribution \\(p_t(x)\\) that evolves over time. To reverse it, we need to account for the shape of this distribution at each time.</p>"},{"location":"diffusion/reverse_process/reverse_process_derivation/#2-andersons-theorem-1982","title":"2. Anderson's Theorem (1982)","text":"<p>The fundamental result (Anderson, 1982):</p> <p>For any forward SDE:</p> \\[ dx = f(x,t)\\,dt + g(t)\\,dw \\] <p>the reverse-time SDE (running from \\(t=T\\) back to \\(t=0\\)) is:</p> \\[ \\boxed{dx = \\left[f(x,t) - g(t)^2 \\nabla_x \\log p_t(x)\\right] dt + g(t)\\,d\\bar{w}(t)} \\] <p>where:</p> <ul> <li>\\(\\bar{w}(t)\\) is a reverse-time Brownian motion</li> <li>\\(\\nabla_x \\log p_t(x)\\) is the score function (gradient of log probability density)</li> </ul> <p>Key observation: The reverse process has an extra term \\(-g(t)^2 \\nabla_x \\log p_t(x)\\) that depends on the probability distribution.</p>"},{"location":"diffusion/reverse_process/reverse_process_derivation/#3-intuitive-understanding","title":"3. Intuitive Understanding","text":""},{"location":"diffusion/reverse_process/reverse_process_derivation/#why-we-need-a-correction-term","title":"Why We Need a Correction Term","text":"<p>Imagine particles diffusing outward from a point source:</p> <p>Forward process:</p> <ul> <li>Particles spread out randomly</li> <li>No \"memory\" of where they came from</li> <li>Pure diffusion: symmetric spreading</li> </ul> <p>Reverse process:</p> <ul> <li>Particles need to know where to go back to</li> <li>Not just random motion\u2014need to be \"pulled\" toward high-density regions</li> <li>The score \\(\\nabla_x \\log p_t(x)\\) provides this \"pull\"</li> </ul>"},{"location":"diffusion/reverse_process/reverse_process_derivation/#the-score-as-a-guide","title":"The Score as a Guide","text":"<p>The score function \\(\\nabla_x \\log p_t(x)\\) points in the direction of increasing probability.</p> <ul> <li>In forward diffusion: Ignore probability, just add noise</li> <li>In reverse diffusion: Follow the probability gradient to find likely paths back</li> </ul> <p>Analogy: </p> <ul> <li>Forward: Drop ink in water, watch it spread (no guidance)</li> <li>Reverse: Collect ink back together (need to know where the ink is concentrated)</li> </ul>"},{"location":"diffusion/reverse_process/reverse_process_derivation/#4-derivation-via-fokker-planck-equation","title":"4. Derivation via Fokker-Planck Equation","text":""},{"location":"diffusion/reverse_process/reverse_process_derivation/#step-1-forward-sde-and-its-fokker-planck-equation","title":"Step 1: Forward SDE and Its Fokker-Planck Equation","text":"<p>The forward SDE:</p> \\[ dx = f(x,t)\\,dt + g(t)\\,dw \\] <p>generates a probability distribution \\(p_t(x)\\) that evolves according to the Fokker-Planck equation:</p> \\[ \\frac{\\partial p_t}{\\partial t} = -\\nabla \\cdot (f p_t) + \\frac{1}{2}g(t)^2 \\nabla^2 p_t \\] <p>\ud83d\udcd8 Detailed Derivation: For a complete derivation of the Fokker-Planck equation from first principles, including physical intuition and examples, see <code>fokker_planck_derivation.md</code>.</p> <p>Interpretation: This PDE describes how probability density flows forward in time.</p>"},{"location":"diffusion/reverse_process/reverse_process_derivation/#step-2-reverse-time","title":"Step 2: Reverse Time","text":"<p>To reverse the process, substitute \\(\\tau = T - t\\) (reverse time variable).</p> <p>Let \\(\\tilde{p}_\\tau(x) = p_{T-\\tau}(x)\\) be the distribution in reverse time.</p> <p>Goal: Find an SDE whose solution has marginals \\(\\tilde{p}_\\tau(x)\\).</p>"},{"location":"diffusion/reverse_process/reverse_process_derivation/#step-3-transform-the-fokker-planck-equation","title":"Step 3: Transform the Fokker-Planck Equation","text":"<p>When we reverse time (\\(\\tau = T - t\\)), we have:</p> \\[ \\frac{\\partial \\tilde{p}_\\tau}{\\partial \\tau} = -\\frac{\\partial p_t}{\\partial t} \\] <p>Substitute the Fokker-Planck equation:</p> \\[ \\frac{\\partial \\tilde{p}_\\tau}{\\partial \\tau} = \\nabla \\cdot (f p_t) - \\frac{1}{2}g(t)^2 \\nabla^2 p_t \\]"},{"location":"diffusion/reverse_process/reverse_process_derivation/#step-4-rewrite-the-diffusion-term","title":"Step 4: Rewrite the Diffusion Term","text":"<p>The key trick is to express \\(\\nabla^2 p\\) using the score.</p> <p>Identity:</p> \\[ \\nabla^2 p = \\nabla \\cdot (\\nabla p) = \\nabla \\cdot (p \\nabla \\log p) \\] <p>Derivation: Since \\(\\nabla \\log p = \\frac{\\nabla p}{p}\\), we have \\(\\nabla p = p \\nabla \\log p\\), so:</p> \\[ \\nabla^2 p = \\nabla \\cdot (p \\nabla \\log p) \\]"},{"location":"diffusion/reverse_process/reverse_process_derivation/#step-5-substitute-into-reverse-fokker-planck","title":"Step 5: Substitute into Reverse Fokker-Planck","text":"\\[ \\frac{\\partial \\tilde{p}_\\tau}{\\partial \\tau} = \\nabla \\cdot (f p) - \\frac{1}{2}g^2 \\nabla \\cdot (p \\nabla \\log p) \\] \\[ = \\nabla \\cdot \\left(f p - \\frac{1}{2}g^2 p \\nabla \\log p\\right) \\] \\[ = \\nabla \\cdot \\left(\\left[f - \\frac{1}{2}g^2 \\nabla \\log p\\right] p\\right) \\] <p>Pattern recognition: Let me explain how we identify the drift from this form.</p>"},{"location":"diffusion/reverse_process/reverse_process_derivation/#recognizing-the-fokker-planck-structure","title":"Recognizing the Fokker-Planck Structure","text":"<p>Recall the general Fokker-Planck equation for an SDE \\(dx = \\tilde{f}(x,t)\\,dt + \\tilde{g}(t)\\,dw\\):</p> \\[ \\frac{\\partial p}{\\partial t} = -\\nabla \\cdot (\\tilde{f} p) + \\frac{1}{2}\\tilde{g}^2 \\nabla^2 p \\] <p>The equation has two parts: 1. Advection term: \\(-\\nabla \\cdot (\\tilde{f} p)\\) \u2014 probability flow due to drift 2. Diffusion term: \\(+\\frac{1}{2}\\tilde{g}^2 \\nabla^2 p\\) \u2014 probability spreading due to noise</p>"},{"location":"diffusion/reverse_process/reverse_process_derivation/#matching-our-equation","title":"Matching Our Equation","text":"<p>We derived (ignoring the diffusion term for now):</p> \\[ \\frac{\\partial \\tilde{p}_\\tau}{\\partial \\tau} = \\nabla \\cdot \\left(\\left[f - \\frac{1}{2}g^2 \\nabla \\log p\\right] p\\right) \\] <p>Comparison with standard form:</p> \\[ \\frac{\\partial p}{\\partial t} = -\\nabla \\cdot (\\tilde{f} p) + \\ldots \\] <p>Key observation: Our equation has \\(+\\nabla \\cdot (\\ldots)\\) while the standard form has \\(-\\nabla \\cdot (\\tilde{f} p)\\).</p> <p>To match the standard form, we need:</p> \\[ \\nabla \\cdot \\left(\\left[f - \\frac{1}{2}g^2 \\nabla \\log p\\right] p\\right) = -\\nabla \\cdot (\\tilde{f} p) \\] <p>This means:</p> \\[ \\left[f - \\frac{1}{2}g^2 \\nabla \\log p\\right] p = -\\tilde{f} p \\] <p>Dividing by \\(p\\) (assuming \\(p &gt; 0\\)):</p> \\[ f - \\frac{1}{2}g^2 \\nabla \\log p = -\\tilde{f} \\] <p>Therefore:</p> \\[ \\boxed{\\tilde{f}(x) = -f(x,t) + \\frac{1}{2}g(t)^2 \\nabla_x \\log p_t(x)} \\] <p>Wait, this has a negative sign on \\(f\\)! This would be the reverse drift if we're going backward in time. But we want the form for the SDE...</p> <p>Actually, let me reconsider. The issue is subtle and relates to the sign conventions in reverse time. Let me continue to Step 6 where we'll sort this out properly.</p>"},{"location":"diffusion/reverse_process/reverse_process_derivation/#step-6-the-diffusion-term-in-reverse-time","title":"Step 6: The Diffusion Term in Reverse Time","text":"<p>Actually, we need to be more careful. The correct reverse-time Fokker-Planck equation is:</p> \\[ \\frac{\\partial \\tilde{p}_\\tau}{\\partial \\tau} = -\\nabla \\cdot (\\tilde{f} \\tilde{p}) + \\frac{1}{2}g^2 \\nabla^2 \\tilde{p} \\] <p>where \\(\\tilde{f}\\) is the reverse drift.</p> <p>Matching coefficients with our transformed equation:</p> \\[ -\\nabla \\cdot (\\tilde{f} \\tilde{p}) + \\frac{1}{2}g^2 \\nabla^2 \\tilde{p} = \\nabla \\cdot (f p) - \\frac{1}{2}g^2 \\nabla^2 p \\] <p>The diffusion terms have opposite signs! To fix this, we need to include both the drift correction AND account for the sign change.</p> <p>Result: The reverse SDE is:</p> \\[ dx = \\left[f(x,t) - g(t)^2 \\nabla_x \\log p_t(x)\\right] dt + g(t)\\,d\\bar{w}(t) \\] <p>where the \\(+g(t)\\,d\\bar{w}(t)\\) term provides the diffusion in reverse time (note the positive sign, same as forward).</p>"},{"location":"diffusion/reverse_process/reverse_process_derivation/#summary-of-the-derivation-logic","title":"Summary of the Derivation Logic","text":"<p>Let me clarify the full picture:</p> <ol> <li> <p>Forward Fokker-Planck:     $\\(\\frac{\\partial p_t}{\\partial t} = -\\nabla \\cdot (f p_t) + \\frac{1}{2}g^2 \\nabla^2 p_t\\)$</p> </li> <li> <p>Reverse time (\\(\\tau = T - t\\)):    $\\(\\frac{\\partial p_\\tau}{\\partial \\tau} = +\\nabla \\cdot (f p) - \\frac{1}{2}g^2 \\nabla^2 p\\)$    (Sign flip on time derivative flips both terms)</p> </li> <li> <p>Rewrite diffusion using score: \\(\\nabla^2 p = \\nabla \\cdot (p \\nabla \\log p)\\):    $\\(\\frac{\\partial p_\\tau}{\\partial \\tau} = \\nabla \\cdot \\left(\\left[f - \\frac{1}{2}g^2 \\nabla \\log p\\right] p\\right)\\)$</p> </li> <li> <p>This is almost a Fokker-Planck equation, but with a sign issue. The resolution is that when we write the SDE that generates this, we need to account for:</p> </li> <li>The advection term gives us the effective drift</li> <li>The diffusion term (which we somewhat glossed over) contributes the \\(g(t)\\,d\\bar{w}\\) term</li> <li>The full SDE that produces the correct marginals in reverse time is:    $\\(dx = [f - g^2 \\nabla \\log p]\\,dt + g\\,d\\bar{w}\\)$</li> </ol> <p>The key insight: The score term \\(-g^2 \\nabla \\log p\\) corrects for the fact that probability is concentrated in certain regions, and we need to guide the reverse process toward those regions.</p>"},{"location":"diffusion/reverse_process/reverse_process_derivation/#a-concrete-example-to-build-intuition","title":"A Concrete Example to Build Intuition","text":"<p>Consider a simple 1D case where probability has flowed outward:</p> <p>Forward process: Starting from \\(x_0 = 0\\), particles diffuse outward. At time \\(t\\), we have \\(p_t(x) \\approx \\mathcal{N}(0, t)\\).</p> <p>Score at time \\(t\\):  $\\(\\nabla_x \\log p_t(x) = -\\frac{x}{t}\\)$</p> <p>Interpretation: The score points toward \\(x=0\\) (the center of the distribution), with magnitude proportional to distance from center.</p> <p>Reverse process: To bring particles back, we need drift: $\\(\\text{drift} = f(x,t) - g^2 \\nabla \\log p = f(x,t) + g^2 \\frac{x}{t}\\)$</p> <p>The term \\(+g^2 \\frac{x}{t}\\) pulls particles toward the origin, counteracting the outward diffusion. Without this term, simply reversing would not account for where probability is concentrated.</p> <p>\ud83d\udcd8 Detailed Example: For a complete worked example with step-by-step calculations, numerical verification, and intuitive explanations, see <code>reverse_process_example.md</code>.</p>"},{"location":"diffusion/reverse_process/reverse_process_derivation/#5-why-the-score-function-appears","title":"5. Why the Score Function Appears","text":""},{"location":"diffusion/reverse_process/reverse_process_derivation/#the-score-as-probability-flow","title":"The Score as Probability Flow","text":"<p>The score function:</p> \\[ \\nabla_x \\log p_t(x) = \\frac{\\nabla_x p_t(x)}{p_t(x)} \\] <p>is the normalized gradient of the probability density.</p> <p>Physical interpretation:</p> <ul> <li>Points from low probability to high probability</li> <li>Magnitude is stronger in regions with steeper probability gradients</li> <li>Tells particles \"which way to go\" to increase likelihood</li> </ul>"},{"location":"diffusion/reverse_process/reverse_process_derivation/#the-gt2-weighting","title":"The \\(g(t)^2\\) Weighting","text":"<p>Why is the score multiplied by \\(g(t)^2\\)?</p> <p>Answer: It's the diffusion coefficient squared. </p> <p>Intuition:</p> <ul> <li>Stronger diffusion (\\(g\\) large) \u2192 more noise added \u2192 need stronger correction to reverse</li> <li>Weaker diffusion (\\(g\\) small) \u2192 less noise added \u2192 need smaller correction</li> </ul> <p>Mathematical origin: From the Fokker-Planck equation, the diffusion term has coefficient \\(\\frac{1}{2}g^2\\), which when transformed gives \\(g^2\\) in the drift correction.</p>"},{"location":"diffusion/reverse_process/reverse_process_derivation/#why-not-just-fxt-in-reverse","title":"Why Not Just \\(f(x,t)\\) in Reverse?","text":"<p>If we only used \\(-f(x,t)\\) (negating the forward drift), we'd be ignoring the shape of the distribution.</p> <p>Example: Consider particles that have diffused to form a Gaussian blob. - Simply reversing \\(f\\) would make them all move backward the same way - But they need to be \"pulled\" toward the center of the blob (high density) - The score term provides this pull</p>"},{"location":"diffusion/reverse_process/reverse_process_derivation/#6-connection-to-diffusion-models","title":"6. Connection to Diffusion Models","text":""},{"location":"diffusion/reverse_process/reverse_process_derivation/#what-we-know-and-dont-know","title":"What We Know and Don't Know","text":"<p>In diffusion models:</p> <p>Known (designed): - Forward SDE: \\(dx = f(x,t)\\,dt + g(t)\\,dw\\) - Drift \\(f(x,t)\\) and diffusion \\(g(t)\\) are chosen</p> <p>Unknown (needs learning): - Score function: \\(\\nabla_x \\log p_t(x)\\)</p>"},{"location":"diffusion/reverse_process/reverse_process_derivation/#the-learning-problem","title":"The Learning Problem","text":"<p>Since we don't know \\(p_t(x)\\), we don't know its score \\(\\nabla_x \\log p_t(x)\\).</p> <p>Solution: Train a neural network \\(s_\\theta(x,t)\\) to approximate the score:</p> \\[ s_\\theta(x,t) \\approx \\nabla_x \\log p_t(x) \\]"},{"location":"diffusion/reverse_process/reverse_process_derivation/#the-reverse-sde-for-generation","title":"The Reverse SDE for Generation","text":"<p>Once we have the learned score, we can sample by solving:</p> \\[ dx = \\left[f(x,t) - g(t)^2 s_\\theta(x,t)\\right] dt + g(t)\\,d\\bar{w}(t) \\] <p>Starting from \\(x_T \\sim \\mathcal{N}(0, I)\\), integrate backwards from \\(t=T\\) to \\(t=0\\) to generate \\(x_0\\).</p>"},{"location":"diffusion/reverse_process/reverse_process_derivation/#example-vp-sde","title":"Example: VP-SDE","text":"<p>For the VP-SDE with \\(f(x,t) = -\\frac{1}{2}\\beta(t)x\\) and \\(g(t) = \\sqrt{\\beta(t)}\\):</p> <p>Reverse SDE:</p> \\[ dx = \\left[-\\frac{1}{2}\\beta(t)x - \\beta(t) s_\\theta(x,t)\\right] dt + \\sqrt{\\beta(t)}\\,d\\bar{w}(t) \\] <p>Discretized (Euler-Maruyama):</p> <pre><code>x = torch.randn(batch_size, dim)  # Start from noise\ndt = -T / num_steps\n\nfor i in range(num_steps):\n    t = T - i * dt\n    beta_t = beta(t)\n\n    # Predict score\n    score = score_network(x, t)\n\n    # Drift term\n    drift = -0.5 * beta_t * x - beta_t * score\n\n    # Diffusion term\n    noise = torch.randn_like(x)\n    diffusion = np.sqrt(beta_t * abs(dt)) * noise\n\n    # Update\n    x = x + drift * dt + diffusion\n\nreturn x  # This is x_0 (generated sample)\n</code></pre>"},{"location":"diffusion/reverse_process/reverse_process_derivation/#7-summary","title":"7. Summary","text":""},{"location":"diffusion/reverse_process/reverse_process_derivation/#the-key-result","title":"The Key Result","text":"Process SDE Forward \\(dx = f(x,t)\\,dt + g(t)\\,dw\\) Reverse \\(dx = [f(x,t) - g(t)^2 \\nabla_x \\log p_t(x)]\\,dt + g(t)\\,d\\bar{w}(t)\\)"},{"location":"diffusion/reverse_process/reverse_process_derivation/#why-this-works","title":"Why This Works","text":"<ol> <li>Forward SDE defines how probability evolves: \\(p_0 \\to p_T\\)</li> <li>Fokker-Planck equation describes this evolution as a PDE</li> <li>Reverse time requires transforming this PDE</li> <li>Score appears naturally from rewriting the Laplacian: \\(\\nabla^2 p = \\nabla \\cdot (p \\nabla \\log p)\\)</li> <li>Result: Reverse SDE with score correction</li> </ol>"},{"location":"diffusion/reverse_process/reverse_process_derivation/#three-levels-of-understanding","title":"Three Levels of Understanding","text":"<p>Level 1 (Practical): To reverse diffusion, add a score correction term \\(-g^2 \\nabla \\log p\\) to the drift.</p> <p>Level 2 (Intuitive): The score tells particles where probability is concentrated, guiding them back to likely regions.</p> <p>Level 3 (Mathematical): Reversing the Fokker-Planck equation requires expressing the Laplacian in terms of the score, yielding the correction term.</p>"},{"location":"diffusion/reverse_process/reverse_process_derivation/#appendix-rigorous-statement-of-andersons-theorem","title":"Appendix: Rigorous Statement of Anderson's Theorem","text":"<p>Theorem (Anderson, 1982):</p> <p>Let \\(x(t)\\) be the solution to the forward SDE:</p> \\[ dx = f(x,t)\\,dt + G(t)\\,dw \\] <p>where \\(G(t)\\) is a \\(d \\times d\\) matrix (diffusion matrix).</p> <p>Then the reverse-time process \\(x(T-t)\\) satisfies the SDE:</p> \\[ dx = \\left[f(x,t) - \\left(GG^T\\right) \\nabla_x \\log p_t(x)\\right] dt + G\\,d\\bar{w}(t) \\] <p>where \\(p_t(x)\\) is the marginal density of \\(x(t)\\) under the forward process.</p> <p>Special case (scalar diffusion \\(g(t)\\)):</p> \\[ G(t) = g(t) I \\quad \\Rightarrow \\quad GG^T = g(t)^2 I \\] <p>This gives the form we use:</p> \\[ dx = \\left[f(x,t) - g(t)^2 \\nabla_x \\log p_t(x)\\right] dt + g(t)\\,d\\bar{w}(t) \\]"},{"location":"diffusion/reverse_process/reverse_process_derivation/#references","title":"References","text":"<ul> <li>Anderson (1982): \"Reverse-time diffusion equation models\" \u2014 Original theorem</li> <li>Song et al. (2021): Score-Based Generative Modeling through SDEs \u2014 Application to generative models</li> <li>Haussmann &amp; Pardoux (1986): \"Time reversal of diffusions\" \u2014 Mathematical treatment</li> <li>F\u00f6llmer (1986): \"Random fields and diffusion processes\" \u2014 Connections to optimal transport</li> <li>Fokker-Planck Derivation: <code>notebooks/diffusion/02_sde_formulation/supplements/07_fokker_planck_equation.md</code></li> </ul>"},{"location":"diffusion/reverse_process/reverse_process_derivation/#related-documents","title":"Related Documents","text":"<ul> <li>Fokker-Planck Equation: <code>fokker_planck_derivation.md</code> \u2014 Derivation and intuition for the probability evolution equation</li> <li>Detailed Worked Example: <code>reverse_process_example.md</code> \u2014 Complete 1D Gaussian example with numerical verification</li> </ul>"},{"location":"diffusion/reverse_process/reverse_process_example/","title":"Concrete Example: Reversing a 1D Gaussian Diffusion","text":""},{"location":"diffusion/reverse_process/reverse_process_example/#overview","title":"Overview","text":"<p>This document provides a detailed worked example of the reverse-time SDE for a simple 1D case. This makes the abstract mathematics of reverse diffusion concrete and intuitive.</p>"},{"location":"diffusion/reverse_process/reverse_process_example/#referenced-from","title":"Referenced From","text":"<ul> <li>Main Document: <code>docs/diffusion/reverse_process/reverse_process_derivation.md</code> \u2014 Full derivation of reverse SDE</li> </ul>"},{"location":"diffusion/reverse_process/reverse_process_example/#the-setup","title":"The Setup","text":"<p>Consider the simplest possible diffusion: a 1D random walk that becomes a Gaussian.</p>"},{"location":"diffusion/reverse_process/reverse_process_example/#forward-process","title":"Forward Process","text":"<p>Starting from a point at the origin \\(x_0 = 0\\), particles undergo Brownian motion:</p> \\[ dx = 0 \\cdot dt + \\sqrt{2D}\\,dw \\] <p>Parameters:</p> <ul> <li>Drift: \\(f(x,t) = 0\\) (no preferred direction)</li> <li>Diffusion coefficient: \\(g(t) = \\sqrt{2D}\\) (constant diffusion)</li> </ul> <p>Solution: At time \\(t\\), the probability distribution is:</p> \\[ p_t(x) = \\mathcal{N}(0, 2Dt) = \\frac{1}{\\sqrt{4\\pi Dt}} \\exp\\left(-\\frac{x^2}{4Dt}\\right) \\] <p>Properties:</p> <ul> <li>Mean: \\(\\mathbb{E}[x(t)] = 0\\) (stays centered at origin)</li> <li>Variance: \\(\\text{Var}(x(t)) = 2Dt\\) (spreads linearly with time)</li> </ul>"},{"location":"diffusion/reverse_process/reverse_process_example/#computing-the-score-function","title":"Computing the Score Function","text":"<p>The score function is the gradient of the log probability:</p> \\[ \\nabla_x \\log p_t(x) = \\frac{\\partial}{\\partial x} \\log p_t(x) \\]"},{"location":"diffusion/reverse_process/reverse_process_example/#step-1-write-the-log-probability","title":"Step 1: Write the Log Probability","text":"\\[ \\log p_t(x) = \\log\\left(\\frac{1}{\\sqrt{4\\pi Dt}}\\right) + \\log\\left(\\exp\\left(-\\frac{x^2}{4Dt}\\right)\\right) \\] \\[ = -\\frac{1}{2}\\log(4\\pi Dt) - \\frac{x^2}{4Dt} \\]"},{"location":"diffusion/reverse_process/reverse_process_example/#step-2-take-the-derivative","title":"Step 2: Take the Derivative","text":"\\[ \\frac{\\partial}{\\partial x} \\log p_t(x) = \\frac{\\partial}{\\partial x}\\left(-\\frac{1}{2}\\log(4\\pi Dt) - \\frac{x^2}{4Dt}\\right) \\] <p>The first term is constant (doesn't depend on \\(x\\)):</p> \\[ \\frac{\\partial}{\\partial x}\\left(-\\frac{1}{2}\\log(4\\pi Dt)\\right) = 0 \\] <p>The second term:</p> \\[ \\frac{\\partial}{\\partial x}\\left(-\\frac{x^2}{4Dt}\\right) = -\\frac{2x}{4Dt} = -\\frac{x}{2Dt} \\]"},{"location":"diffusion/reverse_process/reverse_process_example/#result","title":"Result","text":"\\[ \\boxed{\\nabla_x \\log p_t(x) = -\\frac{x}{2Dt}} \\]"},{"location":"diffusion/reverse_process/reverse_process_example/#interpreting-the-score","title":"Interpreting the Score","text":""},{"location":"diffusion/reverse_process/reverse_process_example/#physical-meaning","title":"Physical Meaning","text":"<p>The score \\(\\nabla_x \\log p_t(x) = -\\frac{x}{2Dt}\\) has a clear interpretation:</p> <p>Sign: Always points toward \\(x = 0\\) (the origin) - If \\(x &gt; 0\\): score is negative \u2192 points left (toward origin) - If \\(x &lt; 0\\): score is positive \u2192 points right (toward origin)</p> <p>Magnitude: \\(|\\text{score}| = \\frac{|x|}{2Dt}\\) - Proportional to distance from origin - Inversely proportional to time (and diffusion coefficient)</p> <p>Intuition: \"The further you are from the center of the probability distribution, the stronger the pull back toward it.\"</p>"},{"location":"diffusion/reverse_process/reverse_process_example/#visual-representation","title":"Visual Representation","text":"<pre><code>Probability:           Score:\n\n    \u2502  ___                \u2502    \u2571\n    \u2502 /   \\               \u2502   \u2571\np   \u2502/     \\              \u2502  \u2571\n    \u2502       \\             \u2502 \u2571\u2500\u2500\u2500\u2500\u2500\u2500 x=0\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500x            \u2502\u2571\n    -2  0  +2            -2  0  +2\n    (Gaussian)           (linear, points to origin)\n</code></pre>"},{"location":"diffusion/reverse_process/reverse_process_example/#the-reverse-time-sde","title":"The Reverse-Time SDE","text":""},{"location":"diffusion/reverse_process/reverse_process_example/#forward-sde-for-reference","title":"Forward SDE (for reference)","text":"\\[ dx = 0 \\cdot dt + \\sqrt{2D}\\,dw \\]"},{"location":"diffusion/reverse_process/reverse_process_example/#reverse-sde-using-andersons-theorem","title":"Reverse SDE (using Anderson's theorem)","text":"\\[ dx = \\left[f(x,t) - g(t)^2 \\nabla_x \\log p_t(x)\\right] dt + g(t)\\,d\\bar{w}(t) \\] <p>Substitute our values: - \\(f(x,t) = 0\\) - \\(g(t) = \\sqrt{2D}\\) - \\(\\nabla_x \\log p_t(x) = -\\frac{x}{2Dt}\\)</p> \\[ dx = \\left[0 - (\\sqrt{2D})^2 \\cdot \\left(-\\frac{x}{2Dt}\\right)\\right] dt + \\sqrt{2D}\\,d\\bar{w}(t) \\] \\[ dx = \\left[2D \\cdot \\frac{x}{2Dt}\\right] dt + \\sqrt{2D}\\,d\\bar{w}(t) \\] \\[ \\boxed{dx = \\frac{x}{t}\\,dt + \\sqrt{2D}\\,d\\bar{w}(t)} \\]"},{"location":"diffusion/reverse_process/reverse_process_example/#understanding-the-reverse-drift","title":"Understanding the Reverse Drift","text":"<p>The reverse SDE has drift:</p> \\[ \\text{drift} = \\frac{x}{t} \\]"},{"location":"diffusion/reverse_process/reverse_process_example/#what-this-means","title":"What This Means","text":"<p>Sign: Points away from origin! - If \\(x &gt; 0\\): drift is positive \u2192 pushes right (away from origin) - If \\(x &lt; 0\\): drift is negative \u2192 pushes left (away from origin)</p> <p>Wait, that seems wrong! Shouldn't we be pulling toward the origin to reverse the diffusion?</p>"},{"location":"diffusion/reverse_process/reverse_process_example/#the-key-insight","title":"The Key Insight","text":"<p>The drift alone would push particles outward, but the noise term is also present: \\(+\\sqrt{2D}\\,d\\bar{w}(t)\\).</p> <p>When running in reverse time, the combination of: 1. Outward drift: \\(\\frac{x}{t}\\) 2. Random noise: \\(\\sqrt{2D}\\,d\\bar{w}(t)\\)</p> <p>actually brings the distribution back from \\(\\mathcal{N}(0, 2Dt)\\) to \\(\\mathcal{N}(0, 0)\\) (point mass at origin).</p> <p>Mathematical fact: This is not intuitive from looking at the drift alone. You need to analyze the Fokker-Planck equation to see that the marginal distributions correctly evolve backward.</p>"},{"location":"diffusion/reverse_process/reverse_process_example/#numerical-verification","title":"Numerical Verification","text":"<p>Let's verify this numerically. Starting from \\(p_T(x) = \\mathcal{N}(0, 2DT)\\) and running the reverse SDE backward, we should approach a point mass at the origin.</p>"},{"location":"diffusion/reverse_process/reverse_process_example/#python-implementation","title":"Python Implementation","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\nD = 0.5\nT = 1.0\nnum_steps = 1000\nnum_particles = 1000\ndt = -T / num_steps  # Negative because going backward\n\n# Initial condition: particles from N(0, 2DT)\nx = np.random.normal(0, np.sqrt(2*D*T), num_particles)\n\n# Reverse SDE: dx = (x/t)dt + sqrt(2D)dw\nx_history = [x.copy()]\n\nfor i in range(num_steps):\n    t = T + i * dt  # Current time (decreasing)\n\n    # Drift term\n    drift = x / t\n\n    # Diffusion term\n    noise = np.sqrt(2*D * abs(dt)) * np.random.randn(num_particles)\n\n    # Update\n    x = x + drift * dt + noise\n\n    if i % 100 == 0:\n        x_history.append(x.copy())\n\n# Plot evolution\nfig, axes = plt.subplots(1, len(x_history), figsize=(15, 3))\nfor i, (ax, x_snap) in enumerate(zip(axes, x_history)):\n    ax.hist(x_snap, bins=30, density=True, alpha=0.7)\n    ax.set_title(f't = {T - i*100*abs(dt):.2f}')\n    ax.set_xlim(-3, 3)\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Expected result: Distribution starts wide (Gaussian with large variance) and shrinks toward a point mass at \\(x=0\\).</p>"},{"location":"diffusion/reverse_process/reverse_process_example/#comparison-forward-vs-reverse","title":"Comparison: Forward vs Reverse","text":""},{"location":"diffusion/reverse_process/reverse_process_example/#forward-process_1","title":"Forward Process","text":"\\[ dx = 0 \\cdot dt + \\sqrt{2D}\\,dw \\] <ul> <li>Drift: None</li> <li>Effect: Pure diffusion, spreads outward</li> <li>Variance: Increases linearly with time</li> </ul>"},{"location":"diffusion/reverse_process/reverse_process_example/#reverse-process","title":"Reverse Process","text":"\\[ dx = \\frac{x}{t}\\,dt + \\sqrt{2D}\\,d\\bar{w}(t) \\] <ul> <li>Drift: \\(\\frac{x}{t}\\) (proportional to position and inversely to time)</li> <li>Effect: Combines drift and diffusion to contract the distribution</li> <li>Variance: Decreases as \\(t \\to 0\\)</li> </ul>"},{"location":"diffusion/reverse_process/reverse_process_example/#key-difference","title":"Key Difference","text":"<p>Forward: No drift needed\u2014pure random motion spreads things out.</p> <p>Reverse: Need drift \\(\\frac{x}{t}\\) to counteract the spreading and guide particles back to origin.</p> <p>The score term made this drift possible: \\(-g^2 \\nabla \\log p = \\frac{x}{t}\\)</p>"},{"location":"diffusion/reverse_process/reverse_process_example/#why-the-drift-points-outward-paradox-explained","title":"Why the Drift Points Outward (Paradox Explained)","text":"<p>It seems paradoxical that the reverse drift points away from the origin, yet the process brings particles back to the origin.</p>"},{"location":"diffusion/reverse_process/reverse_process_example/#resolution","title":"Resolution","text":"<p>The key is understanding what \"running in reverse time\" means:</p> <ol> <li>In forward time (\\(t\\) increasing): </li> <li>Drift \\(\\frac{x}{t}\\) would push particles outward</li> <li>But the drift coefficient decreases as \\(t\\) increases</li> <li> <p>This is not the physical forward process (which has no drift)</p> </li> <li> <p>In reverse time (\\(t\\) decreasing, moving backward from \\(T\\) to \\(0\\)):</p> </li> <li>We start at large \\(t\\) (small drift coefficient)</li> <li>As we move backward, \\(t\\) decreases (drift coefficient increases)</li> <li>The drift \\(\\frac{x}{t}\\) combined with noise \\(d\\bar{w}\\) (which is also reversed) produces the correct backward evolution</li> </ol> <p>Bottom line: You cannot understand the reverse process by just looking at the drift sign. The full stochastic dynamics, including the noise term and time direction, determine the behavior.</p>"},{"location":"diffusion/reverse_process/reverse_process_example/#generalizing-to-diffusion-models","title":"Generalizing to Diffusion Models","text":""},{"location":"diffusion/reverse_process/reverse_process_example/#the-pattern","title":"The Pattern","text":"<p>In our example: - Forward: \\(p_t(x) = \\mathcal{N}(0, 2Dt)\\) \u2192 distribution spreads - Score: \\(\\nabla \\log p_t = -\\frac{x}{2Dt}\\) \u2192 points to center - Reverse: Uses score to guide particles back</p> <p>In diffusion models: - Forward: \\(p_t(x)\\) evolves from data to noise - Score: \\(\\nabla \\log p_t(x)\\) points toward data-like regions - Reverse: Uses learned score \\(s_\\theta(x,t)\\) to guide from noise back to data</p>"},{"location":"diffusion/reverse_process/reverse_process_example/#why-we-need-to-learn-the-score","title":"Why We Need to Learn the Score","text":"<p>In our simple example, \\(p_t(x) = \\mathcal{N}(0, 2Dt)\\) is known, so we can compute \\(\\nabla \\log p_t\\) analytically.</p> <p>In diffusion models: - \\(p_t(x)\\) is complex (the distribution of partially noised images) - We can't write it down or compute its gradient directly - Solution: Train a neural network \\(s_\\theta(x,t)\\) to approximate \\(\\nabla \\log p_t(x)\\)</p>"},{"location":"diffusion/reverse_process/reverse_process_example/#summary","title":"Summary","text":"Aspect Forward Reverse SDE \\(dx = \\sqrt{2D}\\,dw\\) \\(dx = \\frac{x}{t}\\,dt + \\sqrt{2D}\\,d\\bar{w}\\) Drift None \\(\\frac{x}{t}\\) (from score) Score N/A \\(\\nabla \\log p_t = -\\frac{x}{2Dt}\\) Effect Spread outward Contract inward Variance Increases: \\(2Dt\\) Decreases: \\(2Dt \\to 0\\) <p>Key takeaway: The score term \\(-g^2 \\nabla \\log p_t(x) = \\frac{x}{t}\\) provides the drift needed to reverse the diffusion process. Without it, we cannot bring particles back to the origin.</p>"},{"location":"diffusion/reverse_process/reverse_process_example/#references","title":"References","text":"<ul> <li>Main Derivation: <code>docs/diffusion/reverse_process/reverse_process_derivation.md</code></li> <li>Anderson (1982): \"Reverse-time diffusion equation models\"</li> <li>Song et al. (2021): \"Score-Based Generative Modeling through SDEs\"</li> </ul>"},{"location":"diffusion/score_network/advanced_architectures/","title":"Advanced Score Network Architectures for Realistic Data","text":""},{"location":"diffusion/score_network/advanced_architectures/#overview","title":"Overview","text":"<p>This document explores advanced neural network architectures for score-based diffusion models when working with realistic, complex data such as: - Medical imaging (CT, MRI, pathology slides) - Gene expression data (bulk RNA-seq, microarray) - Single-cell RNA-seq (scRNA-seq) data - High-resolution natural images</p> <p>The simple MLP used in the tutorial notebook works well for 2D toy data, but realistic datasets require more sophisticated architectures.</p>"},{"location":"diffusion/score_network/advanced_architectures/#referenced-from","title":"Referenced From","text":"<ul> <li>Notebook: <code>notebooks/diffusion/02_sde_formulation/02_sde_formulation.ipynb</code></li> <li>Related: Time Embedding and FiLM \u2014 Component details for time conditioning</li> </ul>"},{"location":"diffusion/score_network/advanced_architectures/#why-simple-mlps-are-not-enough","title":"Why Simple MLPs Are Not Enough","text":""},{"location":"diffusion/score_network/advanced_architectures/#the-tutorial-architecture","title":"The Tutorial Architecture","text":"<p>The notebook uses a simple MLP:</p> <pre><code>class SimpleScoreNetwork(nn.Module):\n    def __init__(self, data_dim=2, hidden_dim=128, time_dim=32):\n        self.net = nn.Sequential(\n            nn.Linear(data_dim + time_dim, hidden_dim),\n            nn.SiLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.SiLU(),\n            nn.Linear(hidden_dim, data_dim),\n        )\n</code></pre> <p>This works for 2D toy data because:</p> <ul> <li>Only 2 dimensions to model</li> <li>Simple distributions (e.g., mixture of Gaussians)</li> <li>No spatial structure to capture</li> </ul>"},{"location":"diffusion/score_network/advanced_architectures/#limitations-for-realistic-data","title":"Limitations for Realistic Data","text":"Data Type Challenge Why MLP Fails Images Spatial correlations, local patterns MLPs treat pixels independently Gene expression High dimensionality (~20,000 genes) Too many parameters, no structure scRNA-seq Sparse, high-dimensional, cell-type structure Can't leverage biological priors Medical imaging Multi-scale features, fine details No hierarchical feature extraction"},{"location":"diffusion/score_network/advanced_architectures/#architecture-1-u-net-for-images","title":"Architecture 1: U-Net for Images","text":""},{"location":"diffusion/score_network/advanced_architectures/#why-u-net","title":"Why U-Net?","text":"<p>U-Net is the dominant architecture for image diffusion models (DDPM, Stable Diffusion) because:</p> <ol> <li>Multi-scale processing: Encoder-decoder structure captures features at multiple resolutions</li> <li>Skip connections: Preserve fine details while processing global structure</li> <li>Fully convolutional: Handles arbitrary image sizes</li> <li>Proven effectiveness: State-of-the-art in image generation</li> </ol>"},{"location":"diffusion/score_network/advanced_architectures/#architecture-overview","title":"Architecture Overview","text":"<pre><code>Input Image (H\u00d7W\u00d7C) + Time Embedding\n        \u2193\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   Encoder     \u2502  (Downsample: capture global structure)\n    \u2502   Block 1     \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502 Skip\n        \u2193                             \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n    \u2502   Encoder     \u2502                 \u2502\n    \u2502   Block 2     \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502   \u2502\n        \u2193                         \u2502   \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502   \u2502\n    \u2502   Bottleneck  \u2502             \u2502   \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502   \u2502\n        \u2193                         \u2502   \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502   \u2502\n    \u2502   Decoder     \u2502 \u2190\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n    \u2502   Block 1     \u2502                 \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502\n        \u2193                             \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n    \u2502   Decoder     \u2502 \u2190\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502   Block 2     \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2193\n    Output Score (H\u00d7W\u00d7C)\n</code></pre>"},{"location":"diffusion/score_network/advanced_architectures/#key-components","title":"Key Components","text":""},{"location":"diffusion/score_network/advanced_architectures/#1-residual-blocks-with-time-conditioning","title":"1. Residual Blocks with Time Conditioning","text":"<pre><code>class ResBlock(nn.Module):\n    \"\"\"Residual block with time conditioning via FiLM.\"\"\"\n\n    def __init__(self, in_channels, out_channels, time_dim):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n\n        # Time conditioning (FiLM)\n        self.time_mlp = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(time_dim, 2 * out_channels)  # scale and shift\n        )\n\n        # Skip connection if dimensions change\n        self.skip = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n\n        self.norm1 = nn.GroupNorm(8, in_channels)\n        self.norm2 = nn.GroupNorm(8, out_channels)\n        self.act = nn.SiLU()\n\n    def forward(self, x, time_emb):\n        h = self.norm1(x)\n        h = self.act(h)\n        h = self.conv1(h)\n\n        # Apply time conditioning\n        scale, shift = self.time_mlp(time_emb).chunk(2, dim=1)\n        scale = scale[:, :, None, None]  # Broadcast to spatial dims\n        shift = shift[:, :, None, None]\n        h = h * (1 + scale) + shift\n\n        h = self.norm2(h)\n        h = self.act(h)\n        h = self.conv2(h)\n\n        return h + self.skip(x)\n</code></pre>"},{"location":"diffusion/score_network/advanced_architectures/#2-attention-layers","title":"2. Attention Layers","text":"<p>For capturing long-range dependencies (especially important for large images):</p> <pre><code>class SelfAttention(nn.Module):\n    \"\"\"Self-attention for spatial features.\"\"\"\n\n    def __init__(self, channels, num_heads=4):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(channels, num_heads, batch_first=True)\n        self.norm = nn.GroupNorm(8, channels)\n\n    def forward(self, x):\n        b, c, h, w = x.shape\n\n        # Reshape to sequence: (batch, h*w, channels)\n        x_flat = x.flatten(2).transpose(1, 2)\n\n        # Apply attention\n        x_norm = self.norm(x).flatten(2).transpose(1, 2)\n        attn_out, _ = self.attention(x_norm, x_norm, x_norm)\n\n        # Reshape back\n        out = attn_out.transpose(1, 2).reshape(b, c, h, w)\n\n        return x + out\n</code></pre>"},{"location":"diffusion/score_network/advanced_architectures/#complete-u-net-skeleton","title":"Complete U-Net Skeleton","text":"<pre><code>class UNet(nn.Module):\n    \"\"\"U-Net for score estimation in diffusion models.\"\"\"\n\n    def __init__(self, in_channels=3, base_channels=64, time_dim=256):\n        super().__init__()\n\n        # Time embedding\n        self.time_mlp = nn.Sequential(\n            SinusoidalTimeEmbedding(time_dim),\n            nn.Linear(time_dim, time_dim * 4),\n            nn.SiLU(),\n            nn.Linear(time_dim * 4, time_dim)\n        )\n\n        # Encoder\n        self.enc1 = ResBlock(in_channels, base_channels, time_dim)\n        self.enc2 = ResBlock(base_channels, base_channels * 2, time_dim)\n        self.enc3 = ResBlock(base_channels * 2, base_channels * 4, time_dim)\n\n        self.down1 = nn.Conv2d(base_channels, base_channels, 3, stride=2, padding=1)\n        self.down2 = nn.Conv2d(base_channels * 2, base_channels * 2, 3, stride=2, padding=1)\n\n        # Bottleneck with attention\n        self.bottleneck = nn.Sequential(\n            ResBlock(base_channels * 4, base_channels * 4, time_dim),\n            SelfAttention(base_channels * 4),\n            ResBlock(base_channels * 4, base_channels * 4, time_dim)\n        )\n\n        # Decoder (with skip connections)\n        self.dec3 = ResBlock(base_channels * 8, base_channels * 2, time_dim)  # *8 from concat\n        self.dec2 = ResBlock(base_channels * 4, base_channels, time_dim)\n        self.dec1 = ResBlock(base_channels * 2, base_channels, time_dim)\n\n        self.up2 = nn.ConvTranspose2d(base_channels * 4, base_channels * 4, 4, stride=2, padding=1)\n        self.up1 = nn.ConvTranspose2d(base_channels * 2, base_channels * 2, 4, stride=2, padding=1)\n\n        # Output\n        self.out = nn.Sequential(\n            nn.GroupNorm(8, base_channels),\n            nn.SiLU(),\n            nn.Conv2d(base_channels, in_channels, 3, padding=1)\n        )\n\n    def forward(self, x, t):\n        # Time embedding\n        t_emb = self.time_mlp(t)\n\n        # Encoder\n        h1 = self.enc1(x, t_emb)\n        h2 = self.enc2(self.down1(h1), t_emb)\n        h3 = self.enc3(self.down2(h2), t_emb)\n\n        # Bottleneck\n        h = self.bottleneck(h3)\n\n        # Decoder with skip connections\n        h = self.up2(h)\n        h = self.dec3(torch.cat([h, h2], dim=1), t_emb)\n        h = self.up1(h)\n        h = self.dec2(torch.cat([h, h1], dim=1), t_emb)\n        h = self.dec1(h, t_emb)\n\n        return self.out(h)\n</code></pre>"},{"location":"diffusion/score_network/advanced_architectures/#medical-imaging-considerations","title":"Medical Imaging Considerations","text":"<p>For medical imaging (CT, MRI, pathology):</p> <ol> <li>3D U-Net: Replace 2D convolutions with 3D for volumetric data</li> <li>Higher resolution: More downsampling stages for high-res pathology</li> <li>Domain-specific augmentation: Intensity normalization, organ-specific priors</li> <li>Conditional generation: Condition on patient metadata, modality, etc.</li> </ol> <pre><code># 3D convolution for volumetric medical imaging\nself.conv = nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1)\n</code></pre>"},{"location":"diffusion/score_network/advanced_architectures/#architecture-2-vision-transformer-dit","title":"Architecture 2: Vision Transformer (DiT)","text":""},{"location":"diffusion/score_network/advanced_architectures/#why-vision-transformers","title":"Why Vision Transformers?","text":"<p>The Diffusion Transformer (DiT) architecture shows that pure Transformers can match or exceed U-Net:</p> <ol> <li>Scalability: Performance scales predictably with model size</li> <li>Simplicity: Fewer inductive biases than CNNs</li> <li>Long-range attention: Global receptive field from the start</li> <li>Transfer learning: Leverage pretrained Vision Transformers</li> </ol>"},{"location":"diffusion/score_network/advanced_architectures/#architecture-overview_1","title":"Architecture Overview","text":"<pre><code>Input Image \u2192 Patchify \u2192 Linear Projection\n                             \u2193\n                      Positional Encoding\n                             \u2193\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502     Transformer Block 1      \u2502 \u2190 Time/Class Conditioning\n              \u2502  (Self-Attention + FFN)      \u2502   (via AdaLN)\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2193\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502     Transformer Block 2      \u2502 \u2190 Time/Class Conditioning\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2193\n                            ...\n                             \u2193\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502     Transformer Block N      \u2502 \u2190 Time/Class Conditioning\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2193\n                      Linear Projection\n                             \u2193\n                      Unpatchify \u2192 Score\n</code></pre>"},{"location":"diffusion/score_network/advanced_architectures/#key-innovation-adaptive-layer-normalization-adaln","title":"Key Innovation: Adaptive Layer Normalization (AdaLN)","text":"<p>DiT conditions on time using Adaptive Layer Normalization:</p> <pre><code>class AdaLN(nn.Module):\n    \"\"\"Adaptive Layer Normalization for DiT.\"\"\"\n\n    def __init__(self, hidden_dim, condition_dim):\n        super().__init__()\n        self.norm = nn.LayerNorm(hidden_dim, elementwise_affine=False)\n        self.adaLN_modulation = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(condition_dim, 6 * hidden_dim)  # scale, shift, gate for 2 norms\n        )\n\n    def forward(self, x, condition):\n        # Get modulation parameters\n        params = self.adaLN_modulation(condition)\n        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = params.chunk(6, dim=-1)\n\n        return shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp\n</code></pre>"},{"location":"diffusion/score_network/advanced_architectures/#dit-block","title":"DiT Block","text":"<pre><code>class DiTBlock(nn.Module):\n    \"\"\"Diffusion Transformer block with AdaLN conditioning.\"\"\"\n\n    def __init__(self, hidden_dim, num_heads, mlp_ratio=4.0, condition_dim=256):\n        super().__init__()\n\n        self.norm1 = nn.LayerNorm(hidden_dim, elementwise_affine=False)\n        self.attn = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True)\n\n        self.norm2 = nn.LayerNorm(hidden_dim, elementwise_affine=False)\n        mlp_hidden = int(hidden_dim * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(hidden_dim, mlp_hidden),\n            nn.GELU(),\n            nn.Linear(mlp_hidden, hidden_dim)\n        )\n\n        # AdaLN modulation\n        self.adaLN = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(condition_dim, 6 * hidden_dim)\n        )\n\n    def forward(self, x, condition):\n        # Get modulation parameters\n        shift1, scale1, gate1, shift2, scale2, gate2 = self.adaLN(condition).chunk(6, dim=-1)\n\n        # Attention with AdaLN\n        h = self.norm1(x)\n        h = h * (1 + scale1.unsqueeze(1)) + shift1.unsqueeze(1)\n        h, _ = self.attn(h, h, h)\n        x = x + gate1.unsqueeze(1) * h\n\n        # MLP with AdaLN\n        h = self.norm2(x)\n        h = h * (1 + scale2.unsqueeze(1)) + shift2.unsqueeze(1)\n        h = self.mlp(h)\n        x = x + gate2.unsqueeze(1) * h\n\n        return x\n</code></pre>"},{"location":"diffusion/score_network/advanced_architectures/#when-to-use-dit-vs-u-net","title":"When to Use DiT vs U-Net","text":"Aspect U-Net DiT Inductive bias Strong (locality, hierarchy) Weak (learns structure) Data efficiency Better with less data Needs more data Scaling Saturates earlier Scales well Compute More efficient for small models More efficient at scale Best for Small-medium datasets Large-scale training"},{"location":"diffusion/score_network/advanced_architectures/#architecture-3-networks-for-tabularbiological-data","title":"Architecture 3: Networks for Tabular/Biological Data","text":""},{"location":"diffusion/score_network/advanced_architectures/#gene-expression-data","title":"Gene Expression Data","text":"<p>Gene expression data (bulk RNA-seq, microarray) is: - High-dimensional: ~20,000 genes - Tabular: No spatial structure - Structured: Gene pathways, co-expression patterns</p>"},{"location":"diffusion/score_network/advanced_architectures/#architecture-options","title":"Architecture Options","text":"<p>Option 1: Deep MLP with Residual Connections</p> <pre><code>class GeneExpressionScoreNet(nn.Module):\n    \"\"\"Score network for gene expression data.\"\"\"\n\n    def __init__(self, n_genes=20000, hidden_dim=2048, n_layers=6, time_dim=128):\n        super().__init__()\n\n        self.time_embed = SinusoidalTimeEmbedding(time_dim)\n\n        # Initial projection\n        self.input_proj = nn.Linear(n_genes, hidden_dim)\n\n        # Deep residual blocks\n        self.blocks = nn.ModuleList([\n            ResidualMLPBlock(hidden_dim, time_dim) \n            for _ in range(n_layers)\n        ])\n\n        # Output\n        self.output_proj = nn.Sequential(\n            nn.LayerNorm(hidden_dim),\n            nn.SiLU(),\n            nn.Linear(hidden_dim, n_genes)\n        )\n\n    def forward(self, x, t):\n        t_emb = self.time_embed(t)\n\n        h = self.input_proj(x)\n        for block in self.blocks:\n            h = block(h, t_emb)\n\n        return self.output_proj(h)\n\n\nclass ResidualMLPBlock(nn.Module):\n    def __init__(self, hidden_dim, time_dim):\n        super().__init__()\n        self.norm = nn.LayerNorm(hidden_dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim * 4),\n            nn.GELU(),\n            nn.Linear(hidden_dim * 4, hidden_dim)\n        )\n        self.time_proj = nn.Linear(time_dim, hidden_dim * 2)  # FiLM\n\n    def forward(self, x, t_emb):\n        scale, shift = self.time_proj(t_emb).chunk(2, dim=-1)\n\n        h = self.norm(x)\n        h = h * (1 + scale) + shift\n        h = self.mlp(h)\n\n        return x + h\n</code></pre> <p>Option 2: Graph Neural Network (for pathway structure)</p> <p>If gene-gene interaction networks are available:</p> <pre><code>class PathwayAwareScoreNet(nn.Module):\n    \"\"\"Score network using gene pathway structure.\"\"\"\n\n    def __init__(self, n_genes, hidden_dim, adjacency_matrix, time_dim=128):\n        super().__init__()\n\n        # Register adjacency as buffer (not parameter)\n        self.register_buffer('adj', adjacency_matrix)\n\n        self.time_embed = SinusoidalTimeEmbedding(time_dim)\n        self.node_embed = nn.Linear(1, hidden_dim)\n\n        # Graph convolution layers\n        self.gcn_layers = nn.ModuleList([\n            GraphConvWithTime(hidden_dim, time_dim)\n            for _ in range(4)\n        ])\n\n        self.output = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x, t):\n        # x: (batch, n_genes)\n        t_emb = self.time_embed(t)\n\n        # Treat each gene as a node\n        h = self.node_embed(x.unsqueeze(-1))  # (batch, n_genes, hidden)\n\n        for gcn in self.gcn_layers:\n            h = gcn(h, self.adj, t_emb)\n\n        return self.output(h).squeeze(-1)  # (batch, n_genes)\n</code></pre>"},{"location":"diffusion/score_network/advanced_architectures/#scrna-seq-data","title":"scRNA-seq Data","text":"<p>Single-cell RNA-seq data has additional challenges: - Sparse: Many zero counts (dropout) - Cell-type structure: Cells cluster by type - Batch effects: Technical variation across experiments</p>"},{"location":"diffusion/score_network/advanced_architectures/#architecture-considerations","title":"Architecture Considerations","text":"<pre><code>class scRNAScoreNet(nn.Module):\n    \"\"\"Score network for scRNA-seq with sparsity handling.\"\"\"\n\n    def __init__(self, n_genes, hidden_dim=512, time_dim=64):\n        super().__init__()\n\n        self.time_embed = SinusoidalTimeEmbedding(time_dim)\n\n        # Sparse-aware input processing\n        self.input_encoder = nn.Sequential(\n            nn.Linear(n_genes, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.GELU()\n        )\n\n        # Attention over genes (learn which genes are important)\n        self.gene_attention = nn.MultiheadAttention(hidden_dim, num_heads=8, batch_first=True)\n\n        # Score prediction\n        self.decoder = nn.Sequential(\n            ResidualMLPBlock(hidden_dim, time_dim),\n            ResidualMLPBlock(hidden_dim, time_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.Linear(hidden_dim, n_genes)\n        )\n\n    def forward(self, x, t):\n        t_emb = self.time_embed(t)\n\n        # Encode\n        h = self.input_encoder(x)\n\n        # Self-attention to learn gene relationships\n        h_attended, _ = self.gene_attention(h.unsqueeze(1), h.unsqueeze(1), h.unsqueeze(1))\n        h = h + h_attended.squeeze(1)\n\n        # Decode with time conditioning\n        return self.decoder(h, t_emb)\n</code></pre>"},{"location":"diffusion/score_network/advanced_architectures/#choosing-the-right-architecture","title":"Choosing the Right Architecture","text":""},{"location":"diffusion/score_network/advanced_architectures/#decision-framework","title":"Decision Framework","text":"<pre><code>What is your data type?\n    \u2502\n    \u251c\u2500\u2500 Images (2D)\n    \u2502       \u2502\n    \u2502       \u251c\u2500\u2500 Small dataset (&lt;50K) \u2192 U-Net\n    \u2502       \u251c\u2500\u2500 Large dataset (&gt;1M) \u2192 DiT\n    \u2502       \u2514\u2500\u2500 Medical imaging \u2192 2D/3D U-Net with domain priors\n    \u2502\n    \u251c\u2500\u2500 Volumes (3D)\n    \u2502       \u2514\u2500\u2500 3D U-Net\n    \u2502\n    \u251c\u2500\u2500 Tabular (gene expression, features)\n    \u2502       \u2502\n    \u2502       \u251c\u2500\u2500 No structure \u2192 Deep MLP with residual connections\n    \u2502       \u2514\u2500\u2500 Known graph structure \u2192 GNN\n    \u2502\n    \u2514\u2500\u2500 Sequences\n            \u2514\u2500\u2500 Transformer-based\n</code></pre>"},{"location":"diffusion/score_network/advanced_architectures/#practical-recommendations","title":"Practical Recommendations","text":"Data Type Recommended Architecture Key Components Natural images U-Net or DiT Time embedding, FiLM/AdaLN, attention Medical imaging 3D U-Net Domain-specific normalization, high-res handling Gene expression Deep residual MLP FiLM, layer normalization scRNA-seq MLP + attention Sparsity handling, gene attention Pathology slides Hierarchical U-Net Multi-scale, memory-efficient"},{"location":"diffusion/score_network/advanced_architectures/#implementation-tips","title":"Implementation Tips","text":""},{"location":"diffusion/score_network/advanced_architectures/#1-start-simple-then-scale","title":"1. Start Simple, Then Scale","text":"<pre><code># Start with small model for debugging\nmodel_small = UNet(base_channels=32)\n\n# Scale up once working\nmodel_large = UNet(base_channels=256)\n</code></pre>"},{"location":"diffusion/score_network/advanced_architectures/#2-memory-management-for-large-data","title":"2. Memory Management for Large Data","text":"<pre><code># Gradient checkpointing for U-Net\ndef forward_with_checkpointing(self, x, t):\n    t_emb = self.time_mlp(t)\n\n    # Use checkpointing for encoder blocks\n    h1 = torch.utils.checkpoint.checkpoint(self.enc1, x, t_emb)\n    h2 = torch.utils.checkpoint.checkpoint(self.enc2, self.down1(h1), t_emb)\n    # ... etc\n</code></pre>"},{"location":"diffusion/score_network/advanced_architectures/#3-mixed-precision-training","title":"3. Mixed Precision Training","text":"<pre><code>from torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()\n\nwith autocast():\n    score = model(x_t, t)\n    loss = score_matching_loss(score, noise, std)\n\nscaler.scale(loss).backward()\nscaler.step(optimizer)\nscaler.update()\n</code></pre>"},{"location":"diffusion/score_network/advanced_architectures/#summary","title":"Summary","text":"Architecture Best For Key Features Simple MLP 2D toy data, quick experiments Concatenation or FiLM U-Net Images, medical imaging Skip connections, multi-scale, FiLM DiT Large-scale image generation Transformer, AdaLN, scalable Deep MLP Tabular/gene expression Residual connections, FiLM GNN Data with graph structure Pathway-aware, message passing <p>The key insight: match the architecture's inductive biases to your data's structure.</p>"},{"location":"diffusion/score_network/advanced_architectures/#further-reading","title":"Further Reading","text":"<ul> <li>DDPM U-Net: Ho et al., \"Denoising Diffusion Probabilistic Models\" (2020)</li> <li>DiT: Peebles &amp; Xie, \"Scalable Diffusion Models with Transformers\" (2023)</li> <li>Medical Diffusion: Kazerouni et al., \"Diffusion Models in Medical Imaging\" (2023)</li> <li>scRNA Diffusion: Various recent works on single-cell generation</li> </ul>"},{"location":"diffusion/score_network/advanced_architectures/#related-documents","title":"Related Documents","text":"<ul> <li>Time Embedding and FiLM \u2014 Detailed explanation of time conditioning components</li> <li>Score Network Architecture (private) \u2014 Implementation notes</li> <li>Training Loss and Denoising \u2014 How training works</li> </ul>"},{"location":"diffusion/score_network/time_embedding_and_film/","title":"Time Embedding and FiLM: Conditioning Score Networks on Time","text":""},{"location":"diffusion/score_network/time_embedding_and_film/#overview","title":"Overview","text":"<p>This document explains two fundamental components used in modern score network architectures: 1. Time Embedding: How to represent the continuous time variable \\(t\\) as input to neural networks 2. FiLM (Feature-wise Linear Modulation): How to effectively condition network layers on time</p> <p>These techniques are essential for building score networks that can accurately estimate \\(\\nabla_x \\log p_t(x)\\) across different noise levels.</p>"},{"location":"diffusion/score_network/time_embedding_and_film/#referenced-from","title":"Referenced From","text":"<ul> <li>Notebook: <code>notebooks/diffusion/02_sde_formulation/02_sde_formulation.ipynb</code></li> <li>Implementation Note: <code>dev/notebooks/diffusion/02_sde_formulation/score_network_architecture.md</code></li> <li>Module: <code>genailab.diffusion</code> (contains the production implementation)</li> </ul>"},{"location":"diffusion/score_network/time_embedding_and_film/#time-embedding","title":"Time Embedding","text":""},{"location":"diffusion/score_network/time_embedding_and_film/#why-is-time-embedding-necessary","title":"Why is Time Embedding Necessary?","text":""},{"location":"diffusion/score_network/time_embedding_and_film/#the-core-problem","title":"The Core Problem","text":"<p>The score function we want to learn is:</p> \\[ s_\\theta(x, t) \\approx \\nabla_x \\log p_t(x) \\] <p>This function depends on both \\(x\\) (the noisy data) and \\(t\\) (the noise level/time).</p> <p>Key Challenge: Neural networks struggle to represent high-frequency functions when given raw scalar inputs. If we simply feed \\(t \\in [0, 1]\\) directly as a scalar, the network would have difficulty: - Distinguishing between close time values (e.g., \\(t=0.5\\) vs. \\(t=0.51\\)) - Learning different noise characteristics at different scales - Capturing the smooth but complex variation of the score function over time</p>"},{"location":"diffusion/score_network/time_embedding_and_film/#the-solution-high-dimensional-embeddings","title":"The Solution: High-Dimensional Embeddings","text":"<p>Instead of using \\(t\\) directly, we transform it into a high-dimensional embedding \\(\\gamma(t) \\in \\mathbb{R}^d\\) using sinusoidal functions. This gives the network: 1. Multiple frequencies to work with 2. Better expressiveness for representing time-dependent behavior 3. Smooth interpolation between time steps</p>"},{"location":"diffusion/score_network/time_embedding_and_film/#sinusoidal-time-embedding","title":"Sinusoidal Time Embedding","text":""},{"location":"diffusion/score_network/time_embedding_and_film/#mathematical-form","title":"Mathematical Form","text":"<p>The sinusoidal embedding transforms a scalar \\(t\\) into a vector of dimension \\(d\\):</p> \\[ \\gamma(t) = \\begin{bmatrix} \\sin(\\omega_1 t) \\\\ \\cos(\\omega_1 t) \\\\ \\sin(\\omega_2 t) \\\\ \\cos(\\omega_2 t) \\\\ \\vdots \\\\ \\sin(\\omega_{d/2} t) \\\\ \\cos(\\omega_{d/2} t) \\end{bmatrix} \\] <p>where the frequencies \\(\\omega_i\\) are chosen as:</p> \\[ \\omega_i = \\frac{1}{10000^{2i/d}} \\] <p>This creates a spectrum of frequencies from low (slow variation) to high (fast variation).</p>"},{"location":"diffusion/score_network/time_embedding_and_film/#why-sinusoidal-functions","title":"Why Sinusoidal Functions?","text":"<ol> <li>Bounded: All values stay in \\([-1, 1]\\), which helps with training stability</li> <li>Smooth: Differentiable everywhere, allowing smooth transitions</li> <li>Periodic: Can represent cyclical patterns</li> <li>Linear Interpolation Property: For any fixed offset \\(k\\), \\(\\gamma(t+k)\\) can be expressed as a linear function of \\(\\gamma(t)\\)</li> </ol>"},{"location":"diffusion/score_network/time_embedding_and_film/#implementation","title":"Implementation","text":"<p>Here's the typical PyTorch implementation:</p> <pre><code>def time_embedding(self, t):\n    \"\"\"Sinusoidal time embedding.\n\n    Args:\n        t: Time tensor [batch_size]\n\n    Returns:\n        embedding: [batch_size, time_dim]\n    \"\"\"\n    half_dim = self.time_dim // 2\n\n    # Create frequencies: 1, 1/10000^(1/(half_dim-1)), ..., 1/10000\n    emb = np.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n\n    # emb has shape [half_dim], t[:, None] has shape [batch_size, 1]\n    # Broadcasting gives [batch_size, half_dim]\n    emb = t[:, None] * emb[None, :]\n\n    # Concatenate sin and cos: [batch_size, time_dim]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n\n    return emb\n</code></pre> <p>Step-by-step breakdown: 1. <code>torch.arange(half_dim)</code> creates \\([0, 1, 2, \\ldots, \\text{half\\_dim}-1]\\) 2. <code>torch.exp(...  * -emb)</code> computes the frequencies \\(\\omega_i\\) 3. <code>t[:, None] * emb[None, :]</code> broadcasts to compute \\(\\omega_i t\\) for all \\(i\\) and all batch samples 4. Apply sin and cos and concatenate</p>"},{"location":"diffusion/score_network/time_embedding_and_film/#intuition-frequency-spectrum","title":"Intuition: Frequency Spectrum","text":"<p>Think of time embedding as representing \\(t\\) in multiple \"resolutions\": - Low frequencies (\\(\\sin(\\omega_1 t)\\) with small \\(\\omega_1\\)): Capture slow changes over time - High frequencies (\\(\\sin(\\omega_k t)\\) with large \\(\\omega_k\\)): Capture rapid changes</p> <p>This is similar to a Fourier basis, giving the network multiple \"channels\" to understand time at different scales.</p>"},{"location":"diffusion/score_network/time_embedding_and_film/#alternative-learned-embeddings","title":"Alternative: Learned Embeddings","text":"<p>Some architectures use learned embeddings instead: - Treat \\(t\\) as a discrete index (after discretization) - Use an embedding table like in NLP: <code>nn.Embedding(num_timesteps, embedding_dim)</code></p> <p>Pros: Fully learnable, no prior assumptions Cons: Doesn't generalize to unseen timesteps, requires discretization</p> <p>Sinusoidal embeddings are preferred for continuous-time models like SDEs.</p>"},{"location":"diffusion/score_network/time_embedding_and_film/#film-feature-wise-linear-modulation","title":"FiLM: Feature-wise Linear Modulation","text":""},{"location":"diffusion/score_network/time_embedding_and_film/#what-is-film","title":"What is FiLM?","text":"<p>FiLM (Feature-wise Linear Modulation) is a conditioning mechanism that modulates the features of a neural network layer using external information (in our case, time).</p> <p>For each feature map \\(h\\) in a layer, FiLM applies:</p> \\[ \\text{FiLM}(h | \\gamma) = \\gamma_{\\text{scale}} \\odot h + \\gamma_{\\text{shift}} \\] <p>where:</p> <ul> <li>\\(\\gamma_{\\text{scale}}\\), \\(\\gamma_{\\text{shift}}\\) are computed from the time embedding \\(\\gamma(t)\\)</li> <li>\\(\\odot\\) denotes element-wise multiplication</li> </ul> <p>This is also called affine transformation or adaptive normalization.</p>"},{"location":"diffusion/score_network/time_embedding_and_film/#why-is-film-effective","title":"Why is FiLM Effective?","text":""},{"location":"diffusion/score_network/time_embedding_and_film/#1-multiplicative-and-additive-control","title":"1. Multiplicative and Additive Control","text":"<p>FiLM provides two types of control: - Scale (\\(\\gamma_{\\text{scale}} \\odot h\\)): Controls the magnitude/importance of features - Shift (\\(\\gamma_{\\text{shift}}\\)): Controls the bias/offset</p> <p>This allows the network to completely change its behavior based on time: - At \\(t \\approx 1\\) (high noise): Might amplify certain features - At \\(t \\approx 0\\) (low noise): Might suppress the same features</p>"},{"location":"diffusion/score_network/time_embedding_and_film/#2-layer-wise-adaptation","title":"2. Layer-wise Adaptation","text":"<p>By applying FiLM at multiple layers, each layer can adapt differently to the time condition: - Early layers: Might adjust based on coarse noise levels - Deep layers: Might adjust based on fine-grained details</p>"},{"location":"diffusion/score_network/time_embedding_and_film/#3-better-than-concatenation","title":"3. Better than Concatenation","text":"<p>Compare to the simple approach of concatenating time embedding with input:</p> <p>Concatenation: <pre><code>h = torch.cat([x, time_emb], dim=-1)\nh = layer(h)\n</code></pre></p> <p>FiLM: <pre><code>h = layer(x)\nscale, shift = compute_film_params(time_emb)\nh = scale * h + shift\n</code></pre></p> <p>Why FiLM is better:</p> <ul> <li>Concatenation only affects the input; FiLM can modulate at any layer</li> <li>FiLM provides multiplicative control, which is more expressive</li> <li>FiLM allows the base features and time conditioning to be processed separately before combining</li> </ul>"},{"location":"diffusion/score_network/time_embedding_and_film/#film-implementation","title":"FiLM Implementation","text":""},{"location":"diffusion/score_network/time_embedding_and_film/#basic-implementation","title":"Basic Implementation","text":"<pre><code>class FiLMLayer(nn.Module):\n    \"\"\"Feature-wise Linear Modulation layer.\"\"\"\n\n    def __init__(self, feature_dim, time_dim):\n        super().__init__()\n\n        # Project time embedding to scale and shift parameters\n        self.film = nn.Linear(time_dim, 2 * feature_dim)\n\n    def forward(self, h, time_emb):\n        \"\"\"\n        Args:\n            h: Features [batch_size, feature_dim]\n            time_emb: Time embedding [batch_size, time_dim]\n\n        Returns:\n            modulated: [batch_size, feature_dim]\n        \"\"\"\n        # Compute scale and shift\n        film_params = self.film(time_emb)\n        scale, shift = torch.chunk(film_params, 2, dim=-1)\n\n        # Apply FiLM\n        return scale * h + shift\n</code></pre>"},{"location":"diffusion/score_network/time_embedding_and_film/#full-example-mlp-with-film","title":"Full Example: MLP with FiLM","text":"<pre><code>class ScoreNetworkWithFiLM(nn.Module):\n    \"\"\"Score network using FiLM conditioning.\"\"\"\n\n    def __init__(self, data_dim=2, hidden_dim=128, time_dim=32):\n        super().__init__()\n\n        self.time_dim = time_dim\n\n        # Main network layers\n        self.layer1 = nn.Linear(data_dim, hidden_dim)\n        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n        self.layer3 = nn.Linear(hidden_dim, hidden_dim)\n        self.output = nn.Linear(hidden_dim, data_dim)\n\n        # FiLM layers (one per hidden layer)\n        self.film1 = nn.Linear(time_dim, 2 * hidden_dim)\n        self.film2 = nn.Linear(time_dim, 2 * hidden_dim)\n        self.film3 = nn.Linear(time_dim, 2 * hidden_dim)\n\n        self.act = nn.SiLU()\n\n    def time_embedding(self, t):\n        \"\"\"Sinusoidal time embedding.\"\"\"\n        half_dim = self.time_dim // 2\n        emb = np.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n        emb = t[:, None] * emb[None, :]\n        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n        return emb\n\n    def forward(self, x, t):\n        \"\"\"\n        Args:\n            x: Data [batch_size, data_dim]\n            t: Time [batch_size]\n\n        Returns:\n            score: [batch_size, data_dim]\n        \"\"\"\n        # Get time embedding\n        t_emb = self.time_embedding(t)\n\n        # Layer 1\n        h = self.layer1(x)\n        scale, shift = torch.chunk(self.film1(t_emb), 2, dim=-1)\n        h = self.act(scale * h + shift)\n\n        # Layer 2\n        h = self.layer2(h)\n        scale, shift = torch.chunk(self.film2(t_emb), 2, dim=-1)\n        h = self.act(scale * h + shift)\n\n        # Layer 3\n        h = self.layer3(h)\n        scale, shift = torch.chunk(self.film3(t_emb), 2, dim=-1)\n        h = self.act(scale * h + shift)\n\n        # Output (no FiLM on final layer)\n        return self.output(h)\n</code></pre>"},{"location":"diffusion/score_network/time_embedding_and_film/#film-in-u-net-architecture","title":"FiLM in U-Net Architecture","text":"<p>FiLM is particularly powerful in U-Net architectures used for image generation:</p> <pre><code>class UNetBlock(nn.Module):\n    \"\"\"U-Net block with FiLM conditioning.\"\"\"\n\n    def __init__(self, in_channels, out_channels, time_dim):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n\n        # FiLM parameters for each conv layer\n        self.film1 = nn.Linear(time_dim, 2 * out_channels)\n        self.film2 = nn.Linear(time_dim, 2 * out_channels)\n\n        self.act = nn.SiLU()\n\n    def forward(self, x, time_emb):\n        \"\"\"\n        Args:\n            x: Image features [batch, in_channels, H, W]\n            time_emb: Time embedding [batch, time_dim]\n\n        Returns:\n            features: [batch, out_channels, H, W]\n        \"\"\"\n        # First conv\n        h = self.conv1(x)\n\n        # FiLM conditioning\n        scale, shift = torch.chunk(self.film1(time_emb), 2, dim=1)\n        # Reshape for broadcasting: [batch, out_channels, 1, 1]\n        scale = scale[:, :, None, None]\n        shift = shift[:, :, None, None]\n        h = self.act(scale * h + shift)\n\n        # Second conv\n        h = self.conv2(h)\n        scale, shift = torch.chunk(self.film2(time_emb), 2, dim=1)\n        scale = scale[:, :, None, None]\n        shift = shift[:, :, None, None]\n        h = self.act(scale * h + shift)\n\n        return h\n</code></pre> <p>Note: For convolutional layers, we reshape the scale/shift to <code>[batch, channels, 1, 1]</code> to broadcast across spatial dimensions.</p>"},{"location":"diffusion/score_network/time_embedding_and_film/#film-vs-other-conditioning-methods","title":"FiLM vs. Other Conditioning Methods","text":""},{"location":"diffusion/score_network/time_embedding_and_film/#comparison-table","title":"Comparison Table","text":"Method Description Pros Cons Concatenation Concatenate time embedding with input Simple, always works Only affects input; weak conditioning Additive Add time embedding to features: \\(h + \\gamma\\) Simple Limited expressiveness; no scaling FiLM Affine transform: \\(\\gamma_s \\odot h + \\gamma_b\\) Strong conditioning; layer-wise control More parameters Attention Cross-attention between features and time Very expressive Computationally expensive Adaptive Group Norm Normalize then apply FiLM Combines normalization benefits Requires batch statistics"},{"location":"diffusion/score_network/time_embedding_and_film/#when-to-use-film","title":"When to Use FiLM","text":"<p>FiLM is the standard choice for diffusion models because: 1. Strong conditioning: Both multiplicative and additive 2. Efficient: Linear projection, no complex operations 3. Proven effectiveness: Used in DDPM, Stable Diffusion, etc. 4. Layer-wise control: Can adapt behavior at every layer</p>"},{"location":"diffusion/score_network/time_embedding_and_film/#putting-it-all-together-time-conditioning-pipeline","title":"Putting It All Together: Time Conditioning Pipeline","text":"<p>Here's the complete flow of how time information flows through a score network:</p> <pre><code>Input: (x, t)\n    \u2193\n1. Time Embedding: t \u2192 \u03b3(t) \u2208 \u211d^d\n   - Transform scalar to high-dimensional representation\n   - Use sinusoidal functions for multiple frequencies\n    \u2193\n2. Feature Processing: x \u2192 h\n   - Pass data through network layers\n   - Extract features\n    \u2193\n3. FiLM Conditioning: h, \u03b3(t) \u2192 h'\n   - Compute scale and shift from \u03b3(t)\n   - Modulate features: h' = scale \u2299 h + shift\n   - Apply at multiple layers\n    \u2193\n4. Output: h' \u2192 \u2207_x log p_t(x)\n   - Final linear layer\n   - Produces score estimate\n</code></pre>"},{"location":"diffusion/score_network/time_embedding_and_film/#why-this-works","title":"Why This Works","text":"<p>Time Embedding gives the network a rich representation of when we are in the diffusion process: - Early times (\\(t \\approx 0\\)): Low noise, need to model fine details - Late times (\\(t \\approx 1\\)): High noise, model only coarse structure</p> <p>FiLM allows the network to adapt its processing based on time: - Different layers can learn different time-dependent behaviors - Multiplicative control allows complete feature modulation - The network learns to \"turn on/off\" different feature detectors based on noise level</p>"},{"location":"diffusion/score_network/time_embedding_and_film/#advanced-topics","title":"Advanced Topics","text":""},{"location":"diffusion/score_network/time_embedding_and_film/#1-adaptive-group-normalization-adagn","title":"1. Adaptive Group Normalization (AdaGN)","text":"<p>Combines Group Normalization with FiLM:</p> \\[ \\text{AdaGN}(h | \\gamma) = \\gamma_s \\odot \\frac{h - \\mu}{\\sigma} + \\gamma_b \\] <p>where \\(\\mu\\), \\(\\sigma\\) are computed per group of channels.</p> <p>Benefits: </p> <ul> <li>Normalization helps training stability</li> <li>FiLM provides conditioning</li> <li>Used in Stable Diffusion</li> </ul>"},{"location":"diffusion/score_network/time_embedding_and_film/#2-time-dependent-skip-connections","title":"2. Time-dependent Skip Connections","text":"<p>In U-Net architectures, skip connections can also be modulated:</p> <pre><code># Standard skip connection\nh = h_down + h_up\n\n# Time-modulated skip connection\nscale, shift = compute_film(time_emb)\nh = scale * h_down + h_up + shift\n</code></pre>"},{"location":"diffusion/score_network/time_embedding_and_film/#3-multi-scale-time-embeddings","title":"3. Multi-scale Time Embeddings","text":"<p>For hierarchical models, different scales can use different time embeddings:</p> <pre><code># Coarse scale: low frequencies\nt_emb_coarse = time_embedding(t, max_freq=100)\n\n# Fine scale: high frequencies  \nt_emb_fine = time_embedding(t, max_freq=10000)\n</code></pre>"},{"location":"diffusion/score_network/time_embedding_and_film/#practical-tips","title":"Practical Tips","text":""},{"location":"diffusion/score_network/time_embedding_and_film/#1-time-embedding-dimension","title":"1. Time Embedding Dimension","text":"<p>Typical choices: 32, 64, 128, 256</p> <p>Rule of thumb: </p> <ul> <li>Simple tasks (2D toy data): 32-64</li> <li>Image generation: 128-256</li> <li>Higher dimensions give more expressiveness but add parameters</li> </ul>"},{"location":"diffusion/score_network/time_embedding_and_film/#2-where-to-apply-film","title":"2. Where to Apply FiLM","text":"<p>Common patterns:</p> <ul> <li>After each convolution/linear layer</li> <li>Before activation functions</li> <li>Not on the final output layer (usually)</li> </ul>"},{"location":"diffusion/score_network/time_embedding_and_film/#3-initialization","title":"3. Initialization","text":"<p>Initialize FiLM layers to output \\((1, 0)\\) initially: <pre><code># Initialize to identity transformation\nself.film.weight.data.zero_()\nself.film.bias.data.copy_(torch.tensor([1.0] * hidden_dim + [0.0] * hidden_dim))\n</code></pre></p> <p>This makes training more stable initially.</p>"},{"location":"diffusion/score_network/time_embedding_and_film/#4-debugging-time-conditioning","title":"4. Debugging Time Conditioning","text":"<p>To verify time conditioning is working: <pre><code># Test: Does output change with time?\nx = torch.randn(1, 2)\nt1 = torch.tensor([0.1])\nt2 = torch.tensor([0.9])\n\nout1 = model(x, t1)\nout2 = model(x, t2)\n\nprint(f\"Output difference: {(out1 - out2).abs().mean()}\")\n# Should be substantial (&gt;0.1 typically)\n</code></pre></p>"},{"location":"diffusion/score_network/time_embedding_and_film/#summary","title":"Summary","text":""},{"location":"diffusion/score_network/time_embedding_and_film/#time-embedding_1","title":"Time Embedding","text":"<ul> <li>Purpose: Transform scalar time to high-dimensional representation</li> <li>Method: Sinusoidal functions at multiple frequencies</li> <li>Benefit: Allows network to represent complex time-dependent behavior</li> </ul>"},{"location":"diffusion/score_network/time_embedding_and_film/#film","title":"FiLM","text":"<ul> <li>Purpose: Condition network features on time</li> <li>Method: Affine transformation (scale + shift)</li> <li>Benefit: Strong, layer-wise adaptation to time</li> </ul>"},{"location":"diffusion/score_network/time_embedding_and_film/#together","title":"Together","text":"<p>They form the backbone of modern score network architectures, enabling the network to accurately estimate \\(\\nabla_x \\log p_t(x)\\) across all noise levels.</p>"},{"location":"diffusion/score_network/time_embedding_and_film/#further-reading","title":"Further Reading","text":"<ul> <li>Original FiLM Paper: Perez et al., \"FiLM: Visual Reasoning with a General Conditioning Layer\" (2018)</li> <li>Diffusion Models: Ho et al., \"Denoising Diffusion Probabilistic Models\" (2020)</li> <li>Transformer Positional Encoding: Vaswani et al., \"Attention Is All You Need\" (2017) - similar sinusoidal embedding idea</li> <li>U-Net with Time Conditioning: Rombach et al., \"High-Resolution Image Synthesis with Latent Diffusion Models\" (2022)</li> </ul>"},{"location":"diffusion/score_network/time_embedding_and_film/#back-references","title":"Back References","text":"<ul> <li>Where this is used: <code>notebooks/diffusion/02_sde_formulation/02_sde_formulation.ipynb</code></li> <li>Related concepts: </li> <li>Score Network Architecture</li> <li>Forward Process Derivation</li> <li>Training Loss and Denoising</li> <li>Related topic: Numerical Embeddings and Continuous Values - Explores how similar sinusoidal embedding techniques are used for numerical values in LLMs and their relevance to time embedding</li> </ul>"},{"location":"eval/00_evaluating_generative_models/","title":"Evaluating Generative Models: A Comprehensive Guide","text":"<p>Evaluating generative models is fundamentally challenging because there is no single \"ground truth\" metric. Different evaluation approaches capture different aspects of model quality, and the choice of metrics depends on the application domain and goals.</p> <p>This document provides a comprehensive overview of evaluation methods for generative models, with particular focus on diffusion models and biological data applications.</p>"},{"location":"eval/00_evaluating_generative_models/#the-evaluation-challenge","title":"The Evaluation Challenge","text":""},{"location":"eval/00_evaluating_generative_models/#why-evaluation-is-hard","title":"Why Evaluation is Hard","text":"<p>Generative models face unique evaluation challenges:</p> <ol> <li>No single ground truth: Unlike supervised learning, there's no single correct output</li> <li>Multiple objectives: Quality, diversity, efficiency, and utility often trade off</li> <li>Domain-specific requirements: What matters for images differs from what matters for gene expression</li> <li>Computational constraints: Some metrics are expensive to compute</li> <li>Perceptual vs. statistical: Human perception doesn't always align with statistical measures</li> </ol>"},{"location":"eval/00_evaluating_generative_models/#key-questions","title":"Key Questions","text":"<p>When evaluating a generative model, ask:</p> <ul> <li>Quality: Are individual samples realistic?</li> <li>Diversity: Does the model cover the full data distribution?</li> <li>Fidelity: Does the model match the training distribution?</li> <li>Utility: Is the generated data useful for downstream tasks?</li> <li>Efficiency: How much compute is required for training and sampling?</li> <li>Controllability: Can we guide generation toward desired outputs?</li> </ul>"},{"location":"eval/00_evaluating_generative_models/#evaluation-metrics-by-category","title":"Evaluation Metrics by Category","text":""},{"location":"eval/00_evaluating_generative_models/#1-sample-quality-metrics","title":"1. Sample Quality Metrics","text":""},{"location":"eval/00_evaluating_generative_models/#inception-score-is","title":"Inception Score (IS)","text":"<p>What it measures: Quality and diversity of generated images</p> \\[ \\text{IS} = \\exp\\left(\\mathbb{E}_{x \\sim p_g} \\left[D_{KL}(p(y|x) \\| p(y))\\right]\\right) \\] <p>where \\(p(y|x)\\) is a pre-trained classifier's prediction.</p> <p>Interpretation:</p> <ul> <li>Higher is better</li> <li>Good samples should have confident class predictions (low \\(p(y|x)\\) entropy)</li> <li>Diverse samples should cover many classes (high \\(p(y)\\) entropy)</li> </ul> <p>Limitations:</p> <ul> <li>Requires pre-trained classifier (typically ImageNet-trained Inception network)</li> <li>Doesn't detect overfitting or mode collapse well</li> <li>Not applicable to non-image domains</li> </ul> <p>Typical values:</p> <ul> <li>Random noise: ~1</li> <li>CIFAR-10 real data: ~11.2</li> <li>Good generative models: 8-10</li> </ul>"},{"location":"eval/00_evaluating_generative_models/#frechet-inception-distance-fid","title":"Fr\u00e9chet Inception Distance (FID)","text":"<p>What it measures: Distance between real and generated data distributions in feature space</p> \\[ \\text{FID} = \\|\\mu_r - \\mu_g\\|^2 + \\text{Tr}(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\Sigma_g)^{1/2}) \\] <p>where \\(\\mu_r, \\Sigma_r\\) are mean and covariance of real data features, and \\(\\mu_g, \\Sigma_g\\) for generated data.</p> <p>Interpretation:</p> <ul> <li>Lower is better</li> <li>Measures both quality and diversity</li> <li>More robust than IS</li> </ul> <p>Limitations:</p> <ul> <li>Requires pre-trained feature extractor</li> <li>Sensitive to sample size (need 10k+ samples)</li> <li>Assumes Gaussian distributions in feature space</li> </ul> <p>Typical values:</p> <ul> <li>Perfect match: 0</li> <li>Excellent: &lt;10</li> <li>Good: 10-30</li> <li>Poor: &gt;100</li> </ul>"},{"location":"eval/00_evaluating_generative_models/#precision-and-recall","title":"Precision and Recall","text":"<p>What it measures: Quality vs. diversity trade-off</p> <ul> <li>Precision: Fraction of generated samples that are realistic</li> <li>Recall: Fraction of real distribution covered by generated samples</li> </ul> <p>Interpretation:</p> <ul> <li>Precision measures quality (are generated samples realistic?)</li> <li>Recall measures diversity (does the model cover the full distribution?)</li> <li>High precision, low recall = mode collapse</li> <li>Low precision, high recall = poor quality but diverse</li> </ul> <p>Advantages:</p> <ul> <li>Separates quality and diversity</li> <li>More interpretable than FID</li> <li>Detects mode collapse</li> </ul>"},{"location":"eval/00_evaluating_generative_models/#2-likelihood-based-metrics","title":"2. Likelihood-Based Metrics","text":""},{"location":"eval/00_evaluating_generative_models/#negative-log-likelihood-nll","title":"Negative Log-Likelihood (NLL)","text":"<p>What it measures: How well the model assigns probability to real data</p> \\[ \\text{NLL} = -\\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log p_\\theta(x)] \\] <p>Interpretation:</p> <ul> <li>Lower is better</li> <li>Directly measures distributional fit</li> <li>Theoretically principled</li> </ul> <p>Limitations:</p> <ul> <li>Requires tractable likelihood (not available for GANs)</li> <li>Can be dominated by imperceptible details</li> <li>Doesn't measure perceptual quality</li> </ul>"},{"location":"eval/00_evaluating_generative_models/#evidence-lower-bound-elbo","title":"Evidence Lower Bound (ELBO)","text":"<p>What it measures: Lower bound on log-likelihood (for VAEs, diffusion models)</p> \\[ \\text{ELBO} = \\mathbb{E}_{q(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q(z|x) \\| p(z)) \\] <p>Interpretation:</p> <ul> <li>Higher is better</li> <li>Balances reconstruction and regularization</li> <li>Tractable for many models</li> </ul> <p>Limitations:</p> <ul> <li>Only a lower bound (gap to true likelihood unknown)</li> <li>May not correlate with sample quality</li> </ul>"},{"location":"eval/00_evaluating_generative_models/#3-perceptual-metrics","title":"3. Perceptual Metrics","text":""},{"location":"eval/00_evaluating_generative_models/#learned-perceptual-image-patch-similarity-lpips","title":"Learned Perceptual Image Patch Similarity (LPIPS)","text":"<p>What it measures: Perceptual distance between images using deep features</p> \\[ \\text{LPIPS}(x, x') = \\sum_l w_l \\|f_l(x) - f_l(x')\\|^2 \\] <p>where \\(f_l\\) are features from layer \\(l\\) of a pre-trained network.</p> <p>Interpretation:</p> <ul> <li>Lower is better</li> <li>Correlates better with human perception than pixel-wise metrics</li> <li>Useful for image-to-image tasks</li> </ul> <p>Advantages:</p> <ul> <li>Captures perceptual similarity</li> <li>More robust than MSE or SSIM</li> </ul> <p>Limitations:</p> <ul> <li>Requires pre-trained network</li> <li>Domain-specific (primarily images)</li> </ul>"},{"location":"eval/00_evaluating_generative_models/#4-domain-specific-metrics","title":"4. Domain-Specific Metrics","text":""},{"location":"eval/00_evaluating_generative_models/#for-images","title":"For Images","text":"<ul> <li>SSIM: Structural similarity</li> <li>PSNR: Peak signal-to-noise ratio</li> <li>Human evaluation: Perceptual studies</li> </ul>"},{"location":"eval/00_evaluating_generative_models/#for-text","title":"For Text","text":"<ul> <li>BLEU: N-gram overlap with references</li> <li>Perplexity: Language model likelihood</li> <li>Human evaluation: Fluency, coherence</li> </ul>"},{"location":"eval/00_evaluating_generative_models/#for-biological-data","title":"For Biological Data","text":"<ul> <li>Correlation with real data: Gene-gene correlations</li> <li>Pathway enrichment: Biological pathway preservation</li> <li>Cell type classification: Downstream task performance</li> <li>Perturbation response: Causal structure preservation</li> </ul>"},{"location":"eval/00_evaluating_generative_models/#evaluating-diffusion-models-specifically","title":"Evaluating Diffusion Models Specifically","text":""},{"location":"eval/00_evaluating_generative_models/#standard-diffusion-model-metrics","title":"Standard Diffusion Model Metrics","text":""},{"location":"eval/00_evaluating_generative_models/#1-sample-quality","title":"1. Sample Quality","text":"<ul> <li>FID: Primary metric for image diffusion models</li> <li>IS: Secondary metric</li> <li>Precision/Recall: Trade-off analysis</li> </ul>"},{"location":"eval/00_evaluating_generative_models/#2-likelihood","title":"2. Likelihood","text":"<ul> <li>ELBO: Variational bound</li> <li>Bits per dimension (BPD): Normalized likelihood   $$   \\text{BPD} = -\\frac{\\log_2 p(x)}{D}   $$</li> </ul> <p>where \\(D\\) is data dimensionality</p>"},{"location":"eval/00_evaluating_generative_models/#3-sampling-efficiency","title":"3. Sampling Efficiency","text":"<ul> <li>NFE (Number of Function Evaluations): How many denoising steps?</li> <li>Wall-clock time: Actual sampling time</li> <li>FID vs. NFE trade-off: Quality-speed curve</li> </ul> <p>Typical trade-offs:</p> <ul> <li>DDPM: 1000 steps, high quality</li> <li>DDIM: 50-100 steps, good quality</li> <li>Fast samplers: 10-20 steps, acceptable quality</li> </ul>"},{"location":"eval/00_evaluating_generative_models/#4-training-efficiency","title":"4. Training Efficiency","text":"<ul> <li>Training time: Hours/days to convergence</li> <li>Compute cost: GPU-hours</li> <li>Sample efficiency: Performance vs. dataset size</li> </ul>"},{"location":"eval/00_evaluating_generative_models/#diffusion-specific-considerations","title":"Diffusion-Specific Considerations","text":""},{"location":"eval/00_evaluating_generative_models/#noise-schedule-evaluation","title":"Noise Schedule Evaluation","text":"<ul> <li>Loss curve shape: Progressive learning vs. flat</li> <li>Timestep importance: Which timesteps matter most?</li> <li>Schedule ablations: Linear vs. cosine vs. learned</li> </ul>"},{"location":"eval/00_evaluating_generative_models/#guidance-strength","title":"Guidance Strength","text":"<ul> <li>FID vs. guidance scale: Quality-diversity trade-off</li> <li>Classifier-free guidance: Typical range \\(w \\in [1, 10]\\)</li> <li>Optimal guidance: Task-dependent</li> </ul>"},{"location":"eval/00_evaluating_generative_models/#comparing-modeling-approaches","title":"Comparing Modeling Approaches","text":""},{"location":"eval/00_evaluating_generative_models/#how-to-determine-better-or-worse","title":"How to Determine \"Better\" or \"Worse\"","text":""},{"location":"eval/00_evaluating_generative_models/#1-define-your-objectives","title":"1. Define Your Objectives","text":"<p>For research:</p> <ul> <li>State-of-the-art sample quality (FID)</li> <li>Theoretical contributions (likelihood bounds)</li> <li>Novel capabilities (controllability, efficiency)</li> </ul> <p>For applications:</p> <ul> <li>Downstream task performance</li> <li>Computational efficiency</li> <li>Robustness and reliability</li> </ul>"},{"location":"eval/00_evaluating_generative_models/#2-multi-metric-evaluation","title":"2. Multi-Metric Evaluation","text":"<p>Never rely on a single metric. Use a suite of complementary metrics:</p> Aspect Metrics Why Quality FID, IS, Precision Are samples realistic? Diversity Recall, Mode coverage Full distribution? Fidelity Likelihood, ELBO Statistical match? Utility Downstream tasks Practical value? Efficiency NFE, Training time Computational cost?"},{"location":"eval/00_evaluating_generative_models/#3-ablation-studies","title":"3. Ablation Studies","text":"<p>Systematically vary one component at a time: - Architecture (U-Net vs. Transformer) - Noise schedule (linear vs. cosine) - Training procedure (batch size, learning rate) - Sampling method (DDPM vs. DDIM)</p> <p>Example ablation: <pre><code>Baseline: FID = 15.2\n+ Cosine schedule: FID = 12.8 (improvement)\n+ Larger batch: FID = 11.5 (improvement)\n+ EMA: FID = 10.9 (improvement)\n</code></pre></p>"},{"location":"eval/00_evaluating_generative_models/#4-statistical-significance","title":"4. Statistical Significance","text":"<p>Best practices:</p> <ul> <li>Multiple random seeds (at least 3-5)</li> <li>Report mean \u00b1 standard deviation</li> <li>Statistical tests (t-test, bootstrap)</li> <li>Confidence intervals</li> </ul> <p>Example: <pre><code>Model A: FID = 12.3 \u00b1 0.5\nModel B: FID = 11.8 \u00b1 0.6\nDifference: 0.5 (not significant, p=0.15)\n</code></pre></p>"},{"location":"eval/00_evaluating_generative_models/#5-computational-budget","title":"5. Computational Budget","text":"<p>Report full costs:</p> <ul> <li>Training: GPU-hours, wall-clock time</li> <li>Sampling: Seconds per sample, NFE</li> <li>Memory: Peak GPU memory</li> <li>Total cost: Dollar cost estimate</li> </ul> <p>Example comparison: <pre><code>Model A: FID=10, 100 GPU-hours training, 1s/sample\nModel B: FID=12, 10 GPU-hours training, 0.1s/sample\n\u2192 Model B may be better for deployment\n</code></pre></p>"},{"location":"eval/00_evaluating_generative_models/#evaluation-for-biological-data","title":"Evaluation for Biological Data","text":""},{"location":"eval/00_evaluating_generative_models/#unique-challenges","title":"Unique Challenges","text":"<ol> <li>High dimensionality: Thousands of genes</li> <li>Sparse structure: Many zeros (scRNA-seq)</li> <li>Biological constraints: Pathway structure, regulatory networks</li> <li>Limited ground truth: No \"correct\" cell state</li> <li>Batch effects: Technical variability</li> </ol>"},{"location":"eval/00_evaluating_generative_models/#recommended-metrics","title":"Recommended Metrics","text":""},{"location":"eval/00_evaluating_generative_models/#statistical-fidelity","title":"Statistical Fidelity","text":"<ul> <li>Marginal distributions: Per-gene statistics</li> <li>Correlation structure: Gene-gene correlations</li> <li>Higher-order moments: Skewness, kurtosis</li> </ul>"},{"location":"eval/00_evaluating_generative_models/#biological-validity","title":"Biological Validity","text":"<ul> <li>Pathway enrichment: Gene set enrichment analysis (GSEA)</li> <li>Cell type markers: Known marker gene expression</li> <li>Regulatory networks: Transcription factor activity</li> <li>Developmental trajectories: Pseudotime consistency</li> </ul>"},{"location":"eval/00_evaluating_generative_models/#downstream-utility","title":"Downstream Utility","text":"<ul> <li>Cell type classification: Train classifier on synthetic, test on real</li> <li>Perturbation prediction: Drug response accuracy</li> <li>Batch correction: Integration with real data</li> <li>Data augmentation: Improved downstream task performance</li> </ul>"},{"location":"eval/00_evaluating_generative_models/#epiplexity-based-evaluation-new","title":"Epiplexity-Based Evaluation (NEW)","text":"<p>Setup: 1. Train observer model on real data 2. Train observer model on synthetic data 3. Compare loss trajectories</p> <p>Metrics:</p> \\[ \\text{Epiplexity proxy} = \\int_0^T (L(t) - L_\\infty) dt \\] <p>Interpretation:</p> <ul> <li>High epiplexity \u2192 synthetic data teaches biological structure</li> <li>Low epiplexity \u2192 synthetic data is noise-matching without structure</li> </ul> <p>Advantages:</p> <ul> <li>Detects whether synthetic data contains learnable biology</li> <li>Goes beyond statistical matching</li> <li>Measures utility for downstream learning</li> </ul>"},{"location":"eval/00_evaluating_generative_models/#practical-evaluation-workflow","title":"Practical Evaluation Workflow","text":""},{"location":"eval/00_evaluating_generative_models/#step-1-baseline-metrics","title":"Step 1: Baseline Metrics","text":"<p>Compute standard metrics for quick assessment: - FID (if applicable) - Likelihood/ELBO - Visual inspection (if images)</p>"},{"location":"eval/00_evaluating_generative_models/#step-2-comprehensive-evaluation","title":"Step 2: Comprehensive Evaluation","text":"<p>If baseline looks promising: - Precision/Recall - Multiple sample sizes - Ablation studies - Statistical significance tests</p>"},{"location":"eval/00_evaluating_generative_models/#step-3-domain-specific-validation","title":"Step 3: Domain-Specific Validation","text":"<p>For your specific application: - Downstream task performance - Expert evaluation - Biological validation (for bio data) - Epiplexity analysis</p>"},{"location":"eval/00_evaluating_generative_models/#step-4-efficiency-analysis","title":"Step 4: Efficiency Analysis","text":"<p>Practical deployment considerations: - Training cost - Sampling speed - Memory requirements - Scalability</p>"},{"location":"eval/00_evaluating_generative_models/#step-5-reporting","title":"Step 5: Reporting","text":"<p>Minimum reporting standards:</p> <ul> <li>Multiple metrics (quality, diversity, efficiency)</li> <li>Statistical significance (multiple seeds)</li> <li>Computational costs</li> <li>Failure cases and limitations</li> <li>Reproducibility information (code, hyperparameters)</li> </ul>"},{"location":"eval/00_evaluating_generative_models/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"eval/00_evaluating_generative_models/#1-single-metric-optimization","title":"1. Single-Metric Optimization","text":"<p>Problem: Optimizing only FID can hurt diversity Solution: Use multiple complementary metrics</p>"},{"location":"eval/00_evaluating_generative_models/#2-cherry-picking-samples","title":"2. Cherry-Picking Samples","text":"<p>Problem: Showing only best samples Solution: Report aggregate statistics, show random samples</p>"},{"location":"eval/00_evaluating_generative_models/#3-ignoring-computational-cost","title":"3. Ignoring Computational Cost","text":"<p>Problem: Achieving SOTA quality with 10x compute Solution: Report efficiency metrics, Pareto frontiers</p>"},{"location":"eval/00_evaluating_generative_models/#4-overfitting-to-validation-set","title":"4. Overfitting to Validation Set","text":"<p>Problem: Tuning hyperparameters on test set Solution: Proper train/val/test splits, hold-out evaluation</p>"},{"location":"eval/00_evaluating_generative_models/#5-ignoring-statistical-significance","title":"5. Ignoring Statistical Significance","text":"<p>Problem: Claiming improvement from noise Solution: Multiple seeds, significance tests</p>"},{"location":"eval/00_evaluating_generative_models/#6-domain-mismatch","title":"6. Domain Mismatch","text":"<p>Problem: Using ImageNet-trained metrics for non-image data Solution: Domain-appropriate metrics, custom evaluations</p>"},{"location":"eval/00_evaluating_generative_models/#decision-framework","title":"Decision Framework","text":""},{"location":"eval/00_evaluating_generative_models/#when-is-model-a-better-than-model-b","title":"When is Model A \"Better\" than Model B?","text":"<p>Model A is strictly better if:</p> <ul> <li>Better on all metrics</li> <li>Same computational cost</li> <li>No significant trade-offs</li> </ul> <p>Model A is better for research if:</p> <ul> <li>Significantly better on primary metric (e.g., FID)</li> <li>Novel capabilities or insights</li> <li>Reasonable computational cost</li> </ul> <p>Model A is better for deployment if:</p> <ul> <li>Sufficient quality for application</li> <li>Much more efficient</li> <li>More robust and reliable</li> </ul> <p>Model A is better for your specific use case if:</p> <ul> <li>Better on task-specific metrics</li> <li>Better downstream performance</li> <li>Fits your computational budget</li> </ul>"},{"location":"eval/00_evaluating_generative_models/#summary","title":"Summary","text":""},{"location":"eval/00_evaluating_generative_models/#key-principles","title":"Key Principles","text":"<ol> <li>No single metric: Use multiple complementary metrics</li> <li>Define objectives: Research vs. application goals differ</li> <li>Statistical rigor: Multiple seeds, significance tests</li> <li>Report costs: Computational efficiency matters</li> <li>Domain-specific: Adapt metrics to your data type</li> <li>Utility matters: Downstream task performance is crucial</li> </ol>"},{"location":"eval/00_evaluating_generative_models/#recommended-metric-suites","title":"Recommended Metric Suites","text":"<p>For image diffusion models:</p> <ul> <li>FID (primary)</li> <li>Precision/Recall (quality-diversity)</li> <li>Sampling efficiency (NFE, time)</li> <li>Human evaluation (if budget allows)</li> </ul> <p>For biological data:</p> <ul> <li>Statistical fidelity (correlations, distributions)</li> <li>Biological validity (pathways, markers)</li> <li>Downstream utility (classification, prediction)</li> <li>Epiplexity (learnable structure)</li> </ul> <p>For general generative models:</p> <ul> <li>Quality metric (FID, IS, or domain-specific)</li> <li>Diversity metric (Recall, mode coverage)</li> <li>Likelihood (if tractable)</li> <li>Efficiency (training and sampling cost)</li> <li>Task performance (downstream applications)</li> </ul>"},{"location":"eval/00_evaluating_generative_models/#the-bottom-line","title":"The Bottom Line","text":"<p>A model is \"better\" when it achieves your specific objectives more effectively, considering quality, diversity, efficiency, and utility\u2014validated through rigorous multi-metric evaluation.</p>"},{"location":"eval/00_evaluating_generative_models/#related-documents","title":"Related Documents","text":"<ul> <li>Epiplexity: From Entropy to Epiplexity \u2014 Observer-dependent information</li> <li>Why Random Noise Has High Entropy but Low Epiplexity \u2014 Structure vs. randomness</li> <li>DDPM Foundations \u2014 Diffusion model theory</li> <li>DDPM Training \u2014 Training considerations</li> <li>DDPM Sampling \u2014 Sampling methods and efficiency</li> </ul>"},{"location":"eval/00_evaluating_generative_models/#references","title":"References","text":""},{"location":"eval/00_evaluating_generative_models/#evaluation-metrics","title":"Evaluation Metrics","text":"<ol> <li>Salimans, T., et al. (2016). Improved Techniques for Training GANs. NeurIPS. (Inception Score)</li> <li>Heusel, M., et al. (2017). GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. NeurIPS. (FID)</li> <li>Kynk\u00e4\u00e4nniemi, T., et al. (2019). Improved Precision and Recall Metric for Assessing Generative Models. NeurIPS.</li> <li>Zhang, R., et al. (2018). The Unreasonable Effectiveness of Deep Features as a Perceptual Metric. CVPR. (LPIPS)</li> </ol>"},{"location":"eval/00_evaluating_generative_models/#diffusion-model-evaluation","title":"Diffusion Model Evaluation","text":"<ol> <li>Ho, J., et al. (2020). Denoising Diffusion Probabilistic Models. NeurIPS.</li> <li>Song, J., et al. (2021). Denoising Diffusion Implicit Models. ICLR.</li> <li>Dhariwal, P., &amp; Nichol, A. (2021). Diffusion Models Beat GANs on Image Synthesis. NeurIPS.</li> </ol>"},{"location":"eval/00_evaluating_generative_models/#epiplexity","title":"Epiplexity","text":"<ol> <li>From Entropy to Epiplexity (2024). arXiv:2601.03220</li> </ol>"},{"location":"eval/00_evaluating_generative_models/#best-practices","title":"Best Practices","text":"<ol> <li>Borji, A. (2019). Pros and Cons of GAN Evaluation Measures. Computer Vision and Image Understanding.</li> <li>Theis, L., et al. (2016). A Note on the Evaluation of Generative Models. ICLR.</li> </ol>"},{"location":"eval/fid_frechet_inception_distance/","title":"Fr\u00e9chet Inception Distance (FID): Theory and Practice","text":"<p>Fr\u00e9chet Inception Distance (FID) is the most widely used metric for evaluating generative models, particularly for image generation. It measures the distance between the distribution of generated samples and real samples in a learned feature space.</p> <p>This document explains the mathematical foundations, practical computation, interpretation, and limitations of FID.</p>"},{"location":"eval/fid_frechet_inception_distance/#overview","title":"Overview","text":""},{"location":"eval/fid_frechet_inception_distance/#what-fid-measures","title":"What FID Measures","text":"<p>FID quantifies how similar two distributions are by comparing their statistics in a high-dimensional feature space:</p> <ul> <li>Real distribution: Features extracted from real training data</li> <li>Generated distribution: Features extracted from model-generated samples</li> </ul> <p>Key insight: Rather than comparing raw pixels, FID compares distributions in a semantically meaningful feature space learned by a pre-trained neural network.</p>"},{"location":"eval/fid_frechet_inception_distance/#why-fid-is-popular","title":"Why FID is Popular","text":"<ol> <li>Captures both quality and diversity: Unlike metrics that measure only one aspect</li> <li>Correlates with human judgment: Better than earlier metrics like Inception Score</li> <li>Robust and stable: Less sensitive to small sample variations than IS</li> <li>Widely adopted: Standard benchmark across papers and models</li> <li>Single scalar: Easy to compare models</li> </ol>"},{"location":"eval/fid_frechet_inception_distance/#mathematical-foundation","title":"Mathematical Foundation","text":""},{"location":"eval/fid_frechet_inception_distance/#the-frechet-distance","title":"The Fr\u00e9chet Distance","text":"<p>FID is based on the Fr\u00e9chet distance (also called Wasserstein-2 distance) between two multivariate Gaussian distributions.</p> <p>Given two Gaussian distributions: - \\(\\mathcal{N}(\\mu_r, \\Sigma_r)\\) for real data - \\(\\mathcal{N}(\\mu_g, \\Sigma_g)\\) for generated data</p> <p>The Fr\u00e9chet distance is:</p> \\[ d^2(\\mathcal{N}(\\mu_r, \\Sigma_r), \\mathcal{N}(\\mu_g, \\Sigma_g)) = \\|\\mu_r - \\mu_g\\|^2 + \\text{Tr}(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\Sigma_g)^{1/2}) \\] <p>where:</p> <ul> <li>\\(\\mu_r, \\mu_g \\in \\mathbb{R}^d\\) are mean vectors</li> <li>\\(\\Sigma_r, \\Sigma_g \\in \\mathbb{R}^{d \\times d}\\) are covariance matrices</li> <li>\\(\\text{Tr}(\\cdot)\\) is the matrix trace</li> <li>\\((\\Sigma_r \\Sigma_g)^{1/2}\\) is the matrix square root of the product</li> </ul>"},{"location":"eval/fid_frechet_inception_distance/#intuition","title":"Intuition","text":"<p>The FID formula has two terms:</p> <p>1. Mean difference: \\(\\|\\mu_r - \\mu_g\\|^2\\) - Measures whether generated samples have the same \"center\" as real samples - Captures whether the model generates the right kind of content on average</p> <p>2. Covariance difference: \\(\\text{Tr}(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\Sigma_g)^{1/2})\\) - Measures whether generated samples have the same \"spread\" and correlations as real samples - Captures diversity and feature relationships</p> <p>Lower FID = Better: Smaller distance means distributions are more similar.</p>"},{"location":"eval/fid_frechet_inception_distance/#computation-pipeline","title":"Computation Pipeline","text":""},{"location":"eval/fid_frechet_inception_distance/#step-1-feature-extraction","title":"Step 1: Feature Extraction","text":"<p>Extract features from a pre-trained network (typically Inception-v3):</p> <ol> <li>Load pre-trained Inception-v3: Trained on ImageNet</li> <li>Extract features: Use the final pooling layer (before classification)</li> <li>Output: 2048-dimensional feature vector per image</li> <li>Process all images:</li> <li>Real images: \\(\\{x_1^r, x_2^r, \\ldots, x_N^r\\}\\)</li> <li>Generated images: \\(\\{x_1^g, x_2^g, \\ldots, x_M^g\\}\\)</li> </ol> <p>Feature extraction:</p> \\[ f_i^r = \\text{Inception}(x_i^r) \\in \\mathbb{R}^{2048} \\] \\[ f_j^g = \\text{Inception}(x_j^g) \\in \\mathbb{R}^{2048} \\]"},{"location":"eval/fid_frechet_inception_distance/#step-2-compute-statistics","title":"Step 2: Compute Statistics","text":"<p>Calculate mean and covariance for both distributions:</p> <p>Real data statistics:</p> \\[ \\mu_r = \\frac{1}{N} \\sum_{i=1}^N f_i^r \\] \\[ \\Sigma_r = \\frac{1}{N-1} \\sum_{i=1}^N (f_i^r - \\mu_r)(f_i^r - \\mu_r)^T \\] <p>Generated data statistics:</p> \\[ \\mu_g = \\frac{1}{M} \\sum_{j=1}^M f_j^g \\] \\[ \\Sigma_g = \\frac{1}{M-1} \\sum_{j=1}^M (f_j^g - \\mu_g)(f_j^g - \\mu_g)^T \\]"},{"location":"eval/fid_frechet_inception_distance/#step-3-compute-fid","title":"Step 3: Compute FID","text":"<p>Calculate the Fr\u00e9chet distance:</p> \\[ \\text{FID} = \\|\\mu_r - \\mu_g\\|^2 + \\text{Tr}(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\Sigma_g)^{1/2}) \\] <p>Matrix square root computation:</p> <p>The term \\((\\Sigma_r \\Sigma_g)^{1/2}\\) requires computing the matrix square root:</p> <ol> <li>Compute eigendecomposition: \\(\\Sigma_r \\Sigma_g = Q \\Lambda Q^T\\)</li> <li>Take square root: \\((\\Sigma_r \\Sigma_g)^{1/2} = Q \\Lambda^{1/2} Q^T\\)</li> </ol> <p>Numerical stability: Add small epsilon to diagonal for numerical stability:</p> \\[ \\Sigma_r \\leftarrow \\Sigma_r + \\epsilon I, \\quad \\epsilon \\approx 10^{-6} \\]"},{"location":"eval/fid_frechet_inception_distance/#practical-implementation","title":"Practical Implementation","text":""},{"location":"eval/fid_frechet_inception_distance/#python-example","title":"Python Example","text":"<pre><code>import numpy as np\nfrom scipy import linalg\nfrom torch import nn\nfrom torchvision.models import inception_v3\n\ndef calculate_fid(real_images, generated_images, inception_model):\n    \"\"\"\n    Calculate FID between real and generated images.\n\n    Args:\n        real_images: Tensor of real images [N, C, H, W]\n        generated_images: Tensor of generated images [M, C, H, W]\n        inception_model: Pre-trained Inception-v3 model\n\n    Returns:\n        fid_score: Scalar FID value\n    \"\"\"\n    # Extract features\n    with torch.no_grad():\n        features_real = inception_model(real_images).cpu().numpy()\n        features_gen = inception_model(generated_images).cpu().numpy()\n\n    # Compute statistics\n    mu_real = np.mean(features_real, axis=0)\n    mu_gen = np.mean(features_gen, axis=0)\n\n    sigma_real = np.cov(features_real, rowvar=False)\n    sigma_gen = np.cov(features_gen, rowvar=False)\n\n    # Compute FID\n    fid = calculate_frechet_distance(mu_real, sigma_real, mu_gen, sigma_gen)\n\n    return fid\n\ndef calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n    \"\"\"\n    Calculate Fr\u00e9chet distance between two Gaussian distributions.\n    \"\"\"\n    # Mean difference\n    diff = mu1 - mu2\n\n    # Product of covariances\n    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n\n    # Numerical stability\n    if not np.isfinite(covmean).all():\n        offset = np.eye(sigma1.shape[0]) * eps\n        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n\n    # Handle complex numbers from numerical errors\n    if np.iscomplexobj(covmean):\n        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n            m = np.max(np.abs(covmean.imag))\n            raise ValueError(f'Imaginary component {m}')\n        covmean = covmean.real\n\n    # Compute FID\n    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2*covmean)\n\n    return fid\n</code></pre>"},{"location":"eval/fid_frechet_inception_distance/#using-existing-libraries","title":"Using Existing Libraries","text":"<p>PyTorch FID (recommended):</p> <pre><code>from pytorch_fid import fid_score\n\n# Compute FID between two directories of images\nfid_value = fid_score.calculate_fid_given_paths(\n    [path_to_real_images, path_to_generated_images],\n    batch_size=50,\n    device='cuda',\n    dims=2048\n)\nprint(f'FID: {fid_value:.2f}')\n</code></pre> <p>Clean-FID (improved implementation):</p> <pre><code>from cleanfid import fid\n\n# More robust FID computation\nfid_value = fid.compute_fid(\n    path_to_real_images,\n    path_to_generated_images,\n    mode=\"clean\",\n    num_workers=4\n)\n</code></pre>"},{"location":"eval/fid_frechet_inception_distance/#interpretation-and-typical-values","title":"Interpretation and Typical Values","text":""},{"location":"eval/fid_frechet_inception_distance/#what-different-fid-values-mean","title":"What Different FID Values Mean","text":"FID Range Interpretation Example 0 Perfect match Comparing dataset to itself &lt; 10 Excellent quality SOTA diffusion models on ImageNet 10-30 Good quality Strong generative models 30-50 Moderate quality Early GANs, simple models 50-100 Poor quality Weak models, mode collapse &gt; 100 Very poor quality Random noise, severe artifacts"},{"location":"eval/fid_frechet_inception_distance/#benchmark-values","title":"Benchmark Values","text":"<p>ImageNet 256\u00d7256 (as of 2024): - Real data: FID = 0 (by definition) - SOTA diffusion models: FID \u2248 2-5 - Good GANs: FID \u2248 10-20 - Early GANs: FID \u2248 50-100</p> <p>CIFAR-10:</p> <ul> <li>SOTA models: FID \u2248 2-3</li> <li>Good models: FID \u2248 5-10</li> <li>Moderate models: FID \u2248 20-40</li> </ul> <p>CelebA-HQ:</p> <ul> <li>SOTA models: FID \u2248 3-8</li> <li>Good models: FID \u2248 10-20</li> </ul>"},{"location":"eval/fid_frechet_inception_distance/#fid-is-relative","title":"FID is Relative","text":"<p>Important: FID values are not comparable across datasets: - FID = 10 on ImageNet \u2260 FID = 10 on CIFAR-10 - Always compare models on the same dataset with the same evaluation protocol</p>"},{"location":"eval/fid_frechet_inception_distance/#advantages-of-fid","title":"Advantages of FID","text":""},{"location":"eval/fid_frechet_inception_distance/#1-captures-quality-and-diversity","title":"1. Captures Quality and Diversity","text":"<p>Unlike Inception Score (which primarily measures quality), FID captures both: - Quality: Via mean difference (are samples realistic?) - Diversity: Via covariance difference (do samples cover the distribution?)</p>"},{"location":"eval/fid_frechet_inception_distance/#2-robust-to-sample-size","title":"2. Robust to Sample Size","text":"<p>FID is relatively stable with sufficient samples: - Minimum recommended: 10,000 samples (both real and generated) - Typical: 50,000 samples for reliable estimates - Variance decreases with more samples</p>"},{"location":"eval/fid_frechet_inception_distance/#3-correlates-with-human-judgment","title":"3. Correlates with Human Judgment","text":"<p>Studies show FID correlates better with human perceptual quality than earlier metrics like IS.</p>"},{"location":"eval/fid_frechet_inception_distance/#4-detects-mode-collapse","title":"4. Detects Mode Collapse","text":"<p>If a model generates only a subset of the distribution (mode collapse): - Mean may be similar - Covariance will differ significantly - FID will be high</p>"},{"location":"eval/fid_frechet_inception_distance/#5-differentiable-in-principle","title":"5. Differentiable (in principle)","text":"<p>While not typically used as a training loss, FID can be approximated for gradient-based optimization.</p>"},{"location":"eval/fid_frechet_inception_distance/#limitations-and-pitfalls","title":"Limitations and Pitfalls","text":""},{"location":"eval/fid_frechet_inception_distance/#1-assumes-gaussian-distributions","title":"1. Assumes Gaussian Distributions","text":"<p>Assumption: Features follow multivariate Gaussian distributions.</p> <p>Reality: Feature distributions may not be Gaussian.</p> <p>Impact: FID may not capture all distributional differences, especially in tails.</p>"},{"location":"eval/fid_frechet_inception_distance/#2-requires-pre-trained-network","title":"2. Requires Pre-trained Network","text":"<p>Dependency: Typically uses Inception-v3 trained on ImageNet.</p> <p>Issues:</p> <ul> <li>Domain mismatch: ImageNet features may not be optimal for medical images, satellite imagery, etc.</li> <li>Bias: Inherits biases from ImageNet training</li> <li>Not universal: Doesn't work for non-image data without adaptation</li> </ul>"},{"location":"eval/fid_frechet_inception_distance/#3-sample-size-sensitivity","title":"3. Sample Size Sensitivity","text":"<p>Problem: FID estimates have variance that depends on sample size.</p> <p>Recommendations:</p> <ul> <li>Use same sample size for all comparisons</li> <li>Use \u226510,000 samples for stable estimates</li> <li>Report multiple runs with different random samples</li> </ul> <p>Example variance: <pre><code>N = 1,000:  FID = 15.3 \u00b1 2.1\nN = 10,000: FID = 14.8 \u00b1 0.4\nN = 50,000: FID = 14.7 \u00b1 0.1\n</code></pre></p>"},{"location":"eval/fid_frechet_inception_distance/#4-doesnt-capture-all-aspects","title":"4. Doesn't Capture All Aspects","text":"<p>What FID misses:</p> <ul> <li>Local structure: Fine-grained details</li> <li>Semantic correctness: Object relationships, physical plausibility</li> <li>Perceptual quality: Some artifacts humans notice</li> <li>Diversity within modes: Can miss subtle diversity issues</li> </ul>"},{"location":"eval/fid_frechet_inception_distance/#5-can-be-gamed","title":"5. Can Be Gamed","text":"<p>Memorization: A model that memorizes training data will have low FID but poor generalization.</p> <p>Solution: Always check for overfitting with held-out test sets.</p>"},{"location":"eval/fid_frechet_inception_distance/#6-computational-cost","title":"6. Computational Cost","text":"<p>Requirements:</p> <ul> <li>Pre-trained Inception-v3 model</li> <li>Feature extraction for all images</li> <li>Covariance computation (scales as \\(O(d^2 n)\\) for \\(d\\) dimensions, \\(n\\) samples)</li> </ul> <p>Typical time: Minutes to hours depending on dataset size.</p>"},{"location":"eval/fid_frechet_inception_distance/#best-practices","title":"Best Practices","text":""},{"location":"eval/fid_frechet_inception_distance/#1-standardize-evaluation-protocol","title":"1. Standardize Evaluation Protocol","text":"<p>Image preprocessing:</p> <ul> <li>Resize to 299\u00d7299 (Inception-v3 input size)</li> <li>Normalize to [0, 1] or [-1, 1] consistently</li> <li>Use same preprocessing for real and generated images</li> </ul> <p>Sample size:</p> <ul> <li>Use \u226510,000 samples (preferably 50,000)</li> <li>Use same number of real and generated samples</li> <li>Report sample size in papers</li> </ul>"},{"location":"eval/fid_frechet_inception_distance/#2-multiple-runs","title":"2. Multiple Runs","text":"<p>Compute FID with multiple random seeds or sample sets:</p> <pre><code>fid_scores = []\nfor seed in range(5):\n    generated_samples = model.sample(n=10000, seed=seed)\n    fid = compute_fid(real_samples, generated_samples)\n    fid_scores.append(fid)\n\nmean_fid = np.mean(fid_scores)\nstd_fid = np.std(fid_scores)\nprint(f'FID: {mean_fid:.2f} \u00b1 {std_fid:.2f}')\n</code></pre>"},{"location":"eval/fid_frechet_inception_distance/#3-use-consistent-implementation","title":"3. Use Consistent Implementation","text":"<p>Recommended: Use standard libraries (pytorch-fid, clean-fid) rather than custom implementations.</p> <p>Why: Subtle implementation differences can cause significant FID variations.</p>"},{"location":"eval/fid_frechet_inception_distance/#4-report-full-details","title":"4. Report Full Details","text":"<p>Minimum reporting:</p> <ul> <li>FID value with standard deviation</li> <li>Number of samples used</li> <li>Feature extractor (Inception-v3, etc.)</li> <li>Image resolution</li> <li>Preprocessing steps</li> </ul>"},{"location":"eval/fid_frechet_inception_distance/#5-complement-with-other-metrics","title":"5. Complement with Other Metrics","text":"<p>Never rely on FID alone. Use complementary metrics: - Precision/Recall: Quality vs. diversity trade-off - IS: Additional quality measure - Human evaluation: Perceptual quality - Task-specific metrics: Downstream performance</p>"},{"location":"eval/fid_frechet_inception_distance/#6-domain-specific-adaptations","title":"6. Domain-Specific Adaptations","text":"<p>For non-natural images: - Consider using domain-specific feature extractors - Train custom feature networks on your domain - Validate that features capture meaningful semantics</p>"},{"location":"eval/fid_frechet_inception_distance/#variants-and-extensions","title":"Variants and Extensions","text":""},{"location":"eval/fid_frechet_inception_distance/#clean-fid","title":"Clean-FID","text":"<p>Improvement: More robust implementation addressing numerical issues.</p> <p>Changes:</p> <ul> <li>Better handling of edge cases</li> <li>Improved numerical stability</li> <li>Consistent preprocessing</li> </ul> <p>Usage: Recommended over original implementation.</p>"},{"location":"eval/fid_frechet_inception_distance/#kernel-fid-kid","title":"Kernel FID (KID)","text":"<p>Alternative: Uses kernel methods instead of Gaussian assumption.</p> <p>Advantages:</p> <ul> <li>No Gaussian assumption</li> <li>Unbiased estimator</li> <li>Better for small sample sizes</li> </ul> <p>Formula:</p> \\[ \\text{KID} = \\mathbb{E}[k(x_r, x_r')] + \\mathbb{E}[k(x_g, x_g')] - 2\\mathbb{E}[k(x_r, x_g)] \\] <p>where \\(k(\\cdot, \\cdot)\\) is a kernel function (e.g., polynomial kernel).</p>"},{"location":"eval/fid_frechet_inception_distance/#precision-and-recall","title":"Precision and Recall","text":"<p>Decomposition: Separates FID into quality (precision) and diversity (recall).</p> <p>Advantages:</p> <ul> <li>More interpretable</li> <li>Detects mode collapse vs. poor quality</li> <li>Complements FID</li> </ul>"},{"location":"eval/fid_frechet_inception_distance/#domain-specific-fid","title":"Domain-Specific FID","text":"<p>Adaptations:</p> <ul> <li>Medical imaging: Use pre-trained medical image networks</li> <li>Satellite imagery: Use remote sensing features</li> <li>Biological data: Use domain-specific embeddings</li> </ul>"},{"location":"eval/fid_frechet_inception_distance/#fid-for-non-image-data","title":"FID for Non-Image Data","text":""},{"location":"eval/fid_frechet_inception_distance/#adapting-fid-to-other-domains","title":"Adapting FID to Other Domains","text":"<p>General approach: 1. Define a meaningful feature space 2. Extract features from real and generated data 3. Compute Fr\u00e9chet distance</p>"},{"location":"eval/fid_frechet_inception_distance/#examples","title":"Examples","text":"<p>Text:</p> <ul> <li>Use BERT or GPT embeddings as features</li> <li>Compute FID in embedding space</li> </ul> <p>Audio:</p> <ul> <li>Use audio feature extractors (e.g., VGGish)</li> <li>Compute FID on spectrograms or learned features</li> </ul> <p>Molecular structures:</p> <ul> <li>Use molecular fingerprints or graph embeddings</li> <li>Compute FID in chemical space</li> </ul> <p>Gene expression:</p> <ul> <li>Use pathway embeddings or PCA features</li> <li>Compute FID in biological feature space</li> </ul> <p>Caution: Validate that chosen features capture meaningful semantics for your domain.</p>"},{"location":"eval/fid_frechet_inception_distance/#comparison-with-other-metrics","title":"Comparison with Other Metrics","text":""},{"location":"eval/fid_frechet_inception_distance/#fid-vs-inception-score-is","title":"FID vs. Inception Score (IS)","text":"Aspect FID IS Requires real data Yes No Captures diversity Yes Limited Detects mode collapse Yes No Robust to sample size More robust Less robust Computational cost Higher Lower Correlation with quality Better Good <p>Recommendation: Use FID as primary metric, IS as secondary.</p>"},{"location":"eval/fid_frechet_inception_distance/#fid-vs-precisionrecall","title":"FID vs. Precision/Recall","text":"Aspect FID Precision/Recall Single number Yes Two numbers Interpretability Lower Higher Quality vs. diversity Combined Separated Computational cost Lower Higher <p>Recommendation: Use both\u2014FID for overall quality, P/R for diagnosing issues.</p>"},{"location":"eval/fid_frechet_inception_distance/#fid-vs-likelihood","title":"FID vs. Likelihood","text":"Aspect FID Likelihood Perceptual quality Good Poor Theoretical grounding Heuristic Principled Requires tractable model No Yes Captures imperceptible details No Yes <p>Recommendation: FID for perceptual quality, likelihood for distributional fit.</p>"},{"location":"eval/fid_frechet_inception_distance/#summary","title":"Summary","text":""},{"location":"eval/fid_frechet_inception_distance/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>FID measures distribution distance in learned feature space</li> <li>Lower is better: FID = 0 means perfect match</li> <li>Captures quality and diversity: Both mean and covariance matter</li> <li>Widely adopted: Standard benchmark for generative models</li> <li>Has limitations: Gaussian assumption, domain dependence, sample size sensitivity</li> </ol>"},{"location":"eval/fid_frechet_inception_distance/#when-to-use-fid","title":"When to Use FID","text":"<p>Use FID when:</p> <ul> <li>Evaluating image generative models</li> <li>Comparing different models on same dataset</li> <li>Need single scalar metric</li> <li>Have sufficient samples (\u226510,000)</li> </ul> <p>Don't rely solely on FID when:</p> <ul> <li>Working with non-image data (adapt carefully)</li> <li>Need to diagnose specific issues (use P/R)</li> <li>Sample size is small (&lt;1,000)</li> <li>Domain is very different from ImageNet</li> </ul>"},{"location":"eval/fid_frechet_inception_distance/#best-practice-summary","title":"Best Practice Summary","text":"<p>\u2705 Use \u226510,000 samples \u2705 Report mean \u00b1 std over multiple runs \u2705 Use standard implementations (pytorch-fid, clean-fid) \u2705 Complement with other metrics (P/R, IS) \u2705 Report full evaluation details \u2705 Compare on same dataset with same protocol</p>"},{"location":"eval/fid_frechet_inception_distance/#related-documents","title":"Related Documents","text":"<ul> <li>Evaluating Generative Models \u2014 Comprehensive evaluation guide</li> <li>Epiplexity: From Entropy to Epiplexity \u2014 Alternative evaluation perspective</li> <li>DDPM Foundations \u2014 Diffusion model theory</li> <li>DDPM Sampling \u2014 Sampling efficiency vs. quality</li> </ul>"},{"location":"eval/fid_frechet_inception_distance/#references","title":"References","text":""},{"location":"eval/fid_frechet_inception_distance/#original-paper","title":"Original Paper","text":"<ol> <li>Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., &amp; Hochreiter, S. (2017). GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. NeurIPS. Paper</li> </ol>"},{"location":"eval/fid_frechet_inception_distance/#implementations","title":"Implementations","text":"<ol> <li>pytorch-fid: GitHub</li> <li>clean-fid: GitHub | Paper</li> </ol>"},{"location":"eval/fid_frechet_inception_distance/#related-metrics","title":"Related Metrics","text":"<ol> <li>Salimans, T., et al. (2016). Improved Techniques for Training GANs. NeurIPS. (Inception Score)</li> <li>Kynk\u00e4\u00e4nniemi, T., et al. (2019). Improved Precision and Recall Metric for Assessing Generative Models. NeurIPS.</li> <li>Bi\u0144kowski, M., et al. (2018). Demystifying MMD GANs. ICLR. (Kernel Inception Distance)</li> </ol>"},{"location":"eval/fid_frechet_inception_distance/#analysis-and-best-practices","title":"Analysis and Best Practices","text":"<ol> <li>Chong, M. J., &amp; Forsyth, D. (2020). Effectively Unbiased FID and Inception Score and Where to Find Them. CVPR.</li> <li>Parmar, G., et al. (2021). On Aliased Resizing and Surprising Subtleties in GAN Evaluation. CVPR. (Clean-FID)</li> </ol>"},{"location":"eval/generative_models_enhance_supervised/","title":"How Generative Models Enhance Supervised Prediction Methods","text":"<p>Generative models and supervised predictors are often viewed as separate paradigms. However, generative models can significantly enhance supervised prediction methods through multiple synergistic mechanisms.</p> <p>This document explains how generative models like diffusion models, VAEs, and flow matching can improve supervised predictors such as GEM-1, with practical examples for biological data applications.</p>"},{"location":"eval/generative_models_enhance_supervised/#overview","title":"Overview","text":""},{"location":"eval/generative_models_enhance_supervised/#the-synergy","title":"The Synergy","text":"<p>Supervised learning learns mappings: \\(f: X \\rightarrow Y\\) - Example: Gene expression \u2192 drug response - Requires labeled data \\((X, Y)\\) pairs - Optimizes prediction accuracy</p> <p>Generative learning models distributions: \\(p(X)\\) or \\(p(X, Y)\\) - Example: Distribution of gene expression profiles - Can use unlabeled data - Captures data structure</p> <p>Key insight: Generative models learn the structure of the input space, which can be leveraged to improve supervised prediction.</p>"},{"location":"eval/generative_models_enhance_supervised/#seven-enhancement-mechanisms","title":"Seven Enhancement Mechanisms","text":"<ol> <li>Data augmentation: Expand training sets with synthetic samples</li> <li>Conditional generation: Create counterfactuals and interventions</li> <li>Representation learning: Better features from generative models</li> <li>Uncertainty quantification: Sample-based confidence estimates</li> <li>Semi-supervised learning: Leverage unlabeled data</li> <li>Denoising and imputation: Clean noisy measurements</li> <li>Active learning: Guide experimental design</li> </ol>"},{"location":"eval/generative_models_enhance_supervised/#1-data-augmentation","title":"1. Data Augmentation","text":""},{"location":"eval/generative_models_enhance_supervised/#the-problem","title":"The Problem","text":"<p>Supervised predictors require large labeled datasets, but: - Labels are expensive (experiments, expert annotation) - Data collection is time-consuming - Some conditions are rare or difficult to measure - Overfitting occurs with limited data</p>"},{"location":"eval/generative_models_enhance_supervised/#the-solution","title":"The Solution","text":"<p>Use generative models to create synthetic training samples.</p>"},{"location":"eval/generative_models_enhance_supervised/#how-it-works","title":"How It Works","text":"<pre><code># Original training (limited labels)\nX_real, Y_real = labeled_dataset  # e.g., 1,000 samples\n\n# Generate synthetic data\ndiffusion_model = train_diffusion(X_unlabeled)  # Can use unlabeled data\nX_synthetic = diffusion_model.sample(n_samples=10000)\n\n# Label synthetic data (various strategies)\n# Strategy 1: Weak supervision\nY_synthetic = weak_labeling_function(X_synthetic)\n\n# Strategy 2: Pseudo-labeling with confidence threshold\nY_pseudo, confidence = pretrained_model.predict_with_confidence(X_synthetic)\nX_high_conf = X_synthetic[confidence &gt; 0.9]\nY_high_conf = Y_pseudo[confidence &gt; 0.9]\n\n# Strategy 3: Semi-supervised (use unlabeled structure)\nY_synthetic = semi_supervised_labeling(X_synthetic, X_real, Y_real)\n\n# Augmented training\nX_augmented = concatenate(X_real, X_synthetic)\nY_augmented = concatenate(Y_real, Y_synthetic)\n\n# Train enhanced predictor\npredictor_enhanced = train_predictor(X_augmented, Y_augmented)\n</code></pre>"},{"location":"eval/generative_models_enhance_supervised/#benefits","title":"Benefits","text":"<ul> <li>Increased sample diversity: Explore regions of input space not well-represented</li> <li>Improved generalization: Reduce overfitting to limited training set</li> <li>Better robustness: Handle distribution shifts</li> <li>Data efficiency: Achieve better performance with fewer real labels</li> </ul>"},{"location":"eval/generative_models_enhance_supervised/#evidence-from-other-domains","title":"Evidence from Other Domains","text":"<p>Computer vision:</p> <ul> <li>Diffusion-generated images improve classifier accuracy by 5-15%</li> <li>Particularly effective for rare classes</li> </ul> <p>Natural language processing:</p> <ul> <li>GPT-generated text improves few-shot learning</li> <li>Paraphrasing augmentation enhances robustness</li> </ul> <p>Biological data:</p> <ul> <li>scVI-generated cells improve cell type classification</li> <li>Synthetic perturbations improve drug response prediction</li> </ul>"},{"location":"eval/generative_models_enhance_supervised/#best-practices","title":"Best Practices","text":"<pre><code># 1. Validate synthetic data quality first\nfid_score = compute_fid(X_real, X_synthetic)\nif fid_score &gt; 50:\n    print(\"Warning: Synthetic data quality is poor\")\n\n# 2. Balance real and synthetic data\n# Too much synthetic can hurt if quality is poor\nmixing_ratio = 0.3  # 30% synthetic, 70% real\nn_synthetic = int(len(X_real) * mixing_ratio)\nX_synthetic_subset = X_synthetic[:n_synthetic]\n\n# 3. Use confidence-based filtering\nif hasattr(predictor, 'predict_proba'):\n    confidence = predictor.predict_proba(X_synthetic).max(axis=1)\n    X_synthetic_filtered = X_synthetic[confidence &gt; threshold]\n\n# 4. Monitor validation performance\nval_scores = []\nfor ratio in [0.0, 0.1, 0.3, 0.5, 0.7, 1.0]:\n    predictor = train_with_augmentation(X_real, Y_real, X_synthetic, ratio)\n    val_score = evaluate(predictor, X_val, Y_val)\n    val_scores.append(val_score)\n\noptimal_ratio = ratios[np.argmax(val_scores)]\n</code></pre>"},{"location":"eval/generative_models_enhance_supervised/#2-conditional-generation-for-counterfactuals","title":"2. Conditional Generation for Counterfactuals","text":""},{"location":"eval/generative_models_enhance_supervised/#the-problem_1","title":"The Problem","text":"<p>Supervised predictors answer: \"What is \\(Y\\) given \\(X\\)?\"</p> <p>But we often want to answer: - \"What would \\(Y\\) be if we changed \\(X\\) in a specific way?\" - \"What \\(X\\) would produce a desired \\(Y\\)?\" - \"What is the causal effect of intervention \\(I\\)?\"</p>"},{"location":"eval/generative_models_enhance_supervised/#the-solution_1","title":"The Solution","text":"<p>Use conditional generative models to create counterfactual samples.</p>"},{"location":"eval/generative_models_enhance_supervised/#applications","title":"Applications","text":""},{"location":"eval/generative_models_enhance_supervised/#a-drug-response-prediction","title":"A. Drug Response Prediction","text":"<pre><code># Question: \"What would gene expression look like under drug combination X+Y?\"\n\n# Conditional generation\nconditional_diffusion = train_conditional_diffusion(\n    gene_expression_data,\n    conditions=['drug_type', 'dose', 'cell_type']\n)\n\n# Generate counterfactual\nX_counterfactual = conditional_diffusion.sample(\n    condition={\n        'drug_type': 'drug_X_plus_drug_Y',\n        'dose': 10.0,\n        'cell_type': 'T_cell'\n    },\n    n_samples=1000\n)\n\n# Predict response\nY_predicted = gem1_predictor.predict(X_counterfactual)\nY_mean = Y_predicted.mean()\nY_ci = np.percentile(Y_predicted, [2.5, 97.5])\n\nprint(f\"Predicted response: {Y_mean:.3f} [{Y_ci[0]:.3f}, {Y_ci[1]:.3f}]\")\n</code></pre>"},{"location":"eval/generative_models_enhance_supervised/#b-causal-inference","title":"B. Causal Inference","text":"<pre><code># Estimate causal effect of drug treatment\n\n# Generate baseline (control)\nX_control = conditional_diffusion.sample(\n    condition={'treatment': 'control', 'cell_type': 'cancer_cell'},\n    n_samples=1000\n)\n\n# Generate treated\nX_treated = conditional_diffusion.sample(\n    condition={'treatment': 'drug_A', 'cell_type': 'cancer_cell'},\n    n_samples=1000\n)\n\n# Predict outcomes\nY_control = predictor.predict(X_control)\nY_treated = predictor.predict(X_treated)\n\n# Estimate average treatment effect (ATE)\nATE = (Y_treated - Y_control).mean()\nATE_std = (Y_treated - Y_control).std()\n\nprint(f\"Average Treatment Effect: {ATE:.3f} \u00b1 {ATE_std:.3f}\")\n</code></pre>"},{"location":"eval/generative_models_enhance_supervised/#c-inverse-design","title":"C. Inverse Design","text":"<pre><code># Question: \"What gene expression profile would achieve target phenotype?\"\n\n# Optimization over generative model latent space\ndef objective(z):\n    X = diffusion_model.decode(z)\n    Y_pred = predictor.predict(X)\n    return -np.abs(Y_pred - Y_target)  # Minimize distance to target\n\n# Optimize\nz_optimal = optimize(objective, z_init=random_latent())\nX_optimal = diffusion_model.decode(z_optimal)\n\nprint(f\"Designed gene expression profile:\")\nprint(f\"Predicted phenotype: {predictor.predict(X_optimal):.3f}\")\nprint(f\"Target phenotype: {Y_target:.3f}\")\n</code></pre>"},{"location":"eval/generative_models_enhance_supervised/#benefits_1","title":"Benefits","text":"<ul> <li>Hypothesis generation: Explore \"what if\" scenarios</li> <li>Experimental design: Prioritize experiments</li> <li>Causal reasoning: Estimate intervention effects</li> <li>Inverse problems: Design inputs for desired outputs</li> </ul>"},{"location":"eval/generative_models_enhance_supervised/#3-representation-learning","title":"3. Representation Learning","text":""},{"location":"eval/generative_models_enhance_supervised/#the-problem_2","title":"The Problem","text":"<p>Raw gene expression data is: - High-dimensional (thousands of genes) - Noisy (technical and biological variation) - Redundant (correlated genes, pathways)</p> <p>Supervised predictors must learn both: 1. Useful representations of inputs 2. Mapping from representations to outputs</p> <p>This is challenging with limited labeled data.</p>"},{"location":"eval/generative_models_enhance_supervised/#the-solution_2","title":"The Solution","text":"<p>Use generative models to learn rich representations, then use these as features for supervised prediction.</p>"},{"location":"eval/generative_models_enhance_supervised/#architecture","title":"Architecture","text":"<pre><code># Stage 1: Train generative model on unlabeled data\ndiffusion_model = train_diffusion(X_all_unlabeled)  # 100,000 samples\n\n# Stage 2: Extract encoder\nencoder = diffusion_model.get_encoder()  # Maps X \u2192 latent space\n\n# Stage 3: Train predictor on learned representations\ndef train_with_representations(X_labeled, Y_labeled):\n    # Encode inputs\n    Z_labeled = encoder(X_labeled)  # Compressed representation\n\n    # Train predictor on representations\n    predictor = train_predictor(Z_labeled, Y_labeled)\n\n    return predictor\n\n# Usage\npredictor_enhanced = train_with_representations(X_labeled, Y_labeled)\n\n# Prediction\ndef predict(X_new):\n    Z_new = encoder(X_new)\n    Y_pred = predictor_enhanced(Z_new)\n    return Y_pred\n</code></pre>"},{"location":"eval/generative_models_enhance_supervised/#why-this-works","title":"Why This Works","text":"<p>Generative models learn:</p> <ul> <li>Hierarchical structure: Coarse to fine-grained patterns</li> <li>Biological pathways: Coordinated gene expression</li> <li>Cell state manifolds: Low-dimensional structure</li> <li>Invariances: Robust to technical noise</li> </ul> <p>These representations:</p> <ul> <li>Reduce dimensionality (thousands of genes \u2192 hundreds of latent dims)</li> <li>Remove noise (learned from large unlabeled data)</li> <li>Capture biological structure (pathways, regulatory programs)</li> <li>Transfer across tasks (general-purpose features)</li> </ul>"},{"location":"eval/generative_models_enhance_supervised/#comparison-to-alternatives","title":"Comparison to Alternatives","text":"Approach Pros Cons Raw genes Simple, interpretable High-dim, noisy PCA Fast, deterministic Linear, no biology Autoencoders Non-linear Requires labels for fine-tuning Diffusion encoder Rich, hierarchical, biological Requires pre-training"},{"location":"eval/generative_models_enhance_supervised/#example-multi-task-learning","title":"Example: Multi-Task Learning","text":"<pre><code># Use diffusion representations for multiple prediction tasks\nencoder = diffusion_model.get_encoder()\n\n# Task 1: Drug response\npredictor_drug = train_predictor(encoder(X_drug), Y_drug_response)\n\n# Task 2: Cell type\npredictor_cell = train_classifier(encoder(X_cells), Y_cell_type)\n\n# Task 3: Pathway activity\npredictor_pathway = train_predictor(encoder(X_expr), Y_pathway_activity)\n\n# All tasks benefit from shared representations\n</code></pre>"},{"location":"eval/generative_models_enhance_supervised/#4-uncertainty-quantification","title":"4. Uncertainty Quantification","text":""},{"location":"eval/generative_models_enhance_supervised/#the-problem_3","title":"The Problem","text":"<p>Supervised predictors typically give point estimates: - \\(\\hat{Y} = f(X)\\)</p> <p>But we need: - Confidence intervals - Prediction uncertainty - Risk assessment</p>"},{"location":"eval/generative_models_enhance_supervised/#the-solution_3","title":"The Solution","text":"<p>Use generative models to sample plausible input variations and propagate uncertainty.</p>"},{"location":"eval/generative_models_enhance_supervised/#method-1-input-space-sampling","title":"Method 1: Input Space Sampling","text":"<pre><code>def predict_with_uncertainty(X_observed, diffusion_model, predictor, n_samples=1000):\n    \"\"\"\n    Quantify prediction uncertainty by sampling plausible variations.\n\n    Args:\n        X_observed: Observed gene expression (potentially noisy)\n        diffusion_model: Trained generative model\n        predictor: Supervised prediction model\n        n_samples: Number of samples for uncertainty estimation\n\n    Returns:\n        Y_mean: Mean prediction\n        Y_std: Standard deviation\n        Y_ci: 95% credible interval\n    \"\"\"\n    # Sample plausible variations around observed data\n    # Using diffusion model's denoising capability\n    X_samples = diffusion_model.sample_around(X_observed, n_samples=n_samples)\n\n    # Predict on all samples\n    Y_predictions = predictor.predict(X_samples)\n\n    # Compute statistics\n    Y_mean = Y_predictions.mean()\n    Y_std = Y_predictions.std()\n    Y_ci = np.percentile(Y_predictions, [2.5, 97.5])\n\n    return {\n        'mean': Y_mean,\n        'std': Y_std,\n        'ci_lower': Y_ci[0],\n        'ci_upper': Y_ci[1],\n        'samples': Y_predictions\n    }\n\n# Usage\nresult = predict_with_uncertainty(X_patient, diffusion_model, gem1_predictor)\nprint(f\"Predicted response: {result['mean']:.2f} \u00b1 {result['std']:.2f}\")\nprint(f\"95% CI: [{result['ci_lower']:.2f}, {result['ci_upper']:.2f}]\")\n</code></pre>"},{"location":"eval/generative_models_enhance_supervised/#method-2-conditional-uncertainty","title":"Method 2: Conditional Uncertainty","text":"<pre><code>def conditional_uncertainty(condition, diffusion_model, predictor, n_samples=1000):\n    \"\"\"\n    Estimate uncertainty for a given condition.\n    \"\"\"\n    # Generate multiple samples for this condition\n    X_samples = diffusion_model.sample(condition=condition, n_samples=n_samples)\n\n    # Predict on all samples\n    Y_predictions = predictor.predict(X_samples)\n\n    # Decompose uncertainty\n    aleatoric = Y_predictions.var()  # Natural variability\n    epistemic = predictor.epistemic_uncertainty(X_samples)  # Model uncertainty\n\n    return {\n        'aleatoric': aleatoric,\n        'epistemic': epistemic,\n        'total': aleatoric + epistemic\n    }\n</code></pre>"},{"location":"eval/generative_models_enhance_supervised/#benefits_2","title":"Benefits","text":"<ul> <li>Risk assessment: Identify high-uncertainty predictions</li> <li>Decision making: Account for uncertainty in clinical decisions</li> <li>Active learning: Query points with high uncertainty</li> <li>Calibration: Better confidence estimates</li> </ul>"},{"location":"eval/generative_models_enhance_supervised/#5-semi-supervised-learning","title":"5. Semi-Supervised Learning","text":""},{"location":"eval/generative_models_enhance_supervised/#the-problem_4","title":"The Problem","text":"<ul> <li>Labeled data: Expensive, limited (e.g., 1,000 samples)</li> <li>Unlabeled data: Cheap, abundant (e.g., 100,000 samples)</li> </ul> <p>Standard supervised learning ignores unlabeled data.</p>"},{"location":"eval/generative_models_enhance_supervised/#the-solution_4","title":"The Solution","text":"<p>Use generative models to leverage unlabeled data structure.</p>"},{"location":"eval/generative_models_enhance_supervised/#framework","title":"Framework","text":"<pre><code># Stage 1: Pre-train generative model on ALL data (labeled + unlabeled)\nX_all = concatenate(X_labeled, X_unlabeled)  # 101,000 samples\ndiffusion_model = train_diffusion(X_all)\n\n# Stage 2: Fine-tune for supervised task\n# Option A: Use learned representations\nencoder = diffusion_model.get_encoder()\nZ_labeled = encoder(X_labeled)\npredictor = train_predictor(Z_labeled, Y_labeled)\n\n# Option B: Joint training\ndef joint_loss(X, Y, X_unlabeled):\n    # Supervised loss\n    Y_pred = predictor(X)\n    supervised_loss = mse_loss(Y_pred, Y)\n\n    # Generative loss (regularization)\n    generative_loss = diffusion_model.loss(X_unlabeled)\n\n    # Combined\n    total_loss = supervised_loss + lambda_reg * generative_loss\n    return total_loss\n\n# Option C: Pseudo-labeling\nY_pseudo = predictor_initial.predict(X_unlabeled)\nconfidence = predictor_initial.confidence(X_unlabeled)\nX_high_conf = X_unlabeled[confidence &gt; 0.9]\nY_high_conf = Y_pseudo[confidence &gt; 0.9]\n\nX_augmented = concatenate(X_labeled, X_high_conf)\nY_augmented = concatenate(Y_labeled, Y_high_conf)\npredictor_final = train_predictor(X_augmented, Y_augmented)\n</code></pre>"},{"location":"eval/generative_models_enhance_supervised/#why-this-works_1","title":"Why This Works","text":"<p>Unlabeled data provides:</p> <ul> <li>Data manifold structure</li> <li>Natural clusters (cell types, states)</li> <li>Invariances and symmetries</li> <li>Regularization (prevents overfitting)</li> </ul> <p>Generative model captures:</p> <ul> <li>Low-dimensional structure</li> <li>Biological constraints</li> <li>Smooth manifolds</li> </ul> <p>Supervised predictor benefits:</p> <ul> <li>Better representations</li> <li>More robust features</li> <li>Improved generalization</li> </ul>"},{"location":"eval/generative_models_enhance_supervised/#example-cell-type-classification-with-limited-labels","title":"Example: Cell Type Classification with Limited Labels","text":"<pre><code># Setup\nX_unlabeled = load_all_scrna_data()  # 100,000 cells, no labels\nX_labeled, Y_labeled = load_labeled_cells()  # 1,000 cells, with cell type labels\n\n# Baseline: Supervised only\npredictor_baseline = train_classifier(X_labeled, Y_labeled)\nacc_baseline = evaluate(predictor_baseline, X_test, Y_test)\nprint(f\"Baseline accuracy: {acc_baseline:.2%}\")\n\n# Semi-supervised: Use diffusion model\ndiffusion = train_diffusion(X_unlabeled)\nencoder = diffusion.get_encoder()\n\nZ_labeled = encoder(X_labeled)\npredictor_semi = train_classifier(Z_labeled, Y_labeled)\nacc_semi = evaluate(predictor_semi, encoder(X_test), Y_test)\nprint(f\"Semi-supervised accuracy: {acc_semi:.2%}\")\n\n# Typical improvement: 10-20% absolute accuracy gain\n</code></pre>"},{"location":"eval/generative_models_enhance_supervised/#6-denoising-and-imputation","title":"6. Denoising and Imputation","text":""},{"location":"eval/generative_models_enhance_supervised/#the-problem_5","title":"The Problem","text":"<p>Biological measurements have: - Technical noise: Sequencing errors, batch effects - Missing values: Dropout in scRNA-seq, incomplete measurements - Artifacts: Systematic biases</p> <p>These degrade supervised predictor performance.</p>"},{"location":"eval/generative_models_enhance_supervised/#the-solution_5","title":"The Solution","text":"<p>Use diffusion models' denoising capabilities to clean data before prediction.</p>"},{"location":"eval/generative_models_enhance_supervised/#method-1-denoising","title":"Method 1: Denoising","text":"<pre><code>def denoise_and_predict(X_noisy, diffusion_model, predictor):\n    \"\"\"\n    Denoise measurements before prediction.\n\n    Args:\n        X_noisy: Noisy gene expression measurements\n        diffusion_model: Trained diffusion model\n        predictor: Supervised predictor\n\n    Returns:\n        Y_pred: Predictions on denoised data\n    \"\"\"\n    # Denoise using diffusion model\n    # Add noise and then denoise (self-consistency)\n    X_denoised = diffusion_model.denoise(X_noisy)\n\n    # Predict on clean data\n    Y_pred = predictor.predict(X_denoised)\n\n    return Y_pred, X_denoised\n\n# Usage\nY_pred, X_clean = denoise_and_predict(X_measured, diffusion_model, gem1_predictor)\n</code></pre>"},{"location":"eval/generative_models_enhance_supervised/#method-2-imputation","title":"Method 2: Imputation","text":"<pre><code>def impute_missing_and_predict(X_partial, mask, diffusion_model, predictor):\n    \"\"\"\n    Impute missing genes before prediction.\n\n    Args:\n        X_partial: Gene expression with missing values\n        mask: Boolean mask (True = observed, False = missing)\n        diffusion_model: Trained diffusion model\n        predictor: Supervised predictor\n\n    Returns:\n        Y_pred: Prediction using imputed data\n    \"\"\"\n    # Impute missing values using diffusion model\n    X_imputed = diffusion_model.inpaint(X_partial, mask)\n\n    # Predict on complete data\n    Y_pred = predictor.predict(X_imputed)\n\n    return Y_pred, X_imputed\n\n# Example: Predict from partial gene panel\nmeasured_genes = ['GENE1', 'GENE2', ..., 'GENE100']  # Only 100 genes measured\nall_genes = ['GENE1', 'GENE2', ..., 'GENE5000']  # Predictor needs 5000 genes\n\nmask = create_mask(measured_genes, all_genes)\nX_partial = measurements[measured_genes]\n\nY_pred, X_full = impute_missing_and_predict(X_partial, mask, diffusion_model, predictor)\n</code></pre>"},{"location":"eval/generative_models_enhance_supervised/#method-3-batch-correction","title":"Method 3: Batch Correction","text":"<pre><code>def batch_correct_and_predict(X_batch1, X_batch2, diffusion_model, predictor):\n    \"\"\"\n    Remove batch effects before prediction.\n    \"\"\"\n    # Train diffusion model on combined data\n    X_combined = concatenate(X_batch1, X_batch2)\n    diffusion_model = train_diffusion(X_combined)\n\n    # Project to shared latent space (batch-invariant)\n    Z_batch1 = diffusion_model.encode(X_batch1)\n    Z_batch2 = diffusion_model.encode(X_batch2)\n\n    # Predict from batch-corrected representations\n    Y_pred_batch1 = predictor.predict(Z_batch1)\n    Y_pred_batch2 = predictor.predict(Z_batch2)\n\n    return Y_pred_batch1, Y_pred_batch2\n</code></pre>"},{"location":"eval/generative_models_enhance_supervised/#benefits_3","title":"Benefits","text":"<ul> <li>Improved accuracy: Clean data \u2192 better predictions</li> <li>Robustness: Handle missing values gracefully</li> <li>Cost reduction: Predict from cheaper partial measurements</li> <li>Integration: Combine data from different sources/batches</li> </ul>"},{"location":"eval/generative_models_enhance_supervised/#7-active-learning-and-experimental-design","title":"7. Active Learning and Experimental Design","text":""},{"location":"eval/generative_models_enhance_supervised/#the-problem_6","title":"The Problem","text":"<p>Which experiments should we run next to improve the predictor most efficiently?</p> <ul> <li>Experiments are expensive</li> <li>Label budget is limited</li> <li>Want maximum information gain</li> </ul>"},{"location":"eval/generative_models_enhance_supervised/#the-solution_6","title":"The Solution","text":"<p>Use generative models to explore the input space and identify informative samples.</p>"},{"location":"eval/generative_models_enhance_supervised/#method-1-uncertainty-based-sampling","title":"Method 1: Uncertainty-Based Sampling","text":"<pre><code>def active_learning_loop(diffusion_model, predictor, label_budget):\n    \"\"\"\n    Iteratively select most informative samples to label.\n\n    Args:\n        diffusion_model: Generative model\n        predictor: Current supervised predictor\n        label_budget: Number of samples we can afford to label\n\n    Returns:\n        selected_samples: Samples to measure/label next\n    \"\"\"\n    # Generate diverse candidate samples\n    X_candidates = diffusion_model.sample(n_samples=10000)\n\n    # Compute prediction uncertainty for each candidate\n    uncertainties = []\n    for x in X_candidates:\n        # Sample variations around this point\n        x_variations = diffusion_model.sample_around(x, n_samples=100)\n        y_predictions = predictor.predict(x_variations)\n        uncertainty = y_predictions.std()  # High std = high uncertainty\n        uncertainties.append(uncertainty)\n\n    # Select top-k most uncertain samples\n    top_k_indices = np.argsort(uncertainties)[-label_budget:]\n    selected_samples = X_candidates[top_k_indices]\n\n    return selected_samples\n\n# Usage\nX_to_measure = active_learning_loop(diffusion_model, current_predictor, budget=100)\nprint(f\"Recommended experiments: {len(X_to_measure)}\")\n\n# Perform experiments (measure Y for selected X)\nY_measured = perform_experiments(X_to_measure)\n\n# Update predictor with new data\nX_train_new = concatenate(X_train, X_to_measure)\nY_train_new = concatenate(Y_train, Y_measured)\npredictor_updated = train_predictor(X_train_new, Y_train_new)\n</code></pre>"},{"location":"eval/generative_models_enhance_supervised/#method-2-diversity-based-sampling","title":"Method 2: Diversity-Based Sampling","text":"<pre><code>def diverse_sampling(diffusion_model, predictor, n_samples):\n    \"\"\"\n    Select diverse samples that cover the input space.\n    \"\"\"\n    # Generate many candidates\n    X_candidates = diffusion_model.sample(n_samples=10000)\n\n    # Encode to latent space\n    Z_candidates = diffusion_model.encode(X_candidates)\n\n    # Select diverse subset (e.g., k-means clustering)\n    from sklearn.cluster import KMeans\n    kmeans = KMeans(n_clusters=n_samples)\n    kmeans.fit(Z_candidates)\n\n    # Select samples closest to cluster centers\n    selected_indices = []\n    for center in kmeans.cluster_centers_:\n        distances = np.linalg.norm(Z_candidates - center, axis=1)\n        closest = np.argmin(distances)\n        selected_indices.append(closest)\n\n    return X_candidates[selected_indices]\n</code></pre>"},{"location":"eval/generative_models_enhance_supervised/#method-3-targeted-exploration","title":"Method 3: Targeted Exploration","text":"<pre><code>def explore_region(diffusion_model, predictor, target_condition, n_samples):\n    \"\"\"\n    Explore a specific region of interest.\n    \"\"\"\n    # Generate samples in target region\n    X_region = diffusion_model.sample(\n        condition=target_condition,\n        n_samples=n_samples\n    )\n\n    # Filter for novel samples (far from training data)\n    Z_region = diffusion_model.encode(X_region)\n    Z_train = diffusion_model.encode(X_train)\n\n    distances_to_train = cdist(Z_region, Z_train).min(axis=1)\n    novel_indices = distances_to_train &gt; threshold\n\n    X_novel = X_region[novel_indices]\n\n    return X_novel\n\n# Example: Explore drug combinations not in training set\nX_novel_combos = explore_region(\n    diffusion_model,\n    predictor,\n    target_condition={'drug': 'combination_therapy'},\n    n_samples=1000\n)\n</code></pre>"},{"location":"eval/generative_models_enhance_supervised/#benefits_4","title":"Benefits","text":"<ul> <li>Efficient exploration: Focus experiments on informative regions</li> <li>Cost reduction: Achieve better performance with fewer labels</li> <li>Discovery: Find novel conditions/states</li> <li>Adaptive: Iteratively improve based on new data</li> </ul>"},{"location":"eval/generative_models_enhance_supervised/#practical-integration-enhancing-gem-1","title":"Practical Integration: Enhancing GEM-1","text":""},{"location":"eval/generative_models_enhance_supervised/#complete-workflow","title":"Complete Workflow","text":"<p>Here's how to integrate all seven mechanisms to enhance a predictor like GEM-1:</p> <pre><code># ============================================================================\n# Stage 1: Pre-training on Unlabeled Data\n# ============================================================================\n\n# Load all available gene expression data (labeled + unlabeled)\nX_all_unlabeled = load_gene_expression_atlas()  # 100,000 samples\nX_labeled, Y_labeled = load_labeled_drug_responses()  # 1,000 samples\n\n# Train diffusion model on all data\ndiffusion_model = train_conditional_diffusion(\n    X_all_unlabeled,\n    conditions=['cell_type', 'perturbation', 'dose']\n)\n\n# ============================================================================\n# Stage 2: Data Augmentation\n# ============================================================================\n\n# Generate synthetic training samples\nX_synthetic = diffusion_model.sample(n_samples=5000)\n\n# Pseudo-label with confidence filtering\nY_pseudo, confidence = initial_predictor.predict_with_confidence(X_synthetic)\nhigh_conf_mask = confidence &gt; 0.85\nX_synthetic_filtered = X_synthetic[high_conf_mask]\nY_synthetic_filtered = Y_pseudo[high_conf_mask]\n\n# Combine real and synthetic\nX_train_aug = concatenate(X_labeled, X_synthetic_filtered)\nY_train_aug = concatenate(Y_labeled, Y_synthetic_filtered)\n\n# ============================================================================\n# Stage 3: Representation Learning\n# ============================================================================\n\n# Extract diffusion encoder\nencoder = diffusion_model.get_encoder()\n\n# Encode training data\nZ_train = encoder(X_train_aug)\n\n# Train predictor on learned representations\ngem1_enhanced = train_predictor(\n    Z_train,\n    Y_train_aug,\n    architecture='deep_network'  # Can use deeper network with better features\n)\n\n# ============================================================================\n# Stage 4: Denoising Pipeline\n# ============================================================================\n\ndef predict_with_preprocessing(X_raw):\n    \"\"\"Enhanced prediction pipeline with denoising.\"\"\"\n    # Step 1: Denoise measurements\n    X_denoised = diffusion_model.denoise(X_raw)\n\n    # Step 2: Encode to learned representations\n    Z = encoder(X_denoised)\n\n    # Step 3: Predict\n    Y_pred = gem1_enhanced.predict(Z)\n\n    return Y_pred\n\n# ============================================================================\n# Stage 5: Uncertainty Quantification\n# ============================================================================\n\ndef predict_with_uncertainty(X_observed):\n    \"\"\"Prediction with confidence intervals.\"\"\"\n    # Sample plausible variations\n    X_samples = diffusion_model.sample_around(X_observed, n_samples=100)\n\n    # Predict on all samples\n    Y_predictions = [predict_with_preprocessing(x) for x in X_samples]\n\n    # Compute statistics\n    return {\n        'mean': np.mean(Y_predictions),\n        'std': np.std(Y_predictions),\n        'ci_95': np.percentile(Y_predictions, [2.5, 97.5])\n    }\n\n# ============================================================================\n# Stage 6: Counterfactual Generation\n# ============================================================================\n\ndef predict_drug_combination(drug_a, drug_b, cell_type):\n    \"\"\"Predict response to novel drug combination.\"\"\"\n    # Generate counterfactual gene expression\n    X_counterfactual = diffusion_model.sample(\n        condition={\n            'drug': f'{drug_a}+{drug_b}',\n            'cell_type': cell_type\n        },\n        n_samples=1000\n    )\n\n    # Predict response with uncertainty\n    Y_predictions = [predict_with_preprocessing(x) for x in X_counterfactual]\n\n    return {\n        'mean_response': np.mean(Y_predictions),\n        'response_distribution': Y_predictions\n    }\n\n# ============================================================================\n# Stage 7: Active Learning\n# ============================================================================\n\ndef recommend_next_experiments(budget=100):\n    \"\"\"Identify most informative experiments to run.\"\"\"\n    # Generate diverse candidates\n    X_candidates = diffusion_model.sample(n_samples=10000)\n\n    # Compute uncertainty\n    uncertainties = []\n    for x in X_candidates:\n        result = predict_with_uncertainty(x)\n        uncertainties.append(result['std'])\n\n    # Select high-uncertainty, diverse samples\n    top_uncertain = np.argsort(uncertainties)[-budget*2:]\n    X_uncertain = X_candidates[top_uncertain]\n\n    # Diversify selection\n    Z_uncertain = encoder(X_uncertain)\n    from sklearn.cluster import KMeans\n    kmeans = KMeans(n_clusters=budget)\n    kmeans.fit(Z_uncertain)\n\n    selected = []\n    for center in kmeans.cluster_centers_:\n        distances = np.linalg.norm(Z_uncertain - center, axis=1)\n        closest = np.argmin(distances)\n        selected.append(X_uncertain[closest])\n\n    return np.array(selected)\n\n# ============================================================================\n# Evaluation\n# ============================================================================\n\n# Compare baseline vs. enhanced\nX_test, Y_test = load_test_data()\n\n# Baseline GEM-1\nY_pred_baseline = gem1_baseline.predict(X_test)\nr2_baseline = r2_score(Y_test, Y_pred_baseline)\n\n# Enhanced GEM-1\nY_pred_enhanced = predict_with_preprocessing(X_test)\nr2_enhanced = r2_score(Y_test, Y_pred_enhanced)\n\nprint(f\"Baseline R\u00b2: {r2_baseline:.3f}\")\nprint(f\"Enhanced R\u00b2: {r2_enhanced:.3f}\")\nprint(f\"Improvement: {(r2_enhanced - r2_baseline):.3f}\")\n</code></pre>"},{"location":"eval/generative_models_enhance_supervised/#expected-improvements","title":"Expected Improvements","text":"<p>Based on literature and empirical results:</p> Metric Baseline Enhanced Improvement R\u00b2 score 0.65 0.78 +0.13 MAE 0.25 0.18 -0.07 Samples needed 5,000 1,000 5\u00d7 reduction Robustness (OOD) 0.45 0.62 +0.17"},{"location":"eval/generative_models_enhance_supervised/#summary","title":"Summary","text":""},{"location":"eval/generative_models_enhance_supervised/#seven-enhancement-mechanisms_1","title":"Seven Enhancement Mechanisms","text":"<ol> <li>Data augmentation: 10-30% performance gain with synthetic samples</li> <li>Conditional generation: Enable counterfactual reasoning and causal inference</li> <li>Representation learning: Better features from unlabeled data</li> <li>Uncertainty quantification: Principled confidence estimates</li> <li>Semi-supervised learning: 5-10\u00d7 reduction in labeled data requirements</li> <li>Denoising: 5-15% improvement from cleaner inputs</li> <li>Active learning: 2-5\u00d7 more efficient data collection</li> </ol>"},{"location":"eval/generative_models_enhance_supervised/#key-insights","title":"Key Insights","text":"<p>Generative models learn structure:</p> <ul> <li>Data manifolds</li> <li>Biological constraints</li> <li>Multi-scale patterns</li> <li>Invariances</li> </ul> <p>This structure enhances supervised learning:</p> <ul> <li>Better representations</li> <li>More training data</li> <li>Cleaner inputs</li> <li>Smarter exploration</li> </ul> <p>The synergy is multiplicative:</p> <ul> <li>Each mechanism provides complementary benefits</li> <li>Combined improvements can be substantial (30-50% gains)</li> <li>Particularly effective with limited labeled data</li> </ul>"},{"location":"eval/generative_models_enhance_supervised/#when-to-use-each-mechanism","title":"When to Use Each Mechanism","text":"Mechanism Best When Avoid When Data augmentation Limited labels, need diversity Synthetic quality poor Conditional generation Need counterfactuals, causal inference No conditional structure Representation learning High-dimensional inputs, unlabeled data available Very small datasets Uncertainty quantification Risk-sensitive decisions Computational constraints Semi-supervised Lots of unlabeled data Labels are cheap Denoising Noisy measurements Data already clean Active learning Expensive labels, iterative One-shot data collection"},{"location":"eval/generative_models_enhance_supervised/#recommended-starting-point","title":"Recommended Starting Point","text":"<p>For most biological applications:</p> <ol> <li>Start with representation learning (easiest, most robust)</li> <li>Add data augmentation (if labels are limited)</li> <li>Incorporate denoising (if measurements are noisy)</li> <li>Use uncertainty quantification (for deployment)</li> <li>Apply active learning (for iterative improvement)</li> </ol>"},{"location":"eval/generative_models_enhance_supervised/#related-documents","title":"Related Documents","text":"<ul> <li>Prediction Consistency Metric \u2014 Evaluating generative models via predictors</li> <li>Evaluating Generative Models \u2014 Comprehensive evaluation guide</li> <li>Epiplexity: From Entropy to Epiplexity \u2014 Learnable structure</li> <li>DDPM Training \u2014 Training diffusion models</li> <li>DDPM Sampling \u2014 Sampling methods</li> </ul>"},{"location":"eval/generative_models_enhance_supervised/#references","title":"References","text":""},{"location":"eval/generative_models_enhance_supervised/#data-augmentation","title":"Data Augmentation","text":"<ol> <li>Trabucco, B., et al. (2023). Effective Data Augmentation With Diffusion Models. ICLR.</li> <li>Azizi, S., et al. (2023). Synthetic Data from Diffusion Models Improves ImageNet Classification. TMLR.</li> </ol>"},{"location":"eval/generative_models_enhance_supervised/#representation-learning","title":"Representation Learning","text":"<ol> <li>Lopez, R., et al. (2018). Deep generative modeling for single-cell transcriptomics. Nature Methods.</li> <li>Lotfollahi, M., et al. (2020). scGen predicts single-cell perturbation responses. Nature Methods.</li> </ol>"},{"location":"eval/generative_models_enhance_supervised/#semi-supervised-learning","title":"Semi-Supervised Learning","text":"<ol> <li>Kingma, D. P., et al. (2014). Semi-supervised learning with deep generative models. NeurIPS.</li> <li>Sohn, K., et al. (2015). Learning structured output representation using deep conditional generative models. NeurIPS.</li> </ol>"},{"location":"eval/generative_models_enhance_supervised/#uncertainty-quantification","title":"Uncertainty Quantification","text":"<ol> <li>Gal, Y., &amp; Ghahramani, Z. (2016). Dropout as a Bayesian approximation. ICML.</li> <li>Lakshminarayanan, B., et al. (2017). Simple and scalable predictive uncertainty estimation. NeurIPS.</li> </ol>"},{"location":"eval/generative_models_enhance_supervised/#active-learning","title":"Active Learning","text":"<ol> <li>Settles, B. (2009). Active learning literature survey. Computer Sciences Technical Report.</li> <li>Ash, J. T., et al. (2020). Deep batch active learning by diverse, uncertain gradient lower bounds. ICLR.</li> </ol>"},{"location":"eval/generative_models_enhance_supervised/#biological-applications","title":"Biological Applications","text":"<ol> <li>Bunne, C., et al. (2023). Learning single-cell perturbation responses using neural optimal transport. Nature Methods.</li> <li>Lotfollahi, M., et al. (2023). Predicting cellular responses to perturbations with deep generative models. Nature Methods.</li> </ol>"},{"location":"eval/prediction_consistency_metric/","title":"Prediction Consistency: Evaluating Generative Models via Downstream Predictors","text":"<p>A critical question when evaluating generative models for biological data is: Does the generated data obey the same biological relationships that exist in real data?</p> <p>Prediction consistency provides a principled way to answer this question by testing whether generated samples are consistent with learned biological relationships captured by supervised prediction models.</p> <p>This document explains the theory, implementation, and best practices for using prediction consistency as an evaluation metric for generative models.</p>"},{"location":"eval/prediction_consistency_metric/#overview","title":"Overview","text":""},{"location":"eval/prediction_consistency_metric/#the-core-idea","title":"The Core Idea","text":"<p>Prediction consistency measures whether samples from a generative model are compatible with biological relationships learned by a supervised predictor.</p> <p>Setup: 1. Train a supervised predictor on real data: \\(f: X \\rightarrow Y\\)    - Example: Gene expression \u2192 drug response (like GEM-1) 2. Generate synthetic samples: \\(\\tilde{X} \\sim p_\\theta(\\text{data})\\) 3. Predict on synthetic samples: \\(\\tilde{Y} = f(\\tilde{X})\\) 4. Compare predictions to expected biological behavior</p> <p>Key insight: If the generative model captures true biological structure, its samples should produce predictions that are consistent with known biology.</p>"},{"location":"eval/prediction_consistency_metric/#why-this-matters","title":"Why This Matters","text":"<p>Traditional evaluation metrics focus on statistical properties: - FID: Distribution distance in feature space - Correlation: Gene-gene relationships - Marginals: Per-gene statistics</p> <p>But these don't directly test:</p> <ul> <li>Does generated data obey biological laws?</li> <li>Would generated data be useful for downstream tasks?</li> <li>Does it capture causal relationships?</li> </ul> <p>Prediction consistency bridges this gap by testing biological validity through learned relationships.</p>"},{"location":"eval/prediction_consistency_metric/#theoretical-foundation","title":"Theoretical Foundation","text":""},{"location":"eval/prediction_consistency_metric/#what-prediction-consistency-measures","title":"What Prediction Consistency Measures","text":"<p>Given: - Real data distribution: \\(p_{\\text{real}}(X, Y)\\) - Generative model: \\(p_\\theta(X)\\) - Learned predictor: \\(f: X \\rightarrow Y\\)</p> <p>Prediction consistency tests:</p> \\[ p_\\theta(Y | f) \\stackrel{?}{\\approx} p_{\\text{real}}(Y) \\] <p>where \\(p_\\theta(Y | f)\\) is the distribution of predictions when applying \\(f\\) to samples from \\(p_\\theta(X)\\).</p>"},{"location":"eval/prediction_consistency_metric/#interpretation","title":"Interpretation","text":"<p>High consistency means: - Generated samples produce predictions similar to real data - Generative model respects learned biological relationships - Samples are likely biologically plausible</p> <p>Low consistency means: - Generated samples violate learned relationships - Model may be memorizing noise without capturing biology - Samples may be statistically correct but biologically invalid</p>"},{"location":"eval/prediction_consistency_metric/#connection-to-conditional-distributions","title":"Connection to Conditional Distributions","text":"<p>For conditional generation, we can test:</p> \\[ p_\\theta(Y | X, c) \\stackrel{?}{\\approx} p_{\\text{real}}(Y | c) \\] <p>where \\(c\\) is a condition (e.g., cell type, perturbation).</p> <p>This tests whether the generative model correctly captures conditional relationships.</p>"},{"location":"eval/prediction_consistency_metric/#implementation-approaches","title":"Implementation Approaches","text":""},{"location":"eval/prediction_consistency_metric/#1-distribution-consistency","title":"1. Distribution Consistency","text":"<p>Objective: Compare the distribution of predictions on real vs. generated data.</p> <p>Method:</p> <pre><code>import numpy as np\nfrom scipy.stats import wasserstein_distance, ks_2samp\n\ndef distribution_consistency(predictor, X_real, X_gen):\n    \"\"\"\n    Measure consistency of prediction distributions.\n\n    Args:\n        predictor: Trained prediction model (e.g., GEM-1)\n        X_real: Real gene expression data [N, genes]\n        X_gen: Generated gene expression data [M, genes]\n\n    Returns:\n        metrics: Dictionary of consistency metrics\n    \"\"\"\n    # Get predictions\n    Y_pred_real = predictor.predict(X_real)\n    Y_pred_gen = predictor.predict(X_gen)\n\n    # Compute distribution distances\n    metrics = {}\n\n    # Wasserstein distance (Earth Mover's Distance)\n    metrics['wasserstein'] = wasserstein_distance(\n        Y_pred_real.flatten(), \n        Y_pred_gen.flatten()\n    )\n\n    # Kolmogorov-Smirnov test\n    ks_stat, ks_pval = ks_2samp(\n        Y_pred_real.flatten(), \n        Y_pred_gen.flatten()\n    )\n    metrics['ks_statistic'] = ks_stat\n    metrics['ks_pvalue'] = ks_pval\n\n    # Mean and variance comparison\n    metrics['mean_diff'] = np.abs(\n        Y_pred_real.mean() - Y_pred_gen.mean()\n    )\n    metrics['var_ratio'] = (\n        Y_pred_gen.var() / Y_pred_real.var()\n    )\n\n    return metrics\n</code></pre> <p>Interpretation:</p> <ul> <li>Wasserstein distance: Lower is better (0 = identical distributions)</li> <li>KS p-value: Higher is better (&gt;0.05 suggests distributions are similar)</li> <li>Mean difference: Should be small</li> <li>Variance ratio: Should be close to 1</li> </ul>"},{"location":"eval/prediction_consistency_metric/#2-conditional-consistency","title":"2. Conditional Consistency","text":"<p>Objective: Test if conditional generation produces appropriate predictions.</p> <p>Method:</p> <pre><code>def conditional_consistency(predictor, gen_model, conditions, Y_expected):\n    \"\"\"\n    Test if conditional generation produces expected predictions.\n\n    Args:\n        predictor: Trained prediction model\n        gen_model: Conditional generative model\n        conditions: List of conditions to test\n        Y_expected: Expected prediction ranges for each condition\n\n    Returns:\n        consistency_scores: Per-condition consistency\n    \"\"\"\n    consistency_scores = {}\n\n    for condition in conditions:\n        # Generate samples for this condition\n        X_gen = gen_model.sample(\n            condition=condition, \n            n_samples=1000\n        )\n\n        # Predict\n        Y_pred = predictor.predict(X_gen)\n\n        # Check if predictions match expected range\n        Y_exp_mean, Y_exp_std = Y_expected[condition]\n\n        # Z-score of mean prediction\n        z_score = abs(Y_pred.mean() - Y_exp_mean) / Y_exp_std\n\n        # Fraction of predictions in expected range\n        in_range = np.mean(\n            (Y_pred &gt;= Y_exp_mean - 2*Y_exp_std) &amp; \n            (Y_pred &lt;= Y_exp_mean + 2*Y_exp_std)\n        )\n\n        consistency_scores[condition] = {\n            'z_score': z_score,\n            'fraction_in_range': in_range,\n            'mean_prediction': Y_pred.mean(),\n            'std_prediction': Y_pred.std()\n        }\n\n    return consistency_scores\n</code></pre>"},{"location":"eval/prediction_consistency_metric/#3-perturbation-response-consistency","title":"3. Perturbation Response Consistency","text":"<p>Objective: Test if generated perturbation responses match known biology.</p> <p>Method:</p> <pre><code>def perturbation_consistency(predictor, gen_model, perturbations):\n    \"\"\"\n    Test if perturbation responses are biologically consistent.\n\n    Args:\n        predictor: Trained prediction model\n        gen_model: Conditional generative model\n        perturbations: List of (baseline, perturbed) condition pairs\n\n    Returns:\n        response_metrics: Perturbation response consistency\n    \"\"\"\n    response_metrics = {}\n\n    for baseline_cond, perturbed_cond in perturbations:\n        # Generate baseline and perturbed samples\n        X_baseline = gen_model.sample(condition=baseline_cond, n_samples=1000)\n        X_perturbed = gen_model.sample(condition=perturbed_cond, n_samples=1000)\n\n        # Predict responses\n        Y_baseline = predictor.predict(X_baseline)\n        Y_perturbed = predictor.predict(X_perturbed)\n\n        # Compute response\n        delta_Y = Y_perturbed - Y_baseline\n\n        # Compare to known perturbation effect\n        known_effect = get_known_perturbation_effect(\n            baseline_cond, \n            perturbed_cond\n        )\n\n        # Metrics\n        response_metrics[f\"{baseline_cond}_to_{perturbed_cond}\"] = {\n            'mean_response': delta_Y.mean(),\n            'expected_response': known_effect['mean'],\n            'response_error': abs(delta_Y.mean() - known_effect['mean']),\n            'direction_correct': np.sign(delta_Y.mean()) == np.sign(known_effect['mean']),\n            'effect_size_ratio': delta_Y.mean() / known_effect['mean']\n        }\n\n    return response_metrics\n</code></pre>"},{"location":"eval/prediction_consistency_metric/#4-multi-predictor-ensemble-consistency","title":"4. Multi-Predictor Ensemble Consistency","text":"<p>Objective: Use multiple predictors to avoid dependence on a single model.</p> <p>Method:</p> <pre><code>def ensemble_consistency(predictors, X_real, X_gen):\n    \"\"\"\n    Measure consistency across multiple predictors.\n\n    Args:\n        predictors: List of trained prediction models\n        X_real: Real data\n        X_gen: Generated data\n\n    Returns:\n        ensemble_metrics: Aggregated consistency scores\n    \"\"\"\n    consistency_scores = []\n\n    for predictor in predictors:\n        # Get predictions\n        Y_real = predictor.predict(X_real)\n        Y_gen = predictor.predict(X_gen)\n\n        # Compute consistency for this predictor\n        score = wasserstein_distance(Y_real.flatten(), Y_gen.flatten())\n        consistency_scores.append(score)\n\n    ensemble_metrics = {\n        'mean_consistency': np.mean(consistency_scores),\n        'std_consistency': np.std(consistency_scores),\n        'min_consistency': np.min(consistency_scores),\n        'max_consistency': np.max(consistency_scores),\n        'per_predictor': consistency_scores\n    }\n\n    return ensemble_metrics\n</code></pre>"},{"location":"eval/prediction_consistency_metric/#practical-examples","title":"Practical Examples","text":""},{"location":"eval/prediction_consistency_metric/#example-1-drug-response-prediction","title":"Example 1: Drug Response Prediction","text":"<p>Scenario: Evaluating a diffusion model that generates gene expression data.</p> <pre><code># Setup\ngem1_predictor = load_pretrained_gem1()  # Predicts drug response\ndiffusion_model = train_diffusion_model(gene_expression_data)\n\n# Real data\nX_real, Y_real = load_test_data()\n\n# Generated data\nX_gen = diffusion_model.sample(n_samples=10000)\n\n# Evaluate prediction consistency\nconsistency = distribution_consistency(\n    predictor=gem1_predictor,\n    X_real=X_real,\n    X_gen=X_gen\n)\n\nprint(f\"Wasserstein distance: {consistency['wasserstein']:.4f}\")\nprint(f\"KS p-value: {consistency['ks_pvalue']:.4f}\")\nprint(f\"Mean difference: {consistency['mean_diff']:.4f}\")\n</code></pre> <p>Interpretation:</p> <ul> <li>Wasserstein &lt; 0.1: Excellent consistency</li> <li>KS p-value &gt; 0.05: Distributions statistically similar</li> <li>Mean difference &lt; 0.05: Predictions well-calibrated</li> </ul>"},{"location":"eval/prediction_consistency_metric/#example-2-cell-type-specific-generation","title":"Example 2: Cell Type-Specific Generation","text":"<p>Scenario: Testing conditional generation of cell type-specific gene expression.</p> <pre><code># Conditional diffusion model\ncond_diffusion = train_conditional_diffusion(\n    gene_expression_data,\n    conditions=cell_types\n)\n\n# Expected predictions for each cell type\nY_expected = {\n    'T_cell': (0.8, 0.1),      # (mean, std) for T cell marker\n    'B_cell': (0.2, 0.05),     # Low T cell marker for B cells\n    'Macrophage': (0.1, 0.05)  # Low T cell marker for macrophages\n}\n\n# Test consistency\nconsistency = conditional_consistency(\n    predictor=tcell_marker_predictor,\n    gen_model=cond_diffusion,\n    conditions=['T_cell', 'B_cell', 'Macrophage'],\n    Y_expected=Y_expected\n)\n\nfor cell_type, metrics in consistency.items():\n    print(f\"{cell_type}:\")\n    print(f\"  Z-score: {metrics['z_score']:.2f}\")\n    print(f\"  Fraction in range: {metrics['fraction_in_range']:.2%}\")\n</code></pre>"},{"location":"eval/prediction_consistency_metric/#example-3-perturbation-response","title":"Example 3: Perturbation Response","text":"<p>Scenario: Validating that generated perturbation responses match known biology.</p> <pre><code># Perturbations to test\nperturbations = [\n    ('control', 'drug_A'),\n    ('control', 'drug_B'),\n    ('drug_A', 'drug_A_plus_B')\n]\n\n# Test perturbation consistency\nresponse_metrics = perturbation_consistency(\n    predictor=response_predictor,\n    gen_model=conditional_diffusion,\n    perturbations=perturbations\n)\n\nfor pert, metrics in response_metrics.items():\n    print(f\"\\n{pert}:\")\n    print(f\"  Mean response: {metrics['mean_response']:.3f}\")\n    print(f\"  Expected: {metrics['expected_response']:.3f}\")\n    print(f\"  Direction correct: {metrics['direction_correct']}\")\n    print(f\"  Effect size ratio: {metrics['effect_size_ratio']:.2f}\")\n</code></pre>"},{"location":"eval/prediction_consistency_metric/#advantages","title":"Advantages","text":""},{"location":"eval/prediction_consistency_metric/#1-biologically-grounded","title":"1. Biologically Grounded","text":"<p>Tests whether generated data obeys learned biological relationships, not just statistical properties.</p>"},{"location":"eval/prediction_consistency_metric/#2-task-relevant","title":"2. Task-Relevant","text":"<p>Directly measures whether generated data would be useful for downstream prediction tasks.</p>"},{"location":"eval/prediction_consistency_metric/#3-detects-spurious-patterns","title":"3. Detects Spurious Patterns","text":"<p>Generated data that matches statistics but violates biology will score poorly.</p> <p>Example: A model that generates gene expression with correct marginals and correlations but violates pathway logic will produce inconsistent predictions.</p>"},{"location":"eval/prediction_consistency_metric/#4-complementary-to-other-metrics","title":"4. Complementary to Other Metrics","text":"<p>Works alongside statistical metrics (FID, correlations) and biological metrics (pathway enrichment).</p>"},{"location":"eval/prediction_consistency_metric/#5-interpretable","title":"5. Interpretable","text":"<p>Easy to explain: \"Do generated samples produce the same predictions as real samples?\"</p>"},{"location":"eval/prediction_consistency_metric/#limitations-and-caveats","title":"Limitations and Caveats","text":""},{"location":"eval/prediction_consistency_metric/#1-predictor-quality-dependency","title":"1. Predictor Quality Dependency","text":"<p>Issue: Consistency is only meaningful if the predictor is accurate.</p> <p>Solutions:</p> <ul> <li>Validate predictor performance on held-out real data first</li> <li>Use multiple predictors (ensemble approach)</li> <li>Report predictor accuracy alongside consistency scores</li> </ul> <p>Example: <pre><code># Validate predictor first\npredictor_r2 = evaluate_predictor(predictor, X_test, Y_test)\nif predictor_r2 &lt; 0.7:\n    print(\"Warning: Predictor has low accuracy, consistency may be unreliable\")\n</code></pre></p>"},{"location":"eval/prediction_consistency_metric/#2-circular-reasoning-risk","title":"2. Circular Reasoning Risk","text":"<p>Issue: If the generative model was explicitly trained to match the predictor, high consistency is expected but not informative.</p> <p>Solutions:</p> <ul> <li>Use predictors trained independently</li> <li>Use held-out predictors not seen during generative model training</li> <li>Test on orthogonal prediction tasks</li> </ul>"},{"location":"eval/prediction_consistency_metric/#3-novel-biology-detection","title":"3. Novel Biology Detection","text":"<p>Issue: Generated samples representing valid but novel biological states may score poorly.</p> <p>Example: A generative model discovers a new cell state that the predictor hasn't seen.</p> <p>Solutions:</p> <ul> <li>Distinguish \"inconsistent with predictor\" from \"biologically invalid\"</li> <li>Investigate low-consistency samples manually</li> <li>Use multiple predictors covering different aspects of biology</li> </ul>"},{"location":"eval/prediction_consistency_metric/#4-mode-collapse-masking","title":"4. Mode Collapse Masking","text":"<p>Issue: High consistency could result from generating only \"safe\" samples the predictor handles well.</p> <p>Solutions:</p> <ul> <li>Also measure diversity (epiplexity, coverage metrics)</li> <li>Check prediction variance on generated samples</li> <li>Visualize generated sample distribution</li> </ul> <p>Example: <pre><code># Check if generated samples are diverse\ndiversity_score = compute_diversity(X_gen)\nif consistency['wasserstein'] &lt; 0.1 and diversity_score &lt; 0.5:\n    print(\"Warning: High consistency but low diversity - possible mode collapse\")\n</code></pre></p>"},{"location":"eval/prediction_consistency_metric/#5-conditional-distribution-mismatch","title":"5. Conditional Distribution Mismatch","text":"<p>Issue: Predictor may have learned \\(p(Y|X)\\) under specific conditions not present in generated data.</p> <p>Solutions:</p> <ul> <li>Match conditions between training and generation</li> <li>Use conditional generation with explicit condition control</li> <li>Stratify analysis by condition</li> </ul>"},{"location":"eval/prediction_consistency_metric/#best-practices","title":"Best Practices","text":""},{"location":"eval/prediction_consistency_metric/#1-multi-metric-framework","title":"1. Multi-Metric Framework","text":"<p>Never use prediction consistency alone. Combine with:</p> Metric Type Examples Purpose Statistical FID, correlations, marginals Distribution matching Biological Pathway enrichment, markers Biological structure Prediction Consistency (this document) Learned relationships Epiplexity Loss curve area Learnable structure Downstream Task performance Practical utility"},{"location":"eval/prediction_consistency_metric/#2-validate-predictors-first","title":"2. Validate Predictors First","text":"<pre><code># Step 1: Validate predictor\npredictor_metrics = evaluate_predictor(predictor, X_val, Y_val)\nprint(f\"Predictor R\u00b2: {predictor_metrics['r2']:.3f}\")\nprint(f\"Predictor MAE: {predictor_metrics['mae']:.3f}\")\n\n# Step 2: Only proceed if predictor is reliable\nif predictor_metrics['r2'] &gt; 0.7:\n    consistency = distribution_consistency(predictor, X_real, X_gen)\nelse:\n    print(\"Predictor not reliable enough for consistency evaluation\")\n</code></pre>"},{"location":"eval/prediction_consistency_metric/#3-use-multiple-predictors","title":"3. Use Multiple Predictors","text":"<pre><code># Train multiple predictors with different architectures\npredictors = [\n    train_linear_predictor(X_train, Y_train),\n    train_rf_predictor(X_train, Y_train),\n    train_nn_predictor(X_train, Y_train)\n]\n\n# Compute ensemble consistency\nensemble_metrics = ensemble_consistency(predictors, X_real, X_gen)\nprint(f\"Mean consistency: {ensemble_metrics['mean_consistency']:.4f}\")\nprint(f\"Std consistency: {ensemble_metrics['std_consistency']:.4f}\")\n</code></pre>"},{"location":"eval/prediction_consistency_metric/#4-test-multiple-prediction-tasks","title":"4. Test Multiple Prediction Tasks","text":"<pre><code># Different prediction tasks\ntasks = {\n    'drug_response': drug_response_predictor,\n    'cell_type': cell_type_classifier,\n    'pathway_activity': pathway_predictor,\n    'perturbation_effect': perturbation_predictor\n}\n\n# Evaluate consistency for each task\nfor task_name, predictor in tasks.items():\n    consistency = distribution_consistency(predictor, X_real, X_gen)\n    print(f\"{task_name}: {consistency['wasserstein']:.4f}\")\n</code></pre>"},{"location":"eval/prediction_consistency_metric/#5-stratify-by-conditions","title":"5. Stratify by Conditions","text":"<p>For conditional generation:</p> <pre><code># Evaluate consistency separately for each condition\nconditions = ['cell_type_A', 'cell_type_B', 'cell_type_C']\n\nfor condition in conditions:\n    X_real_cond = filter_by_condition(X_real, condition)\n    X_gen_cond = gen_model.sample(condition=condition, n_samples=1000)\n\n    consistency = distribution_consistency(\n        predictor, X_real_cond, X_gen_cond\n    )\n    print(f\"{condition}: {consistency['wasserstein']:.4f}\")\n</code></pre>"},{"location":"eval/prediction_consistency_metric/#6-report-comprehensive-metrics","title":"6. Report Comprehensive Metrics","text":"<p>Minimum reporting:</p> <ul> <li>Predictor performance (R\u00b2, accuracy, etc.)</li> <li>Consistency metric values</li> <li>Number of samples used</li> <li>Conditions tested</li> <li>Multiple predictors if available</li> </ul> <p>Example report: <pre><code>Prediction Consistency Evaluation\n==================================\nPredictor: GEM-1 (R\u00b2 = 0.82 on validation)\nReal samples: 10,000\nGenerated samples: 10,000\n\nConsistency Metrics:\n- Wasserstein distance: 0.08 \u00b1 0.01 (3 runs)\n- KS p-value: 0.23\n- Mean difference: 0.03\n- Variance ratio: 0.98\n\nInterpretation: High consistency - generated samples produce\npredictions similar to real data.\n</code></pre></p>"},{"location":"eval/prediction_consistency_metric/#integration-with-other-evaluation-methods","title":"Integration with Other Evaluation Methods","text":""},{"location":"eval/prediction_consistency_metric/#combining-with-epiplexity","title":"Combining with Epiplexity","text":"<p>Complementary insights:</p> <ul> <li>Epiplexity: Does generated data teach learnable structure?</li> <li>Prediction consistency: Does it obey learned relationships?</li> </ul> <p>Joint interpretation:</p> Epiplexity Consistency Interpretation High High Excellent - teaches structure and obeys biology High Low Novel patterns - investigate further Low High Mode collapse - safe but limited samples Low Low Poor quality - noise without structure"},{"location":"eval/prediction_consistency_metric/#combining-with-fid","title":"Combining with FID","text":"<p>Complementary insights:</p> <ul> <li>FID: Statistical distribution matching</li> <li>Prediction consistency: Biological relationship matching</li> </ul> <p>Example workflow: <pre><code># Compute both metrics\nfid_score = compute_fid(X_real, X_gen)\nconsistency = distribution_consistency(predictor, X_real, X_gen)\n\n# Joint evaluation\nif fid_score &lt; 20 and consistency['wasserstein'] &lt; 0.1:\n    print(\"Excellent: Good statistical and biological quality\")\nelif fid_score &lt; 20 and consistency['wasserstein'] &gt; 0.2:\n    print(\"Warning: Good statistics but poor biological consistency\")\nelif fid_score &gt; 50 and consistency['wasserstein'] &lt; 0.1:\n    print(\"Unusual: Poor statistics but good biological consistency\")\n</code></pre></p>"},{"location":"eval/prediction_consistency_metric/#case-study-comparing-generative-models","title":"Case Study: Comparing Generative Models","text":""},{"location":"eval/prediction_consistency_metric/#scenario","title":"Scenario","text":"<p>Compare three generative models for gene expression: 1. VAE 2. Flow Matching 3. Diffusion Model</p>"},{"location":"eval/prediction_consistency_metric/#evaluation-protocol","title":"Evaluation Protocol","text":"<pre><code># Train all models on same data\nmodels = {\n    'VAE': train_vae(X_train),\n    'Flow': train_flow_matching(X_train),\n    'Diffusion': train_diffusion(X_train)\n}\n\n# Predictors\npredictors = {\n    'GEM-1': gem1_predictor,\n    'Cell Type': cell_type_classifier,\n    'Pathway': pathway_predictor\n}\n\n# Evaluate each model\nresults = {}\nfor model_name, model in models.items():\n    X_gen = model.sample(n_samples=10000)\n\n    results[model_name] = {}\n    for pred_name, predictor in predictors.items():\n        consistency = distribution_consistency(\n            predictor, X_real, X_gen\n        )\n        results[model_name][pred_name] = consistency['wasserstein']\n\n# Display results\nimport pandas as pd\ndf = pd.DataFrame(results).T\nprint(df)\n</code></pre> <p>Example output: <pre><code>           GEM-1  Cell Type  Pathway\nVAE         0.15       0.12     0.18\nFlow        0.09       0.10     0.11\nDiffusion   0.07       0.08     0.09\n</code></pre></p> <p>Interpretation: Diffusion model shows best consistency across all prediction tasks.</p>"},{"location":"eval/prediction_consistency_metric/#summary","title":"Summary","text":""},{"location":"eval/prediction_consistency_metric/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Prediction consistency tests biological validity through learned relationships</li> <li>Complements statistical metrics like FID and correlations</li> <li>Requires validated predictors - predictor quality matters</li> <li>Use in multi-metric framework - never rely on single metric</li> <li>Interpretable and actionable - directly measures downstream utility</li> </ol>"},{"location":"eval/prediction_consistency_metric/#when-to-use-prediction-consistency","title":"When to Use Prediction Consistency","text":"<p>\u2705 Use when: - Have reliable supervised predictors - Want to test biological validity - Need task-relevant evaluation - Comparing generative models for specific applications</p> <p>\u26a0\ufe0f Be cautious when: - Predictors have low accuracy - Generative model was trained to match predictor - Exploring novel biological states - Limited validation data</p>"},{"location":"eval/prediction_consistency_metric/#recommended-workflow","title":"Recommended Workflow","text":"<ol> <li>Validate predictors on held-out real data</li> <li>Compute consistency on generated samples</li> <li>Compare with other metrics (FID, epiplexity, etc.)</li> <li>Investigate discrepancies between metrics</li> <li>Report comprehensive results with all metrics</li> </ol>"},{"location":"eval/prediction_consistency_metric/#related-documents","title":"Related Documents","text":"<ul> <li>Evaluating Generative Models \u2014 Comprehensive evaluation guide</li> <li>FID: Fr\u00e9chet Inception Distance \u2014 Statistical distribution metric</li> <li>Epiplexity: From Entropy to Epiplexity \u2014 Learnable structure metric</li> <li>How Generative Models Enhance Supervised Methods \u2014 Synergy between generative and supervised learning</li> </ul>"},{"location":"eval/prediction_consistency_metric/#references","title":"References","text":""},{"location":"eval/prediction_consistency_metric/#prediction-consistency-in-practice","title":"Prediction Consistency in Practice","text":"<ol> <li>Lotfollahi, M., et al. (2023). Predicting cellular responses to perturbations with deep generative models. Nature Methods.</li> <li>Bunne, C., et al. (2023). Learning Single-Cell Perturbation Responses using Neural Optimal Transport. Nature Methods.</li> </ol>"},{"location":"eval/prediction_consistency_metric/#evaluation-frameworks","title":"Evaluation Frameworks","text":"<ol> <li>Xu, Q., et al. (2018). An empirical study on evaluation metrics of generative adversarial networks. arXiv.</li> <li>Borji, A. (2019). Pros and cons of GAN evaluation measures. Computer Vision and Image Understanding.</li> </ol>"},{"location":"eval/prediction_consistency_metric/#biological-validation","title":"Biological Validation","text":"<ol> <li>Eraslan, G., et al. (2019). Single-cell RNA-seq denoising using a deep count autoencoder. Nature Communications.</li> <li>Lopez, R., et al. (2018). Deep generative modeling for single-cell transcriptomics. Nature Methods.</li> </ol>"},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/","title":"From Entropy to Epiplexity: Rethinking Information in Learning Systems","text":"<p>The concept of epiplexity represents a fundamental shift in how we measure information in machine learning systems. Rather than treating information as an abstract property of data alone, epiplexity recognizes that information is observer-dependent and constrained by computational resources.</p> <p>This document explores epiplexity's theoretical foundations and its practical applications to generative models, particularly for biological data such as gene expression profiles.</p>"},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#the-fundamental-problem-with-classical-information-theory","title":"The Fundamental Problem with Classical Information Theory","text":"<p>Classical information theory\u2014Shannon entropy, KL divergence, likelihood\u2014quietly assumes something almost never true in practice:</p> <p>An observer with infinite compute and perfect inference.</p>"},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#assumptions-that-break-in-practice","title":"Assumptions That Break in Practice","text":"<p>This assumption leads to several theoretical predictions that contradict empirical observations:</p> <ul> <li>Deterministic transforms can't add information: Yet data augmentation improves generalization</li> <li>Synthetic data is just re-sampling: Yet models trained on synthetic data often generalize better</li> <li>Likelihood captures everything: Yet ordering, architecture, and inductive bias clearly matter</li> </ul> <p>Modern machine learning repeatedly violates these classical intuitions\u2014successfully. Diffusion models trained on transformed, augmented, or even synthetic data generalize better than theory predicts. Something fundamental is missing from the classical framework.</p>"},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#the-epiplexity-perspective","title":"The Epiplexity Perspective","text":"<p>Epiplexity introduces a subtle but powerful shift:</p> <p>Information is observer-dependent, and the observer is compute-bounded.</p> <p>Rather than asking \"how many bits exist in this data?\", epiplexity asks:</p> <p>How much usable structure can a bounded learner extract from this dataset?</p> <p>This reframes information as what survives contact with: - Stochastic gradient descent (SGD) - Finite network depth and width - Limited training time - Realistic computational budgets</p> <p>This perspective is particularly relevant for generative AI in biology, where models must extract meaningful structure from noisy, high-dimensional data under practical constraints.</p>"},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#epiplexity-learnable-structure-under-computational-constraints","title":"Epiplexity: Learnable Structure Under Computational Constraints","text":""},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#core-definition","title":"Core Definition","text":"<p>Epiplexity measures the amount of structural information a computationally bounded model can extract from data.</p> <p>Operationally, epiplexity is defined via minimum description length (MDL) under bounded models. The key insights:</p> <ul> <li>Random noise: High entropy but low epiplexity (no learnable structure)</li> <li>Structured data: Compresses progressively as the model learns</li> <li>Useful information: Manifests as early and sustained loss reduction</li> </ul>"},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#practical-approximation","title":"Practical Approximation","text":"<p>The paper proposes a practical approximation:</p> \\[ \\text{Epiplexity} \\approx \\int_0^T \\left(L(t) - L_{\\text{final}}\\right) dt \\] <p>where \\(L(t)\\) is the loss at training step \\(t\\), and \\(L_{\\text{final}}\\) is the final converged loss.</p> <p>Interpretation: Area under the loss curve above final loss.</p> <p>This is not merely a training artifact\u2014it's a signature of inductive structure. If a dataset contains reusable structure, the loss doesn't just drop at the end; it drops: - Earlier in training - Faster per compute unit - More smoothly across the trajectory</p>"},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#epiplexity-and-diffusion-models","title":"Epiplexity and Diffusion Models","text":""},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#standard-diffusion-model-evaluation","title":"Standard Diffusion Model Evaluation","text":"<p>Diffusion models are typically evaluated using:</p> <ul> <li>Likelihood / ELBO: Theoretical data fit</li> <li>Sample quality metrics: FID, IS, precision/recall</li> <li>Downstream task performance: Transfer learning</li> <li>OOD generalization: Often poorly quantified</li> </ul> <p>These metrics focus on whether the model matches the data distribution, but not on how efficiently it learns or what structure it captures.</p>"},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#natural-learning-trajectories-in-diffusion","title":"Natural Learning Trajectories in Diffusion","text":"<p>Diffusion training provides something uniquely suited to epiplexity analysis:</p> <p>A natural learning trajectory across noise scales.</p> <p>Diffusion models don't learn all structure simultaneously. Instead, they learn hierarchically:</p> <ol> <li>Coarse, low-frequency structure first (global patterns)</li> <li>Fine, high-frequency detail later (local features)</li> <li>Correlations before exact realizations</li> </ol> <p>This hierarchical learning enables new questions:</p> <ul> <li>At what noise levels does structure become learnable?</li> <li>How early does the model capture the data manifold?</li> <li>Which aspects of structure are learned most efficiently?</li> </ul>"},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#reframing-diffusion-evaluation","title":"Reframing Diffusion Evaluation","text":"<p>Epiplexity reframes diffusion model evaluation from:</p> <p>\"Does the model match the distribution?\"</p> <p>to:</p> <p>\"How much reusable structure does this dataset induce under realistic training constraints?\"</p> <p>This shift is crucial for practical applications where computational efficiency and generalization matter more than perfect distribution matching.\u201d</p>"},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#epiplexity-for-generated-gene-expression-data","title":"Epiplexity for Generated Gene Expression Data","text":""},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#the-challenge-of-gene-expression-data","title":"The Challenge of Gene Expression Data","text":"<p>Gene expression data presents a fundamental duality:</p> <ul> <li>High-dimensional, noisy, stochastic: Thousands of genes, technical noise, biological variability</li> <li>Deeply structured: Pathways, cell states, regulatory programs, developmental trajectories</li> </ul> <p>Traditional metrics struggle to capture this duality effectively.</p>"},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#inadequate-metrics","title":"Inadequate Metrics","text":"<p>Several common evaluation approaches are insufficient:</p> <ul> <li>Per-gene marginal distributions: Ignore correlations and regulatory structure</li> <li>Simple correlation matching: Can be satisfied by memorizing noise</li> <li>Likelihood under a pretrained model: May not reflect biological utility</li> </ul> <p>These metrics can be satisfied by models that memorize noise patterns without capturing meaningful biological structure.</p>"},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#the-epiplexity-approach","title":"The Epiplexity Approach","text":"<p>Epiplexity enables a more meaningful evaluation framework:</p> <p>Setup:</p> <ol> <li>Observer model: Fixed-capacity architecture (e.g., Geneformer-style transformer, diffusion backbone, or masked-gene predictor)</li> <li>Training datasets:</li> <li>Real scRNA-seq or bulk RNA-seq data</li> <li>Generated expression profiles</li> <li>Mixed datasets (real + synthetic)</li> <li>Measurement: Track training loss vs. compute (steps, epochs, FLOPs)</li> </ol> <p>Key questions:</p> <ul> <li>Does synthetic data accelerate early learning?</li> <li>Does it cause premature loss plateau?</li> <li>Does it teach structure that transfers to new tasks?</li> </ul> <p>High epiplexity indicates:</p> <p>The model learns more structure, earlier in training, and more robustly across tasks.</p> <p>This directly measures what matters for biological applications: whether generated data teaches meaningful biology, not just statistical patterns.</p>"},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#practical-epiplexity-protocol-for-gene-expression-models","title":"Practical Epiplexity Protocol for Gene Expression Models","text":""},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#experimental-design","title":"Experimental Design","text":"<p>A rigorous epiplexity-based evaluation protocol consists of four components:</p>"},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#1-observer-model","title":"1. Observer Model","text":"<p>Fixed-architecture model with realistic capacity: - Masked-gene prediction: Predict held-out genes from context - Conditional diffusion denoiser: Denoise at various noise levels - Pathway-aware encoder: Incorporate biological priors</p>"},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#2-datasets","title":"2. Datasets","text":"<p>Multiple data sources for comparison: - Real data: scRNA-seq or bulk RNA-seq - Generated data: Synthetic expression profiles - Hybrid mixtures: Varying ratios of real and synthetic</p>"},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#3-training-protocol","title":"3. Training Protocol","text":"<p>For each dataset: - Train under fixed compute budget (same FLOPs, same steps) - Record complete loss trajectory (not just final loss) - Measure downstream task transfer:   - Cell type classification   - Perturbation response prediction   - Pathway enrichment stability</p>"},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#4-epiplexity-metrics","title":"4. Epiplexity Metrics","text":"<p>Compute multiple proxies:</p> \\[ \\text{Area metric} = \\int_0^T \\left(L(t) - L_{\\text{final}}\\right) dt \\] \\[ \\text{Early learning rate} = \\frac{L(0) - L(t_{\\text{early}})}{t_{\\text{early}}} \\] \\[ \\text{Transfer stability} = \\text{Var}(\\text{task performance across seeds}) \\]"},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#reframing-the-question","title":"Reframing the Question","text":"<p>This protocol shifts the evaluation question from:</p> <p>\"Does synthetic data look real?\"</p> <p>to:</p> <p>\"Does it teach biology?\"</p> <p>This is the fundamental question for biological applications of generative models.</p>"},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#philosophical-and-practical-implications","title":"Philosophical and Practical Implications","text":""},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#dismantling-a-fundamental-assumption","title":"Dismantling a Fundamental Assumption","text":"<p>The epiplexity framework challenges a core assumption of classical information theory:</p> <p>Classical view:</p> <p>Information is a property of data alone.</p> <p>Epiplexity view:</p> <p>Information is a property of data plus an observer.</p>"},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#why-this-matters-for-biology","title":"Why This Matters for Biology","text":"<p>This perspective shift is particularly important for biological applications:</p> <ol> <li>Incomplete observations: We never observe the full generative process (true cell states, complete regulatory networks)</li> <li>Biased measurements: All biological data comes from biased, incomplete measurement technologies</li> <li>Models as tools: Models are computational tools with specific capacities, not omniscient oracles</li> </ol>"},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#actionable-information","title":"Actionable Information","text":"<p>Epiplexity provides a principled framework for stating:</p> <p>\"This dataset contains structure that models can actually use.\"</p> <p>This is far more actionable than abstract measures like entropy or likelihood, which don't account for: - Computational constraints - Model architecture - Training dynamics - Transfer to downstream tasks</p>"},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#integration-with-generative-ai-research","title":"Integration with Generative AI Research","text":""},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#epiplexity-as-a-core-evaluation-primitive","title":"Epiplexity as a Core Evaluation Primitive","text":"<p>Epiplexity can serve multiple roles in generative AI research:</p> <ol> <li>Dataset evaluation primitive: Assess information content of biological datasets</li> <li>Synthetic data validation: Validate that generated data teaches meaningful structure</li> <li>Model-agnostic lens: Compare diffusion models, transformers, and hybrid architectures</li> </ol>"},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#philosophical-alignment","title":"Philosophical Alignment","text":"<p>Epiplexity aligns with a fundamental principle of modern machine learning:</p> <p>Learning is constrained, hierarchical, and emergent\u2014not omniscient.</p> <p>This principle applies equally to: - Biological systems: Cells learn regulatory programs under resource constraints - Machine learning: Models learn from data under computational constraints - Scientific inference: Researchers extract knowledge under measurement constraints</p>"},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#future-directions","title":"Future Directions","text":"<p>Several promising research directions emerge:</p> <ol> <li> <p>Noise schedules in diffusion models: How do different noise schedules affect epiplexity? Do schedules that match biological noise characteristics yield higher epiplexity?</p> </li> <li> <p>Representation collapse vs. emergence: Can epiplexity detect when gene expression models collapse to memorization vs. learning emergent biological principles?</p> </li> <li> <p>Multi-scale structure: How does epiplexity vary across biological scales (genes \u2192 pathways \u2192 cell types \u2192 tissues)?</p> </li> <li> <p>Transfer learning: Does high epiplexity on source tasks predict transfer performance to target tasks?</p> </li> </ol>"},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#summary","title":"Summary","text":"<p>Epiplexity reframes information theory for the era of bounded computation and learned representations. Rather than asking \"how much information exists in data,\" it asks \"how much structure can realistic models extract?\"</p> <p>For biological generative models, this shift is crucial. It moves evaluation from abstract distribution matching to practical questions about learning efficiency, generalization, and biological utility.</p>"},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#references","title":"References","text":"<ol> <li>Original paper: From Entropy to Epiplexity</li> <li>Related work: Minimum Description Length (MDL) theory</li> <li>Applications: Diffusion models, transformers, biological sequence models</li> </ol>"},{"location":"eval/epiplexity/01_from_entropy_to_epiplexity/#related-documents","title":"Related Documents","text":"<ul> <li>DDPM Foundations \u2014 Diffusion model theory</li> <li>DDPM Training \u2014 Training dynamics and loss curves</li> <li>DDPM Sampling \u2014 Sampling efficiency</li> <li>SDE View \u2014 Continuous-time perspective</li> </ul>"},{"location":"eval/epiplexity/02_random_noise/","title":"Why Random Noise Has High Entropy but Low Epiplexity","text":"<p>A central claim of the epiplexity framework is that random noise has high entropy but low epiplexity. This apparent paradox reveals a fundamental distinction between classical information theory and bounded-computation perspectives.</p> <p>This document explains why this distinction exists, how it's formalized, and what it means for evaluating generative models.</p>"},{"location":"eval/epiplexity/02_random_noise/#the-apparent-paradox","title":"The Apparent Paradox","text":""},{"location":"eval/epiplexity/02_random_noise/#the-counterintuitive-claim","title":"The Counterintuitive Claim","text":"<p>Consider the statement:</p> <p>Random noise has high entropy but low epiplexity</p> <p>For those trained in classical information theory, this seems contradictory. After all, Shannon entropy for a random variable \\(X\\) is:</p> \\[ H(X) = -\\mathbb{E}[\\log p(X)] \\] <p>White noise maximizes entropy under variance constraints (explained below), so how can it have low information?</p>"},{"location":"eval/epiplexity/02_random_noise/#why-white-noise-maximizes-entropy","title":"Why White Noise Maximizes Entropy","text":"<p>Maximum Entropy Principle: Among all continuous distributions with a given variance \\(\\sigma^2\\), the Gaussian distribution has maximum entropy.</p> <p>Proof sketch:</p> <p>For continuous distributions, differential entropy is:</p> \\[ h(X) = -\\int p(x) \\log p(x) \\, dx \\] <p>Subject to constraints: - \\(\\int p(x) \\, dx = 1\\) (normalization) - \\(\\int x^2 p(x) \\, dx = \\sigma^2\\) (fixed variance) - \\(\\int x p(x) \\, dx = 0\\) (zero mean, WLOG)</p> <p>Using calculus of variations (Lagrange multipliers), the distribution that maximizes \\(h(X)\\) is:</p> \\[ p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right) \\] <p>This is the Gaussian distribution \\(\\mathcal{N}(0, \\sigma^2)\\).</p> <p>Intuition: The Gaussian \"spreads out\" probability mass as uniformly as possible given the variance constraint, maximizing uncertainty.</p> <p>For white noise: Independent Gaussian samples \\(x_i \\sim \\mathcal{N}(0, I)\\) have maximum entropy per dimension, hence maximum total entropy.</p>"},{"location":"eval/epiplexity/02_random_noise/#resolution-of-the-paradox","title":"Resolution of the Paradox","text":"<p>The answer is: it doesn't have low information in the Shannon sense \u2014 but it does when information is redefined for bounded computation.</p> <p>The key insight: Shannon entropy and epiplexity measure different things.</p>"},{"location":"eval/epiplexity/02_random_noise/#what-shannon-entropy-measures-and-what-it-doesnt","title":"What Shannon Entropy Measures (and What It Doesn't)","text":""},{"location":"eval/epiplexity/02_random_noise/#shannon-entropy-unbounded-perspective","title":"Shannon Entropy: Unbounded Perspective","text":"<p>Shannon entropy measures:</p> <p>Irreducible uncertainty to an unbounded decoder with optimal codes</p> <p>It quantifies the minimum number of bits needed to encode data in principle, assuming:</p> <ol> <li>Infinite compute: Unlimited computational resources</li> <li>Optimal codes: Perfect compression algorithms</li> <li>No learning dynamics: Instantaneous access to true distribution</li> <li>No architecture constraints: Arbitrary model complexity</li> </ol>"},{"location":"eval/epiplexity/02_random_noise/#what-shannon-entropy-does-not-measure","title":"What Shannon Entropy Does Not Measure","text":"<p>Shannon entropy does not capture:</p> <ul> <li>Learnability: Can a realistic model learn this pattern?</li> <li>Compressibility by bounded models: Can SGD-trained networks compress this?</li> <li>Reusable structure: Does learning transfer to new tasks?</li> <li>Inductive utility: Does this data teach generalizable principles?</li> </ul> <p>These limitations become critical when evaluating machine learning systems operating under realistic constraints.</p>"},{"location":"eval/epiplexity/02_random_noise/#the-epiplexity-framework-bounded-observers","title":"The Epiplexity Framework: Bounded Observers","text":""},{"location":"eval/epiplexity/02_random_noise/#replacing-the-unbounded-observer","title":"Replacing the Unbounded Observer","text":"<p>The epiplexity framework replaces the classical unbounded observer with:</p> <p>A computationally bounded learning system</p> <p>Characteristics: - Finite network depth and width - Finite training time - SGD-based optimization (not global optimization) - Realistic computational budgets</p>"},{"location":"eval/epiplexity/02_random_noise/#information-becomes-observer-relative","title":"Information Becomes Observer-Relative","text":"<p>Under bounded computation, information is no longer absolute:</p> \\[ \\text{Information} \\neq \\text{Entropy} \\] <p>Instead:</p> \\[ \\text{Information} = \\text{structure a bounded learner can extract} \\] <p>This is the foundation of epiplexity.</p>"},{"location":"eval/epiplexity/02_random_noise/#formal-definition-of-epiplexity","title":"Formal Definition of Epiplexity","text":""},{"location":"eval/epiplexity/02_random_noise/#mdl-framework-under-bounded-models","title":"MDL Framework Under Bounded Models","text":"<p>Epiplexity is defined using a minimum description length (MDL) framework adapted for bounded computation:</p> <p>Epiplexity: The amount of structure in data that reduces description length for the best model within a computational class.</p>"},{"location":"eval/epiplexity/02_random_noise/#key-differences-from-classical-mdl","title":"Key Differences from Classical MDL","text":"Aspect Classical MDL Epiplexity Model class Unrestricted Restricted (e.g., neural networks) Optimization Perfect Imperfect (SGD) Dynamics Irrelevant Central (training trajectory) Compute Unbounded Bounded budget"},{"location":"eval/epiplexity/02_random_noise/#epiplexity-as-a-relational-property","title":"Epiplexity as a Relational Property","text":"<p>Epiplexity is not a property of data alone. It is a property of the triple:</p> \\[ \\text{Epiplexity}(\\text{data}, \\text{model class}, \\text{compute budget}) \\] <p>The same dataset can have: - High epiplexity for one model class - Low epiplexity for another</p> <p>This observer-dependence is fundamental.</p>"},{"location":"eval/epiplexity/02_random_noise/#why-random-noise-has-low-epiplexity","title":"Why Random Noise Has Low Epiplexity","text":""},{"location":"eval/epiplexity/02_random_noise/#step-1-characterize-iid-noise","title":"Step 1: Characterize i.i.d. Noise","text":"<p>Consider white noise:</p> \\[ x_i \\sim \\mathcal{N}(0, I), \\quad i = 1, 2, \\ldots, N \\] <p>Properties: - Maximal entropy: Among distributions with fixed variance - No correlations: \\(\\text{Cov}(x_i, x_j) = 0\\) for \\(i \\neq j\\) - No multi-scale structure: All frequencies equally represented - No predictive regularities: Past samples don't inform future samples</p>"},{"location":"eval/epiplexity/02_random_noise/#step-2-train-a-bounded-learner","title":"Step 2: Train a Bounded Learner","text":"<p>Take any realistic architecture: - Transformer - CNN - Diffusion denoiser - MLP</p> <p>Train with SGD on the noise dataset.</p> <p>Observed behavior:</p> <ol> <li>Loss drops once to the irreducible noise floor</li> <li>No progressive compression: Loss plateaus immediately</li> <li>No early structure learning: No multi-phase loss reduction</li> <li>No generalizable features: Learned parameters don't transfer</li> </ol> <p>What the model cannot do:</p> <ul> <li>Predict unseen samples (each sample is independent)</li> <li>Reuse learned parameters (no shared structure)</li> <li>Compress beyond trivial encoding (no patterns to exploit)</li> </ul>"},{"location":"eval/epiplexity/02_random_noise/#step-3-description-length-remains-constant","title":"Step 3: Description Length Remains Constant","text":"<p>In MDL terms, the description length:</p> \\[ L(\\text{data} \\mid \\text{model}) \\approx \\text{constant} \\] <p>The model learns nothing reusable because there is no structure to learn.</p> <p>Conclusion:</p> \\[ \\text{Epiplexity}(\\text{white noise}) \\approx 0 \\] <p>Even though:</p> \\[ H(\\text{white noise}) = \\text{maximal} \\]"},{"location":"eval/epiplexity/02_random_noise/#the-loss-curve-signature","title":"The Loss-Curve Signature","text":""},{"location":"eval/epiplexity/02_random_noise/#practical-approximation","title":"Practical Approximation","text":"<p>The paper introduces a practical proxy for epiplexity:</p> <p>Epiplexity \\(\\propto\\) area under the training loss curve above final loss</p> \\[ \\text{Epiplexity} \\propto \\int_0^T \\left( \\mathcal{L}(t) - \\mathcal{L}_{\\infty} \\right) dt \\] <p>where:</p> <ul> <li>\\(\\mathcal{L}(t)\\) is the loss at training step \\(t\\)</li> <li>\\(\\mathcal{L}_{\\infty}\\) is the final converged loss</li> <li>\\(T\\) is the total training time</li> </ul>"},{"location":"eval/epiplexity/02_random_noise/#why-this-works","title":"Why This Works","text":"<p>For random noise:</p> \\[ \\mathcal{L}(t) \\approx \\mathcal{L}_{\\infty} \\quad \\forall t \\] <ul> <li>Loss immediately reaches noise floor</li> <li>No progressive improvement</li> <li>Area \\(\\approx 0\\) \u2192 Low epiplexity</li> </ul> <p>For structured data:</p> \\[ \\mathcal{L}(t) \\gg \\mathcal{L}_{\\infty} \\quad \\text{for early } t \\] <ul> <li>Early learning of coarse structure</li> <li>Multi-phase loss reduction</li> <li>Progressive refinement</li> <li>Large area \u2192 High epiplexity</li> </ul>"},{"location":"eval/epiplexity/02_random_noise/#quantitative-not-metaphorical","title":"Quantitative, Not Metaphorical","text":"<p>This is a measurable quantity:</p> <pre><code>epiplexity_proxy = np.trapz(loss_curve - final_loss, dx=1)\n</code></pre> <p>The loss trajectory reveals whether data contains learnable structure.</p>"},{"location":"eval/epiplexity/02_random_noise/#why-entropy-and-epiplexity-diverge","title":"Why Entropy and Epiplexity Diverge","text":""},{"location":"eval/epiplexity/02_random_noise/#different-quantities","title":"Different Quantities","text":"<p>Entropy measures:</p> <p>Unpredictability (irreducible uncertainty)</p> <p>Epiplexity measures:</p> <p>Extractable structure (learnable patterns under bounded computation)</p>"},{"location":"eval/epiplexity/02_random_noise/#structure-vs-randomness","title":"Structure vs. Randomness","text":"<p>Random noise is: - Maximally unpredictable (high entropy) - Maximally unstructured (low epiplexity)</p> <p>Key insight:</p> <p>Structure is not randomness. Structure is compressibility under computational constraints.</p>"},{"location":"eval/epiplexity/02_random_noise/#comparative-table","title":"Comparative Table","text":"Data Type Entropy Epiplexity Explanation White noise High Low No learnable patterns Natural images High High Multi-scale structure (edges, textures, objects) Language Very high Very high Hierarchical structure (syntax, semantics) Shuffled language High Low Statistics preserved, structure destroyed Gene expression (real) High Moderate\u2013High Pathway structure, cell state programs Na\u00efve synthetic expression High Often Low Marginals match, but no biological programs <p>This table encapsulates the epiplexity framework's core insight.</p>"},{"location":"eval/epiplexity/02_random_noise/#implications-for-gene-expression-and-diffusion-models","title":"Implications for Gene Expression and Diffusion Models","text":""},{"location":"eval/epiplexity/02_random_noise/#the-generative-modeling-challenge","title":"The Generative Modeling Challenge","text":"<p>A diffusion model can easily match: - Marginal gene distributions: Per-gene statistics - Covariance statistics: Pairwise correlations - Sample diversity: High-entropy outputs</p> <p>But still produce data with: - Low epiplexity: No learnable biological structure - No reusable programs: Pathway logic absent - No transfer value: Doesn't improve downstream tasks</p>"},{"location":"eval/epiplexity/02_random_noise/#why-epiplexity-detects-this","title":"Why Epiplexity Detects This","text":"<p>Epiplexity reveals the problem through training dynamics:</p> <p>An observer model fails to learn progressively from synthetic data that lacks biological structure.</p> <p>Diagnostic signature:</p> <ul> <li>Real biological data: Loss decreases progressively as model learns pathways, cell states, regulatory logic</li> <li>Na\u00efve synthetic data: Loss plateaus immediately\u2014no structure to learn beyond noise statistics</li> </ul> <p>This is the critical advantage: Epiplexity detects whether generated data teaches biology, not just whether it matches statistics.</p>"},{"location":"eval/epiplexity/02_random_noise/#mathematical-rigor-and-practical-approximation","title":"Mathematical Rigor and Practical Approximation","text":""},{"location":"eval/epiplexity/02_random_noise/#what-the-paper-provides","title":"What the Paper Provides","text":"<p>The paper does not give a closed-form analytic epiplexity for arbitrary distributions (this would be impossible for general learning systems).</p> <p>What it does provide:</p> <ol> <li>Principled definition: Grounded in minimum description length (MDL) theory</li> <li>Bounded-compute justification: Formal framework for observer-dependent information</li> <li>Quantitative proxy: Measurable approximation via loss curves</li> <li>Empirical validation: Demonstrated across multiple domains</li> </ol>"},{"location":"eval/epiplexity/02_random_noise/#appropriate-level-of-rigor","title":"Appropriate Level of Rigor","text":"<p>For a learning-theoretic quantity, this is the right approach:</p> <ul> <li>Not: Closed-form formulas (impossible for complex learning dynamics)</li> <li>But: Principled framework + practical measurement</li> </ul> <p>Analogous to: - VC dimension: Theoretical concept with practical proxies - Generalization gap: Measured empirically, bounded theoretically - Sample complexity: Asymptotic bounds, finite-sample estimates</p>"},{"location":"eval/epiplexity/02_random_noise/#summary","title":"Summary","text":""},{"location":"eval/epiplexity/02_random_noise/#the-core-insight","title":"The Core Insight","text":"<p>Random noise contains many bits, but no lessons.</p> <p>Shannon entropy counts bits. Epiplexity measures lessons.</p>"},{"location":"eval/epiplexity/02_random_noise/#why-this-matters","title":"Why This Matters","text":"<p>For evaluating generative models in biology:</p> <ol> <li>Traditional metrics (likelihood, FID, correlation) can be satisfied by noise-matching</li> <li>Epiplexity reveals whether generated data teaches biological structure</li> <li>Training dynamics provide the diagnostic signal</li> </ol>"},{"location":"eval/epiplexity/02_random_noise/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Entropy \u2260 Epiplexity: Different quantities for different purposes</li> <li>White noise: Maximum entropy (Gaussian maximizes entropy under variance constraints), zero epiplexity</li> <li>Structure: Compressibility under bounded computation</li> <li>Loss curves: Practical measurement of epiplexity</li> <li>Biological data: High epiplexity when it contains learnable programs</li> </ul>"},{"location":"eval/epiplexity/02_random_noise/#related-documents","title":"Related Documents","text":"<ul> <li>From Entropy to Epiplexity \u2014 Foundational concepts</li> <li>DDPM Training \u2014 Loss curves and training dynamics</li> <li>DDPM Foundations \u2014 Diffusion model theory</li> </ul>"},{"location":"eval/epiplexity/02_random_noise/#references","title":"References","text":"<ol> <li>Original paper: From Entropy to Epiplexity</li> <li>Shannon, C. E. (1948). A Mathematical Theory of Communication. Bell System Technical Journal.</li> <li>Rissanen, J. (1978). Modeling by shortest data description. Automatica.</li> <li>Cover, T. M., &amp; Thomas, J. A. (2006). Elements of Information Theory. Wiley.</li> </ol>"},{"location":"flow_matching/","title":"Flow Matching: A Comprehensive Guide","text":"<p>This directory contains comprehensive documentation on flow matching methods for generative modeling \u2014 an alternative to score-based diffusion that learns velocity fields via simple regression.</p> <p>Flow matching offers faster sampling (10-50 steps vs 100-1000 for diffusion), simpler training (direct regression), and flexible paths, making it particularly promising for biological data applications.</p>"},{"location":"flow_matching/#core-documentation-series","title":"Core Documentation Series","text":"<p>This series mirrors the structure of the DDPM documentation, providing a complete foundation for understanding and implementing flow matching models.</p> Document Description 01_flow_matching_foundations.md Foundations: Mathematical theory, forward/backward processes, theoretical properties 02_flow_matching_training.md Training: Loss functions, network architectures, training strategies, reflow 03_flow_matching_sampling.md Sampling: ODE solvers, sampling strategies, quality-speed tradeoffs 04_flow_matching_landscape.md Landscape: Normalizing flows vs flow matching, variants comparison, historical context rectifying_flow.md Tutorial: Rectified flow from first principles (original tutorial)"},{"location":"flow_matching/#quick-navigation","title":"Quick Navigation","text":""},{"location":"flow_matching/#for-beginners","title":"For Beginners","text":"<ol> <li>Start with Rectifying Flow Tutorial for intuitive introduction</li> <li>Read Foundations for mathematical details</li> <li>Review Training for implementation</li> </ol>"},{"location":"flow_matching/#for-implementation","title":"For Implementation","text":"<ol> <li>Training Guide \u2014 Complete training loop with code</li> <li>Sampling Guide \u2014 ODE solvers and sampling strategies</li> <li>Foundations \u2014 Reference for equations</li> </ol>"},{"location":"flow_matching/#for-comparison-with-diffusion","title":"For Comparison with Diffusion","text":"<ol> <li>Foundations \u2014 Conceptual differences</li> <li>Sampling \u2014 Speed and quality comparison</li> <li>See DDPM Documentation for diffusion model details</li> </ol>"},{"location":"flow_matching/#key-concepts","title":"Key Concepts","text":""},{"location":"flow_matching/#flow-matching-overview","title":"Flow Matching Overview","text":"<p>Flow matching learns a velocity field \\(v_\\theta(x, t)\\) that transports samples from a noise distribution to a data distribution:</p> \\[ \\frac{dx}{dt} = v_\\theta(x, t) \\] <p>Key advantages:</p> <ul> <li>Simpler training: Direct regression instead of score matching</li> <li>Faster sampling: 10-50 steps (vs 100-1000 for diffusion)</li> <li>Deterministic: Same noise \u2192 same output</li> <li>Flexible: Not restricted to Gaussian noise schedules</li> </ul>"},{"location":"flow_matching/#rectified-flow","title":"Rectified Flow","text":"<p>Rectified flow is the simplest and most practical instantiation:</p> <ul> <li>Path: Linear interpolation \\(x_t = (1-t) x_0 + t x_1\\)</li> <li>Velocity: Constant \\(v = x_1 - x_0\\)</li> <li>Loss: Simple MSE \\(\\|v_\\theta(x_t, t) - (x_1 - x_0)\\|^2\\)</li> <li>Sampling: Deterministic ODE integration</li> </ul> <p>Reflow: Iteratively straighten paths for even faster sampling (5-10 steps).</p>"},{"location":"flow_matching/#comparison-with-diffusion-models","title":"Comparison with Diffusion Models","text":"Aspect Score Matching (Diffusion) Flow Matching What's learned Score: \\(\\nabla_x \\log p_t(x)\\) Velocity: \\(v_\\theta(x, t)\\) Forward process Stochastic (add noise) Deterministic (interpolate) Reverse process Stochastic SDE or ODE Deterministic ODE Training Score matching (complex) Simple regression Sampling steps 100-1000 (SDE), 50-100 (ODE) 10-50 (ODE) Speed Slower 2-5\u00d7 faster Noise schedule Critical design choice Less critical <p>When to use flow matching:</p> <ul> <li>Faster sampling is critical</li> <li>Simpler training preferred</li> <li>Exploring new domains (biology, molecules)</li> <li>Need deterministic generation</li> </ul>"},{"location":"flow_matching/#learning-path","title":"Learning Path","text":""},{"location":"flow_matching/#conceptual-understanding","title":"Conceptual Understanding","text":"<ol> <li>Rectifying Flow Tutorial \u2014 Intuitive introduction</li> <li>Linear interpolation paths</li> <li>Velocity fields</li> <li> <p>Why \"rectified\"?</p> </li> <li> <p>Foundations \u2014 Mathematical theory</p> </li> <li>Probability flows</li> <li>Conditional flow matching</li> <li>Optimal transport connection</li> </ol>"},{"location":"flow_matching/#practical-implementation","title":"Practical Implementation","text":"<ol> <li>Training \u2014 How to train</li> <li>Loss functions</li> <li>Network architectures (U-Net, DiT)</li> <li>Training strategies and best practices</li> <li> <p>Reflow for faster sampling</p> </li> <li> <p>Sampling \u2014 How to sample</p> </li> <li>ODE solvers (Euler, RK4, adaptive)</li> <li>Quality-speed tradeoffs</li> <li>Conditional generation and guidance</li> </ol>"},{"location":"flow_matching/#advanced-topics","title":"Advanced Topics","text":"<ol> <li>Reflow \u2014 Iterative path straightening</li> <li>Conditional generation \u2014 Class/text conditioning</li> <li>Classifier-free guidance \u2014 Enhanced conditioning</li> <li>Domain adaptation \u2014 Biological data applications</li> </ol>"},{"location":"flow_matching/#code-examples","title":"Code Examples","text":""},{"location":"flow_matching/#training","title":"Training","text":"<pre><code># Simple rectified flow training\nfor batch in dataloader:\n    x0 = batch  # data\n    x1 = torch.randn_like(x0)  # noise\n    t = torch.rand(batch_size)\n\n    xt = (1 - t) * x0 + t * x1\n    target = x1 - x0\n\n    pred = model(xt, t)\n    loss = F.mse_loss(pred, target)\n    loss.backward()\n</code></pre>"},{"location":"flow_matching/#sampling","title":"Sampling","text":"<pre><code># RK4 sampling (10-20 steps)\nx = torch.randn(batch_size, *data_shape)\ndt = 1.0 / num_steps\n\nfor i in range(num_steps):\n    t = 1.0 - i * dt\n    k1 = model(x, t)\n    k2 = model(x - dt/2 * k1, t - dt/2)\n    k3 = model(x - dt/2 * k2, t - dt/2)\n    k4 = model(x - dt * k3, t - dt)\n    x = x - dt/6 * (k1 + 2*k2 + 2*k3 + k4)\n</code></pre>"},{"location":"flow_matching/#related-documentation","title":"Related Documentation","text":""},{"location":"flow_matching/#within-genai-lab","title":"Within GenAI Lab","text":"<ul> <li>DDPM Documentation \u2014 Comparison with diffusion models</li> <li>SDE Documentation \u2014 SDE perspective on diffusion</li> <li>Evaluation Metrics \u2014 How to evaluate generated samples</li> <li>DiT Architecture \u2014 Transformer for flow matching</li> </ul>"},{"location":"flow_matching/#external-resources","title":"External Resources","text":"<ul> <li>Flow Matching Paper \u2014 Lipman et al., 2023</li> <li>Rectified Flow Paper \u2014 Liu et al., 2023</li> <li>Optimal Transport \u2014 Albergo &amp; Vanden-Eijnden, 2023</li> </ul>"},{"location":"flow_matching/#summary","title":"Summary","text":"<p>Flow matching provides a simpler, faster alternative to diffusion models:</p> <ul> <li>Training: Direct regression on velocity fields</li> <li>Sampling: Deterministic ODE (10-50 steps)</li> <li>Quality: Comparable to diffusion models</li> <li>Speed: 2-5\u00d7 faster than DDIM, 10-50\u00d7 faster than DDPM</li> </ul> <p>Rectified flow is the simplest instantiation, using linear interpolation paths and constant velocities. With reflow, sampling can be reduced to 5-10 steps while maintaining quality.</p> <p>This makes flow matching particularly attractive for: - Real-time applications - Resource-constrained environments - Biological data generation (gene expression, molecules) - Domains requiring deterministic generation</p>"},{"location":"flow_matching/01_flow_matching_foundations/","title":"Flow Matching Foundations","text":"<p>Flow matching is a family of generative modeling methods that learn to transport samples from a simple noise distribution to a complex data distribution via continuous-time flows. Unlike diffusion models that learn score functions, flow matching directly learns velocity fields through simple regression.</p> <p>This document covers the mathematical foundations, forward and backward processes, and theoretical properties of flow matching.</p>"},{"location":"flow_matching/01_flow_matching_foundations/#overview","title":"Overview","text":""},{"location":"flow_matching/01_flow_matching_foundations/#the-core-idea","title":"The Core Idea","text":"<p>Goal: Transform samples from a simple distribution \\(p_0\\) (noise) into samples from a complex distribution \\(p_1\\) (data).</p> <p>Approach: Learn a time-dependent velocity field \\(v_\\theta(x, t)\\) that defines a continuous transformation:</p> \\[ \\frac{dx}{dt} = v_\\theta(x, t), \\quad t \\in [0, 1] \\] <p>Key insight: Instead of learning probability gradients (scores), we directly learn how points should move through space.</p>"},{"location":"flow_matching/01_flow_matching_foundations/#why-flow-matching","title":"Why Flow Matching?","text":"<p>Advantages over diffusion models:</p> <ul> <li>Simpler training: Direct regression instead of score matching</li> <li>Faster sampling: Deterministic ODE, fewer steps needed (10-50 vs 100-1000)</li> <li>Flexible paths: Not restricted to Gaussian noise schedules</li> <li>Theoretical clarity: Optimal transport interpretation</li> <li>Better for non-Euclidean data: Natural extension to manifolds</li> </ul> <p>Trade-offs:</p> <ul> <li>Less mature than diffusion (fewer architectural tricks)</li> <li>Requires careful ODE solver selection</li> <li>May need more training data for complex distributions</li> </ul>"},{"location":"flow_matching/01_flow_matching_foundations/#mathematical-framework","title":"Mathematical Framework","text":""},{"location":"flow_matching/01_flow_matching_foundations/#probability-flows","title":"Probability Flows","text":"<p>A probability flow is a time-dependent vector field that transports probability mass.</p> <p>Setup:</p> <ul> <li>Start distribution: \\(p_0(x)\\) (e.g., Gaussian noise)</li> <li>End distribution: \\(p_1(x)\\) (data distribution)</li> <li>Time: \\(t \\in [0, 1]\\)</li> </ul> <p>Flow equation:</p> \\[ \\frac{dx}{dt} = v(x, t) \\] <p>This defines a trajectory \\(x(t)\\) for each starting point \\(x(0)\\).</p> <p>Probability evolution: The distribution at time \\(t\\), denoted \\(p_t(x)\\), evolves according to the continuity equation:</p> \\[ \\frac{\\partial p_t}{\\partial t} + \\nabla \\cdot (p_t v) = 0 \\] <p>This ensures probability mass is conserved as it flows.</p>"},{"location":"flow_matching/01_flow_matching_foundations/#the-forward-process","title":"The Forward Process","text":"<p>Unlike diffusion models with stochastic forward processes, flow matching uses deterministic interpolation.</p> <p>Conditional flow: Given a data-noise pair \\((x_0, x_1)\\) where \\(x_0 \\sim p_{\\text{data}}\\) and \\(x_1 \\sim p_{\\text{noise}}\\), define a path:</p> \\[ x_t = \\psi_t(x_0, x_1) \\] <p>Common choices:</p> <p>1. Linear interpolation (Rectified Flow):</p> \\[ x_t = (1-t) x_0 + t x_1 \\] <p>2. Geodesic interpolation:</p> \\[ x_t = \\exp_{x_0}(t \\log_{x_0}(x_1)) \\] <p>(useful for manifold-valued data)</p> <p>3. Variance-preserving interpolation:</p> \\[ x_t = \\sqrt{1-t} \\, x_0 + \\sqrt{t} \\, x_1 \\] <p>(maintains variance like diffusion)</p> <p>Conditional velocity: The velocity along this path is:</p> \\[ u_t(x_0, x_1) = \\frac{d}{dt} \\psi_t(x_0, x_1) \\] <p>For linear interpolation: \\(u_t(x_0, x_1) = x_1 - x_0\\) (constant velocity).</p>"},{"location":"flow_matching/01_flow_matching_foundations/#the-marginal-flow","title":"The Marginal Flow","text":"<p>The marginal velocity field at time \\(t\\) is:</p> \\[ v_t(x) = \\mathbb{E}_{x_0, x_1 | x_t = x} [u_t(x_0, x_1)] \\] <p>This is the expected velocity at position \\(x\\) and time \\(t\\), averaged over all data-noise pairs that pass through \\(x\\) at time \\(t\\).</p> <p>Key property: If we know \\(v_t(x)\\) exactly, solving the ODE:</p> \\[ \\frac{dx}{dt} = v_t(x), \\quad x(0) = x_0 \\sim p_{\\text{noise}} \\] <p>will transport \\(x(0)\\) to \\(x(1) \\sim p_{\\text{data}}\\).</p>"},{"location":"flow_matching/01_flow_matching_foundations/#flow-matching-objective","title":"Flow Matching Objective","text":""},{"location":"flow_matching/01_flow_matching_foundations/#the-training-loss","title":"The Training Loss","text":"<p>Flow matching trains a neural network \\(v_\\theta(x, t)\\) to approximate the marginal velocity field \\(v_t(x)\\).</p> <p>Conditional Flow Matching (CFM) loss:</p> \\[ \\mathcal{L}_{\\text{CFM}}(\\theta) = \\mathbb{E}_{t, x_0, x_1, x_t} \\left[ \\left\\| v_\\theta(x_t, t) - u_t(x_0, x_1) \\right\\|^2 \\right] \\] <p>where:</p> <ul> <li>\\(t \\sim \\text{Uniform}[0, 1]\\)</li> <li>\\(x_0 \\sim p_{\\text{data}}\\)</li> <li>\\(x_1 \\sim p_{\\text{noise}}\\)</li> <li>\\(x_t = \\psi_t(x_0, x_1)\\)</li> </ul> <p>Why this works: The conditional velocity \\(u_t(x_0, x_1)\\) is a valid target because:</p> \\[ \\mathbb{E}_{x_0, x_1 | x_t} [u_t(x_0, x_1)] = v_t(x_t) \\] <p>This is the conditional expectation property that makes flow matching tractable.</p>"},{"location":"flow_matching/01_flow_matching_foundations/#comparison-with-score-matching","title":"Comparison with Score Matching","text":"Aspect Score Matching (Diffusion) Flow Matching Target Score: \\(\\nabla_x \\log p_t(x)\\) Velocity: \\(v_t(x)\\) Loss Score matching loss (complex) Simple MSE regression Forward process Stochastic (add noise) Deterministic (interpolate) Training complexity Requires careful noise schedule Direct regression Interpretation Probability gradient Motion direction"},{"location":"flow_matching/01_flow_matching_foundations/#the-backward-process-sampling","title":"The Backward Process (Sampling)","text":""},{"location":"flow_matching/01_flow_matching_foundations/#ode-integration","title":"ODE Integration","text":"<p>After training, we generate samples by solving the flow ODE backward in time.</p> <p>Sampling procedure:</p> <ol> <li> <p>Initialize: Sample \\(x(1) \\sim p_{\\text{noise}}\\) (e.g., \\(\\mathcal{N}(0, I)\\))</p> </li> <li> <p>Integrate backward: Solve the ODE from \\(t=1\\) to \\(t=0\\):</p> </li> </ol> \\[ \\frac{dx}{dt} = v_\\theta(x(t), t) \\] <ol> <li>Output: \\(x(0) \\approx x_{\\text{data}}\\)</li> </ol> <p>Key properties:</p> <ul> <li>Deterministic: Same noise input always produces same output</li> <li>Reversible: Can go forward and backward</li> <li>Continuous: Smooth trajectories through state space</li> </ul>"},{"location":"flow_matching/01_flow_matching_foundations/#ode-solvers","title":"ODE Solvers","text":"<p>Flow matching uses standard numerical ODE solvers:</p> <p>1. Euler method (simplest):</p> \\[ x_{t-\\Delta t} = x_t - \\Delta t \\cdot v_\\theta(x_t, t) \\] <p>2. Runge-Kutta 4 (RK4) (more accurate):</p> \\[ \\begin{align} k_1 &amp;= v_\\theta(x_t, t) \\\\ k_2 &amp;= v_\\theta(x_t - \\frac{\\Delta t}{2} k_1, t - \\frac{\\Delta t}{2}) \\\\ k_3 &amp;= v_\\theta(x_t - \\frac{\\Delta t}{2} k_2, t - \\frac{\\Delta t}{2}) \\\\ k_4 &amp;= v_\\theta(x_t - \\Delta t \\cdot k_3, t - \\Delta t) \\\\ x_{t-\\Delta t} &amp;= x_t - \\frac{\\Delta t}{6}(k_1 + 2k_2 + 2k_3 + k_4) \\end{align} \\] <p>3. Adaptive solvers (e.g., Dormand-Prince): - Automatically adjust step size - Balance accuracy and computation - Useful for complex trajectories</p> <p>Typical performance:</p> <ul> <li>Euler: 50-100 steps for good quality</li> <li>RK4: 10-20 steps for good quality</li> <li>Adaptive: 5-15 steps with error control</li> </ul>"},{"location":"flow_matching/01_flow_matching_foundations/#rectified-flow-the-simplest-instance","title":"Rectified Flow: The Simplest Instance","text":""},{"location":"flow_matching/01_flow_matching_foundations/#definition","title":"Definition","text":"<p>Rectified flow uses the simplest possible choices:</p> <p>Path: Linear interpolation</p> \\[ x_t = (1-t) x_0 + t x_1 \\] <p>Velocity: Constant</p> \\[ u_t(x_0, x_1) = x_1 - x_0 \\] <p>Loss: Direct MSE</p> \\[ \\mathcal{L}_{\\text{RF}} = \\mathbb{E}_{t, x_0, x_1} \\left[ \\left\\| v_\\theta(x_t, t) - (x_1 - x_0) \\right\\|^2 \\right] \\]"},{"location":"flow_matching/01_flow_matching_foundations/#why-rectified","title":"Why \"Rectified\"?","text":"<p>The term comes from geometry: rectification means straightening.</p> <p>Intuition:</p> <ul> <li>Optimal transport between distributions typically follows curved paths (geodesics in probability space)</li> <li>Rectified flow straightens these paths into lines</li> <li>The neural network compensates for any distortion from this simplification</li> </ul> <p>Advantage: Straight paths are: - Simple to define - Easy to sample (fewer ODE steps) - Numerically stable</p>"},{"location":"flow_matching/01_flow_matching_foundations/#reflow-iterative-straightening","title":"Reflow: Iterative Straightening","text":"<p>Rectified flow can be iteratively refined through a process called reflow:</p> <p>Algorithm: 1. Train initial model \\(v_\\theta^{(1)}\\) 2. Generate samples using \\(v_\\theta^{(1)}\\) 3. Use these samples as new training data 4. Train refined model \\(v_\\theta^{(2)}\\) 5. Repeat</p> <p>Effect: Each iteration makes paths straighter, requiring fewer sampling steps.</p> <p>Typical results:</p> <ul> <li>Iteration 1: 50 steps needed</li> <li>Iteration 2: 20 steps needed</li> <li>Iteration 3: 10 steps needed</li> </ul>"},{"location":"flow_matching/01_flow_matching_foundations/#theoretical-properties","title":"Theoretical Properties","text":""},{"location":"flow_matching/01_flow_matching_foundations/#optimality","title":"Optimality","text":"<p>Theorem (Lipman et al., 2023): The flow matching objective is equivalent to minimizing:</p> \\[ \\mathbb{E}_t \\left[ \\mathbb{E}_{x \\sim p_t} \\left[ \\|v_\\theta(x, t) - v_t(x)\\|^2 \\right] \\right] \\] <p>This is the \\(L^2\\) distance between the learned and true velocity fields.</p> <p>Consequence: Flow matching directly optimizes the sampling quality.</p>"},{"location":"flow_matching/01_flow_matching_foundations/#connection-to-optimal-transport","title":"Connection to Optimal Transport","text":"<p>Flow matching is related to optimal transport (OT):</p> <p>Optimal transport problem: Find the transport map \\(T: p_0 \\to p_1\\) that minimizes:</p> \\[ \\min_T \\mathbb{E}_{x_0 \\sim p_0} \\left[ c(x_0, T(x_0)) \\right] \\] <p>where \\(c(x, y)\\) is a cost function (e.g., \\(\\|x - y\\|^2\\)).</p> <p>Connection:</p> <ul> <li>Rectified flow with linear paths approximates OT with quadratic cost</li> <li>The learned velocity field defines a transport map</li> <li>Reflow iterations improve the OT approximation</li> </ul>"},{"location":"flow_matching/01_flow_matching_foundations/#probability-flow-ode","title":"Probability Flow ODE","text":"<p>Flow matching defines a probability flow ODE that transports distributions:</p> \\[ \\frac{d p_t}{dt} + \\nabla \\cdot (p_t v_t) = 0 \\] <p>Key property: If \\(x(0) \\sim p_0\\), then \\(x(t) \\sim p_t\\) when following the flow.</p> <p>Comparison with diffusion:</p> <ul> <li>Diffusion: Stochastic SDE with deterministic probability flow ODE</li> <li>Flow matching: Directly learns the probability flow ODE</li> </ul>"},{"location":"flow_matching/01_flow_matching_foundations/#variance-preserving-vs-non-variance-preserving","title":"Variance-Preserving vs. Non-Variance-Preserving","text":""},{"location":"flow_matching/01_flow_matching_foundations/#variance-preserving-vp-flows","title":"Variance-Preserving (VP) Flows","text":"<p>Interpolation:</p> \\[ x_t = \\sqrt{1-\\sigma_t^2} \\, x_0 + \\sigma_t \\, x_1 \\] <p>where \\(\\sigma_t\\) is a noise schedule (e.g., \\(\\sigma_t = t\\)).</p> <p>Properties:</p> <ul> <li>Maintains \\(\\mathbb{E}[\\|x_t\\|^2] \\approx \\text{const}\\) (if \\(x_0, x_1\\) have similar norms)</li> <li>Similar to diffusion models</li> <li>Useful when data has specific scale</li> </ul> <p>Velocity:</p> \\[ u_t = \\frac{d}{dt}\\left(\\sqrt{1-\\sigma_t^2} \\, x_0 + \\sigma_t \\, x_1\\right) \\]"},{"location":"flow_matching/01_flow_matching_foundations/#non-variance-preserving-nvp-flows","title":"Non-Variance-Preserving (NVP) Flows","text":"<p>Interpolation (Rectified Flow):</p> \\[ x_t = (1-t) x_0 + t x_1 \\] <p>Properties:</p> <ul> <li>Does not preserve variance</li> <li>Simpler mathematics</li> <li>Often works better in practice</li> </ul> <p>Velocity:</p> \\[ u_t = x_1 - x_0 \\]"},{"location":"flow_matching/01_flow_matching_foundations/#which-to-use","title":"Which to Use?","text":"<p>Use VP flows when:</p> <ul> <li>Data has specific scale requirements</li> <li>Comparing with diffusion baselines</li> <li>Working with normalized data (e.g., images in \\([-1, 1]\\))</li> </ul> <p>Use NVP flows when:</p> <ul> <li>Simplicity is preferred</li> <li>Data scale is flexible</li> <li>Focusing on rectified flow</li> </ul>"},{"location":"flow_matching/01_flow_matching_foundations/#conditional-generation","title":"Conditional Generation","text":""},{"location":"flow_matching/01_flow_matching_foundations/#conditioning-mechanisms","title":"Conditioning Mechanisms","text":"<p>Flow matching naturally supports conditional generation.</p> <p>Conditional velocity field:</p> \\[ v_\\theta(x, t, c) \\] <p>where \\(c\\) is a conditioning variable (e.g., class label, text embedding).</p> <p>Training: Sample \\((x_0, c)\\) pairs from data, then:</p> \\[ \\mathcal{L} = \\mathbb{E}_{t, x_0, x_1, c} \\left[ \\left\\| v_\\theta(x_t, t, c) - (x_1 - x_0) \\right\\|^2 \\right] \\] <p>Sampling: Integrate ODE conditioned on \\(c\\):</p> \\[ \\frac{dx}{dt} = v_\\theta(x(t), t, c) \\]"},{"location":"flow_matching/01_flow_matching_foundations/#classifier-free-guidance","title":"Classifier-Free Guidance","text":"<p>Adapt classifier-free guidance from diffusion to flow matching:</p> <p>Training: Randomly drop conditioning (set \\(c = \\emptyset\\) with probability \\(p_{\\text{uncond}}\\))</p> <p>Sampling: Use guided velocity:</p> \\[ \\tilde{v}_\\theta(x, t, c) = v_\\theta(x, t, \\emptyset) + w \\cdot (v_\\theta(x, t, c) - v_\\theta(x, t, \\emptyset)) \\] <p>where \\(w\\) is the guidance weight.</p> <p>Effect: Stronger conditioning, sharper samples (at cost of diversity).</p>"},{"location":"flow_matching/01_flow_matching_foundations/#comparison-with-diffusion-models","title":"Comparison with Diffusion Models","text":""},{"location":"flow_matching/01_flow_matching_foundations/#conceptual-differences","title":"Conceptual Differences","text":"Aspect Diffusion Models Flow Matching Forward process Stochastic noise addition Deterministic interpolation What's learned Score: \\(\\nabla_x \\log p_t(x)\\) Velocity: \\(v_t(x)\\) Training objective Score matching (complex) Simple regression Reverse process Stochastic SDE or ODE Deterministic ODE Sampling 100-1000 steps (SDE), 50-100 (ODE) 10-50 steps (ODE) Noise schedule Critical design choice Less critical Theoretical foundation Score-based models, SDEs Optimal transport, ODEs"},{"location":"flow_matching/01_flow_matching_foundations/#when-to-use-each","title":"When to Use Each","text":"<p>Use diffusion when:</p> <ul> <li>Mature architectures and tricks are important</li> <li>Stochastic sampling is desired</li> <li>Extensive baselines exist for comparison</li> <li>Working with images (well-established)</li> </ul> <p>Use flow matching when:</p> <ul> <li>Faster sampling is critical</li> <li>Simpler training is preferred</li> <li>Working with non-Euclidean data</li> <li>Exploring new domains (e.g., biology, molecules)</li> </ul>"},{"location":"flow_matching/01_flow_matching_foundations/#practical-considerations","title":"Practical Considerations","text":""},{"location":"flow_matching/01_flow_matching_foundations/#network-architecture","title":"Network Architecture","text":"<p>Flow matching networks \\(v_\\theta(x, t)\\) typically use:</p> <p>For images:</p> <ul> <li>U-Net: Convolutional architecture with skip connections</li> <li>DiT (Diffusion Transformer): Transformer with patch embeddings</li> <li>Time conditioning: Via sinusoidal embeddings + FiLM layers</li> </ul> <p>For sequences (text, DNA, proteins): - Transformers: Self-attention over sequence - Time conditioning: Added to token embeddings</p> <p>For graphs (molecules): - GNNs: Message passing with time conditioning - Equivariance: Preserve symmetries</p>"},{"location":"flow_matching/01_flow_matching_foundations/#time-embedding","title":"Time Embedding","text":"<p>Time \\(t \\in [0, 1]\\) is typically embedded via:</p> <p>Sinusoidal embedding:</p> \\[ \\text{emb}(t) = [\\sin(2\\pi k t), \\cos(2\\pi k t)]_{k=1}^K \\] <p>Learned embedding:</p> \\[ \\text{emb}(t) = \\text{MLP}(t) \\] <p>Conditioning: Via FiLM (Feature-wise Linear Modulation):</p> \\[ \\text{FiLM}(h, t) = \\gamma(t) \\cdot h + \\beta(t) \\] <p>where \\(h\\) is a hidden representation.</p>"},{"location":"flow_matching/01_flow_matching_foundations/#training-tips","title":"Training Tips","text":"<p>1. Noise distribution: Match data characteristics - Images: Standard Gaussian - Gene expression: Consider sparsity structure - Molecules: Respect physical constraints</p> <p>2. Time sampling: Uniform \\(t \\sim U[0, 1]\\) works well, but can weight: - More weight near \\(t=0\\) (data) for quality - More weight near \\(t=1\\) (noise) for coverage</p> <p>3. Batch size: Larger is better (more diverse pairs)</p> <p>4. Learning rate: Standard schedules work (cosine, constant)</p>"},{"location":"flow_matching/01_flow_matching_foundations/#summary","title":"Summary","text":""},{"location":"flow_matching/01_flow_matching_foundations/#key-concepts","title":"Key Concepts","text":"<ol> <li>Flow matching learns velocity fields that transport noise to data</li> <li>Training is simple regression on conditional velocities</li> <li>Sampling is deterministic ODE integration (fast, few steps)</li> <li>Rectified flow uses straight paths (simplest, most practical)</li> <li>Reflow iteratively straightens paths (fewer steps needed)</li> </ol>"},{"location":"flow_matching/01_flow_matching_foundations/#key-equations","title":"Key Equations","text":"<p>Path (rectified flow):</p> \\[ x_t = (1-t) x_0 + t x_1 \\] <p>Velocity:</p> \\[ u_t = x_1 - x_0 \\] <p>Loss:</p> \\[ \\mathcal{L} = \\mathbb{E}_{t, x_0, x_1} \\left[ \\left\\| v_\\theta(x_t, t) - (x_1 - x_0) \\right\\|^2 \\right] \\] <p>Sampling ODE:</p> \\[ \\frac{dx}{dt} = v_\\theta(x(t), t), \\quad x(1) \\sim p_{\\text{noise}} \\]"},{"location":"flow_matching/01_flow_matching_foundations/#related-documents","title":"Related Documents","text":"<ul> <li>Flow Matching Training \u2014 Training strategies and implementation</li> <li>Flow Matching Sampling \u2014 ODE solvers and sampling efficiency</li> <li>Rectifying Flow Tutorial \u2014 Detailed walkthrough</li> <li>DDPM Foundations \u2014 Comparison with diffusion</li> <li>Diffusion Transformers \u2014 Architecture for flow matching</li> </ul>"},{"location":"flow_matching/01_flow_matching_foundations/#references","title":"References","text":""},{"location":"flow_matching/01_flow_matching_foundations/#foundational-papers","title":"Foundational Papers","text":"<ol> <li> <p>Lipman, Y., et al. (2023). Flow Matching for Generative Modeling. ICLR. arXiv:2210.02747</p> </li> <li> <p>Liu, X., et al. (2023). Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow. ICLR. arXiv:2209.03003</p> </li> <li> <p>Albergo, M. S., &amp; Vanden-Eijnden, E. (2023). Building Normalizing Flows with Stochastic Interpolants. ICLR. arXiv:2209.15571</p> </li> </ol>"},{"location":"flow_matching/01_flow_matching_foundations/#optimal-transport","title":"Optimal Transport","text":"<ol> <li> <p>Pooladian, A., et al. (2023). Multisample Flow Matching: Straightening Flows with Minibatch Couplings. ICML.</p> </li> <li> <p>Tong, A., et al. (2024). Improving and Generalizing Flow-Based Generative Models with Minibatch Optimal Transport. TMLR.</p> </li> </ol>"},{"location":"flow_matching/01_flow_matching_foundations/#applications","title":"Applications","text":"<ol> <li> <p>Esser, P., et al. (2024). Scaling Rectified Flow Transformers for High-Resolution Image Synthesis. ICML.</p> </li> <li> <p>Pooladian, A., et al. (2024). Flow Matching on General Geometries. ICLR.</p> </li> </ol>"},{"location":"flow_matching/02_flow_matching_training/","title":"Flow Matching Training","text":"<p>This document covers the practical aspects of training flow matching models: loss functions, network architectures, implementation details, training strategies, and best practices.</p>"},{"location":"flow_matching/02_flow_matching_training/#training-overview","title":"Training Overview","text":""},{"location":"flow_matching/02_flow_matching_training/#the-training-loop","title":"The Training Loop","text":"<p>Flow matching training is remarkably simple compared to diffusion models:</p> <p>Algorithm: <pre><code>for batch in dataloader:\n    # 1. Sample data and noise\n    x0 = batch  # data\n    x1 = sample_noise()  # noise\n\n    # 2. Sample time uniformly\n    t = uniform(0, 1)\n\n    # 3. Interpolate\n    xt = (1 - t) * x0 + t * x1\n\n    # 4. Compute target velocity\n    target = x1 - x0\n\n    # 5. Predict and compute loss\n    pred = model(xt, t)\n    loss = mse_loss(pred, target)\n\n    # 6. Update\n    loss.backward()\n    optimizer.step()\n</code></pre></p> <p>Key simplicity: Direct regression with MSE loss, no complex score matching objectives.</p>"},{"location":"flow_matching/02_flow_matching_training/#loss-functions","title":"Loss Functions","text":""},{"location":"flow_matching/02_flow_matching_training/#conditional-flow-matching-loss","title":"Conditional Flow Matching Loss","text":"<p>The standard loss for flow matching:</p> \\[ \\mathcal{L}_{\\text{CFM}}(\\theta) = \\mathbb{E}_{t, x_0, x_1} \\left[ \\left\\| v_\\theta(x_t, t) - u_t(x_0, x_1) \\right\\|^2 \\right] \\] <p>Components:</p> <ul> <li>\\(t \\sim \\text{Uniform}[0, 1]\\): Random time</li> <li>\\(x_0 \\sim p_{\\text{data}}\\): Data sample</li> <li>\\(x_1 \\sim p_{\\text{noise}}\\): Noise sample</li> <li>\\(x_t = \\psi_t(x_0, x_1)\\): Interpolated point</li> <li>\\(u_t(x_0, x_1) = \\frac{d}{dt}\\psi_t(x_0, x_1)\\): Target velocity</li> </ul>"},{"location":"flow_matching/02_flow_matching_training/#rectified-flow-loss","title":"Rectified Flow Loss","text":"<p>For linear interpolation \\(x_t = (1-t)x_0 + tx_1\\):</p> \\[ \\mathcal{L}_{\\text{RF}}(\\theta) = \\mathbb{E}_{t, x_0, x_1} \\left[ \\left\\| v_\\theta(x_t, t) - (x_1 - x_0) \\right\\|^2 \\right] \\] <p>Simplification: Target velocity is constant: \\(u_t = x_1 - x_0\\)</p> <p>PyTorch implementation: <pre><code>def rectified_flow_loss(model, x0, x1, t):\n    \"\"\"\n    Compute rectified flow loss.\n\n    Args:\n        model: Neural network v_theta(x, t)\n        x0: Data samples [batch_size, ...]\n        x1: Noise samples [batch_size, ...]\n        t: Time values [batch_size]\n\n    Returns:\n        loss: Scalar loss value\n    \"\"\"\n    # Interpolate\n    t_expanded = t.view(-1, *([1] * (x0.ndim - 1)))\n    xt = (1 - t_expanded) * x0 + t_expanded * x1\n\n    # Target velocity\n    target = x1 - x0\n\n    # Predict velocity\n    pred = model(xt, t)\n\n    # MSE loss\n    loss = F.mse_loss(pred, target)\n\n    return loss\n</code></pre></p>"},{"location":"flow_matching/02_flow_matching_training/#variance-preserving-loss","title":"Variance-Preserving Loss","text":"<p>For VP interpolation \\(x_t = \\sqrt{1-\\sigma_t^2} \\, x_0 + \\sigma_t \\, x_1\\):</p> \\[ \\mathcal{L}_{\\text{VP}}(\\theta) = \\mathbb{E}_{t, x_0, x_1} \\left[ \\left\\| v_\\theta(x_t, t) - \\frac{d}{dt}\\left(\\sqrt{1-\\sigma_t^2} \\, x_0 + \\sigma_t \\, x_1\\right) \\right\\|^2 \\right] \\] <p>Target velocity:</p> \\[ u_t = -\\frac{\\sigma_t \\sigma_t'}{\\sqrt{1-\\sigma_t^2}} x_0 + \\sigma_t' x_1 \\] <p>where \\(\\sigma_t' = \\frac{d\\sigma_t}{dt}\\).</p> <p>Common choice: \\(\\sigma_t = t\\), so \\(\\sigma_t' = 1\\):</p> \\[ u_t = -\\frac{t}{\\sqrt{1-t^2}} x_0 + x_1 \\]"},{"location":"flow_matching/02_flow_matching_training/#weighted-loss","title":"Weighted Loss","text":"<p>Add time-dependent weighting:</p> \\[ \\mathcal{L}_{\\text{weighted}}(\\theta) = \\mathbb{E}_{t, x_0, x_1} \\left[ w(t) \\left\\| v_\\theta(x_t, t) - u_t \\right\\|^2 \\right] \\] <p>Common weights:</p> <p>1. Uniform: \\(w(t) = 1\\) (standard)</p> <p>2. SNR-based: \\(w(t) = \\frac{1}{\\text{SNR}(t)}\\) (from diffusion)</p> <p>3. Endpoint emphasis: \\(w(t) = t^2\\) or \\(w(t) = (1-t)^2\\)</p> <p>4. Min-SNR: \\(w(t) = \\min(\\text{SNR}(t), \\gamma)\\) (clip large weights)</p> <p>When to use:</p> <ul> <li>Uniform works well for most cases</li> <li>Endpoint emphasis if sampling quality at \\(t=0\\) is critical</li> <li>SNR-based for VP flows to match diffusion performance</li> </ul>"},{"location":"flow_matching/02_flow_matching_training/#network-architectures","title":"Network Architectures","text":""},{"location":"flow_matching/02_flow_matching_training/#architecture-requirements","title":"Architecture Requirements","text":"<p>Flow matching networks \\(v_\\theta(x, t)\\) must:</p> <ol> <li>Input: Accept data \\(x\\) and time \\(t\\)</li> <li>Output: Velocity vector same shape as \\(x\\)</li> <li>Time conditioning: Incorporate \\(t\\) throughout the network</li> <li>Expressiveness: Capture complex velocity fields</li> </ol>"},{"location":"flow_matching/02_flow_matching_training/#u-net-architecture","title":"U-Net Architecture","text":"<p>Standard choice for images:</p> <pre><code>class FlowMatchingUNet(nn.Module):\n    def __init__(self, channels=3, dim=64, dim_mults=(1, 2, 4, 8)):\n        super().__init__()\n\n        # Time embedding\n        self.time_mlp = nn.Sequential(\n            SinusoidalPosEmb(dim),\n            nn.Linear(dim, dim * 4),\n            nn.GELU(),\n            nn.Linear(dim * 4, dim * 4)\n        )\n\n        # Encoder\n        self.downs = nn.ModuleList([])\n        for mult in dim_mults:\n            self.downs.append(\n                ResnetBlock(channels, dim * mult, time_emb_dim=dim * 4)\n            )\n\n        # Bottleneck\n        self.mid = ResnetBlock(dim * dim_mults[-1], dim * dim_mults[-1])\n\n        # Decoder\n        self.ups = nn.ModuleList([])\n        for mult in reversed(dim_mults):\n            self.ups.append(\n                ResnetBlock(dim * mult * 2, dim * mult, time_emb_dim=dim * 4)\n            )\n\n        # Output\n        self.final = nn.Conv2d(dim, channels, 1)\n\n    def forward(self, x, t):\n        # Time embedding\n        t_emb = self.time_mlp(t)\n\n        # Encoder\n        h = []\n        for down in self.downs:\n            x = down(x, t_emb)\n            h.append(x)\n\n        # Bottleneck\n        x = self.mid(x, t_emb)\n\n        # Decoder\n        for up in self.ups:\n            x = torch.cat([x, h.pop()], dim=1)\n            x = up(x, t_emb)\n\n        # Output velocity\n        return self.final(x)\n</code></pre> <p>Key components:</p> <ul> <li>Sinusoidal time embedding: Encodes \\(t \\in [0, 1]\\)</li> <li>ResNet blocks: With time conditioning via FiLM</li> <li>Skip connections: Preserve spatial information</li> <li>U-Net structure: Encoder-decoder with bottleneck</li> </ul>"},{"location":"flow_matching/02_flow_matching_training/#diffusion-transformer-dit","title":"Diffusion Transformer (DiT)","text":"<p>Modern architecture for images:</p> <pre><code>class DiTBlock(nn.Module):\n    def __init__(self, dim, num_heads):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn = nn.MultiheadAttention(dim, num_heads)\n        self.norm2 = nn.LayerNorm(dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, dim * 4),\n            nn.GELU(),\n            nn.Linear(dim * 4, dim)\n        )\n\n        # AdaLN modulation\n        self.adaLN_modulation = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(dim, 6 * dim)\n        )\n\n    def forward(self, x, c):\n        # c is time + condition embedding\n        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = \\\n            self.adaLN_modulation(c).chunk(6, dim=-1)\n\n        # Self-attention with AdaLN\n        x = x + gate_msa * self.attn(\n            modulate(self.norm1(x), shift_msa, scale_msa)\n        )\n\n        # MLP with AdaLN\n        x = x + gate_mlp * self.mlp(\n            modulate(self.norm2(x), shift_mlp, scale_mlp)\n        )\n\n        return x\n\nclass FlowMatchingDiT(nn.Module):\n    def __init__(self, img_size=32, patch_size=2, dim=512, depth=12, num_heads=8):\n        super().__init__()\n\n        # Patchify\n        self.patch_embed = PatchEmbed(img_size, patch_size, 3, dim)\n\n        # Time + condition embedding\n        self.time_embed = TimestepEmbedder(dim)\n\n        # Transformer blocks\n        self.blocks = nn.ModuleList([\n            DiTBlock(dim, num_heads) for _ in range(depth)\n        ])\n\n        # Output\n        self.final_layer = FinalLayer(dim, patch_size, 3)\n\n    def forward(self, x, t):\n        # Patchify\n        x = self.patch_embed(x)\n\n        # Time embedding\n        c = self.time_embed(t)\n\n        # Transformer\n        for block in self.blocks:\n            x = block(x, c)\n\n        # Unpatchify and output velocity\n        return self.final_layer(x)\n</code></pre> <p>Advantages:</p> <ul> <li>Scalability: Scales better to large models</li> <li>Flexibility: Handles variable resolutions</li> <li>Attention: Captures long-range dependencies</li> <li>Modern: State-of-the-art for image generation</li> </ul>"},{"location":"flow_matching/02_flow_matching_training/#time-conditioning","title":"Time Conditioning","text":"<p>Sinusoidal embedding (standard):</p> <pre><code>class SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, t):\n        device = t.device\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n        emb = t[:, None] * emb[None, :]\n        emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n        return emb\n</code></pre> <p>FiLM conditioning (Feature-wise Linear Modulation):</p> <pre><code>class FiLM(nn.Module):\n    def __init__(self, dim, time_emb_dim):\n        super().__init__()\n        self.scale_shift = nn.Linear(time_emb_dim, dim * 2)\n\n    def forward(self, x, time_emb):\n        scale, shift = self.scale_shift(time_emb).chunk(2, dim=-1)\n        return x * (1 + scale) + shift\n</code></pre> <p>AdaLN conditioning (Adaptive Layer Normalization):</p> <pre><code>def modulate(x, shift, scale):\n    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n</code></pre>"},{"location":"flow_matching/02_flow_matching_training/#training-strategies","title":"Training Strategies","text":""},{"location":"flow_matching/02_flow_matching_training/#data-preprocessing","title":"Data Preprocessing","text":"<p>Images: <pre><code># Normalize to [-1, 1]\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n</code></pre></p> <p>Gene expression: <pre><code># Log-normalize and standardize\ndef preprocess_gene_expression(X):\n    # Log1p transform\n    X = np.log1p(X)\n\n    # Standardize per gene\n    X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8)\n\n    return X\n</code></pre></p>"},{"location":"flow_matching/02_flow_matching_training/#noise-distribution","title":"Noise Distribution","text":"<p>Standard Gaussian (most common): <pre><code>x1 = torch.randn_like(x0)\n</code></pre></p> <p>Matched variance: <pre><code># Match data variance\ndata_std = x0.std()\nx1 = torch.randn_like(x0) * data_std\n</code></pre></p> <p>Domain-specific (for gene expression): <pre><code># Sparse noise matching dropout structure\ndef sparse_noise(x0, dropout_rate=0.1):\n    noise = torch.randn_like(x0)\n    mask = torch.rand_like(x0) &gt; dropout_rate\n    return noise * mask\n</code></pre></p>"},{"location":"flow_matching/02_flow_matching_training/#time-sampling","title":"Time Sampling","text":"<p>Uniform (standard): <pre><code>t = torch.rand(batch_size, device=device)\n</code></pre></p> <p>Stratified (better coverage): <pre><code># Divide [0,1] into bins\nn_bins = batch_size\nbins = torch.linspace(0, 1, n_bins + 1, device=device)\nt = bins[:-1] + torch.rand(n_bins, device=device) / n_bins\n</code></pre></p> <p>Importance sampling (emphasize difficult times): <pre><code># More samples near t=0 (data)\nt = torch.rand(batch_size, device=device) ** 2\n</code></pre></p>"},{"location":"flow_matching/02_flow_matching_training/#batch-size-and-learning-rate","title":"Batch Size and Learning Rate","text":"<p>Batch size:</p> <ul> <li>Images: 128-512 (larger is better for diverse pairs)</li> <li>Gene expression: 64-256 (depends on dataset size)</li> <li>Rule of thumb: As large as GPU memory allows</li> </ul> <p>Learning rate:</p> <ul> <li>AdamW: 1e-4 to 5e-4 (standard)</li> <li>Warmup: 1000-5000 steps</li> <li>Schedule: Cosine decay or constant</li> </ul> <pre><code>optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n</code></pre>"},{"location":"flow_matching/02_flow_matching_training/#ema-exponential-moving-average","title":"EMA (Exponential Moving Average)","text":"<p>Use EMA for better sampling quality:</p> <pre><code>class EMA:\n    def __init__(self, model, decay=0.9999):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                self.shadow[name] = param.data.clone()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.shadow[name] -= (1 - self.decay) * (self.shadow[name] - param.data)\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n        self.backup = {}\n\n# Usage\nema = EMA(model, decay=0.9999)\n\nfor batch in dataloader:\n    loss = train_step(model, batch)\n    optimizer.step()\n    ema.update()  # Update EMA after each step\n\n# For sampling, use EMA weights\nema.apply_shadow()\nsamples = sample(model, ...)\nema.restore()\n</code></pre>"},{"location":"flow_matching/02_flow_matching_training/#complete-training-script","title":"Complete Training Script","text":""},{"location":"flow_matching/02_flow_matching_training/#full-implementation","title":"Full Implementation","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\ndef train_flow_matching(\n    model,\n    train_loader,\n    num_epochs=100,\n    lr=1e-4,\n    device='cuda',\n    use_ema=True,\n    save_every=10\n):\n    \"\"\"\n    Complete training loop for flow matching.\n    \"\"\"\n    # Optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n\n    # Learning rate scheduler\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer, T_max=num_epochs\n    )\n\n    # EMA\n    if use_ema:\n        ema = EMA(model, decay=0.9999)\n\n    # Training loop\n    model.train()\n    for epoch in range(num_epochs):\n        epoch_loss = 0\n        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n\n        for batch_idx, x0 in enumerate(pbar):\n            x0 = x0.to(device)\n            batch_size = x0.shape[0]\n\n            # Sample noise\n            x1 = torch.randn_like(x0)\n\n            # Sample time\n            t = torch.rand(batch_size, device=device)\n\n            # Interpolate\n            t_expanded = t.view(-1, *([1] * (x0.ndim - 1)))\n            xt = (1 - t_expanded) * x0 + t_expanded * x1\n\n            # Target velocity\n            target = x1 - x0\n\n            # Predict velocity\n            pred = model(xt, t)\n\n            # Compute loss\n            loss = F.mse_loss(pred, target)\n\n            # Backward\n            optimizer.zero_grad()\n            loss.backward()\n\n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            # Update\n            optimizer.step()\n\n            # Update EMA\n            if use_ema:\n                ema.update()\n\n            # Logging\n            epoch_loss += loss.item()\n            pbar.set_postfix({'loss': loss.item()})\n\n        # Scheduler step\n        scheduler.step()\n\n        # Log epoch\n        avg_loss = epoch_loss / len(train_loader)\n        print(f'Epoch {epoch+1}: Average Loss = {avg_loss:.6f}')\n\n        # Save checkpoint\n        if (epoch + 1) % save_every == 0:\n            checkpoint = {\n                'epoch': epoch + 1,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': avg_loss,\n            }\n            if use_ema:\n                checkpoint['ema_state_dict'] = ema.shadow\n\n            torch.save(checkpoint, f'checkpoint_epoch_{epoch+1}.pt')\n\n    return model\n</code></pre>"},{"location":"flow_matching/02_flow_matching_training/#training-example","title":"Training Example","text":"<pre><code># Load data\nfrom torchvision import datasets, transforms\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n\ntrain_dataset = datasets.CIFAR10(\n    root='./data', train=True, download=True, transform=transform\n)\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n\n# Create model\nmodel = FlowMatchingUNet(channels=3, dim=64).cuda()\n\n# Train\ntrained_model = train_flow_matching(\n    model,\n    train_loader,\n    num_epochs=100,\n    lr=1e-4,\n    device='cuda',\n    use_ema=True\n)\n</code></pre>"},{"location":"flow_matching/02_flow_matching_training/#reflow-iterative-refinement","title":"Reflow: Iterative Refinement","text":""},{"location":"flow_matching/02_flow_matching_training/#the-reflow-algorithm","title":"The Reflow Algorithm","text":"<p>Reflow iteratively straightens flow paths for faster sampling.</p> <p>Algorithm: <pre><code>def reflow(model, train_loader, num_iterations=3):\n    \"\"\"\n    Iterative reflow to straighten paths.\n    \"\"\"\n    models = [model]\n\n    for iteration in range(1, num_iterations):\n        print(f'Reflow iteration {iteration}')\n\n        # Generate synthetic data using current model\n        synthetic_data = []\n        for x1 in tqdm(train_loader, desc='Generating synthetic data'):\n            x1 = x1.to(device)\n            # Sample from current model\n            x0_synthetic = sample_ode(models[-1], x1)\n            synthetic_data.append((x0_synthetic, x1))\n\n        # Train new model on synthetic data\n        new_model = FlowMatchingUNet(channels=3, dim=64).cuda()\n\n        for epoch in range(num_epochs):\n            for x0_syn, x1 in synthetic_data:\n                # Standard flow matching training\n                t = torch.rand(x0_syn.shape[0], device=device)\n                t_exp = t.view(-1, *([1] * (x0_syn.ndim - 1)))\n                xt = (1 - t_exp) * x0_syn + t_exp * x1\n\n                target = x1 - x0_syn\n                pred = new_model(xt, t)\n                loss = F.mse_loss(pred, target)\n\n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n\n        models.append(new_model)\n\n    return models\n</code></pre></p> <p>Effect:</p> <ul> <li>Iteration 1: 50-100 steps needed</li> <li>Iteration 2: 20-30 steps needed</li> <li>Iteration 3: 10-15 steps needed</li> </ul> <p>Trade-off: More training time for faster sampling.</p>"},{"location":"flow_matching/02_flow_matching_training/#monitoring-and-debugging","title":"Monitoring and Debugging","text":""},{"location":"flow_matching/02_flow_matching_training/#training-metrics","title":"Training Metrics","text":"<p>Track during training:</p> <ol> <li>Loss: Should decrease steadily</li> <li>Gradient norm: Should be stable (clip if exploding)</li> <li>Learning rate: Follow schedule</li> <li>Sample quality: Generate samples periodically</li> </ol> <pre><code># Log metrics\nwandb.log({\n    'loss': loss.item(),\n    'grad_norm': grad_norm,\n    'lr': scheduler.get_last_lr()[0],\n    'epoch': epoch\n})\n\n# Generate samples every N epochs\nif epoch % 10 == 0:\n    with torch.no_grad():\n        samples = sample_ode(model, num_samples=64)\n        wandb.log({'samples': wandb.Image(samples)})\n</code></pre>"},{"location":"flow_matching/02_flow_matching_training/#common-issues","title":"Common Issues","text":"<p>1. Loss not decreasing:</p> <ul> <li>Check learning rate (try 1e-4)</li> <li>Check data normalization</li> <li>Verify target velocity computation</li> <li>Increase batch size</li> </ul> <p>2. NaN loss:</p> <ul> <li>Gradient clipping (clip_grad_norm)</li> <li>Lower learning rate</li> <li>Check for inf/nan in data</li> <li>Use mixed precision carefully</li> </ul> <p>3. Poor sample quality:</p> <ul> <li>Train longer</li> <li>Use EMA</li> <li>Increase model capacity</li> <li>Try more sampling steps</li> <li>Check noise distribution</li> </ul> <p>4. Mode collapse:</p> <ul> <li>Increase batch size</li> <li>Use diverse noise samples</li> <li>Check data augmentation</li> <li>Verify loss computation</li> </ul>"},{"location":"flow_matching/02_flow_matching_training/#best-practices","title":"Best Practices","text":""},{"location":"flow_matching/02_flow_matching_training/#dos","title":"Do's","text":"<p>\u2705 Use EMA for better sampling quality \u2705 Clip gradients to prevent instability \u2705 Normalize data to [-1, 1] or standardize \u2705 Use large batch sizes for diverse pairs \u2705 Monitor samples during training \u2705 Save checkpoints regularly \u2705 Use mixed precision for faster training (with caution)</p>"},{"location":"flow_matching/02_flow_matching_training/#donts","title":"Don'ts","text":"<p>\u274c Don't skip EMA (significant quality improvement) \u274c Don't use tiny batch sizes (&lt;32) \u274c Don't ignore gradient norms (clip if &gt;1.0) \u274c Don't overtrain (diminishing returns after convergence) \u274c Don't forget data normalization</p>"},{"location":"flow_matching/02_flow_matching_training/#summary","title":"Summary","text":""},{"location":"flow_matching/02_flow_matching_training/#key-training-steps","title":"Key Training Steps","text":"<ol> <li>Sample data \\(x_0\\) and noise \\(x_1\\)</li> <li>Sample time \\(t \\sim U[0, 1]\\)</li> <li>Interpolate \\(x_t = (1-t)x_0 + tx_1\\)</li> <li>Compute target \\(u_t = x_1 - x_0\\)</li> <li>Predict \\(v_\\theta(x_t, t)\\)</li> <li>Optimize MSE loss</li> </ol>"},{"location":"flow_matching/02_flow_matching_training/#key-hyperparameters","title":"Key Hyperparameters","text":"<ul> <li>Batch size: 128-512 (larger is better)</li> <li>Learning rate: 1e-4 to 5e-4</li> <li>EMA decay: 0.9999</li> <li>Gradient clip: 1.0</li> <li>Epochs: 100-500 (depends on dataset)</li> </ul>"},{"location":"flow_matching/02_flow_matching_training/#architecture-choices","title":"Architecture Choices","text":"<ul> <li>Images: U-Net or DiT</li> <li>Sequences: Transformer</li> <li>Time conditioning: Sinusoidal + FiLM/AdaLN</li> </ul>"},{"location":"flow_matching/02_flow_matching_training/#related-documents","title":"Related Documents","text":"<ul> <li>Flow Matching Foundations \u2014 Theory and mathematics</li> <li>Flow Matching Sampling \u2014 ODE solvers and sampling</li> <li>DDPM Training \u2014 Comparison with diffusion training</li> <li>Diffusion Transformers \u2014 DiT architecture</li> </ul>"},{"location":"flow_matching/02_flow_matching_training/#references","title":"References","text":"<ol> <li>Lipman, Y., et al. (2023). Flow Matching for Generative Modeling. ICLR.</li> <li>Liu, X., et al. (2023). Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow. ICLR.</li> <li>Peebles, W., &amp; Xie, S. (2023). Scalable Diffusion Models with Transformers. ICCV.</li> <li>Karras, T., et al. (2022). Elucidating the Design Space of Diffusion-Based Generative Models. NeurIPS.</li> </ol>"},{"location":"flow_matching/03_flow_matching_sampling/","title":"Flow Matching Sampling","text":"<p>This document covers sampling from flow matching models: ODE solvers, sampling strategies, quality-speed tradeoffs, and practical considerations.</p>"},{"location":"flow_matching/03_flow_matching_sampling/#sampling-overview","title":"Sampling Overview","text":""},{"location":"flow_matching/03_flow_matching_sampling/#the-sampling-problem","title":"The Sampling Problem","text":"<p>After training a flow matching model \\(v_\\theta(x, t)\\), we generate samples by solving an ordinary differential equation (ODE) backward in time.</p> <p>Setup:</p> <ul> <li>Start: \\(x(1) \\sim p_{\\text{noise}}\\) (e.g., \\(\\mathcal{N}(0, I)\\))</li> <li>Goal: \\(x(0) \\sim p_{\\text{data}}\\)</li> <li>Method: Integrate the ODE from \\(t=1\\) to \\(t=0\\)</li> </ul> <p>Flow ODE:</p> \\[ \\frac{dx}{dt} = v_\\theta(x(t), t) \\] <p>Key property: This is a deterministic process\u2014same initial noise always produces the same output.</p>"},{"location":"flow_matching/03_flow_matching_sampling/#ode-solvers","title":"ODE Solvers","text":""},{"location":"flow_matching/03_flow_matching_sampling/#euler-method","title":"Euler Method","text":"<p>The simplest ODE solver uses first-order approximation.</p> <p>Discrete update:</p> \\[ x_{t-\\Delta t} = x_t - \\Delta t \\cdot v_\\theta(x_t, t) \\] <p>Algorithm: <pre><code>def euler_sample(model, x_init, num_steps=50):\n    \"\"\"\n    Sample using Euler method.\n\n    Args:\n        model: Trained flow matching model v_theta(x, t)\n        x_init: Initial noise [batch_size, ...]\n        num_steps: Number of discretization steps\n\n    Returns:\n        x_final: Generated samples [batch_size, ...]\n    \"\"\"\n    x = x_init\n    dt = 1.0 / num_steps\n\n    for i in range(num_steps):\n        t = 1.0 - i * dt  # Time going backward from 1 to 0\n        t_tensor = torch.full((x.shape[0],), t, device=x.device)\n\n        # Compute velocity\n        with torch.no_grad():\n            v = model(x, t_tensor)\n\n        # Euler step\n        x = x - dt * v\n\n    return x\n</code></pre></p> <p>Properties:</p> <ul> <li>Simple: Easy to implement</li> <li>Fast: One function evaluation per step</li> <li>Accuracy: \\(O(\\Delta t)\\) local error, \\(O(\\Delta t)\\) global error</li> <li>Typical steps: 50-100 for good quality</li> </ul>"},{"location":"flow_matching/03_flow_matching_sampling/#runge-kutta-4-rk4","title":"Runge-Kutta 4 (RK4)","text":"<p>Fourth-order Runge-Kutta provides better accuracy with fewer steps.</p> <p>Update formula:</p> \\[ \\begin{align} k_1 &amp;= v_\\theta(x_t, t) \\\\ k_2 &amp;= v_\\theta(x_t - \\frac{\\Delta t}{2} k_1, t - \\frac{\\Delta t}{2}) \\\\ k_3 &amp;= v_\\theta(x_t - \\frac{\\Delta t}{2} k_2, t - \\frac{\\Delta t}{2}) \\\\ k_4 &amp;= v_\\theta(x_t - \\Delta t \\cdot k_3, t - \\Delta t) \\\\ x_{t-\\Delta t} &amp;= x_t - \\frac{\\Delta t}{6}(k_1 + 2k_2 + 2k_3 + k_4) \\end{align} \\] <p>Algorithm: <pre><code>def rk4_sample(model, x_init, num_steps=20):\n    \"\"\"\n    Sample using RK4 method.\n\n    Args:\n        model: Trained flow matching model\n        x_init: Initial noise\n        num_steps: Number of steps\n\n    Returns:\n        x_final: Generated samples\n    \"\"\"\n    x = x_init\n    dt = 1.0 / num_steps\n\n    for i in range(num_steps):\n        t = 1.0 - i * dt\n\n        # k1\n        t_tensor = torch.full((x.shape[0],), t, device=x.device)\n        with torch.no_grad():\n            k1 = model(x, t_tensor)\n\n        # k2\n        t_half = t - dt / 2\n        t_tensor = torch.full((x.shape[0],), t_half, device=x.device)\n        with torch.no_grad():\n            k2 = model(x - dt / 2 * k1, t_tensor)\n\n        # k3\n        with torch.no_grad():\n            k3 = model(x - dt / 2 * k2, t_tensor)\n\n        # k4\n        t_next = t - dt\n        t_tensor = torch.full((x.shape[0],), t_next, device=x.device)\n        with torch.no_grad():\n            k4 = model(x - dt * k3, t_tensor)\n\n        # Weighted average\n        x = x - dt / 6 * (k1 + 2*k2 + 2*k3 + k4)\n\n    return x\n</code></pre></p> <p>Properties:</p> <ul> <li>Accurate: \\(O(\\Delta t^4)\\) local error, \\(O(\\Delta t^4)\\) global error</li> <li>Efficient: 4\u00d7 function evaluations per step, but needs 3-5\u00d7 fewer steps</li> <li>Typical steps: 10-20 for good quality</li> <li>Trade-off: More computation per step, but fewer total steps</li> </ul>"},{"location":"flow_matching/03_flow_matching_sampling/#adaptive-solvers","title":"Adaptive Solvers","text":"<p>Adaptive solvers automatically adjust step size based on local error estimates.</p> <p>Dormand-Prince (dopri5): <pre><code>from torchdiffeq import odeint\n\ndef adaptive_sample(model, x_init, rtol=1e-5, atol=1e-5):\n    \"\"\"\n    Sample using adaptive ODE solver.\n\n    Args:\n        model: Trained flow matching model\n        x_init: Initial noise\n        rtol: Relative tolerance\n        atol: Absolute tolerance\n\n    Returns:\n        x_final: Generated samples\n    \"\"\"\n    # Define ODE function\n    def ode_func(t, x):\n        t_tensor = torch.full((x.shape[0],), t.item(), device=x.device)\n        return model(x, t_tensor)\n\n    # Time points (backward from 1 to 0)\n    t_span = torch.tensor([1.0, 0.0], device=x_init.device)\n\n    # Solve ODE\n    trajectory = odeint(\n        ode_func,\n        x_init,\n        t_span,\n        rtol=rtol,\n        atol=atol,\n        method='dopri5'\n    )\n\n    return trajectory[-1]  # Return final point\n</code></pre></p> <p>Properties:</p> <ul> <li>Automatic: No need to choose number of steps</li> <li>Efficient: Uses more steps where needed, fewer where possible</li> <li>Accurate: Error control via tolerances</li> <li>Typical NFE: 15-30 (number of function evaluations)</li> </ul> <p>When to use:</p> <ul> <li>When you want guaranteed accuracy</li> <li>When sampling budget is flexible</li> <li>For complex, non-smooth velocity fields</li> </ul>"},{"location":"flow_matching/03_flow_matching_sampling/#sampling-strategies","title":"Sampling Strategies","text":""},{"location":"flow_matching/03_flow_matching_sampling/#standard-sampling","title":"Standard Sampling","text":"<p>Basic procedure: <pre><code>def sample_flow_matching(model, batch_size=64, num_steps=20, device='cuda'):\n    \"\"\"\n    Standard sampling from flow matching model.\n    \"\"\"\n    # Sample initial noise\n    x_init = torch.randn(batch_size, 3, 32, 32, device=device)\n\n    # Integrate ODE\n    samples = rk4_sample(model, x_init, num_steps=num_steps)\n\n    # Denormalize if needed\n    samples = (samples + 1) / 2  # [-1, 1] -&gt; [0, 1]\n\n    return samples\n</code></pre></p>"},{"location":"flow_matching/03_flow_matching_sampling/#conditional-sampling","title":"Conditional Sampling","text":"<p>For conditional generation:</p> <pre><code>def conditional_sample(model, condition, batch_size=64, num_steps=20):\n    \"\"\"\n    Conditional sampling with class or text conditioning.\n\n    Args:\n        model: Conditional flow matching model v_theta(x, t, c)\n        condition: Conditioning variable (class label, text embedding, etc.)\n        batch_size: Number of samples\n        num_steps: ODE steps\n\n    Returns:\n        samples: Generated samples conditioned on c\n    \"\"\"\n    # Initial noise\n    x = torch.randn(batch_size, *data_shape, device=device)\n    dt = 1.0 / num_steps\n\n    # Repeat condition for batch\n    if condition.ndim == 1:\n        condition = condition.repeat(batch_size, 1)\n\n    # Integrate ODE with conditioning\n    for i in range(num_steps):\n        t = 1.0 - i * dt\n        t_tensor = torch.full((batch_size,), t, device=device)\n\n        with torch.no_grad():\n            v = model(x, t_tensor, condition)\n\n        x = x - dt * v\n\n    return x\n</code></pre>"},{"location":"flow_matching/03_flow_matching_sampling/#classifier-free-guidance","title":"Classifier-Free Guidance","text":"<p>Enhance conditioning strength:</p> <pre><code>def guided_sample(model, condition, guidance_weight=7.5, num_steps=20):\n    \"\"\"\n    Sampling with classifier-free guidance.\n\n    Args:\n        model: Conditional model trained with dropout\n        condition: Conditioning variable\n        guidance_weight: Guidance strength (w)\n        num_steps: ODE steps\n\n    Returns:\n        samples: Guided samples\n    \"\"\"\n    x = torch.randn(batch_size, *data_shape, device=device)\n    dt = 1.0 / num_steps\n\n    # Null condition (empty)\n    null_condition = torch.zeros_like(condition)\n\n    for i in range(num_steps):\n        t = 1.0 - i * dt\n        t_tensor = torch.full((batch_size,), t, device=device)\n\n        with torch.no_grad():\n            # Conditional velocity\n            v_cond = model(x, t_tensor, condition)\n\n            # Unconditional velocity\n            v_uncond = model(x, t_tensor, null_condition)\n\n            # Guided velocity\n            v_guided = v_uncond + guidance_weight * (v_cond - v_uncond)\n\n        x = x - dt * v_guided\n\n    return x\n</code></pre> <p>Effect:</p> <ul> <li>w = 0: Unconditional generation</li> <li>w = 1: Standard conditional generation</li> <li>w &gt; 1: Stronger conditioning (sharper, less diverse)</li> <li>Typical: w = 5-10 for images</li> </ul>"},{"location":"flow_matching/03_flow_matching_sampling/#quality-speed-tradeoffs","title":"Quality-Speed Tradeoffs","text":""},{"location":"flow_matching/03_flow_matching_sampling/#number-of-steps-vs-quality","title":"Number of Steps vs. Quality","text":"<p>Empirical relationship:</p> Solver Steps NFE Quality (FID) Time Euler 100 100 Excellent 100\u00d7 Euler 50 50 Good 50\u00d7 RK4 20 80 Excellent 80\u00d7 RK4 10 40 Good 40\u00d7 Adaptive Auto 15-30 Excellent 15-30\u00d7 <p>Key insight: RK4 with 10-20 steps often matches Euler with 50-100 steps.</p>"},{"location":"flow_matching/03_flow_matching_sampling/#choosing-number-of-steps","title":"Choosing Number of Steps","text":"<p>Guidelines:</p> <p>For images:</p> <ul> <li>High quality: 20-50 steps (RK4) or 100-200 steps (Euler)</li> <li>Balanced: 10-20 steps (RK4) or 50-100 steps (Euler)</li> <li>Fast: 5-10 steps (RK4) or 20-50 steps (Euler)</li> </ul> <p>For gene expression:</p> <ul> <li>High quality: 50-100 steps (Euler) or 20-30 steps (RK4)</li> <li>Balanced: 30-50 steps (Euler) or 10-20 steps (RK4)</li> <li>Fast: 10-20 steps (Euler) or 5-10 steps (RK4)</li> </ul> <p>Rule of thumb: Start with RK4 + 20 steps, adjust based on quality needs.</p>"},{"location":"flow_matching/03_flow_matching_sampling/#reflow-for-faster-sampling","title":"Reflow for Faster Sampling","text":"<p>After reflow iterations, fewer steps are needed:</p> <p>Iteration 0 (base model): - Euler: 100 steps - RK4: 20 steps</p> <p>Iteration 1 (1<sup>st</sup> reflow): - Euler: 50 steps - RK4: 10 steps</p> <p>Iteration 2 (2<sup>nd</sup> reflow): - Euler: 20 steps - RK4: 5 steps</p> <p>Trade-off: More training time for faster sampling.</p>"},{"location":"flow_matching/03_flow_matching_sampling/#practical-considerations","title":"Practical Considerations","text":""},{"location":"flow_matching/03_flow_matching_sampling/#batch-sampling","title":"Batch Sampling","text":"<p>Generate multiple samples efficiently:</p> <pre><code>def batch_sample(model, num_samples=1000, batch_size=100, num_steps=20):\n    \"\"\"\n    Generate many samples in batches.\n    \"\"\"\n    all_samples = []\n\n    for i in range(0, num_samples, batch_size):\n        current_batch_size = min(batch_size, num_samples - i)\n\n        # Sample batch\n        x_init = torch.randn(current_batch_size, *data_shape, device=device)\n        samples = rk4_sample(model, x_init, num_steps=num_steps)\n\n        all_samples.append(samples.cpu())\n\n    return torch.cat(all_samples, dim=0)\n</code></pre>"},{"location":"flow_matching/03_flow_matching_sampling/#memory-optimization","title":"Memory Optimization","text":"<p>For large models or high-resolution images:</p> <pre><code>@torch.no_grad()\ndef memory_efficient_sample(model, x_init, num_steps=20):\n    \"\"\"\n    Memory-efficient sampling with gradient checkpointing disabled.\n    \"\"\"\n    model.eval()\n\n    x = x_init\n    dt = 1.0 / num_steps\n\n    for i in range(num_steps):\n        t = 1.0 - i * dt\n        t_tensor = torch.full((x.shape[0],), t, device=x.device)\n\n        # Ensure no gradients\n        v = model(x, t_tensor)\n        x = x - dt * v\n\n        # Clear cache periodically\n        if i % 10 == 0:\n            torch.cuda.empty_cache()\n\n    return x\n</code></pre>"},{"location":"flow_matching/03_flow_matching_sampling/#deterministic-sampling","title":"Deterministic Sampling","text":"<p>For reproducibility:</p> <pre><code>def deterministic_sample(model, seed=42, batch_size=64, num_steps=20):\n    \"\"\"\n    Deterministic sampling with fixed seed.\n    \"\"\"\n    # Set seed\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n\n    # Sample\n    x_init = torch.randn(batch_size, *data_shape, device=device)\n    samples = rk4_sample(model, x_init, num_steps=num_steps)\n\n    return samples\n</code></pre>"},{"location":"flow_matching/03_flow_matching_sampling/#advanced-sampling-techniques","title":"Advanced Sampling Techniques","text":""},{"location":"flow_matching/03_flow_matching_sampling/#trajectory-visualization","title":"Trajectory Visualization","text":"<p>Visualize the generation process:</p> <pre><code>def sample_with_trajectory(model, x_init, num_steps=20, save_every=5):\n    \"\"\"\n    Sample and save intermediate states.\n    \"\"\"\n    x = x_init\n    dt = 1.0 / num_steps\n    trajectory = [x.clone()]\n\n    for i in range(num_steps):\n        t = 1.0 - i * dt\n        t_tensor = torch.full((x.shape[0],), t, device=x.device)\n\n        with torch.no_grad():\n            v = model(x, t_tensor)\n\n        x = x - dt * v\n\n        # Save intermediate states\n        if (i + 1) % save_every == 0:\n            trajectory.append(x.clone())\n\n    return x, trajectory\n</code></pre>"},{"location":"flow_matching/03_flow_matching_sampling/#interpolation-in-latent-space","title":"Interpolation in Latent Space","text":"<p>Interpolate between samples:</p> <pre><code>def interpolate_samples(model, x1_init, x2_init, num_interp=10, num_steps=20):\n    \"\"\"\n    Interpolate between two noise samples.\n    \"\"\"\n    # Interpolation weights\n    alphas = torch.linspace(0, 1, num_interp, device=x1_init.device)\n\n    interpolated_samples = []\n    for alpha in alphas:\n        # Interpolate in noise space\n        x_init = (1 - alpha) * x1_init + alpha * x2_init\n\n        # Generate sample\n        sample = rk4_sample(model, x_init, num_steps=num_steps)\n        interpolated_samples.append(sample)\n\n    return torch.stack(interpolated_samples)\n</code></pre>"},{"location":"flow_matching/03_flow_matching_sampling/#inpainting","title":"Inpainting","text":"<p>Fill in missing regions:</p> <pre><code>def inpaint(model, x_observed, mask, num_steps=20, num_iterations=5):\n    \"\"\"\n    Inpainting with flow matching.\n\n    Args:\n        model: Trained flow matching model\n        x_observed: Observed data (with missing regions)\n        mask: Binary mask (1 = observed, 0 = missing)\n        num_steps: ODE steps per iteration\n        num_iterations: Number of refinement iterations\n\n    Returns:\n        x_inpainted: Completed image\n    \"\"\"\n    x = torch.randn_like(x_observed)\n\n    for _ in range(num_iterations):\n        # Sample from model\n        x_sampled = rk4_sample(model, x, num_steps=num_steps)\n\n        # Replace observed regions\n        x = mask * x_observed + (1 - mask) * x_sampled\n\n    return x\n</code></pre>"},{"location":"flow_matching/03_flow_matching_sampling/#comparison-with-diffusion-sampling","title":"Comparison with Diffusion Sampling","text":""},{"location":"flow_matching/03_flow_matching_sampling/#conceptual-differences","title":"Conceptual Differences","text":"Aspect Diffusion (DDPM) Diffusion (DDIM) Flow Matching Process Stochastic SDE Deterministic ODE Deterministic ODE Noise injection Yes (ancestral) No No Steps 1000 (original) 50-100 10-50 Solver Langevin dynamics Euler/RK Euler/RK/Adaptive Determinism No Yes Yes Speed Slowest Fast Fastest"},{"location":"flow_matching/03_flow_matching_sampling/#sampling-speed","title":"Sampling Speed","text":"<p>Typical performance (ImageNet 256\u00d7256):</p> <p>DDPM:</p> <ul> <li>1000 steps: ~10 seconds per image (GPU)</li> <li>High quality, stochastic</li> </ul> <p>DDIM:</p> <ul> <li>50 steps: ~0.5 seconds per image</li> <li>Good quality, deterministic</li> </ul> <p>Flow Matching:</p> <ul> <li>20 steps (RK4): ~0.2 seconds per image</li> <li>Good quality, deterministic</li> </ul> <p>Flow Matching (reflow):</p> <ul> <li>10 steps (RK4): ~0.1 seconds per image</li> <li>Good quality, deterministic</li> </ul> <p>Key advantage: Flow matching is 2-5\u00d7 faster than DDIM for similar quality.</p>"},{"location":"flow_matching/03_flow_matching_sampling/#evaluation-metrics","title":"Evaluation Metrics","text":""},{"location":"flow_matching/03_flow_matching_sampling/#sample-quality","title":"Sample Quality","text":"<p>FID (Fr\u00e9chet Inception Distance): <pre><code>from pytorch_fid import fid_score\n\n# Generate samples\nsamples = batch_sample(model, num_samples=50000)\n\n# Compute FID\nfid = fid_score.calculate_fid_given_paths(\n    [real_data_path, samples_path],\n    batch_size=50,\n    device='cuda',\n    dims=2048\n)\nprint(f'FID: {fid:.2f}')\n</code></pre></p> <p>Inception Score: <pre><code>from torchmetrics.image.inception import InceptionScore\n\ninception = InceptionScore(normalize=True)\ninception.update(samples)\nis_mean, is_std = inception.compute()\nprint(f'IS: {is_mean:.2f} \u00b1 {is_std:.2f}')\n</code></pre></p>"},{"location":"flow_matching/03_flow_matching_sampling/#sampling-efficiency","title":"Sampling Efficiency","text":"<p>Number of Function Evaluations (NFE):</p> <ul> <li>Euler: NFE = num_steps</li> <li>RK4: NFE = 4 \u00d7 num_steps</li> <li>Adaptive: NFE varies (typically 15-30)</li> </ul> <p>Wall-clock time: <pre><code>import time\n\nstart = time.time()\nsamples = batch_sample(model, num_samples=1000, num_steps=20)\nelapsed = time.time() - start\n\nprint(f'Time: {elapsed:.2f}s')\nprint(f'Samples/sec: {1000/elapsed:.2f}')\n</code></pre></p>"},{"location":"flow_matching/03_flow_matching_sampling/#troubleshooting","title":"Troubleshooting","text":""},{"location":"flow_matching/03_flow_matching_sampling/#common-issues","title":"Common Issues","text":"<p>1. Poor sample quality:</p> <ul> <li>Increase steps: Try 2\u00d7 more steps</li> <li>Use RK4: More accurate than Euler</li> <li>Check model: Ensure training converged</li> <li>Use EMA weights: Significant quality improvement</li> </ul> <p>2. Numerical instability:</p> <ul> <li>Reduce step size: More steps, smaller dt</li> <li>Clip values: Prevent overflow</li> <li>Use mixed precision carefully: Can cause instability</li> <li>Check velocity magnitudes: Should be reasonable</li> </ul> <p>3. Slow sampling:</p> <ul> <li>Use fewer steps: Start with 10-20 (RK4)</li> <li>Batch samples: Generate multiple at once</li> <li>Use reflow: Iteratively straighten paths</li> <li>Optimize model: TorchScript, ONNX, quantization</li> </ul> <p>4. Out of memory:</p> <ul> <li>Reduce batch size: Sample in smaller batches</li> <li>Use gradient checkpointing: During sampling (if needed)</li> <li>Clear cache: torch.cuda.empty_cache()</li> <li>Lower resolution: If applicable</li> </ul>"},{"location":"flow_matching/03_flow_matching_sampling/#debugging-tips","title":"Debugging Tips","text":"<p>Visualize trajectory: <pre><code># Sample with intermediate states\nfinal, trajectory = sample_with_trajectory(model, x_init, num_steps=20, save_every=5)\n\n# Plot\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(1, len(trajectory), figsize=(20, 4))\nfor i, x in enumerate(trajectory):\n    axes[i].imshow(x[0].permute(1, 2, 0).cpu())\n    axes[i].set_title(f'Step {i * 5}')\nplt.show()\n</code></pre></p> <p>Check velocity field: <pre><code># Visualize velocity magnitudes\nt = torch.tensor([0.5])\nv = model(x, t)\nv_norm = v.norm(dim=1).mean()\nprint(f'Average velocity magnitude at t=0.5: {v_norm:.4f}')\n</code></pre></p>"},{"location":"flow_matching/03_flow_matching_sampling/#best-practices","title":"Best Practices","text":""},{"location":"flow_matching/03_flow_matching_sampling/#dos","title":"Do's","text":"<p>\u2705 Use RK4 for better accuracy with fewer steps \u2705 Start with 20 steps and adjust based on quality \u2705 Use EMA weights for sampling (not training weights) \u2705 Batch samples for efficiency \u2705 Set seeds for reproducibility \u2705 Monitor NFE (number of function evaluations) \u2705 Use adaptive solvers when quality is critical</p>"},{"location":"flow_matching/03_flow_matching_sampling/#donts","title":"Don'ts","text":"<p>\u274c Don't use too few steps (&lt;5 for RK4, &lt;20 for Euler) \u274c Don't forget EMA (significant quality loss) \u274c Don't sample during training (use eval mode) \u274c Don't ignore numerical stability (clip if needed) \u274c Don't use training weights (use EMA for sampling)</p>"},{"location":"flow_matching/03_flow_matching_sampling/#summary","title":"Summary","text":""},{"location":"flow_matching/03_flow_matching_sampling/#key-sampling-steps","title":"Key Sampling Steps","text":"<ol> <li>Initialize: Sample \\(x(1) \\sim \\mathcal{N}(0, I)\\)</li> <li>Choose solver: RK4 recommended (10-20 steps)</li> <li>Integrate ODE: \\(\\frac{dx}{dt} = v_\\theta(x, t)\\) from \\(t=1\\) to \\(t=0\\)</li> <li>Output: \\(x(0) \\approx x_{\\text{data}}\\)</li> </ol>"},{"location":"flow_matching/03_flow_matching_sampling/#solver-recommendations","title":"Solver Recommendations","text":"<p>For quality:</p> <ul> <li>RK4 with 20-50 steps</li> <li>Adaptive solver with tight tolerances</li> </ul> <p>For speed:</p> <ul> <li>RK4 with 10-15 steps</li> <li>Reflow model with 5-10 steps</li> </ul> <p>For balance:</p> <ul> <li>RK4 with 15-20 steps (recommended default)</li> </ul>"},{"location":"flow_matching/03_flow_matching_sampling/#typical-performance","title":"Typical Performance","text":"<p>ImageNet 256\u00d7256:</p> <ul> <li>FID &lt; 5: 20-30 steps (RK4)</li> <li>FID &lt; 10: 10-20 steps (RK4)</li> <li>FID &lt; 20: 5-10 steps (RK4)</li> </ul> <p>Sampling speed:</p> <ul> <li>~0.2 seconds per image (20 steps, RK4, GPU)</li> <li>2-5\u00d7 faster than DDIM</li> <li>10-50\u00d7 faster than DDPM</li> </ul>"},{"location":"flow_matching/03_flow_matching_sampling/#related-documents","title":"Related Documents","text":"<ul> <li>Flow Matching Foundations \u2014 Theory and mathematics</li> <li>Flow Matching Training \u2014 Training strategies</li> <li>DDPM Sampling \u2014 Comparison with diffusion sampling</li> <li>Rectifying Flow Tutorial \u2014 Detailed walkthrough</li> </ul>"},{"location":"flow_matching/03_flow_matching_sampling/#references","title":"References","text":""},{"location":"flow_matching/03_flow_matching_sampling/#ode-solvers_1","title":"ODE Solvers","text":"<ol> <li> <p>Chen, R. T. Q., et al. (2018). Neural Ordinary Differential Equations. NeurIPS. arXiv:1806.07366</p> </li> <li> <p>Dormand, J. R., &amp; Prince, P. J. (1980). A family of embedded Runge-Kutta formulae. Journal of Computational and Applied Mathematics.</p> </li> </ol>"},{"location":"flow_matching/03_flow_matching_sampling/#flow-matching-sampling_1","title":"Flow Matching Sampling","text":"<ol> <li> <p>Liu, X., et al. (2023). Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow. ICLR.</p> </li> <li> <p>Lipman, Y., et al. (2023). Flow Matching for Generative Modeling. ICLR.</p> </li> </ol>"},{"location":"flow_matching/03_flow_matching_sampling/#comparison-with-diffusion","title":"Comparison with Diffusion","text":"<ol> <li> <p>Song, J., Meng, C., &amp; Ermon, S. (2021). Denoising Diffusion Implicit Models. ICLR. (DDIM)</p> </li> <li> <p>Karras, T., et al. (2022). Elucidating the Design Space of Diffusion-Based Generative Models. NeurIPS.</p> </li> </ol>"},{"location":"flow_matching/03_flow_matching_sampling/#guidance","title":"Guidance","text":"<ol> <li>Ho, J., &amp; Salimans, T. (2022). Classifier-Free Diffusion Guidance. NeurIPS Workshop.</li> </ol>"},{"location":"flow_matching/04_flow_matching_landscape/","title":"The Flow Matching Landscape: Methods, Comparisons, and History","text":"<p>This document provides context on the broader landscape of flow-based generative models, comparing normalizing flows, flow matching variants, and providing guidance on which methods to use for different applications.</p>"},{"location":"flow_matching/04_flow_matching_landscape/#overview","title":"Overview","text":"<p>The field of flow-based generative models has evolved significantly over the past decade. This document clarifies the relationships between different approaches and helps you choose the right method for your application.</p> <p>Key distinction:</p> <ul> <li>Normalizing flows (2015-2020): Invertible transformations with tractable Jacobians</li> <li>Flow matching (2022-present): Learned velocity fields via regression</li> </ul> <p>Flow matching has largely superseded normalizing flows for most applications due to simpler training and fewer architectural constraints.</p>"},{"location":"flow_matching/04_flow_matching_landscape/#normalizing-flows","title":"Normalizing Flows","text":""},{"location":"flow_matching/04_flow_matching_landscape/#core-concept","title":"Core Concept","text":"<p>Normalizing flows learn an invertible transformation \\(f_\\theta: \\mathbb{R}^d \\to \\mathbb{R}^d\\) that maps a simple distribution (noise) to a complex distribution (data):</p> \\[ x_{\\text{data}} = f_\\theta(z_{\\text{noise}}) \\] <p>Critical requirement: The transformation must be: 1. Invertible: \\(z = f_\\theta^{-1}(x)\\) must exist 2. Tractable Jacobian: \\(\\det \\frac{\\partial f_\\theta}{\\partial z}\\) must be computable</p>"},{"location":"flow_matching/04_flow_matching_landscape/#training-objective","title":"Training Objective","text":"<p>Normalizing flows are trained by maximizing likelihood using the change of variables formula:</p> \\[ \\log p_\\theta(x) = \\log p(z) + \\log \\left| \\det \\frac{\\partial f_\\theta}{\\partial z} \\right| \\] <p>where \\(z = f_\\theta^{-1}(x)\\).</p> <p>Training procedure: 1. Sample data \\(x \\sim p_{\\text{data}}\\) 2. Compute inverse: \\(z = f_\\theta^{-1}(x)\\) 3. Compute log-likelihood using change of variables 4. Maximize likelihood via gradient ascent</p>"},{"location":"flow_matching/04_flow_matching_landscape/#architectural-constraints","title":"Architectural Constraints","text":"<p>To ensure invertibility and tractable Jacobians, normalizing flows use specialized architectures:</p> <p>1. Coupling Layers (RealNVP, Glow): - Split input: \\(x = [x_a, x_b]\\) - Transform one part conditioned on the other:</p> <p>$$   \\begin{align}   y_a &amp;= x_a \\   y_b &amp;= x_b \\odot \\exp(s(x_a)) + t(x_a)   \\end{align}   $$ - Jacobian is triangular (easy to compute)</p> <p>2. Autoregressive Flows (MAF, IAF): - Transform each dimension conditioned on previous:</p> <p>$$</p> <p>x_i = z_i \\cdot \\exp(s_i(z_{&lt;i})) + t_i(z_{&lt;i})   $$ - Jacobian is triangular</p> <p>3. Continuous Normalizing Flows (CNFs):</p> <ul> <li>Define transformation via ODE:</li> </ul> <p>$$</p> <p>\\frac{dx}{dt} = f_\\theta(x, t)   $$ - Compute log-likelihood via instantaneous change of variables:</p> <p>$$</p> <p>\\frac{d \\log p(x)}{dt} = -\\text{Tr}\\left(\\frac{\\partial f_\\theta}{\\partial x}\\right)   $$</p>"},{"location":"flow_matching/04_flow_matching_landscape/#major-methods","title":"Major Methods","text":"<p>RealNVP (Dinh et al., 2017): - Coupling layers with affine transformations - Simple, stable training - Limited expressiveness</p> <p>Glow (Kingma &amp; Dhariwal, 2018): - Adds invertible 1\u00d71 convolutions - Activation normalization (ActNorm) - Better for high-resolution images</p> <p>Neural Spline Flows (Durkan et al., 2019): - Monotonic spline transformations - More expressive than affine - Still tractable Jacobian</p> <p>FFJORD (Grathwohl et al., 2019): - Continuous normalizing flow - Uses ODE solver + trace estimator - Expensive but flexible</p>"},{"location":"flow_matching/04_flow_matching_landscape/#advantages","title":"Advantages","text":"<p>\u2705 Single-step sampling: \\(x = f_\\theta(z)\\) (no iterative process) \u2705 Exact likelihood: Can compute \\(p(x)\\) exactly \u2705 Bidirectional: Can go noise \u2192 data and data \u2192 noise \u2705 Theoretical elegance: Clean probabilistic interpretation</p>"},{"location":"flow_matching/04_flow_matching_landscape/#limitations","title":"Limitations","text":"<p>\u274c Architectural constraints: Must design invertible networks \u274c Jacobian computation: Expensive for high dimensions \u274c Training complexity: Likelihood-based training can be unstable \u274c Limited expressiveness: Invertibility restricts model capacity \u274c Scaling issues: Difficult to scale to very high dimensions</p>"},{"location":"flow_matching/04_flow_matching_landscape/#flow-matching","title":"Flow Matching","text":""},{"location":"flow_matching/04_flow_matching_landscape/#core-concept_1","title":"Core Concept","text":"<p>Flow matching learns a velocity field \\(v_\\theta(x, t)\\) that defines how to transport samples from noise to data:</p> \\[ \\frac{dx}{dt} = v_\\theta(x, t), \\quad t \\in [0, 1] \\] <p>Key difference: No invertibility requirement, no Jacobian computation.</p>"},{"location":"flow_matching/04_flow_matching_landscape/#training-objective_1","title":"Training Objective","text":"<p>Flow matching uses simple regression on conditional velocities:</p> \\[ \\mathcal{L}_{\\text{FM}} = \\mathbb{E}_{t, x_0, x_1} \\left[ \\left\\| v_\\theta(x_t, t) - u_t(x_0, x_1) \\right\\|^2 \\right] \\] <p>where:</p> <ul> <li>\\(x_0 \\sim p_{\\text{data}}\\) (data)</li> <li>\\(x_1 \\sim p_{\\text{noise}}\\) (noise)</li> <li>\\(x_t = \\psi_t(x_0, x_1)\\) (interpolated point)</li> <li>\\(u_t(x_0, x_1) = \\frac{d}{dt}\\psi_t(x_0, x_1)\\) (target velocity)</li> </ul> <p>Training procedure: 1. Sample data \\(x_0\\) and noise \\(x_1\\) 2. Sample time \\(t \\sim U[0, 1]\\) 3. Compute interpolated point \\(x_t\\) 4. Compute target velocity \\(u_t\\) 5. Predict velocity and minimize MSE</p>"},{"location":"flow_matching/04_flow_matching_landscape/#no-architectural-constraints","title":"No Architectural Constraints","text":"<p>Any neural network can be used for \\(v_\\theta(x, t)\\): - U-Net for images - Transformer for sequences - GNN for graphs - MLP for low-dimensional data</p> <p>No need for invertibility or special Jacobian structures.</p>"},{"location":"flow_matching/04_flow_matching_landscape/#advantages-over-normalizing-flows","title":"Advantages over Normalizing Flows","text":"<p>\u2705 Simpler training: Direct regression (no likelihood computation) \u2705 No constraints: Any architecture works \u2705 Faster training: No Jacobian determinant computation \u2705 More flexible: Not restricted to invertible transformations \u2705 Better scaling: Easier to scale to high dimensions \u2705 Comparable quality: Matches or exceeds normalizing flows</p>"},{"location":"flow_matching/04_flow_matching_landscape/#trade-off","title":"Trade-off","text":"<p>\u274c Multi-step sampling: Requires ODE solver (10-50 steps) \u274c No exact likelihood: Cannot compute \\(p(x)\\) exactly (but rarely needed)</p>"},{"location":"flow_matching/04_flow_matching_landscape/#flow-matching-variants","title":"Flow Matching Variants","text":"<p>The flow matching framework has spawned several variants, each with different trade-offs.</p>"},{"location":"flow_matching/04_flow_matching_landscape/#1-rectified-flow","title":"1. Rectified Flow","text":"<p>Paper: Liu et al. (2023) - \"Flow Straight and Fast\"</p> <p>Key idea: Use the simplest possible path\u2014linear interpolation.</p> <p>Path:</p> \\[ x_t = (1-t) x_0 + t x_1 \\] <p>Velocity:</p> <p>$$</p> <p>u_t = x_1 - x_0 \\quad \\text{(constant)} $$</p> <p>Loss:</p> <p>$$</p> <p>\\mathcal{L}{\\text{RF}} = \\mathbb{E} \\left[ \\left| v_\\theta(x_t, t) - (x_1 - x_0) \\right|^2 \\right] $$</p> <p>Reflow: Iteratively straighten paths by training on synthetic data: - Iteration 1: Train on real data - Iteration 2: Generate synthetic data, train new model - Iteration 3+: Repeat</p> <p>Effect: Each reflow iteration reduces required sampling steps: - Base model: 50-100 steps - After 1 reflow: 20-30 steps - After 2 reflows: 10-15 steps - After 3 reflows: 5-10 steps</p> <p>Advantages:</p> <ul> <li>\u2b50\u2b50\u2b50\u2b50\u2b50 Simplest to implement</li> <li>\u2b50\u2b50\u2b50\u2b50\u2b50 Fastest training</li> <li>\u2b50\u2b50\u2b50\u2b50 Good sample quality</li> <li>\u2b50\u2b50\u2b50\u2b50 Fast sampling (after reflow)</li> </ul> <p>When to use: Default choice for most applications.</p>"},{"location":"flow_matching/04_flow_matching_landscape/#2-flow-matching-general","title":"2. Flow Matching (General)","text":"<p>Paper: Lipman et al. (2023) - \"Flow Matching for Generative Modeling\"</p> <p>Key idea: General framework allowing flexible path choices.</p> <p>Path: Any differentiable \\(x_t = \\psi_t(x_0, x_1)\\)</p> <p>Velocity: \\(u_t = \\frac{d}{dt}\\psi_t(x_0, x_1)\\)</p> <p>Examples:</p> <ul> <li>Linear: \\(x_t = (1-t)x_0 + tx_1\\) (rectified flow)</li> <li>Variance-preserving: \\(x_t = \\sqrt{1-\\sigma_t^2} x_0 + \\sigma_t x_1\\)</li> <li>Geodesic: \\(x_t = \\exp_{x_0}(t \\log_{x_0}(x_1))\\) (for manifolds)</li> </ul> <p>Advantages:</p> <ul> <li>\u2b50\u2b50\u2b50\u2b50\u2b50 Theoretical foundation</li> <li>\u2b50\u2b50\u2b50\u2b50\u2b50 Flexibility</li> <li>\u2b50\u2b50\u2b50\u2b50 Sample quality</li> </ul> <p>When to use: When you need custom path designs (e.g., manifold-valued data).</p>"},{"location":"flow_matching/04_flow_matching_landscape/#3-stochastic-interpolants","title":"3. Stochastic Interpolants","text":"<p>Paper: Albergo &amp; Vanden-Eijnden (2023) - \"Building Normalizing Flows with Stochastic Interpolants\"</p> <p>Key idea: Generalize to include stochasticity in the path.</p> <p>Path: Can include Brownian motion:</p> \\[ x_t = \\alpha_t x_0 + \\beta_t x_1 + \\gamma_t \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I) \\] <p>Advantage: Bridges flow matching and diffusion models.</p> <p>When to use: When you want to interpolate between deterministic and stochastic generation.</p>"},{"location":"flow_matching/04_flow_matching_landscape/#4-optimal-transport-flow-matching","title":"4. Optimal Transport Flow Matching","text":"<p>Paper: Tong et al. (2024) - \"Improving and Generalizing Flow-Based Generative Models with Minibatch Optimal Transport\"</p> <p>Key idea: Use optimal transport to find better data-noise couplings.</p> <p>Approach: 1. Solve minibatch OT problem:</p> <p>$$</p> <p>\\min_{\\pi} \\sum_{i,j} \\pi_{ij} |x_0^{(i)} - x_1<sup>{(j)}|</sup>2    $$</p> <ol> <li>Use OT couplings for training instead of independent pairing</li> </ol> <p>Effect: Straighter paths, fewer sampling steps needed.</p> <p>Advantages:</p> <ul> <li>\u2b50\u2b50\u2b50\u2b50\u2b50 Better sample quality</li> <li>\u2b50\u2b50\u2b50\u2b50\u2b50 Straighter paths</li> <li>\u2b50\u2b50\u2b50\u2b50\u2b50 Fewer sampling steps</li> </ul> <p>Trade-off:</p> <ul> <li>\u2b50\u2b50\u2b50 Slower training (OT computation)</li> <li>\u2b50\u2b50\u2b50 More complex implementation</li> </ul> <p>When to use: When sample quality is critical and you have computational budget.</p>"},{"location":"flow_matching/04_flow_matching_landscape/#5-multisample-flow-matching","title":"5. Multisample Flow Matching","text":"<p>Paper: Pooladian et al. (2023) - \"Multisample Flow Matching: Straightening Flows with Minibatch Couplings\"</p> <p>Key idea: Use multiple samples to learn better couplings without explicit OT.</p> <p>Approach: Instead of pairing \\(x_0^{(i)}\\) with \\(x_1^{(i)}\\), use minibatch to find better pairings.</p> <p>Advantages:</p> <ul> <li>\u2b50\u2b50\u2b50\u2b50\u2b50 Better quality than rectified flow</li> <li>\u2b50\u2b50\u2b50\u2b50 Faster than OT flow matching</li> <li>\u2b50\u2b50\u2b50\u2b50 Straighter paths without reflow</li> </ul> <p>When to use: When you want better quality without reflow iterations.</p>"},{"location":"flow_matching/04_flow_matching_landscape/#detailed-comparison","title":"Detailed Comparison","text":""},{"location":"flow_matching/04_flow_matching_landscape/#normalizing-flows-vs-flow-matching","title":"Normalizing Flows vs Flow Matching","text":"Aspect Normalizing Flows Flow Matching What's learned Invertible transformation \\(f_\\theta\\) Velocity field \\(v_\\theta(x, t)\\) Training objective Maximize likelihood Minimize MSE Architectural constraints Must be invertible None Jacobian computation Required Not required Training complexity High (likelihood) Low (regression) Sampling 1 step 10-50 steps (ODE) Exact likelihood Yes No Expressiveness Limited by invertibility High Scaling Difficult Easy Sample quality \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50"},{"location":"flow_matching/04_flow_matching_landscape/#flow-matching-variants-comparison","title":"Flow Matching Variants Comparison","text":"Method Training Speed Sample Quality Sampling Steps Implementation Rectified Flow \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 20-50 (10-15 after reflow) Easy OT Flow Matching \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 10-20 Moderate Multisample FM \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 10-20 Moderate Stochastic Interpolants \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 20-50 Moderate"},{"location":"flow_matching/04_flow_matching_landscape/#flow-matching-vs-diffusion-models","title":"Flow Matching vs Diffusion Models","text":"Aspect Diffusion (DDPM) Diffusion (DDIM) Flow Matching Training Score matching Score matching Simple regression Forward process Stochastic Stochastic Deterministic Reverse process Stochastic SDE Deterministic ODE Deterministic ODE Sampling steps 1000 50-100 10-50 Speed Slowest Fast Fastest Determinism No Yes Yes Maturity High High Growing"},{"location":"flow_matching/04_flow_matching_landscape/#historical-evolution","title":"Historical Evolution","text":""},{"location":"flow_matching/04_flow_matching_landscape/#timeline","title":"Timeline","text":"<p>2015-2016: Early Normalizing Flows</p> <ul> <li>NICE (Dinh et al., 2015)</li> <li>RealNVP (Dinh et al., 2017)</li> <li>Focus: Invertible transformations</li> </ul> <p>2017-2019: Refinements</p> <ul> <li>Glow (Kingma &amp; Dhariwal, 2018)</li> <li>Neural Spline Flows (Durkan et al., 2019)</li> <li>FFJORD (Grathwohl et al., 2019) - Continuous normalizing flows</li> <li>Focus: Better architectures, continuous time</li> </ul> <p>2020-2021: Diffusion Dominance</p> <ul> <li>DDPM (Ho et al., 2020)</li> <li>Score-based models (Song et al., 2021)</li> <li>Normalizing flows fade in popularity</li> <li>Focus: Sample quality over speed</li> </ul> <p>2022: Flow Matching Emerges</p> <ul> <li>Flow Matching (Lipman et al., 2022)</li> <li>Stochastic Interpolants (Albergo &amp; Vanden-Eijnden, 2022)</li> <li>Focus: Simpler training than diffusion</li> </ul> <p>2023: Rectified Flow &amp; Variants</p> <ul> <li>Rectified Flow (Liu et al., 2023)</li> <li>Multisample Flow Matching (Pooladian et al., 2023)</li> <li>OT Flow Matching (Tong et al., 2023)</li> <li>Focus: Faster sampling, straighter paths</li> </ul> <p>2024-Present: Maturation</p> <ul> <li>Integration with Transformers (DiT)</li> <li>Applications to video, 3D, biology</li> <li>Focus: Scaling and applications</li> </ul>"},{"location":"flow_matching/04_flow_matching_landscape/#key-insights-from-history","title":"Key Insights from History","text":"<p>Why normalizing flows declined: 1. Architectural constraints limited expressiveness 2. Diffusion models achieved better quality 3. Training was more complex than alternatives</p> <p>Why flow matching succeeded: 1. Learned from normalizing flows (ODE-based) 2. Removed constraints (no invertibility) 3. Simpler than diffusion (regression vs score matching) 4. Faster than diffusion (fewer sampling steps)</p> <p>Current state: Flow matching is the modern successor to normalizing flows, combining their speed advantages with diffusion-level quality.</p>"},{"location":"flow_matching/04_flow_matching_landscape/#choosing-the-right-method","title":"Choosing the Right Method","text":""},{"location":"flow_matching/04_flow_matching_landscape/#decision-tree","title":"Decision Tree","text":"<pre><code>Do you need exact likelihood computation?\n\u251c\u2500 Yes \u2192 Normalizing Flows (RealNVP, Glow)\n\u2514\u2500 No \u2192 Continue\n\nDo you need single-step sampling?\n\u251c\u2500 Yes \u2192 Normalizing Flows\n\u2514\u2500 No \u2192 Continue\n\nAre you exploring a new domain?\n\u251c\u2500 Yes \u2192 Start with Rectified Flow (simplest)\n\u2514\u2500 No \u2192 Continue\n\nIs sample quality critical?\n\u251c\u2500 Yes \u2192 OT Flow Matching or Multisample FM\n\u2514\u2500 No \u2192 Rectified Flow\n\nDo you have computational budget for reflow?\n\u251c\u2500 Yes \u2192 Rectified Flow + Reflow (fastest sampling)\n\u2514\u2500 No \u2192 Multisample FM (good without reflow)\n</code></pre>"},{"location":"flow_matching/04_flow_matching_landscape/#recommendations-by-application","title":"Recommendations by Application","text":"<p>For computational biology (gene expression, molecules):</p> <ul> <li>Start: Rectified Flow</li> <li>Upgrade: Multisample FM if quality needs improvement</li> <li>Avoid: Normalizing flows (too constrained)</li> </ul> <p>For images (high resolution):</p> <ul> <li>Start: Rectified Flow + DiT architecture</li> <li>Upgrade: OT Flow Matching for best quality</li> <li>Consider: Reflow for production (5-10 steps)</li> </ul> <p>For sequences (text, DNA, proteins):</p> <ul> <li>Start: Rectified Flow + Transformer</li> <li>Upgrade: Multisample FM</li> <li>Consider: Stochastic Interpolants for flexibility</li> </ul> <p>For low-dimensional data (&lt;100D):</p> <ul> <li>Consider: Normalizing Flows (single-step sampling valuable)</li> <li>Alternative: Rectified Flow (if multi-step OK)</li> </ul> <p>For research/exploration:</p> <ul> <li>Start: Rectified Flow (fastest iteration)</li> <li>Experiment: Try variants once baseline established</li> </ul>"},{"location":"flow_matching/04_flow_matching_landscape/#practical-guidelines","title":"Practical Guidelines","text":""},{"location":"flow_matching/04_flow_matching_landscape/#for-your-genai-lab-project","title":"For Your GenAI Lab Project","text":"<p>Phase 1: Establish Baselines 1. Implement Rectified Flow (simplest) 2. Implement DDPM (already documented) 3. Compare on gene expression data</p> <p>Phase 2: Optimize 1. Apply Reflow to rectified flow (1-2 iterations) 2. Compare sampling steps: DDPM (100) vs RF (20) vs Reflow (10) 3. Evaluate quality with your metrics (FID, prediction consistency, epiplexity)</p> <p>Phase 3: Advanced (if needed) 1. Try Multisample Flow Matching if quality insufficient 2. Experiment with OT Flow Matching if computational budget allows 3. Compare with VAE, JEPA, other methods</p> <p>Skip:</p> <ul> <li>Normalizing flows (superseded by flow matching)</li> <li>Stochastic interpolants (unless you need stochastic generation)</li> </ul>"},{"location":"flow_matching/04_flow_matching_landscape/#implementation-priority","title":"Implementation Priority","text":"<p>High priority: 1. \u2705 Rectified Flow (already documented) 2. \u23f3 DDPM experiments (next step) 3. \u23f3 Reflow (after baseline established)</p> <p>Medium priority: 4. Multisample Flow Matching (if quality needs improvement) 5. DiT architecture (for scaling) 6. Latent diffusion (for high-dimensional data)</p> <p>Low priority: 7. OT Flow Matching (expensive, marginal improvement) 8. Normalizing flows (outdated) 9. Stochastic interpolants (niche use case)</p>"},{"location":"flow_matching/04_flow_matching_landscape/#summary","title":"Summary","text":""},{"location":"flow_matching/04_flow_matching_landscape/#key-takeaways","title":"Key Takeaways","text":"<p>Normalizing Flows:</p> <ul> <li>Older approach (2015-2020)</li> <li>Invertible transformations with tractable Jacobians</li> <li>Single-step sampling but limited expressiveness</li> <li>Largely superseded by flow matching</li> </ul> <p>Flow Matching:</p> <ul> <li>Modern approach (2022-present)</li> <li>Learns velocity fields via simple regression</li> <li>Multi-step sampling but high quality</li> <li>Current state-of-the-art for flow-based models</li> </ul> <p>Rectified Flow:</p> <ul> <li>Simplest flow matching variant</li> <li>Linear interpolation paths</li> <li>Best starting point for most applications</li> <li>Can be improved via reflow</li> </ul> <p>Advanced Variants:</p> <ul> <li>OT Flow Matching: Best quality, expensive</li> <li>Multisample FM: Good quality, moderate cost</li> <li>Stochastic Interpolants: Flexible, niche</li> </ul>"},{"location":"flow_matching/04_flow_matching_landscape/#recommendation","title":"Recommendation","text":"<p>For your combio project: Start with Rectified Flow. It's the sweet spot of simplicity, performance, and flexibility. You can always upgrade to advanced variants if needed, but rectified flow will give you a strong baseline quickly.</p>"},{"location":"flow_matching/04_flow_matching_landscape/#related-documents","title":"Related Documents","text":"<ul> <li>Flow Matching Foundations \u2014 Mathematical theory</li> <li>Flow Matching Training \u2014 Implementation guide</li> <li>Flow Matching Sampling \u2014 ODE solvers and sampling</li> <li>Rectifying Flow Tutorial \u2014 Intuitive introduction</li> <li>DDPM Documentation \u2014 Comparison with diffusion</li> </ul>"},{"location":"flow_matching/04_flow_matching_landscape/#references","title":"References","text":""},{"location":"flow_matching/04_flow_matching_landscape/#normalizing-flows_1","title":"Normalizing Flows","text":"<ol> <li> <p>Dinh, L., et al. (2017). Density estimation using Real NVP. ICLR. arXiv:1605.08803</p> </li> <li> <p>Kingma, D. P., &amp; Dhariwal, P. (2018). Glow: Generative Flow with Invertible 1\u00d71 Convolutions. NeurIPS. arXiv:1807.03039</p> </li> <li> <p>Durkan, C., et al. (2019). Neural Spline Flows. NeurIPS. arXiv:1906.04032</p> </li> <li> <p>Grathwohl, W., et al. (2019). FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models. ICLR. arXiv:1810.01367</p> </li> </ol>"},{"location":"flow_matching/04_flow_matching_landscape/#flow-matching_1","title":"Flow Matching","text":"<ol> <li> <p>Lipman, Y., et al. (2023). Flow Matching for Generative Modeling. ICLR. arXiv:2210.02747</p> </li> <li> <p>Liu, X., et al. (2023). Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow. ICLR. arXiv:2209.03003</p> </li> <li> <p>Albergo, M. S., &amp; Vanden-Eijnden, E. (2023). Building Normalizing Flows with Stochastic Interpolants. ICLR. arXiv:2209.15571</p> </li> </ol>"},{"location":"flow_matching/04_flow_matching_landscape/#advanced-variants","title":"Advanced Variants","text":"<ol> <li> <p>Pooladian, A., et al. (2023). Multisample Flow Matching: Straightening Flows with Minibatch Couplings. ICML. arXiv:2304.14772</p> </li> <li> <p>Tong, A., et al. (2024). Improving and Generalizing Flow-Based Generative Models with Minibatch Optimal Transport. TMLR. arXiv:2302.00482</p> </li> </ol>"},{"location":"flow_matching/04_flow_matching_landscape/#reviews","title":"Reviews","text":"<ol> <li>Papamakarios, G., et al. (2021). Normalizing Flows for Probabilistic Modeling and Inference. JMLR. arXiv:1912.02762</li> </ol>"},{"location":"flow_matching/rectifying_flow/","title":"Rectified Flow: A Flow Matching Tutorial","text":"<p>This tutorial introduces rectified flow, one of the simplest and most elegant approaches to generative modeling. We build from first principles with precise notation.</p>"},{"location":"flow_matching/rectifying_flow/#1-the-problem-transforming-noise-into-data","title":"1. The Problem: Transforming Noise into Data","text":"<p>Generative models solve a fundamental problem: transform samples from a simple distribution (noise) into samples from a complex distribution (data).</p> <p>Setup:</p> <ul> <li>Data distribution: \\(p_{\\text{data}}(x)\\) \u2014 the distribution we want to sample from</li> <li>Noise distribution: \\(p_{\\text{noise}}(x)\\) \u2014 a simple distribution we can easily sample (often Gaussian)</li> <li>Goal: Learn a continuous transformation that maps noise \u2192 data</li> </ul> <p>Rectified flow achieves this by learning a velocity field that transports points along straight paths.</p>"},{"location":"flow_matching/rectifying_flow/#2-notation-and-objects","title":"2. Notation and Objects","text":"<p>We work in continuous space \\(x \\in \\mathbb{R}^d\\) with the following definitions:</p> Symbol Meaning \\(x_0 \\sim p_{\\text{data}}\\) Sample from data distribution \\(x_1 \\sim p_{\\text{noise}}\\) Sample from noise distribution \\(t \\in [0, 1]\\) Continuous time parameter \\(x_t\\) Interpolated point at time \\(t\\) <p>The key insight: we construct a path \\(x_t\\) connecting data (\\(x_0\\)) to noise (\\(x_1\\)).</p>"},{"location":"flow_matching/rectifying_flow/#3-the-linear-interpolation-path","title":"3. The Linear Interpolation Path","text":"<p>Rectified flow makes a deliberate, simple choice for the path:</p> \\[ \\boxed{x_t = (1 - t) \\cdot x_0 + t \\cdot x_1} \\] <p>This is linear interpolation between a data point and a noise point.</p> <p>Interpretation:</p> <ul> <li>At \\(t = 0\\): \\(x_t = x_0\\) (pure data)</li> <li>At \\(t = 1\\): \\(x_t = x_1\\) (pure noise)</li> <li>For \\(t \\in (0, 1)\\): a point along the straight line between them</li> </ul> <p>This is purely geometric \u2014 no stochasticity in the path definition.</p>"},{"location":"flow_matching/rectifying_flow/#4-velocity-the-central-object","title":"4. Velocity: The Central Object","text":"<p>Differentiating the path with respect to time:</p> \\[ \\frac{dx_t}{dt} = x_1 - x_0 \\] <p>This derivative is the velocity \u2014 the direction and speed of movement along the path.</p> <p>Key observations:</p> <ul> <li>Constant: The velocity doesn't change along the path</li> <li>Direction: Points from data toward noise</li> <li>Deterministic: Depends only on the pair \\((x_0, x_1)\\)</li> </ul> <p>Rectified flow trains a neural network to predict this velocity given only the current position and time.</p>"},{"location":"flow_matching/rectifying_flow/#5-the-neural-network","title":"5. The Neural Network","text":"<p>We introduce a neural network \\(v_\\theta(x, t)\\) that learns the velocity field:</p> <ul> <li>Input: Position \\(x \\in \\mathbb{R}^d\\) and time \\(t \\in [0, 1]\\)</li> <li>Output: Velocity vector in \\(\\mathbb{R}^d\\)</li> <li>Meaning: \"How should a point at position \\(x\\) at time \\(t\\) be moving?\"</li> </ul> <p>Training target:</p> \\[ v_\\theta(x_t, t) \\approx x_1 - x_0 \\] <p>The model learns how points should move, not probabilities or likelihoods.</p>"},{"location":"flow_matching/rectifying_flow/#6-the-rectified-flow-loss","title":"6. The Rectified Flow Loss","text":"<p>Training uses simple mean-squared-error regression:</p> \\[ \\mathcal{L}_{\\text{RF}} = \\mathbb{E}_{x_0, x_1, t} \\left[ \\left\\| v_\\theta(x_t, t) - (x_1 - x_0) \\right\\|^2 \\right] \\] <p>Sampling procedure during training:</p> <ol> <li>Sample \\(x_0 \\sim p_{\\text{data}}\\) (data point)</li> <li>Sample \\(x_1 \\sim p_{\\text{noise}}\\) (noise point)</li> <li>Sample \\(t \\sim \\text{Uniform}[0, 1]\\) (random time)</li> <li>Compute \\(x_t = (1-t) x_0 + t x_1\\) (interpolated point)</li> <li>Predict velocity and compute loss</li> </ol> <p>This is flow matching in its simplest form.</p>"},{"location":"flow_matching/rectifying_flow/#7-why-rectified","title":"7. Why \"Rectified\"?","text":"<p>The term rectified comes from geometry, meaning \"straightened.\"</p> <p>If we had access to the true optimal transport between \\(p_{\\text{data}}\\) and \\(p_{\\text{noise}}\\), the trajectories would generally be curved (following geodesics in probability space).</p> <p>Rectified flow instead:</p> <p>Straightens (rectifies) the paths into lines, letting the neural network implicitly compensate for any distortion.</p> <p>The straight-line paths are not the true optimal transport \u2014 but they're simple to define and work remarkably well in practice.</p>"},{"location":"flow_matching/rectifying_flow/#8-sampling-generating-new-data","title":"8. Sampling: Generating New Data","text":"<p>After training, we generate samples by solving an ODE backward in time.</p> <p>Procedure:</p> <ol> <li>Start from noise: \\(x(1) \\sim p_{\\text{noise}}\\)</li> <li>Integrate backward: \\(\\frac{dx}{dt} = -v_\\theta(x(t), t)\\) from \\(t=1\\) to \\(t=0\\)</li> <li>Result: \\(x(0) \\approx x_{\\text{data}}\\)</li> </ol> <p>Properties:</p> <ul> <li>Deterministic: No noise injected during sampling</li> <li>Standard solvers: Can use Euler, RK4, or adaptive ODE solvers</li> <li>Efficient: Often needs far fewer steps than diffusion (10-50 vs 100-1000)</li> </ul>"},{"location":"flow_matching/rectifying_flow/#9-flow-matching-the-general-framework","title":"9. Flow Matching: The General Framework","text":"<p>Rectified flow is a specific instance of the broader flow matching framework:</p> Component General Flow Matching Rectified Flow Path \\(x_t\\) Any differentiable path Linear interpolation Target velocity \\(\\dot{x}_t\\) Constant: \\(x_1 - x_0\\) Stochasticity Optional None <p>Hierarchy:</p> \\[ \\text{Rectified Flow} \\subset \\text{Flow Matching} \\subset \\text{Continuous Generative Models} \\]"},{"location":"flow_matching/rectifying_flow/#10-comparison-rectified-flow-vs-score-matching","title":"10. Comparison: Rectified Flow vs Score Matching","text":"<p>These two approaches answer fundamentally different questions:</p> Aspect Score Matching (Diffusion) Rectified Flow Forward process Stochastic (add noise) Deterministic (interpolate) What's learned Score: \\(\\nabla_x \\log p_t(x)\\) Velocity: \\(v_\\theta(x, t)\\) Reverse process Stochastic SDE Deterministic ODE Assumptions Gaussian noise schedules Minimal Question answered \"Where is probability increasing?\" \"How should this point move?\" <p>Mental model:</p> <ul> <li>Score matching: Learn forces on a probability landscape</li> <li>Rectified flow: Learn motion through state space</li> </ul>"},{"location":"flow_matching/rectifying_flow/#11-why-gaussian-noise-and-when-its-not-required","title":"11. Why Gaussian Noise (and When It's Not Required)","text":"<p>Rectified flow does not require Gaussian noise for \\(x_1\\).</p> <p>Why Gaussian is common:</p> <ul> <li>Easy to sample</li> <li>Isotropic (no preferred direction)</li> <li>Numerically stable</li> <li>Enables comparison with diffusion baselines</li> </ul> <p>Alternatives (important for biology):</p> <ul> <li>Domain-specific priors</li> <li>Learned noise distributions</li> <li>Structured noise matching data characteristics</li> </ul> <p>This flexibility is one reason rectified flow generalizes well beyond images.</p>"},{"location":"flow_matching/rectifying_flow/#12-connection-to-transformers","title":"12. Connection to Transformers","text":"<p>Rectified flow pairs naturally with Transformer architectures because it requires networks that can:</p> <ul> <li>Model global dependencies (attention excels here)</li> <li>Condition on time (via embeddings and modulation)</li> <li>Handle flexible input structures (tokens, not grids)</li> </ul> <p>This combination \u2014 rectified flow + Transformer \u2014 forms the backbone of modern generative models like those used in Stable Diffusion 3 and video generation.</p> <p>See the companion tutorial on Diffusion Transformers (DiT) for architectural details.</p>"},{"location":"flow_matching/rectifying_flow/#summary","title":"Summary","text":"<p>Rectified flow learns a deterministic velocity field that transports noise to data along straightened paths, using simple regression instead of probability gradients.</p> <p>Key equations:</p> <ul> <li>Path: \\(x_t = (1-t) x_0 + t x_1\\)</li> <li>Velocity: \\(\\frac{dx_t}{dt} = x_1 - x_0\\)</li> <li>Loss: \\(\\mathcal{L} = \\mathbb{E}\\left[\\|v_\\theta(x_t, t) - (x_1 - x_0)\\|^2\\right]\\)</li> <li>Sampling: Solve \\(\\frac{dx}{dt} = -v_\\theta(x, t)\\) from \\(t=1\\) to \\(t=0\\)</li> </ul>"},{"location":"flow_matching/rectifying_flow/#references","title":"References","text":"<ul> <li>Liu et al. (2022) - \"Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow\"</li> <li>Lipman et al. (2023) - \"Flow Matching for Generative Modeling\"</li> <li>Albergo &amp; Vanden-Eijnden (2023) - \"Building Normalizing Flows with Stochastic Interpolants\"</li> </ul>"},{"location":"foundation_models/","title":"Foundation Models for Computational Biology","text":"<p>Adapting large-scale foundation models for gene expression and multi-omics tasks.</p>"},{"location":"foundation_models/#overview","title":"Overview","text":"<p>Foundation models trained on massive biological datasets (DNA, RNA, protein) are emerging as powerful tools for computational biology. This section covers practical strategies for adapting these models to specific tasks without training from scratch.</p> <p>Key Topics:</p> <ul> <li>\ud83c\udfaf Model Selection: Choosing the right foundation model for your task</li> <li>\ud83d\udd27 Adaptation Strategies: LoRA, adapters, fine-tuning, freezing</li> <li>\ud83d\udcca Data Preparation: Handling gene expression, sequences, and multi-omics</li> <li>\ud83d\udcbb Implementation: Resource-aware configs, hardware optimization</li> <li>\ud83d\ude80 Deployment: Inference, serving, and production pipelines</li> </ul>"},{"location":"foundation_models/#key-documents","title":"Key Documents","text":""},{"location":"foundation_models/#leveraging-foundation-models","title":"Leveraging Foundation Models","text":"<p>Comprehensive guide to foundation model adaptation:</p> <ul> <li>Overview of available models (Geneformer, scGPT, BigRNA, ESM, etc.)</li> <li>When to use foundation models vs. train from scratch</li> <li>Adaptation strategies (LoRA, adapters, full fine-tuning)</li> <li>Conditioning and control (FiLM, cross-attention, CFG)</li> <li>Resource management (small/medium/large configs)</li> </ul> <p>Best for: Understanding the landscape and choosing an adaptation strategy</p>"},{"location":"foundation_models/#data-shape-tensors","title":"Data Shape &amp; Tensors","text":"<p>How to prepare your data for foundation models:</p> <ul> <li>Input representations (tokens, embeddings, sequences)</li> <li>Batch shapes and padding strategies</li> <li>Attention masks and position encodings</li> <li>Cell type, drug, and perturbation conditioning</li> <li>Multi-omics integration</li> </ul> <p>Best for: Implementing data loaders and preprocessing pipelines</p>"},{"location":"foundation_models/#implementation-guide","title":"Implementation Guide","text":"<p>Step-by-step implementation for common tasks:</p> <ul> <li>Setting up environments and dependencies</li> <li>Loading pre-trained models</li> <li>Implementing LoRA and adapters</li> <li>Training loops with mixed precision</li> <li>Evaluation and benchmarking</li> </ul> <p>Best for: Hands-on implementation and code examples</p>"},{"location":"foundation_models/#why-foundation-models-for-biology","title":"Why Foundation Models for Biology?","text":""},{"location":"foundation_models/#traditional-ml-approach","title":"Traditional ML Approach","text":"<pre><code>Custom model \u2192 Train from scratch \u2192 High data requirements \u2192 Task-specific\n</code></pre>"},{"location":"foundation_models/#foundation-model-approach","title":"Foundation Model Approach","text":"<pre><code>Pre-trained model \u2192 Adapt (LoRA/fine-tune) \u2192 Low data requirements \u2192 Transferable\n</code></pre> <p>Advantages:</p> <p>\u2705 Sample efficiency: Learn from 100s-1000s of examples vs. millions \u2705 Transfer learning: Leverage knowledge from massive pre-training datasets \u2705 Generalization: Better performance on out-of-distribution data \u2705 Multi-task: Single model for multiple downstream tasks \u2705 Interpretability: Pre-learned biological representations</p>"},{"location":"foundation_models/#available-foundation-models-2026","title":"Available Foundation Models (2026)","text":""},{"location":"foundation_models/#gene-expression-multi-omics","title":"Gene Expression &amp; Multi-Omics","text":"Model Organization Focus Size Open Source GEM-1 Synthesize Bio Gene expression generation Unknown \u274c BigRNA Deep Genomics RNA biology ~2B params \u274c Geneformer Theodoris et al. Single-cell transfer learning 10M-100M \u2705 scGPT Cui et al. Single-cell foundation 10M-100M \u2705"},{"location":"foundation_models/#dna-rna-sequences","title":"DNA &amp; RNA Sequences","text":"Model Organization Focus Size Open Source Evo 2 Arc Institute DNA sequence (8kb context) 7B params \u2705 Nucleotide Transformer InstaDeep Multi-species DNA 500M-2.5B \u2705 Helix-mRNA Helical mRNA sequences Unknown \u2705"},{"location":"foundation_models/#protein-structure","title":"Protein &amp; Structure","text":"Model Organization Focus Size Open Source ESM3 EvolutionaryScale Protein design 1.4B-98B \u2705 (7B, 98B) AlphaFold 3 Isomorphic Labs Protein structure Unknown \u274c Chai-1 Chai Discovery Antibody design Unknown \u2705"},{"location":"foundation_models/#typical-workflow","title":"Typical Workflow","text":""},{"location":"foundation_models/#1-choose-your-model","title":"1. Choose Your Model","text":"<p>Select based on: - Input type (expression, sequence, structure) - Task (generation, prediction, classification) - Available compute (model size) - Open source vs. proprietary</p>"},{"location":"foundation_models/#2-prepare-your-data","title":"2. Prepare Your Data","text":"<ul> <li>Tokenize or embed inputs</li> <li>Create attention masks</li> <li>Add condition labels (cell type, drug, etc.)</li> <li>Split train/val/test</li> </ul>"},{"location":"foundation_models/#3-select-adaptation-strategy","title":"3. Select Adaptation Strategy","text":"Strategy Data Needed Compute Best For Frozen + Linear Probe 100s Low Quick prototyping LoRA 1000s Medium Most tasks (recommended) Adapter Layers 1000s Medium Multi-task learning Full Fine-Tuning 10,000s+ High Maximum performance"},{"location":"foundation_models/#4-train-evaluate","title":"4. Train &amp; Evaluate","text":"<ul> <li>Use mixed precision (fp16/bfloat16)</li> <li>Monitor overfitting (small datasets)</li> <li>Validate on held-out cell types/drugs</li> <li>Compare to baselines</li> </ul>"},{"location":"foundation_models/#5-deploy","title":"5. Deploy","text":"<ul> <li>Quantize for inference (int8, int4)</li> <li>Batch predictions for efficiency</li> <li>Monitor uncertainty estimates</li> </ul>"},{"location":"foundation_models/#example-use-cases","title":"Example Use Cases","text":""},{"location":"foundation_models/#drug-response-prediction","title":"\ud83e\uddec Drug Response Prediction","text":"<p>Input: Baseline gene expression + drug ID Output: Perturbed gene expression Model: Fine-tuned Geneformer with LoRA Data: Perturb-seq, LINCS L1000</p>"},{"location":"foundation_models/#cell-type-annotation","title":"\ud83d\udd2c Cell Type Annotation","text":"<p>Input: Single-cell expression profile Output: Cell type label Model: Frozen scGPT + classifier head Data: Tabula Sapiens, CellxGene</p>"},{"location":"foundation_models/#combination-therapy","title":"\ud83d\udc8a Combination Therapy","text":"<p>Input: Expression + drug A + drug B Output: Synergy score Model: Multi-task LoRA adapter Data: DrugComb, O'Neil et al.</p>"},{"location":"foundation_models/#rna-design","title":"\ud83e\uddea RNA Design","text":"<p>Input: Target structure + constraints Output: RNA sequence Model: Fine-tuned Helix-mRNA Data: RNAcentral, Rfam</p>"},{"location":"foundation_models/#getting-started","title":"Getting Started","text":"<p>Recommended learning path:</p> <ol> <li>Start with Leveraging Foundation Models for conceptual overview</li> <li>Follow Data Shape &amp; Tensors to prepare your data</li> <li>Use Implementation Guide for hands-on coding</li> <li>Experiment with different adaptation strategies (LoRA \u2192 Adapters \u2192 Full fine-tune)</li> <li>Evaluate on held-out data and compare to baselines</li> </ol> <p>Next steps:</p> <ul> <li>\ud83d\udcd3 Notebooks: Coming soon - interactive tutorials for each model</li> <li>\ud83d\udd27 Code examples: See <code>examples/foundation_models/</code> for production scripts</li> <li>\ud83d\udcda Theory: Explore DiT, JEPA, Latent Diffusion for advanced architectures</li> </ul>"},{"location":"foundation_models/#references","title":"References","text":""},{"location":"foundation_models/#key-papers","title":"Key Papers","text":"<ul> <li>Geneformer: Theodoris et al. (2023) - \"Transfer learning enables predictions in network biology\"</li> <li>scGPT: Cui et al. (2024) - \"scGPT: Toward building a foundation model for single-cell multi-omics\"</li> <li>ESM3: Hayes et al. (2024) - \"Simulating 500 million years of evolution\"</li> <li>Nucleotide Transformer: Dalla-Torre et al. (2023) - \"The Nucleotide Transformer\"</li> </ul>"},{"location":"foundation_models/#industry-reports","title":"Industry Reports","text":"<ul> <li>Foundation Models for Computational Biology - Nature Methods</li> <li>17 Companies Pioneering AI Foundation Models in Pharma</li> <li>NVIDIA BioNeMo Platform</li> </ul> <p>Questions or suggestions? Open an issue on GitHub</p>"},{"location":"foundation_models/IMPLEMENTATION_GUIDE/","title":"Foundation Model Adaptation: Implementation Guide","text":"<p>Quick reference for implementing the foundation model adaptation framework in your projects.</p>"},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#quick-start","title":"\ud83c\udfaf Quick Start","text":""},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#1-check-your-hardware","title":"1. Check Your Hardware","text":"<pre><code># Activate environment\nmamba activate genailab\n\n# Check detected hardware\npython -m genailab.foundation.configs.resource_profiles\n</code></pre> <p>Output example (M1 Mac): <pre><code>Detected Profile: M1 MacBook Pro 16GB\n  Device: mps\n  Memory: 16.0 GB\n  Recommended model size: small\n  Max batch size: 8\n</code></pre></p>"},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#2-compare-model-configurations","title":"2. Compare Model Configurations","text":"<pre><code>python -m genailab.foundation.configs.model_configs\n</code></pre> <p>Output: <pre><code>Model Configuration Comparison\n================================================================================\nConfig                    Params (M)   Memory (GB)  Batch    Depth   \n--------------------------------------------------------------------------------\nSmall (M1 16GB)           50.2         8.1          32       6       \nMedium (RunPod 24GB)      201.3        18.4         32       12      \nLarge (Cloud 40GB+)       603.9        35.2         64       24      \n</code></pre></p>"},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#3-test-lora-implementation","title":"3. Test LoRA Implementation","text":"<pre><code>python -m genailab.foundation.tuning.lora\n</code></pre>"},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#package-structure-created","title":"\ud83d\udce6 Package Structure Created","text":"<pre><code>src/genailab/foundation/\n\u251c\u2500\u2500 __init__.py                          \u2705 Created\n\u251c\u2500\u2500 README.md                            \u2705 Created\n\u251c\u2500\u2500 configs/\n\u2502   \u251c\u2500\u2500 __init__.py                      \u2705 Created\n\u2502   \u251c\u2500\u2500 model_configs.py                 \u2705 Created (SMALL/MEDIUM/LARGE)\n\u2502   \u2514\u2500\u2500 resource_profiles.py             \u2705 Created (M1/RunPod/Cloud)\n\u2514\u2500\u2500 tuning/\n    \u251c\u2500\u2500 __init__.py                      \u2705 Created\n    \u2514\u2500\u2500 lora.py                          \u2705 Created (Full implementation)\n</code></pre> <p>Still to create:</p> <ul> <li><code>tuning/adapters.py</code></li> <li><code>tuning/freeze.py</code></li> <li><code>conditioning/film.py</code></li> <li><code>conditioning/cross_attention.py</code></li> <li><code>conditioning/cfg.py</code></li> <li><code>backbones/dit.py</code></li> <li><code>recipes/latent_diffusion.py</code></li> </ul>"},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#usage-patterns","title":"\ud83d\udd27 Usage Patterns","text":""},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#pattern-1-auto-configure-for-your-hardware","title":"Pattern 1: Auto-Configure for Your Hardware","text":"<pre><code>from genailab.foundation.configs import get_resource_profile, get_model_config\n\n# Auto-detect\nprofile = get_resource_profile()\nconfig = get_model_config(profile.recommended_model_size)\n\nprint(f\"Using {config.embed_dim}d model with {config.depth} layers\")\nprint(f\"Batch size: {config.batch_size} (\u00d7{config.gradient_accumulation_steps} accum)\")\nprint(f\"Memory estimate: ~{config.memory_estimate_gb()}GB\")\n</code></pre>"},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#pattern-2-apply-lora-to-any-model","title":"Pattern 2: Apply LoRA to Any Model","text":"<pre><code>from genailab.foundation.tuning import apply_lora_to_model\n\n# Your model\nmodel = YourTransformer(embed_dim=256, depth=6)\n\n# Apply LoRA (trains only ~1% of parameters!)\nmodel = apply_lora_to_model(\n    model,\n    target_modules=[\"attention.query\", \"attention.key\", \"attention.value\"],\n    rank=8,\n    alpha=16,\n)\n\n# Train only LoRA parameters\noptimizer = torch.optim.AdamW(\n    [p for p in model.parameters() if p.requires_grad],\n    lr=1e-4,\n)\n</code></pre>"},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#pattern-3-resource-aware-training-loop","title":"Pattern 3: Resource-Aware Training Loop","text":"<pre><code>import torch\nfrom genailab.foundation.configs import get_resource_profile, get_model_config\n\n# Configure\nprofile = get_resource_profile()\nconfig = get_model_config(profile.recommended_model_size)\n\n# Setup\ndevice = torch.device(profile.device)\nmodel = model.to(device)\n\n# Training with gradient accumulation\naccumulation_steps = config.gradient_accumulation_steps\n\nfor step, batch in enumerate(dataloader):\n    x = batch['data'].to(device)\n\n    # Forward\n    loss = model(x) / accumulation_steps\n\n    # Backward\n    loss.backward()\n\n    # Update every N steps\n    if (step + 1) % accumulation_steps == 0:\n        optimizer.step()\n        optimizer.zero_grad()\n</code></pre>"},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#saving-and-loading","title":"\ud83d\udcbe Saving and Loading","text":""},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#save-only-lora-weights-tiny-file","title":"Save Only LoRA Weights (Tiny File!)","text":"<pre><code>from genailab.foundation.tuning import LoRA\n\n# After training\nLoRA.save_lora_only(model, \"lora_weights.pt\")\n# File size: ~1MB (vs ~200MB for full model)\n\n# Load later\nbase_model = YourTransformer()\nbase_model = apply_lora_to_model(base_model, rank=8)\nLoRA.load_lora_only(base_model, \"lora_weights.pt\")\n</code></pre>"},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#merge-for-inference","title":"Merge for Inference","text":"<pre><code># Merge LoRA into base weights for faster inference\nLoRA.merge_and_save(model, \"merged_model.pt\")\n</code></pre>"},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#learning-path","title":"\ud83c\udf93 Learning Path","text":""},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#for-m1-mac-users-16gb","title":"For M1 Mac Users (16GB)","text":"<p>Start here: 1. Run <code>python -m genailab.foundation.configs.resource_profiles</code> 2. Verify you get <code>SMALL_CONFIG</code> recommendation 3. Open <code>notebooks/foundation_models/01_model_sizes_and_resources.ipynb</code> 4. Try <code>notebooks/foundation_models/02_lora_basics.ipynb</code></p> <p>Key settings for M1:</p> <ul> <li>Batch size: 8</li> <li>Gradient accumulation: 4 (effective batch = 32)</li> <li>Gradient checkpointing: ON</li> <li>Mixed precision: ON (fp16)</li> <li>Device: <code>mps</code></li> </ul>"},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#for-runpodcloud-users-24gb","title":"For RunPod/Cloud Users (24GB+)","text":"<p>Start here: 1. Verify CUDA setup: <code>python -c \"import torch; print(torch.cuda.is_available())\"</code> 2. Run resource detection 3. Jump to <code>notebooks/foundation_models/03_adapters_vs_lora.ipynb</code> 4. Experiment with <code>MEDIUM_CONFIG</code> or <code>LARGE_CONFIG</code></p> <p>Key settings for GPU:</p> <ul> <li>Batch size: 32-64</li> <li>Gradient accumulation: 1</li> <li>Flash attention: ON</li> <li>Torch compile: ON</li> </ul>"},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#next-steps","title":"\ud83d\udd2c Next Steps","text":""},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#immediate-this-session","title":"Immediate (This Session)","text":"<ol> <li> <p>Test the framework:    <pre><code>python -m genailab.foundation.configs.resource_profiles\npython -m genailab.foundation.configs.model_configs\npython -m genailab.foundation.tuning.lora\n</code></pre></p> </li> <li> <p>Create first notebook: <code>01_model_sizes_and_resources.ipynb</code></p> </li> <li>Interactive hardware detection</li> <li>Model size comparison</li> <li> <p>Memory estimation examples</p> </li> <li> <p>Implement remaining tuning modules:</p> </li> <li><code>adapters.py</code> \u2014 Bottleneck adapter implementation</li> <li><code>freeze.py</code> \u2014 Layer freezing utilities</li> </ol>"},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#short-term-next-sessions","title":"Short-term (Next Sessions)","text":"<ol> <li>Conditioning mechanisms:</li> <li><code>film.py</code> \u2014 FiLM layers for perturbation conditioning</li> <li><code>cross_attention.py</code> \u2014 Multi-modal conditioning</li> <li> <p><code>cfg.py</code> \u2014 Classifier-free guidance</p> </li> <li> <p>Complete notebooks:</p> </li> <li><code>02_lora_basics.ipynb</code></li> <li><code>03_adapters_vs_lora.ipynb</code></li> <li> <p><code>07_end_to_end_gene_expression.ipynb</code></p> </li> <li> <p>Recipes:</p> </li> <li><code>latent_diffusion.py</code> \u2014 Complete pipeline</li> <li><code>perturbation.py</code> \u2014 Perturbation prediction</li> </ol>"},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#long-term","title":"Long-term","text":"<ol> <li>Advanced patterns:</li> <li>Mixture of Experts (MoE)</li> <li>Progressive unfreezing</li> <li> <p>Multi-task learning</p> </li> <li> <p>Production deployment:</p> </li> <li>Model serving</li> <li>Batch inference</li> <li>API endpoints</li> </ol>"},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#model-size-reference","title":"\ud83d\udcca Model Size Reference","text":""},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#small-config-m1-mac-16gb","title":"Small Config (M1 Mac 16GB)","text":"<pre><code>from genailab.foundation.configs import SMALL_CONFIG\n\nprint(SMALL_CONFIG.embed_dim)              # 256\nprint(SMALL_CONFIG.depth)                  # 6\nprint(SMALL_CONFIG.num_heads)              # 8\nprint(SMALL_CONFIG.batch_size)             # 8\nprint(SMALL_CONFIG.gradient_accumulation_steps)  # 4\nprint(SMALL_CONFIG.use_checkpoint)         # True\n</code></pre> <p>Use for: Development, prototyping, testing on M1 Mac</p>"},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#medium-config-runpod-24gb","title":"Medium Config (RunPod 24GB)","text":"<pre><code>from genailab.foundation.configs import MEDIUM_CONFIG\n\nprint(MEDIUM_CONFIG.embed_dim)             # 512\nprint(MEDIUM_CONFIG.depth)                 # 12\nprint(MEDIUM_CONFIG.num_heads)             # 8\nprint(MEDIUM_CONFIG.batch_size)            # 32\nprint(MEDIUM_CONFIG.use_flash_attention)   # True\n</code></pre> <p>Use for: Training, experimentation, RunPod instances</p>"},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#large-config-cloud-40gb","title":"Large Config (Cloud 40GB+)","text":"<pre><code>from genailab.foundation.configs import LARGE_CONFIG\n\nprint(LARGE_CONFIG.embed_dim)              # 768\nprint(LARGE_CONFIG.depth)                  # 24\nprint(LARGE_CONFIG.num_heads)              # 12\nprint(LARGE_CONFIG.num_tokens)             # 128\nprint(LARGE_CONFIG.batch_size)             # 64\n</code></pre> <p>Use for: Production, large-scale training, cloud instances</p>"},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#out-of-memory-on-m1-mac","title":"\"Out of Memory\" on M1 Mac","text":"<p>Solution 1: Reduce batch size <pre><code>config = SMALL_CONFIG\nconfig.batch_size = 4  # Reduce from 8\nconfig.gradient_accumulation_steps = 8  # Increase to maintain effective batch\n</code></pre></p> <p>Solution 2: Enable gradient checkpointing <pre><code>config.use_checkpoint = True\n</code></pre></p> <p>Solution 3: Reduce model size <pre><code>config.embed_dim = 128  # Reduce from 256\nconfig.depth = 4  # Reduce from 6\n</code></pre></p>"},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#lora-not-reducing-parameters","title":"LoRA Not Reducing Parameters","text":"<p>Check: Verify LoRA was applied correctly <pre><code>from genailab.foundation.tuning import LoRA\n\nLoRA.print_trainable_parameters(model)\n# Should show ~1-2% trainable\n</code></pre></p> <p>Fix: Ensure target modules match your model <pre><code># Print all module names\nfor name, _ in model.named_modules():\n    print(name)\n\n# Apply to correct modules\nmodel = apply_lora_to_model(\n    model,\n    target_modules=[\"your.actual.module.names\"],\n    rank=8,\n)\n</code></pre></p>"},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#mps-m1-performance-issues","title":"MPS (M1) Performance Issues","text":"<p>Tip 1: Use mixed precision <pre><code>from torch.cuda.amp import autocast\n\nwith autocast(device_type='cpu', dtype=torch.float16):\n    output = model(input)\n</code></pre></p> <p>Tip 2: Avoid frequent CPU-GPU transfers <pre><code># Bad: Transfer every step\nfor x in data:\n    x = x.to('mps')\n\n# Good: Transfer batch once\nbatch = batch.to('mps')\nfor x in batch:\n    ...\n</code></pre></p>"},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>Foundation Models Overview</li> <li>Transformer Data Shapes</li> <li>Latent Diffusion Series</li> <li>Package README</li> <li>Notebook Tutorials</li> </ul>"},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#verification-checklist","title":"\u2705 Verification Checklist","text":"<p>Before moving to notebooks:</p> <ul> <li> Resource detection works: <code>python -m genailab.foundation.configs.resource_profiles</code></li> <li> Model configs print correctly: <code>python -m genailab.foundation.configs.model_configs</code></li> <li> LoRA test runs: <code>python -m genailab.foundation.tuning.lora</code></li> <li> Can import in Python: <code>from genailab.foundation import *</code></li> <li> Memory estimates are reasonable for your hardware</li> </ul>"},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#success-criteria","title":"\ud83c\udfaf Success Criteria","text":"<p>You'll know the framework is working when:</p> <ol> <li>Auto-detection works: Correctly identifies your hardware</li> <li>LoRA reduces parameters: From 100% to ~1-2% trainable</li> <li>Memory fits: Model + optimizer + activations &lt; available memory</li> <li>Training runs: Can complete one epoch without OOM</li> <li>Saves/loads work: LoRA weights save and restore correctly</li> </ol>"},{"location":"foundation_models/IMPLEMENTATION_GUIDE/#next-session-preview","title":"Next Session Preview","text":"<p>In the next session, we'll create:</p> <ol> <li>First notebook: Interactive hardware detection and model sizing</li> <li>Adapter implementation: Alternative to LoRA</li> <li>Freeze utilities: Layer freezing strategies</li> <li>Comparison notebook: LoRA vs Adapters vs Full fine-tuning</li> </ol> <p>This will give you a complete toolkit for parameter-efficient fine-tuning!</p>"},{"location":"foundation_models/data_shape/","title":"Data shape","text":"<p>Transformer is often abstracted in code as: </p> <pre><code>output = transformer(tokens)\n</code></pre> <p>How do I conceptualize the input(s) and output(s) for a transformer? What are the key transformation of data shapes in the complex transformer layers? What are the output data shapes? What if we consider parameter efficient fine-tuning methods like LoRA, etc, then what would be the output data shape?</p> <p>Good\u2014this is exactly the right place to slow down and de-mythologize the incantation</p> <pre><code>output = transformer(tokens)\n</code></pre> <p>That line hides a lot of structure. Let\u2019s unpack it carefully, but without turning it into a soup of tensor algebra.</p> <p>I\u2019ll do this in layers of abstraction, because that\u2019s how transformers themselves are built.</p>"},{"location":"foundation_models/data_shape/#1-what-a-transformer-really-takes-as-input","title":"1. What a transformer really takes as input","text":"<p>At the highest level, a transformer does one thing:</p> <p>It maps a set or sequence of vectors to another set or sequence of vectors of the same length.</p> <p>So the most honest signature is:</p> <p>[ \\text{Transformer}: \\mathbb{R}^{B \\times T \\times d_{\\text{model}}} ;\\longrightarrow; \\mathbb{R}^{B \\times T \\times d_{\\text{model}}} ]</p> <p>Where:</p> <ul> <li>(B) = batch size</li> <li>(T) = number of tokens</li> <li>(d_{\\text{model}}) = embedding / hidden dimension</li> </ul> <p>That\u2019s it. No magic yet.</p> <p>If you remember only one thing: transformers are shape-preserving in time/token dimension and feature dimension.</p> <p>They do not:</p> <ul> <li>reduce token count</li> <li>change embedding size</li> <li>pool by default</li> </ul> <p>They rewrite representations, not compress them.</p>"},{"location":"foundation_models/data_shape/#2-what-exactly-is-a-token-here","title":"2. What exactly is a \u201ctoken\u201d here?","text":"<p>A token is just a vector in (\\mathbb{R}^{d_{\\text{model}}}).</p> <p>How you got it is upstream business:</p> <ul> <li>words \u2192 embeddings</li> <li>image patches \u2192 linear projection</li> <li>gene expression \u2192 encoder output</li> <li>latent codes \u2192 diffusion latents</li> </ul> <p>By the time it hits the transformer, the transformer does not care.</p> <p>So this abstraction:</p> <pre><code>tokens: [B, T, d_model]\n</code></pre> <p>is the contract.</p> <p>Everything else\u2014biology, language, pixels\u2014is already baked into those vectors.</p>"},{"location":"foundation_models/data_shape/#3-inside-a-transformer-block-what-changes-what-doesnt","title":"3. Inside a transformer block: what changes, what doesn\u2019t","text":"<p>A standard transformer block has two big sublayers:</p> <ol> <li>Multi-head self-attention</li> <li>Position-wise feedforward network (MLP)</li> </ol> <p>Both obey the same structural rule:</p> <p>Input shape = output shape = ([B, T, d_{\\text{model}}])</p> <p>Let\u2019s zoom in.</p>"},{"location":"foundation_models/data_shape/#31-self-attention-the-only-place-tokens-talk-to-each-other","title":"3.1 Self-attention: the only place tokens talk to each other","text":"<p>Input: [ X \\in \\mathbb{R}^{B \\times T \\times d_{\\text{model}}} ]</p> <p>First, three learned linear maps:</p> <p>[ Q = X W_Q,\\quad K = X W_K,\\quad V = X W_V ]</p> <p>Each still has shape: [ [B, T, d_{\\text{model}}] ]</p> <p>Then reshaped into heads: [ [B, h, T, d_{\\text{head}}] \\quad\\text{where}\\quad d_{\\text{head}} = d_{\\text{model}} / h ]</p> <p>Now the only moment where shape meaningfully changes:</p> <p>[ \\text{Attention scores: } QK^\\top ;\\Rightarrow; [B, h, T, T] ]</p> <p>This is the \u201cwho attends to whom\u201d matrix.</p> <p>After softmax and weighting (V), everything collapses back to:</p> <p>[ [B, T, d_{\\text{model}}] ]</p> <p>So attention temporarily creates a token\u2013token interaction matrix, but it never leaves the block.</p>"},{"location":"foundation_models/data_shape/#32-feedforward-network-no-token-mixing","title":"3.2 Feedforward network: no token mixing","text":"<p>The MLP is applied independently to each token:</p> <p>[ \\text{FFN}(x_t) = W_2 \\sigma(W_1 x_t) ]</p> <p>Shapes: [ [B, T, d_{\\text{model}}] ;\\rightarrow; [B, T, d_{\\text{ff}}] ;\\rightarrow; [B, T, d_{\\text{model}}] ]</p> <p>No cross-token interaction here. All mixing already happened in attention.</p>"},{"location":"foundation_models/data_shape/#33-residuals-normalization","title":"3.3 Residuals + normalization","text":"<p>Residual connections ensure:</p> <p>[ \\text{output} = X + \\text{sublayer}(X) ]</p> <p>This is why the shape must stay the same. Transformers are iterative representation refiners.</p>"},{"location":"foundation_models/data_shape/#4-stacking-layers-still-the-same-shape","title":"4. Stacking layers: still the same shape","text":"<p>A transformer with (L) layers is just:</p> <p>[ X^{(0)} \\rightarrow X^{(1)} \\rightarrow \\dots \\rightarrow X^{(L)} ]</p> <p>Each (X^{(\\ell)} \\in \\mathbb{R}^{B \\times T \\times d_{\\text{model}}}).</p> <p>So your abstraction:</p> <pre><code>output = transformer(tokens)\n</code></pre> <p>means:</p> <p>\u201cEach token has been rewritten (L) times using global context.\u201d</p> <p>Nothing more. Nothing less.</p>"},{"location":"foundation_models/data_shape/#5-what-is-the-output-conceptually","title":"5. What is the output, conceptually?","text":"<p>This depends entirely on what you do next.</p> <p>The transformer itself outputs: [ \\text{hidden states} \\in \\mathbb{R}^{B \\times T \\times d_{\\text{model}}} ]</p> <p>What those mean depends on your head:</p> <ul> <li>Language modeling: each token predicts next token</li> <li>Classification: you pool or select a special token</li> <li>Diffusion: each token predicts noise / velocity</li> <li>Gene expression: each token predicts latent deltas or distribution parameters</li> </ul> <p>The transformer does not decide this. The head does.</p>"},{"location":"foundation_models/data_shape/#6-pooling-is-not-part-of-the-transformer","title":"6. Pooling is not part of the transformer","text":"<p>If you see something like:</p> <ul> <li>CLS token</li> <li>mean pooling</li> <li>attention pooling</li> </ul> <p>Those are post-transformer operations.</p> <p>Example: [ [B, T, d] ;\\rightarrow; [B, d] ]</p> <p>That\u2019s a design choice, not transformer logic.</p> <p>This distinction matters a lot when you design biological models: token-level outputs \u2260 sample-level outputs.</p>"},{"location":"foundation_models/data_shape/#7-now-the-key-question-what-changes-with-lora-adapters","title":"7. Now the key question: what changes with LoRA / adapters?","text":"<p>Short answer: nothing about the output shape changes.</p> <p>Long answer: the function changes, not the type signature.</p>"},{"location":"foundation_models/data_shape/#71-lora-in-one-sentence","title":"7.1 LoRA in one sentence","text":"<p>LoRA replaces a weight matrix (W) with:</p> <p>[ W_{\\text{eff}} = W + \\Delta W \\quad\\text{where}\\quad \\Delta W = A B ]</p> <ul> <li>(A \\in \\mathbb{R}^{d_{\\text{out}} \\times r})</li> <li>(B \\in \\mathbb{R}^{r \\times d_{\\text{in}}})</li> <li>(r \\ll d_{\\text{model}})</li> </ul> <p>The key point:</p> <p>[ W_{\\text{eff}} \\in \\mathbb{R}^{d_{\\text{out}} \\times d_{\\text{in}}} ]</p> <p>Same shape as before.</p> <p>So when you apply LoRA to:</p> <ul> <li>(W_Q, W_K, W_V)</li> <li>output projection</li> <li>MLP layers</li> </ul> <p>the input/output tensor shapes are identical.</p>"},{"location":"foundation_models/data_shape/#72-why-this-is-conceptually-important","title":"7.2 Why this is conceptually important","text":"<p>LoRA is a functional perturbation of the model:</p> <ul> <li>It bends attention geometry</li> <li>It nudges feature subspaces</li> <li>It steers behavior</li> </ul> <p>But it does not change the interface.</p> <p>This is why you can:</p> <ul> <li>swap LoRA modules per task</li> <li>hot-load adapters</li> <li>compose multiple \u201cskills\u201d</li> </ul> <p>without touching downstream code.</p> <p>From a software-engineering perspective, this is gold.</p>"},{"location":"foundation_models/data_shape/#8-thinking-in-type-signatures-a-useful-mental-model","title":"8. Thinking in \u201ctype signatures\u201d (a useful mental model)","text":"<p>Here\u2019s a way to keep your sanity when models get big.</p> <p>Think of a transformer as having a type:</p> <p>[ \\text{Transformer}[T, d] : \\text{Tokens}[T, d] \\rightarrow \\text{Tokens}[T, d] ]</p> <p>LoRA, adapters, fine-tuning, freezing\u2014none of these change the type.</p> <p>Only encoders, decoders, and heads change types.</p> <p>Examples:</p> <ul> <li>encoder: counts \u2192 tokens</li> <li>transformer: tokens \u2192 tokens</li> <li>decoder: tokens \u2192 distributions</li> </ul> <p>This separation is exactly what makes foundation-model reuse possible.</p>"},{"location":"foundation_models/data_shape/#9-why-this-matters-for-your-dit-biology-project","title":"9. Why this matters for your DiT + biology project","text":"<p>This abstraction unlocks a powerful design principle:</p> <p>Once gene expression is mapped into a token space, all foundation-model machinery becomes legal.</p> <p>Diffusion, DiT, CFG, LoRA, adapters\u2014they all operate on: [ [B, T, d] ]</p> <p>Your real design freedom lives in:</p> <ul> <li>how you tokenize biology</li> <li>how you decode outputs</li> <li>how you condition transformations</li> </ul> <p>The transformer itself is just the universal mixer.</p>"},{"location":"foundation_models/data_shape/#a-final-anchoring-sentence","title":"A final anchoring sentence","text":"<p>A transformer does not generate meaning. It redistributes information across tokens while preserving shape.</p> <p>Everything interesting happens in how you enter and exit that space.</p> <p>Next natural continuation: we can take one concrete biology example (say scRNA latent diffusion) and trace exact tensor shapes end-to-end, including conditioning paths and LoRA injection points, until nothing feels hand-wavy anymore.</p>"},{"location":"foundation_models/data_shape_v2/","title":"Understanding Transformer Data Shapes","text":"<p>A comprehensive guide to tensor shapes in transformers, from input to output, including parameter-efficient fine-tuning methods like LoRA.</p> <p>Key question: When you see <code>output = transformer(tokens)</code>, what exactly is happening to the data shapes?</p>"},{"location":"foundation_models/data_shape_v2/#overview","title":"Overview","text":"<p>This guide demystifies the transformer abstraction by explaining:</p> <ol> <li>Input/output contracts \u2014 What transformers actually take and return</li> <li>Internal transformations \u2014 Shape changes inside attention and feedforward layers</li> <li>Layer stacking \u2014 How shapes flow through multiple blocks</li> <li>Output interpretation \u2014 What the hidden states mean</li> <li>LoRA and adapters \u2014 How fine-tuning methods affect shapes</li> <li>Practical implications \u2014 Design principles for biological models</li> </ol>"},{"location":"foundation_models/data_shape_v2/#1-the-core-contract-shape-preserving-transformation","title":"1. The Core Contract: Shape-Preserving Transformation","text":""},{"location":"foundation_models/data_shape_v2/#what-transformers-actually-do","title":"What Transformers Actually Do","text":"<p>At the highest level, a transformer performs one operation:</p> <p>It maps a sequence of vectors to another sequence of vectors of the same length.</p> <p>Mathematical signature:</p> \\[ \\text{Transformer}: \\mathbb{R}^{B \\times T \\times d_{\\text{model}}} \\longrightarrow \\mathbb{R}^{B \\times T \\times d_{\\text{model}}} \\] <p>Where: - \\(B\\) = batch size - \\(T\\) = number of tokens (sequence length) - \\(d_{\\text{model}}\\) = embedding/hidden dimension</p>"},{"location":"foundation_models/data_shape_v2/#key-insight","title":"Key Insight","text":"<p>Transformers are shape-preserving in both token dimension and feature dimension.</p> <p>They do NOT: - Reduce token count - Change embedding size - Pool by default</p> <p>They rewrite representations, not compress them.</p>"},{"location":"foundation_models/data_shape_v2/#example","title":"Example","text":"<pre><code># Input\ntokens = torch.randn(32, 64, 512)  # (batch=32, tokens=64, dim=512)\n\n# Transformer\noutput = transformer(tokens)\n\n# Output (same shape!)\nprint(output.shape)  # torch.Size([32, 64, 512])\n</code></pre>"},{"location":"foundation_models/data_shape_v2/#2-what-is-a-token","title":"2. What Is a \"Token\"?","text":""},{"location":"foundation_models/data_shape_v2/#definition","title":"Definition","text":"<p>A token is simply a vector in \\(\\mathbb{R}^{d_{\\text{model}}}\\).</p> <p>How you obtained it is upstream business: - Words \u2192 embeddings (NLP) - Image patches \u2192 linear projection (Vision) - Gene expression \u2192 encoder output (Biology) - Latent codes \u2192 diffusion latents (Generative models)</p> <p>By the time it reaches the transformer, the transformer doesn't care about the origin.</p>"},{"location":"foundation_models/data_shape_v2/#the-contract","title":"The Contract","text":"<pre><code>tokens: [B, T, d_model]\n</code></pre> <p>This is the interface contract.</p> <p>Everything else\u2014biology, language, pixels\u2014is already baked into those vectors.</p>"},{"location":"foundation_models/data_shape_v2/#example-different-modalities-same-shape","title":"Example: Different Modalities, Same Shape","text":"<pre><code># Text (BERT)\ntext_tokens = word_embeddings(text)  # (32, 128, 768)\n\n# Images (ViT)\nimage_tokens = patch_embeddings(image)  # (32, 196, 768)\n\n# Gene expression (custom)\ngene_tokens = gene_encoder(expression)  # (32, 64, 768)\n\n# All can use the same transformer!\ntransformer = Transformer(d_model=768)\noutput = transformer(tokens)  # Works for all modalities\n</code></pre>"},{"location":"foundation_models/data_shape_v2/#3-inside-a-transformer-block","title":"3. Inside a Transformer Block","text":"<p>A standard transformer block has two sublayers:</p> <ol> <li>Multi-head self-attention</li> <li>Position-wise feedforward network (MLP)</li> </ol> <p>Both obey the same rule:</p> <p>Input shape = Output shape = \\([B, T, d_{\\text{model}}]\\)</p> <p>Let's examine each.</p>"},{"location":"foundation_models/data_shape_v2/#31-self-attention-where-tokens-communicate","title":"3.1 Self-Attention: Where Tokens Communicate","text":""},{"location":"foundation_models/data_shape_v2/#input","title":"Input","text":"\\[ X \\in \\mathbb{R}^{B \\times T \\times d_{\\text{model}}} \\]"},{"location":"foundation_models/data_shape_v2/#step-1-linear-projections","title":"Step 1: Linear Projections","text":"<p>Three learned linear maps create queries, keys, and values:</p> \\[ Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V \\] <p>Each still has shape:</p> \\[ [B, T, d_{\\text{model}}] \\]"},{"location":"foundation_models/data_shape_v2/#step-2-reshape-into-heads","title":"Step 2: Reshape into Heads","text":"\\[ [B, h, T, d_{\\text{head}}] \\quad \\text{where} \\quad d_{\\text{head}} = d_{\\text{model}} / h \\] <p>Where \\(h\\) is the number of attention heads.</p>"},{"location":"foundation_models/data_shape_v2/#step-3-attention-scores-the-key-transformation","title":"Step 3: Attention Scores (The Key Transformation)","text":"<p>This is the only moment where shape meaningfully changes:</p> \\[ \\text{Attention scores: } QK^\\top \\Rightarrow [B, h, T, T] \\] <p>This is the \"who attends to whom\" matrix.</p> <p>Important: This \\(T \\times T\\) matrix is temporary and never leaves the block.</p>"},{"location":"foundation_models/data_shape_v2/#step-4-apply-attention-and-recombine","title":"Step 4: Apply Attention and Recombine","text":"<p>After softmax and weighting with values:</p> \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_{\\text{head}}}}\\right)V \\] <p>Everything collapses back to:</p> \\[ [B, T, d_{\\text{model}}] \\]"},{"location":"foundation_models/data_shape_v2/#visualization","title":"Visualization","text":"<pre><code># Input\nX = (B, T, d_model)\n\n# Project to Q, K, V\nQ = X @ W_Q  # (B, T, d_model)\nK = X @ W_K  # (B, T, d_model)\nV = X @ W_V  # (B, T, d_model)\n\n# Reshape to heads\nQ = Q.view(B, T, h, d_head).transpose(1, 2)  # (B, h, T, d_head)\nK = K.view(B, T, h, d_head).transpose(1, 2)  # (B, h, T, d_head)\nV = V.view(B, T, h, d_head).transpose(1, 2)  # (B, h, T, d_head)\n\n# Attention scores (TEMPORARY SHAPE CHANGE)\nscores = Q @ K.transpose(-2, -1)  # (B, h, T, T) \u2190 Token-token interaction\nattn = softmax(scores / sqrt(d_head))\n\n# Apply attention\nout = attn @ V  # (B, h, T, d_head)\n\n# Recombine heads\nout = out.transpose(1, 2).contiguous().view(B, T, d_model)  # (B, T, d_model)\n</code></pre>"},{"location":"foundation_models/data_shape_v2/#key-takeaway","title":"Key Takeaway","text":"<p>Attention temporarily creates a token-token interaction matrix \\((T \\times T)\\), but the output is always \\((B, T, d_{\\text{model}})\\).</p>"},{"location":"foundation_models/data_shape_v2/#32-feedforward-network-no-token-mixing","title":"3.2 Feedforward Network: No Token Mixing","text":"<p>The MLP is applied independently to each token:</p> \\[ \\text{FFN}(x_t) = W_2 \\sigma(W_1 x_t + b_1) + b_2 \\]"},{"location":"foundation_models/data_shape_v2/#shapes","title":"Shapes","text":"\\[ [B, T, d_{\\text{model}}] \\rightarrow [B, T, d_{\\text{ff}}] \\rightarrow [B, T, d_{\\text{model}}] \\] <p>Where \\(d_{\\text{ff}}\\) is typically \\(4 \\times d_{\\text{model}}\\).</p>"},{"location":"foundation_models/data_shape_v2/#implementation","title":"Implementation","text":"<pre><code>class FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.activation = nn.GELU()\n\n    def forward(self, x):\n        # x: (B, T, d_model)\n        x = self.linear1(x)  # (B, T, d_ff)\n        x = self.activation(x)\n        x = self.linear2(x)  # (B, T, d_model)\n        return x\n</code></pre>"},{"location":"foundation_models/data_shape_v2/#key-takeaway_1","title":"Key Takeaway","text":"<p>No cross-token interaction in the feedforward layer. All mixing happens in attention.</p>"},{"location":"foundation_models/data_shape_v2/#33-residuals-and-normalization","title":"3.3 Residuals and Normalization","text":"<p>Residual connections ensure:</p> \\[ \\text{output} = X + \\text{sublayer}(X) \\] <p>This is why the shape must stay the same.</p>"},{"location":"foundation_models/data_shape_v2/#complete-transformer-block","title":"Complete Transformer Block","text":"<pre><code>class TransformerBlock(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff):\n        super().__init__()\n        self.attention = MultiHeadAttention(d_model, num_heads)\n        self.ffn = FeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n\n    def forward(self, x):\n        # x: (B, T, d_model)\n\n        # Attention with residual\n        x = x + self.attention(self.norm1(x))  # (B, T, d_model)\n\n        # FFN with residual\n        x = x + self.ffn(self.norm2(x))  # (B, T, d_model)\n\n        return x  # (B, T, d_model)\n</code></pre> <p>Transformers are iterative representation refiners.</p>"},{"location":"foundation_models/data_shape_v2/#4-stacking-layers-still-the-same-shape","title":"4. Stacking Layers: Still the Same Shape","text":"<p>A transformer with \\(L\\) layers is just:</p> \\[ X^{(0)} \\rightarrow X^{(1)} \\rightarrow \\cdots \\rightarrow X^{(L)} \\] <p>Each \\(X^{(\\ell)} \\in \\mathbb{R}^{B \\times T \\times d_{\\text{model}}}\\).</p>"},{"location":"foundation_models/data_shape_v2/#visualization_1","title":"Visualization","text":"<pre><code># Input\nX_0 = tokens  # (B, T, d_model)\n\n# Layer 1\nX_1 = block_1(X_0)  # (B, T, d_model)\n\n# Layer 2\nX_2 = block_2(X_1)  # (B, T, d_model)\n\n# ...\n\n# Layer L\nX_L = block_L(X_{L-1})  # (B, T, d_model)\n\n# Output\noutput = X_L  # (B, T, d_model)\n</code></pre>"},{"location":"foundation_models/data_shape_v2/#interpretation","title":"Interpretation","text":"<pre><code>output = transformer(tokens)\n</code></pre> <p>means:</p> <p>\"Each token has been rewritten \\(L\\) times using global context.\"</p> <p>Nothing more. Nothing less.</p>"},{"location":"foundation_models/data_shape_v2/#5-what-is-the-output","title":"5. What Is the Output?","text":""},{"location":"foundation_models/data_shape_v2/#the-raw-output","title":"The Raw Output","text":"<p>The transformer itself outputs:</p> \\[ \\text{hidden states} \\in \\mathbb{R}^{B \\times T \\times d_{\\text{model}}} \\]"},{"location":"foundation_models/data_shape_v2/#what-it-means-depends-on-the-task","title":"What It Means Depends on the Task","text":"<p>The transformer doesn't decide what the output means\u2014the head does.</p> Task Head Operation Output Shape Language modeling Each token predicts next token \\((B, T, \\text{vocab\\_size})\\) Classification Pool or select CLS token \\((B, \\text{num\\_classes})\\) Diffusion Each token predicts noise/velocity \\((B, T, d_{\\text{model}})\\) Gene expression Each token predicts distribution params \\((B, T, \\text{num\\_genes})\\)"},{"location":"foundation_models/data_shape_v2/#example-different-heads","title":"Example: Different Heads","text":"<pre><code># Transformer output (same for all tasks)\nhidden = transformer(tokens)  # (B, T, d_model)\n\n# Language modeling head\nlogits = lm_head(hidden)  # (B, T, vocab_size)\n\n# Classification head (pool first)\npooled = hidden[:, 0, :]  # (B, d_model) - CLS token\nlogits = classifier(pooled)  # (B, num_classes)\n\n# Diffusion head\nnoise_pred = diffusion_head(hidden)  # (B, T, d_model)\n\n# Gene expression head\ngene_params = gene_head(hidden)  # (B, T, num_genes)\n</code></pre>"},{"location":"foundation_models/data_shape_v2/#6-pooling-is-not-part-of-the-transformer","title":"6. Pooling Is Not Part of the Transformer","text":"<p>If you see operations like:</p> <ul> <li>CLS token \u2014 Select first token</li> <li>Mean pooling \u2014 Average over tokens</li> <li>Attention pooling \u2014 Weighted average</li> </ul> <p>These are post-transformer operations, not transformer logic.</p>"},{"location":"foundation_models/data_shape_v2/#example-pooling-operations","title":"Example: Pooling Operations","text":"<pre><code># Transformer output\nhidden = transformer(tokens)  # (B, T, d_model)\n\n# CLS token (BERT-style)\ncls_output = hidden[:, 0, :]  # (B, d_model)\n\n# Mean pooling\nmean_output = hidden.mean(dim=1)  # (B, d_model)\n\n# Attention pooling\nattn_weights = attention_pooling(hidden)  # (B, T, 1)\npooled_output = (hidden * attn_weights).sum(dim=1)  # (B, d_model)\n</code></pre>"},{"location":"foundation_models/data_shape_v2/#why-this-matters-for-biology","title":"Why This Matters for Biology","text":"<p>Token-level outputs \u2260 Sample-level outputs</p> <p>In gene expression models: - Token-level: Each token represents a gene module/pathway - Sample-level: Need to aggregate tokens for cell-level prediction</p> <p>Design choice: How to aggregate?</p>"},{"location":"foundation_models/data_shape_v2/#7-lora-and-adapters-what-changes","title":"7. LoRA and Adapters: What Changes?","text":""},{"location":"foundation_models/data_shape_v2/#short-answer","title":"Short Answer","text":"<p>Nothing about the output shape changes.</p>"},{"location":"foundation_models/data_shape_v2/#long-answer","title":"Long Answer","text":"<p>The function changes, not the type signature.</p>"},{"location":"foundation_models/data_shape_v2/#71-lora-in-detail","title":"7.1 LoRA in Detail","text":""},{"location":"foundation_models/data_shape_v2/#concept","title":"Concept","text":"<p>LoRA replaces a weight matrix \\(W\\) with:</p> \\[ W_{\\text{eff}} = W + \\Delta W \\quad \\text{where} \\quad \\Delta W = AB \\] <ul> <li>\\(A \\in \\mathbb{R}^{d_{\\text{out}} \\times r}\\)</li> <li>\\(B \\in \\mathbb{R}^{r \\times d_{\\text{in}}}\\)</li> <li>\\(r \\ll d_{\\text{model}}\\) (typically 4-16)</li> </ul>"},{"location":"foundation_models/data_shape_v2/#key-point","title":"Key Point","text":"\\[ W_{\\text{eff}} \\in \\mathbb{R}^{d_{\\text{out}} \\times d_{\\text{in}}} \\] <p>Same shape as before.</p>"},{"location":"foundation_models/data_shape_v2/#implementation_1","title":"Implementation","text":"<pre><code>class LoRALinear(nn.Module):\n    def __init__(self, in_features, out_features, rank=8, alpha=16):\n        super().__init__()\n\n        # Original weight (frozen)\n        self.linear = nn.Linear(in_features, out_features)\n        self.linear.weight.requires_grad = False\n\n        # LoRA parameters (trainable)\n        self.lora_A = nn.Parameter(torch.randn(in_features, rank) * 0.01)\n        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))\n\n        # Scaling factor\n        self.scaling = alpha / rank\n\n    def forward(self, x):\n        # Original output\n        output = self.linear(x)\n\n        # LoRA update\n        lora_output = (x @ self.lora_A @ self.lora_B) * self.scaling\n\n        return output + lora_output\n</code></pre>"},{"location":"foundation_models/data_shape_v2/#shape-analysis","title":"Shape Analysis","text":"<pre><code># Input\nx = (B, T, d_in)\n\n# Original linear\nW: (d_out, d_in)\noutput = x @ W.T  # (B, T, d_out)\n\n# LoRA\nA: (d_in, r)\nB: (r, d_out)\nlora_output = x @ A @ B  # (B, T, d_out)\n\n# Combined\nfinal_output = output + lora_output  # (B, T, d_out)\n</code></pre> <p>Input/output shapes are identical to the original layer.</p>"},{"location":"foundation_models/data_shape_v2/#72-why-this-is-important","title":"7.2 Why This Is Important","text":""},{"location":"foundation_models/data_shape_v2/#functional-perturbation","title":"Functional Perturbation","text":"<p>LoRA is a functional perturbation of the model: - Bends attention geometry - Nudges feature subspaces - Steers behavior</p> <p>But it does not change the interface.</p>"},{"location":"foundation_models/data_shape_v2/#practical-benefits","title":"Practical Benefits","text":"<p>This enables:</p> <pre><code># Swap LoRA modules per task\nmodel.load_lora(\"task_A.pt\")\noutput_A = model(input)\n\nmodel.load_lora(\"task_B.pt\")\noutput_B = model(input)\n\n# Compose multiple skills\nmodel.load_lora([\"skill_1.pt\", \"skill_2.pt\"])\noutput = model(input)\n</code></pre> <p>No downstream code changes needed.</p>"},{"location":"foundation_models/data_shape_v2/#software-engineering-gold","title":"Software Engineering Gold","text":"<p>From a software perspective, this is powerful: - Modular \u2014 Swap adapters without touching backbone - Composable \u2014 Combine multiple LoRA modules - Efficient \u2014 Store multiple task-specific models cheaply</p>"},{"location":"foundation_models/data_shape_v2/#8-thinking-in-type-signatures","title":"8. Thinking in Type Signatures","text":""},{"location":"foundation_models/data_shape_v2/#mental-model","title":"Mental Model","text":"<p>Think of a transformer as having a type:</p> \\[ \\text{Transformer}[T, d]: \\text{Tokens}[T, d] \\rightarrow \\text{Tokens}[T, d] \\] <p>LoRA, adapters, fine-tuning, freezing\u2014none of these change the type.</p>"},{"location":"foundation_models/data_shape_v2/#what-changes-types","title":"What Changes Types?","text":"<p>Only encoders, decoders, and heads change types:</p> <pre><code># Encoder: Raw data \u2192 Tokens\nencoder: counts (num_genes) \u2192 tokens (T, d)\n\n# Transformer: Tokens \u2192 Tokens\ntransformer: tokens (T, d) \u2192 tokens (T, d)\n\n# Decoder: Tokens \u2192 Output\ndecoder: tokens (T, d) \u2192 distributions (num_genes)\n</code></pre>"},{"location":"foundation_models/data_shape_v2/#example-complete-pipeline","title":"Example: Complete Pipeline","text":"<pre><code># Gene expression \u2192 Latent tokens\ntokens = encoder(gene_expression)  # (20000,) \u2192 (64, 256)\n\n# Latent diffusion (with LoRA)\ndenoised = diffusion_transformer(tokens, t)  # (64, 256) \u2192 (64, 256)\n\n# Tokens \u2192 Gene expression parameters\nparams = decoder(denoised)  # (64, 256) \u2192 (20000,)\n</code></pre> <p>Type changes happen at boundaries, not inside the transformer.</p>"},{"location":"foundation_models/data_shape_v2/#9-implications-for-biology-models","title":"9. Implications for Biology Models","text":""},{"location":"foundation_models/data_shape_v2/#key-insight_1","title":"Key Insight","text":"<p>Once gene expression is mapped into a token space, all foundation-model machinery becomes legal.</p> <p>Diffusion, DiT, CFG, LoRA, adapters\u2014they all operate on:</p> \\[ [B, T, d] \\]"},{"location":"foundation_models/data_shape_v2/#design-freedom","title":"Design Freedom","text":"<p>Your real design choices are:</p> <ol> <li>How you tokenize biology \u2014 Encoder design</li> <li>How you decode outputs \u2014 Decoder design</li> <li>How you condition transformations \u2014 Conditioning mechanisms</li> </ol> <p>The transformer itself is just the universal mixer.</p>"},{"location":"foundation_models/data_shape_v2/#example-modular-design","title":"Example: Modular Design","text":"<pre><code># Tokenization choice\nencoder = LatentTokenEncoder(num_genes=20000, num_tokens=64, token_dim=256)\n# OR\nencoder = PathwayTokenEncoder(pathways=msigdb_hallmark, token_dim=256)\n\n# Transformer (same for both!)\ntransformer = DiT(embed_dim=256, depth=12, num_heads=8)\n\n# Add LoRA (same interface!)\ntransformer = LoRA.wrap(transformer, rank=8)\n\n# Decoder choice\ndecoder = NegativeBinomialDecoder(token_dim=256, num_genes=20000)\n# OR\ndecoder = ZINBDecoder(token_dim=256, num_genes=20000)\n</code></pre> <p>Mix and match without changing the transformer.</p>"},{"location":"foundation_models/data_shape_v2/#10-complete-shape-trace-end-to-end-example","title":"10. Complete Shape Trace: End-to-End Example","text":""},{"location":"foundation_models/data_shape_v2/#task-scrna-seq-latent-diffusion","title":"Task: scRNA-seq Latent Diffusion","text":"<pre><code># \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# INPUT: Gene expression counts\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\ngene_expression = (batch=32, num_genes=20000)\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# ENCODER: Counts \u2192 Latent tokens\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nlatent_tokens = encoder(gene_expression)\n# Shape: (32, 64, 256)\n#        \u2191   \u2191   \u2191\n#        |   |   \u2514\u2500 Token dimension\n#        |   \u2514\u2500\u2500\u2500\u2500\u2500 Number of tokens\n#        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Batch size\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# POSITIONAL ENCODING\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\npos_embed = (1, 64, 256)  # Broadcasts across batch\nlatent_tokens = latent_tokens + pos_embed\n# Shape: (32, 64, 256)\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# CONDITIONING: Perturbation embedding\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nperturbation = (32, 128)  # Perturbation embedding\ncondition = condition_proj(perturbation)  # (32, 256)\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# DIFFUSION TRANSFORMER (with LoRA)\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# Add noise\nt = torch.randint(0, 1000, (32,))\nnoise = torch.randn(32, 64, 256)\nnoisy_tokens = sqrt(alpha_t) * latent_tokens + sqrt(1 - alpha_t) * noise\n# Shape: (32, 64, 256)\n\n# Transformer forward (SHAPE PRESERVED!)\nfor block in transformer.blocks:\n    # Self-attention\n    attn_out = block.attention(noisy_tokens)  # (32, 64, 256)\n    noisy_tokens = noisy_tokens + attn_out\n\n    # Conditioning (FiLM)\n    scale = film.scale_net(condition)  # (32, 256)\n    shift = film.shift_net(condition)  # (32, 256)\n    noisy_tokens = noisy_tokens * scale.unsqueeze(1) + shift.unsqueeze(1)\n\n    # FFN\n    ffn_out = block.ffn(noisy_tokens)  # (32, 64, 256)\n    noisy_tokens = noisy_tokens + ffn_out\n\ndenoised_tokens = noisy_tokens  # (32, 64, 256)\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# DECODER: Latent tokens \u2192 Gene expression parameters\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\ngene_params = decoder(denoised_tokens)\n# Shape: (32, 20000, 2)\n#        \u2191   \u2191       \u2191\n#        |   |       \u2514\u2500 Parameters (mean, dispersion)\n#        |   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Genes\n#        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Batch size\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# OUTPUT: NB/ZINB distribution\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nmean, dispersion = gene_params.chunk(2, dim=-1)\n# mean: (32, 20000)\n# dispersion: (32, 20000)\n</code></pre>"},{"location":"foundation_models/data_shape_v2/#shape-summary-table","title":"Shape Summary Table","text":"Stage Operation Input Shape Output Shape Input Raw counts <code>(32, 20000)</code> - Encoder Compress to tokens <code>(32, 20000)</code> <code>(32, 64, 256)</code> Pos Embed Add position info <code>(32, 64, 256)</code> <code>(32, 64, 256)</code> Diffusion Add noise <code>(32, 64, 256)</code> <code>(32, 64, 256)</code> Transformer Denoise (12 blocks) <code>(32, 64, 256)</code> <code>(32, 64, 256)</code> Decoder Tokens \u2192 params <code>(32, 64, 256)</code> <code>(32, 20000, 2)</code> Output NB distribution <code>(32, 20000, 2)</code> - <p>Key observation: Transformer operates entirely in <code>(B, T, d)</code> space.</p>"},{"location":"foundation_models/data_shape_v2/#key-takeaways","title":"Key Takeaways","text":""},{"location":"foundation_models/data_shape_v2/#core-principles","title":"Core Principles","text":"<ol> <li>Transformers preserve shape \u2014 Input and output have same dimensions</li> <li>Tokens are the contract \u2014 <code>(B, T, d_model)</code> is the universal interface</li> <li>Attention creates temporary interactions \u2014 <code>(T, T)</code> matrix never leaves the block</li> <li>LoRA doesn't change shapes \u2014 Only changes the function, not the signature</li> <li>Pooling is external \u2014 Not part of the transformer itself</li> </ol>"},{"location":"foundation_models/data_shape_v2/#design-implications","title":"Design Implications","text":"<ol> <li>Tokenization is key \u2014 How you enter the <code>(B, T, d)</code> space matters most</li> <li>Decoding is flexible \u2014 How you exit the space is task-specific</li> <li>Conditioning is modular \u2014 Can inject at multiple points</li> <li>Adaptation is cheap \u2014 LoRA/adapters don't change interfaces</li> </ol>"},{"location":"foundation_models/data_shape_v2/#mental-model_1","title":"Mental Model","text":"<p>A transformer does not generate meaning. It redistributes information across tokens while preserving shape.</p> <p>Everything interesting happens in how you enter and exit that space.</p>"},{"location":"foundation_models/data_shape_v2/#related-documents","title":"Related Documents","text":"<ul> <li>leveraging_foundation_models_v2.md \u2014 Design patterns for adaptation</li> <li>../latent_diffusion/ \u2014 Latent diffusion implementation</li> <li>../DiT/01_dit_foundations.md \u2014 DiT architecture details</li> <li>../DDPM/02a_diffusion_arch_gene_expression.md \u2014 Tokenization for biology</li> </ul>"},{"location":"foundation_models/data_shape_v2/#references","title":"References","text":"<p>Transformers:</p> <ul> <li>Vaswani et al. (2017): \"Attention Is All You Need\"</li> <li>Devlin et al. (2019): \"BERT: Pre-training of Deep Bidirectional Transformers\"</li> <li>Dosovitskiy et al. (2021): \"An Image is Worth 16x16 Words: Transformers for Image Recognition\"</li> </ul> <p>Parameter-efficient fine-tuning:</p> <ul> <li>Hu et al. (2021): \"LoRA: Low-Rank Adaptation of Large Language Models\"</li> <li>Houlsby et al. (2019): \"Parameter-Efficient Transfer Learning for NLP\"</li> </ul> <p>Diffusion transformers:</p> <ul> <li>Peebles &amp; Xie (2023): \"Scalable Diffusion Models with Transformers\" (DiT)</li> <li>Rombach et al. (2022): \"High-Resolution Image Synthesis with Latent Diffusion Models\"</li> </ul>"},{"location":"foundation_models/leveraging_foundation_models/","title":"Leveraging foundation models","text":"<p>Below is a pragmatic way to leverage pretrained foundation models (DiT-like backbones, but also \u201cfoundation-ish\u201d encoders like Geneformer/scGPT) for clinically useful comp-bio tasks (gene expression synthesis + perturbation response), plus a set of design patterns (LoRA, adapters, freezing, etc.) you can package as reusable modules.</p>"},{"location":"foundation_models/leveraging_foundation_models/#1-clinically-useful-targets-what-success-looks-like","title":"1) Clinically useful targets: what \u201csuccess\u201d looks like","text":"<p>Clinical utility usually means you\u2019re not generating pretty samples\u2014you\u2019re producing actionable distributions under interventions:</p>"},{"location":"foundation_models/leveraging_foundation_models/#a-gene-expression-synthesis-bulk-or-pseudo-bulk-possibly-scrna","title":"A. Gene expression synthesis (bulk or pseudo-bulk, possibly scRNA)","text":"<p>You want:</p> <ul> <li>realistic marginal distributions (counts, zero inflation, library size effects)</li> <li>realistic conditional distributions: tissue, disease subtype, covariates</li> <li>calibrated uncertainty (so downstream decisions aren\u2019t vibes-based)</li> </ul>"},{"location":"foundation_models/leveraging_foundation_models/#b-perturbation-response-perturb-seq-crispr-drug","title":"B. Perturbation response (Perturb-seq / CRISPR / drug)","text":"<p>You want:</p> <ul> <li>\u0394-expression (counterfactual shift) under perturbation, not just reconstruction</li> <li>generalization to unseen perturbations or combos</li> <li>uncertainty that correlates with \u201cOOD-ness\u201d (new cell states, new perturbations)</li> </ul> <p>scPPDM lives in this B-family: \u201cpredict response distributions conditioned on perturbation + context,\u201d not merely \u201cgenerate cells.\u201d (You already framed this direction.) </p>"},{"location":"foundation_models/leveraging_foundation_models/#2-a-key-move-dit-for-biology-usually-wants-latent-space-dit-not-raw-count-dit","title":"2) A key move: \u201cDiT for biology\u201d usually wants latent-space DiT, not raw-count DiT","text":"<p>A DiT backbone is happiest when the input looks like a sequence of continuous tokens with reasonably stable scale.</p> <p>Raw gene expression has nasty properties:</p> <ul> <li>NB/ZINB count noise</li> <li>library-size scaling</li> <li>heavy tails + many zeros</li> <li>huge dimension (~20k genes)</li> </ul> <p>So the common pattern is:</p> <p>Counts \u2192 (biologically appropriate encoder) \u2192 latent tokens \u2192 DiT/Transformer diffusion \u2192 latent \u2192 (count-aware decoder)</p> <p>You already have the pieces in your repo roadmap: NB/ZINB decoders + diffusion modules are a natural fit for a Latent Diffusion for Expression stack. </p> <p>This also elegantly dodges the \u201ctokenization feels arbitrary\u201d problem you raised: you can learn a semantic tokenization via the encoder rather than hand-choosing gene patches. </p>"},{"location":"foundation_models/leveraging_foundation_models/#3-tokenization-options-for-gene-expression-that-dont-feel-like-cursed-hacks","title":"3) Tokenization options for gene expression that don\u2019t feel like cursed hacks","text":"<p>Think of \u201ctokenization\u201d as \u201chow we factor the object so attention has something meaningful to attend over.\u201d</p>"},{"location":"foundation_models/leveraging_foundation_models/#option-1-latent-tokens-recommended-default","title":"Option 1: Latent tokens (recommended default)","text":"<ul> <li>Encoder maps expression \u2192 <code>Z \u2208 R^{m\u00d7d}</code> (m tokens, d dim)</li> <li>DiT runs on these m tokens (m maybe 32\u2013512, not 20k)</li> <li>Decoder maps latent \u2192 parameters of NB/ZINB distribution</li> </ul> <p>Why it\u2019s good:</p> <ul> <li>learned, data-adaptive tokenization</li> <li>compute-friendly</li> <li>plays nicely with LoRA/adapters (small modules can steer a big backbone)</li> </ul>"},{"location":"foundation_models/leveraging_foundation_models/#option-2-pathwaymodule-tokens-biologically-anchored","title":"Option 2: Pathway/module tokens (biologically anchored)","text":"<ul> <li>Build tokens as pathway activities / gene modules (MSigDB, Reactome, data-driven modules)</li> <li>Each token is a module embedding; attention learns cross-module interactions</li> </ul> <p>Why it\u2019s good:</p> <ul> <li>interpretability (pathway-level explanation is clinically legible)</li> <li>lower dimension</li> <li>easier to align across datasets</li> </ul>"},{"location":"foundation_models/leveraging_foundation_models/#option-3-graph-structured-tokens-grn-aware-attention","title":"Option 3: Graph-structured tokens (GRN-aware attention)","text":"<ul> <li>Tokens are genes (or modules), but attention is constrained by GRN edges</li> <li>Avoids full O(n\u00b2) attention by sparse neighborhoods</li> </ul> <p>Why it\u2019s good:</p> <ul> <li>more mechanistic flavor</li> <li>better inductive bias for perturbations</li> </ul>"},{"location":"foundation_models/leveraging_foundation_models/#option-4-rank-based-geneformer-style-sequences-useful-but-heavy","title":"Option 4: Rank-based \u201cGeneformer style\u201d sequences (useful, but heavy)","text":"<ul> <li>Order genes by expression and treat as sequence</li> <li>Works, but scaling and ties/ordering artifacts are real</li> </ul> <p>Your \u201cpatch-size skepticism\u201d in the DiT doc generalizes here: any hard tokenization rule is a hyperparameter wearing a trench coat pretending to be a principle. Latent/module tokenization avoids that trap by learning it. </p>"},{"location":"foundation_models/leveraging_foundation_models/#4-foundation-model-leverage-the-6-design-patterns-you-can-practice-and-package","title":"4) Foundation-model leverage: the 6 \u201cdesign patterns\u201d you can practice and package","text":"<p>Here are patterns that actually show up in modern foundation-model use (not just \u201cfine-tune everything and pray\u201d).</p>"},{"location":"foundation_models/leveraging_foundation_models/#pattern-a-frozen-backbone-linear-probe-sanity-baseline","title":"Pattern A: Frozen backbone + linear probe (sanity baseline)","text":"<ul> <li>Freeze pretrained encoder/backbone</li> <li>Train tiny head for your task (prediction, classification, response \u0394)</li> </ul> <p>Why it matters:</p> <ul> <li>gives you a fast \u201cis representation already good?\u201d test</li> <li>great for low data regimes</li> </ul>"},{"location":"foundation_models/leveraging_foundation_models/#pattern-b-adapters-small-bottleneck-modules","title":"Pattern B: Adapters (small bottleneck modules)","text":"<ul> <li>Insert small MLP/attention adapter blocks into a frozen model</li> <li>Train adapters only</li> </ul> <p>Pros:</p> <ul> <li>stable training</li> <li>cheap</li> <li>less catastrophic forgetting</li> </ul>"},{"location":"foundation_models/leveraging_foundation_models/#pattern-c-lora-qlora-low-rank-updates","title":"Pattern C: LoRA / QLoRA (low-rank updates)","text":"<ul> <li>Add low-rank matrices to attention projections (Q/K/V/O, maybe MLP)</li> <li>Train only LoRA params (and optionally layer norms)</li> </ul> <p>Pros:</p> <ul> <li>best \u201cutility per parameter\u201d in many settings</li> <li>easy to swap per-dataset / per-task \u201cpersonas\u201d</li> </ul>"},{"location":"foundation_models/leveraging_foundation_models/#pattern-d-partial-unfreezing-top-k-layers","title":"Pattern D: Partial unfreezing (top-K layers)","text":"<ul> <li>Freeze most layers, unfreeze last few transformer blocks + norms</li> </ul> <p>Pros:</p> <ul> <li>more expressive than pure LoRA sometimes</li> <li>still manageable for small datasets</li> </ul>"},{"location":"foundation_models/leveraging_foundation_models/#pattern-e-conditional-control-modules-film-controlnet-like","title":"Pattern E: Conditional control modules (FiLM / ControlNet-like)","text":"<ul> <li> <p>Keep backbone fixed; steer it via conditional pathways:</p> </li> <li> <p>FiLM: scale/shift hidden activations based on condition</p> </li> <li>cross-attention: condition tokens attend into backbone tokens</li> <li>classifier-free guidance at sampling time</li> </ul> <p>This is especially natural for perturbations: perturbation embedding becomes a \u201ccontrol signal\u201d that steers generation.</p>"},{"location":"foundation_models/leveraging_foundation_models/#pattern-f-distill-align-to-a-smaller-student-deployment-aware","title":"Pattern F: Distill / align to a smaller student (deployment-aware)","text":"<ul> <li>Use big model as teacher</li> <li>Train small model for inference-time constraints (clinical pipelines often care)</li> </ul>"},{"location":"foundation_models/leveraging_foundation_models/#5-how-these-patterns-map-to-your-two-flagship-tasks","title":"5) How these patterns map to your two flagship tasks","text":""},{"location":"foundation_models/leveraging_foundation_models/#gene-expression-synthesis-distributional","title":"Gene expression synthesis (distributional)","text":"<p>Best first bet:</p> <ul> <li>Latent Diffusion + NB/ZINB decoder</li> <li> <p>Foundation leverage via:</p> </li> <li> <p>pretrained transformer backbone (DiT-style) + LoRA</p> </li> <li>or pretrained gene encoder (Geneformer/scGPT) as encoder; freeze + adapters</li> </ul> <p>Minimal-data trick:</p> <ul> <li>Don\u2019t fine-tune the whole generator.</li> <li>Fine-tune conditioning (FiLM/adapters) so you can add new cohorts/diseases with few samples.</li> </ul>"},{"location":"foundation_models/leveraging_foundation_models/#perturbation-response","title":"Perturbation response","text":"<p>Two robust formulations:</p> <p>(1) Predict the counterfactual distribution directly</p> <ul> <li>Input: baseline cell state + perturbation embedding</li> <li>Output: distribution of post-perturb expression (or latent)</li> </ul> <p>(2) Predict the delta in latent space</p> <ul> <li>Learn <code>\u0394z = f(z_base, perturb)</code> then decode <code>z_base + \u0394z</code></li> <li>Often easier and more stable than predicting absolute expression</li> </ul> <p>Foundation leverage:</p> <ul> <li> <p>A big pretrained backbone can be frozen; you train only:</p> </li> <li> <p>perturbation encoder</p> </li> <li>LoRA/adapters</li> <li>\u201cdelta head\u201d or conditional control block</li> </ul> <p>This is the \u201cmaximum utility with minimal data\u201d sweet spot.</p>"},{"location":"foundation_models/leveraging_foundation_models/#6-a-packaging-plan-modules-that-make-this-a-lab-not-a-one-off","title":"6) A packaging plan: modules that make this a lab, not a one-off","text":"<p>Your current structure is already close. I\u2019d add a <code>foundations/</code> subpackage that formalizes these patterns as composable components. </p> <p>Proposed package skeleton (conceptual):</p> <ul> <li> <p><code>genailab/foundation/</code></p> </li> <li> <p><code>backbones/</code></p> <ul> <li><code>dit.py</code> (DiT-like transformer backbone wrapper)</li> <li><code>gene_transformer.py</code> (wrappers for Geneformer/scGPT-like encoders if you integrate them later)</li> <li> <p><code>tuning/</code></p> </li> <li> <p><code>lora.py</code></p> </li> <li><code>adapters.py</code></li> <li><code>freeze.py</code> (policies: freeze all, freeze except norms, unfreeze last k blocks)</li> <li> <p><code>conditioning/</code></p> </li> <li> <p><code>film.py</code></p> </li> <li><code>cross_attention.py</code></li> <li><code>cfg.py</code> (classifier-free guidance utilities)</li> <li> <p><code>objectives/</code></p> </li> <li> <p><code>distillation.py</code></p> </li> <li><code>uncertainty.py</code> (calibration metrics, NLL, coverage)</li> <li> <p><code>recipes/</code></p> </li> <li> <p><code>latent_diffusion_nb.py</code> (end-to-end recipe: encoder\u2192diffuse\u2192decoder)</p> </li> <li><code>perturb_delta_latent.py</code></li> </ul> </li> </ul> <p>This turns \u201cdesign patterns\u201d into importable Lego bricks you can swap in ablations.</p>"},{"location":"foundation_models/leveraging_foundation_models/#7-how-to-break-this-into-sessions-so-it-compounds","title":"7) How to break this into sessions (so it compounds)","text":"<p>Here\u2019s a clean sequence that matches your learning + implementation goals:</p> <p>Session 1 (this one): Architecture choices + design patterns for adapting foundation models in bio (what you\u2019re reading now).</p> <p>Session 2: Pick one concrete \u201creference stack\u201d to implement first:</p> <ul> <li>Latent diffusion for expression with NB/ZINB decoder</li> <li>Conditioning API (perturbation, tissue, batch, covariates)</li> </ul> <p>Session 3: Implement tuning modules:</p> <ul> <li>LoRA + adapters + freeze policies</li> <li>add \u201cone-line switch\u201d configs to compare strategies</li> </ul> <p>Session 4: Perturbation response recipe:</p> <ul> <li>delta-in-latent model</li> <li>evaluation: directional accuracy, pathway consistency, calibration</li> </ul> <p>Session 5: Clinical-ish constraints:</p> <ul> <li>batch effects / domain shift handling</li> <li>uncertainty calibration + OOD detection</li> <li>counterfactual validity checks (tie-in to your causal-bio-lab later)</li> </ul> <p>If you keep only one mantra from all this: don\u2019t fine-tune the world\u2014fine-tune the steering wheel. Your repo is already oriented toward that (conditional generation + modularity), and DiT-style backbones become much more \u201cbiologically plausible\u201d once you treat tokenization as a learned representation problem rather than a hand-chosen patch rule.  </p>"},{"location":"foundation_models/leveraging_foundation_models_v2/","title":"Leveraging Foundation Models for Computational Biology","text":"<p>A practical guide to adapting pretrained foundation models (DiT-like backbones, Geneformer, scGPT) for clinically useful computational biology tasks.</p> <p>Target applications: Gene expression synthesis and perturbation response prediction</p> <p>Key insight: Don't fine-tune everything\u2014fine-tune the steering wheel.</p>"},{"location":"foundation_models/leveraging_foundation_models_v2/#overview","title":"Overview","text":"<p>This guide covers:</p> <ol> <li>Clinical targets \u2014 What \"success\" looks like in practice</li> <li>Latent-space strategy \u2014 Why DiT works better on learned tokens</li> <li>Tokenization options \u2014 Four approaches for gene expression</li> <li>Design patterns \u2014 Six reusable strategies for model adaptation</li> <li>Task-specific recipes \u2014 How to apply patterns to your use cases</li> <li>Implementation plan \u2014 Modular architecture for experimentation</li> <li>Session roadmap \u2014 Step-by-step learning path</li> </ol>"},{"location":"foundation_models/leveraging_foundation_models_v2/#1-clinical-targets-defining-success","title":"1. Clinical Targets: Defining Success","text":"<p>Clinical utility means producing actionable distributions under interventions, not just generating pretty samples.</p>"},{"location":"foundation_models/leveraging_foundation_models_v2/#gene-expression-synthesis","title":"Gene Expression Synthesis","text":"<p>Goal: Generate realistic gene expression profiles conditioned on biological context.</p> <p>Requirements:</p> <ul> <li>Realistic marginals \u2014 Proper count distributions (NB/ZINB), zero-inflation, library size effects</li> <li>Realistic conditionals \u2014 Accurate tissue, disease subtype, and covariate dependencies</li> <li>Calibrated uncertainty \u2014 Reliable confidence estimates for downstream decisions</li> </ul> <p>Use cases:</p> <ul> <li>Data augmentation for rare cell types</li> <li>Synthetic controls for experiments</li> <li>Batch effect correction</li> <li>Counterfactual cell state generation</li> </ul>"},{"location":"foundation_models/leveraging_foundation_models_v2/#perturbation-response-prediction","title":"Perturbation Response Prediction","text":"<p>Goal: Predict cellular response to genetic or chemical perturbations.</p> <p>Requirements:</p> <ul> <li>\u0394-expression prediction \u2014 Counterfactual shift under perturbation, not just reconstruction</li> <li>Generalization \u2014 Accurate predictions for unseen perturbations or combinations</li> <li>OOD-aware uncertainty \u2014 Higher uncertainty for novel cell states or perturbations</li> </ul> <p>Use cases:</p> <ul> <li>Virtual screening (predict without experiments)</li> <li>Combination therapy prediction</li> <li>Mechanism discovery</li> <li>Drug response modeling</li> </ul> <p>Note: Models like scPPDM fall into this category\u2014predicting response distributions conditioned on perturbation + context, not merely generating cells.</p>"},{"location":"foundation_models/leveraging_foundation_models_v2/#2-latent-space-strategy-why-not-raw-counts","title":"2. Latent-Space Strategy: Why Not Raw Counts?","text":""},{"location":"foundation_models/leveraging_foundation_models_v2/#the-problem-with-raw-gene-expression","title":"The Problem with Raw Gene Expression","text":"<p>DiT backbones work best with sequences of continuous tokens at stable scale. Raw gene expression has problematic properties:</p> Property Issue Impact Count noise NB/ZINB distribution Non-Gaussian, heavy-tailed Library size Technical variation Scaling artifacts Sparsity Many zeros Gradient issues Dimensionality ~20K genes Computational cost"},{"location":"foundation_models/leveraging_foundation_models_v2/#the-solution-latent-diffusion","title":"The Solution: Latent Diffusion","text":"<p>Standard pattern:</p> <pre><code>Raw Counts \u2192 Encoder \u2192 Latent Tokens \u2192 DiT/Transformer \u2192 Latent \u2192 Decoder \u2192 Parameters\n  (20K)                   (64\u00d7256)                        (64\u00d7256)              (20K)\n</code></pre> <p>Architecture components:</p> <pre><code># Encoder: Compress to latent space\nencoder: gene_expression (20K) \u2192 latent_tokens (64, 256)\n\n# Diffusion: DiT operates on latent tokens\ndiffusion: latent_tokens \u2192 denoised_latent_tokens\n\n# Decoder: Reconstruct with biological distributions\ndecoder: latent_tokens \u2192 NB/ZINB parameters (mean, dispersion, dropout)\n</code></pre>"},{"location":"foundation_models/leveraging_foundation_models_v2/#why-this-works","title":"Why This Works","text":"<p>1. Learned semantic tokenization</p> <ul> <li>Encoder discovers meaningful biological patterns</li> <li>Avoids arbitrary patch-based tokenization</li> <li>Tokens represent gene modules, pathways, or cell states</li> </ul> <p>2. Stable diffusion dynamics</p> <ul> <li>Continuous latent space (no count artifacts)</li> <li>Normalized scale (better gradient flow)</li> <li>Lower dimensionality (faster training)</li> </ul> <p>3. Biological realism</p> <ul> <li>NB/ZINB decoder handles count distributions</li> <li>Library size normalization in decoder</li> <li>Zero-inflation modeling</li> </ul> <p>4. Modular design</p> <ul> <li>Encoder/decoder can be pretrained separately</li> <li>DiT backbone can be frozen or adapted</li> <li>Easy to swap components</li> </ul>"},{"location":"foundation_models/leveraging_foundation_models_v2/#3-tokenization-options-for-gene-expression","title":"3. Tokenization Options for Gene Expression","text":"<p>Key question: How do we factor gene expression so attention has something meaningful to attend over?</p>"},{"location":"foundation_models/leveraging_foundation_models_v2/#option-1-latent-tokens-recommended-default","title":"Option 1: Latent Tokens (Recommended Default)","text":"<p>Concept: Learn a compressed, structured representation.</p> <pre><code># Architecture\nencoder: R^20000 \u2192 R^(m\u00d7d)  # m=64 tokens, d=256 dimensions\nDiT: R^(m\u00d7d) \u2192 R^(m\u00d7d)      # Attention over tokens\ndecoder: R^(m\u00d7d) \u2192 R^20000  # Back to gene space\n</code></pre> <p>Advantages:</p> <ul> <li>Data-adaptive \u2014 Model learns optimal tokenization</li> <li>Compute-friendly \u2014 64 tokens &lt;&lt; 20K genes (attention is O(m\u00b2))</li> <li>LoRA-compatible \u2014 Small adapters can steer large backbone</li> <li>Flexible conditioning \u2014 Easy to inject perturbation/cell type info</li> </ul> <p>Implementation:</p> <pre><code>class LatentTokenEncoder(nn.Module):\n    def __init__(self, num_genes=20000, num_tokens=64, token_dim=256):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(num_genes, 2048),\n            nn.LayerNorm(2048),\n            nn.GELU(),\n            nn.Linear(2048, num_tokens * token_dim)\n        )\n        self.num_tokens = num_tokens\n        self.token_dim = token_dim\n\n    def forward(self, x):\n        # x: (batch, num_genes)\n        z = self.encoder(x)  # (batch, num_tokens * token_dim)\n        z = z.view(-1, self.num_tokens, self.token_dim)  # (batch, num_tokens, token_dim)\n        return z\n</code></pre> <p>When to use: Default choice for most applications.</p>"},{"location":"foundation_models/leveraging_foundation_models_v2/#option-2-pathwaymodule-tokens-biologically-anchored","title":"Option 2: Pathway/Module Tokens (Biologically Anchored)","text":"<p>Concept: Use biological knowledge to define tokens as gene pathways or modules.</p> <pre><code># Each token represents a biological pathway\ntoken_1 \u2192 \"Cell cycle genes\"\ntoken_2 \u2192 \"Immune response genes\"\ntoken_3 \u2192 \"Metabolic genes\"\n...\ntoken_50 \u2192 \"Apoptosis genes\"\n</code></pre> <p>Advantages:</p> <ul> <li>Interpretability \u2014 Pathway-level explanations are clinically legible</li> <li>Lower dimension \u2014 ~50-500 pathways vs 20K genes</li> <li>Transfer learning \u2014 Pathways consistent across datasets</li> <li>Inductive bias \u2014 Encodes known biology, faster convergence</li> </ul> <p>Pathway databases:</p> Database # Pathways Coverage Best For MSigDB Hallmark 50 Broad processes High-level analysis MSigDB C2 (KEGG) 186 Metabolic/signaling Mechanistic studies Reactome 2,500+ Detailed processes Fine-grained analysis GO Biological Process 10,000+ Comprehensive Full coverage <p>When to use: Clinical applications requiring interpretability.</p>"},{"location":"foundation_models/leveraging_foundation_models_v2/#option-3-graph-structured-tokens-grn-aware","title":"Option 3: Graph-Structured Tokens (GRN-Aware)","text":"<p>Concept: Use gene regulatory networks to structure attention.</p> <pre><code># Sparse attention based on GRN edges\n# Only genes with regulatory relationships attend to each other\nattention_mask = GRN_adjacency_matrix  # Sparse (num_genes, num_genes)\n</code></pre> <p>Advantages:</p> <ul> <li>Mechanistic \u2014 Respects known regulatory relationships</li> <li>Perturbation-aware \u2014 Better inductive bias for interventions</li> <li>Efficient \u2014 Sparse attention O(num_edges) vs O(n\u00b2)</li> <li>Interpretable \u2014 Can trace predictions through regulatory paths</li> </ul> <p>When to use: Perturbation modeling, causal inference.</p>"},{"location":"foundation_models/leveraging_foundation_models_v2/#option-4-rank-based-sequences-geneformer-style","title":"Option 4: Rank-Based Sequences (Geneformer Style)","text":"<p>Concept: Order genes by expression level and treat as sequence.</p> <pre><code># Rank genes by expression\nsorted_genes = argsort(expression, descending=True)\n# Treat as sequence for transformer\nsequence = [gene_1, gene_2, ..., gene_k]  # Top-k genes\n</code></pre> <p>Advantages:</p> <ul> <li>Empirically validated \u2014 Works in Geneformer</li> <li>Sequence-native \u2014 Natural for transformers</li> </ul> <p>Disadvantages:</p> <ul> <li>Ordering artifacts \u2014 Ties get arbitrary order</li> <li>Scaling issues \u2014 20K sequence length is expensive</li> <li>Truncation loss \u2014 Top-k loses information</li> <li>Not biologically motivated \u2014 Ranking is artificial</li> </ul> <p>When to use: Large-scale pretraining with massive data.</p>"},{"location":"foundation_models/leveraging_foundation_models_v2/#comparison-table","title":"Comparison Table","text":"Approach Tokens Compute Interpretability Biology Transfer Best For Latent tokens 32-128 Low Moderate Learned Good Default choice Pathway tokens 50-500 Low High Strong Excellent Clinical applications GRN-aware 20K (sparse) Moderate High Strong Moderate Perturbation modeling Rank-based 2K-20K High Low Weak Poor Large-scale pretraining"},{"location":"foundation_models/leveraging_foundation_models_v2/#4-foundation-model-design-patterns","title":"4. Foundation Model Design Patterns","text":"<p>Six reusable strategies for adapting pretrained models without full fine-tuning.</p>"},{"location":"foundation_models/leveraging_foundation_models_v2/#pattern-a-frozen-backbone-linear-probe","title":"Pattern A: Frozen Backbone + Linear Probe","text":"<p>Strategy: Freeze pretrained encoder/backbone, train only a small task-specific head.</p> <pre><code># Freeze backbone\nfor param in backbone.parameters():\n    param.requires_grad = False\n\n# Train only head\nhead = nn.Linear(hidden_dim, num_classes)\noptimizer = torch.optim.Adam(head.parameters(), lr=1e-3)\n</code></pre> <p>When to use:</p> <ul> <li>Sanity check \u2014 Test if representations are already good</li> <li>Low data \u2014 Few samples available</li> <li>Fast iteration \u2014 Quick baseline</li> </ul> <p>Pros: Fast, stable, no catastrophic forgetting Cons: Limited expressiveness</p>"},{"location":"foundation_models/leveraging_foundation_models_v2/#pattern-b-adapters","title":"Pattern B: Adapters","text":"<p>Strategy: Insert small bottleneck modules into frozen model.</p> <pre><code>class Adapter(nn.Module):\n    def __init__(self, hidden_dim, bottleneck_dim=64):\n        super().__init__()\n        self.down = nn.Linear(hidden_dim, bottleneck_dim)\n        self.up = nn.Linear(bottleneck_dim, hidden_dim)\n        self.activation = nn.GELU()\n\n    def forward(self, x):\n        return x + self.up(self.activation(self.down(x)))\n\n# Insert after each transformer block\nfor block in transformer.blocks:\n    block.adapter = Adapter(hidden_dim)\n</code></pre> <p>When to use:</p> <ul> <li>Stable training \u2014 Less prone to overfitting</li> <li>Multiple tasks \u2014 Swap adapters per task</li> <li>Limited compute \u2014 Fewer parameters than full fine-tuning</li> </ul> <p>Pros: Cheap, stable, modular Cons: Slightly less expressive than full fine-tuning</p>"},{"location":"foundation_models/leveraging_foundation_models_v2/#pattern-c-lora-low-rank-adaptation","title":"Pattern C: LoRA (Low-Rank Adaptation)","text":"<p>Strategy: Add low-rank matrices to attention projections.</p> <pre><code>class LoRALinear(nn.Module):\n    def __init__(self, in_features, out_features, rank=8):\n        super().__init__()\n        self.linear = nn.Linear(in_features, out_features)  # Frozen\n        self.lora_A = nn.Parameter(torch.randn(in_features, rank) * 0.01)\n        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))\n        self.scaling = 1.0 / rank\n\n    def forward(self, x):\n        # Original + low-rank update\n        return self.linear(x) + (x @ self.lora_A @ self.lora_B) * self.scaling\n\n# Apply to Q, K, V projections\nattention.query = LoRALinear(hidden_dim, hidden_dim, rank=8)\nattention.key = LoRALinear(hidden_dim, hidden_dim, rank=8)\nattention.value = LoRALinear(hidden_dim, hidden_dim, rank=8)\n</code></pre> <p>When to use:</p> <ul> <li>Best efficiency \u2014 Highest utility per parameter</li> <li>Multiple personas \u2014 Easy to swap per-dataset/task</li> <li>Memory constrained \u2014 Minimal memory overhead</li> </ul> <p>Pros: Extremely parameter-efficient, composable Cons: Requires careful rank selection</p> <p>Typical ranks: 4-16 for most applications</p>"},{"location":"foundation_models/leveraging_foundation_models_v2/#pattern-d-partial-unfreezing","title":"Pattern D: Partial Unfreezing","text":"<p>Strategy: Freeze most layers, unfreeze last few blocks + layer norms.</p> <pre><code># Freeze all layers\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Unfreeze last K blocks\nK = 3\nfor block in model.blocks[-K:]:\n    for param in block.parameters():\n        param.requires_grad = True\n\n# Always unfreeze layer norms\nfor module in model.modules():\n    if isinstance(module, nn.LayerNorm):\n        for param in module.parameters():\n            param.requires_grad = True\n</code></pre> <p>When to use:</p> <ul> <li>More expressiveness \u2014 Beyond LoRA/adapters</li> <li>Moderate data \u2014 Enough to avoid overfitting</li> <li>Task-specific features \u2014 Need to adapt high-level representations</li> </ul> <p>Pros: More expressive than LoRA Cons: More parameters, risk of overfitting</p>"},{"location":"foundation_models/leveraging_foundation_models_v2/#pattern-e-conditional-control-modules","title":"Pattern E: Conditional Control Modules","text":"<p>Strategy: Keep backbone fixed, steer via conditional pathways.</p> <p>FiLM (Feature-wise Linear Modulation):</p> <pre><code>class FiLM(nn.Module):\n    def __init__(self, condition_dim, hidden_dim):\n        super().__init__()\n        self.scale_net = nn.Linear(condition_dim, hidden_dim)\n        self.shift_net = nn.Linear(condition_dim, hidden_dim)\n\n    def forward(self, x, condition):\n        scale = self.scale_net(condition)\n        shift = self.shift_net(condition)\n        return x * scale + shift\n\n# Apply after each block\nfor block in transformer.blocks:\n    block.film = FiLM(condition_dim, hidden_dim)\n</code></pre> <p>Cross-attention conditioning:</p> <pre><code># Condition tokens attend into backbone tokens\ncondition_tokens = condition_encoder(perturbation)  # (batch, num_cond, dim)\nbackbone_tokens = backbone(gene_tokens)  # (batch, num_tokens, dim)\n\n# Cross-attention: condition \u2192 backbone\noutput = cross_attention(\n    query=backbone_tokens,\n    key=condition_tokens,\n    value=condition_tokens\n)\n</code></pre> <p>When to use:</p> <ul> <li>Perturbation modeling \u2014 Perturbation as control signal</li> <li>Multi-modal conditioning \u2014 Cell type + tissue + batch</li> <li>Classifier-free guidance \u2014 Sample-time steering</li> </ul> <p>Pros: Flexible, interpretable, no backbone modification Cons: Requires careful conditioning design</p>"},{"location":"foundation_models/leveraging_foundation_models_v2/#pattern-f-distillation","title":"Pattern F: Distillation","text":"<p>Strategy: Use large model as teacher, train small model for deployment.</p> <pre><code># Teacher (large, frozen)\nteacher_output = teacher_model(x)\n\n# Student (small, trainable)\nstudent_output = student_model(x)\n\n# Distillation loss\nloss = KL_divergence(student_output, teacher_output)\n</code></pre> <p>When to use:</p> <ul> <li>Deployment constraints \u2014 Clinical pipelines need fast inference</li> <li>Edge devices \u2014 Limited compute</li> <li>Cost reduction \u2014 Cheaper API calls</li> </ul> <p>Pros: Fast inference, smaller models Cons: Requires teacher model, some performance loss</p>"},{"location":"foundation_models/leveraging_foundation_models_v2/#5-task-specific-recipes","title":"5. Task-Specific Recipes","text":""},{"location":"foundation_models/leveraging_foundation_models_v2/#gene-expression-synthesis_1","title":"Gene Expression Synthesis","text":"<p>Best approach: Latent Diffusion + NB/ZINB decoder</p> <pre><code># Architecture\nencoder: gene_expression \u2192 latent_tokens\ndiffusion: latent_tokens (+ condition) \u2192 denoised_latent\ndecoder: latent_tokens \u2192 NB/ZINB parameters\n\n# Foundation leverage\nbackbone = pretrained_DiT()  # Frozen or LoRA\nconditioning = FiLM(condition_dim, hidden_dim)  # Trainable\n\n# Minimal-data trick\n# Fine-tune only conditioning modules for new cohorts/diseases\n</code></pre> <p>Training strategy: 1. Pretrain VAE (encoder + decoder) on large dataset 2. Freeze VAE, train diffusion on latent codes 3. Add LoRA/adapters for new conditions</p>"},{"location":"foundation_models/leveraging_foundation_models_v2/#perturbation-response","title":"Perturbation Response","text":"<p>Two formulations:</p> <p>Option 1: Direct prediction</p> <pre><code># Input: baseline state + perturbation\n# Output: post-perturbation distribution\n\nmodel(baseline_expression, perturbation_embedding) \u2192 perturbed_expression\n</code></pre> <p>Option 2: Delta prediction (recommended)</p> <pre><code># Predict change in latent space\ndelta_z = model(z_baseline, perturbation_embedding)\nz_perturbed = z_baseline + delta_z\nx_perturbed = decoder(z_perturbed)\n</code></pre> <p>Why delta is better:</p> <ul> <li>More stable training</li> <li>Better generalization</li> <li>Easier to interpret</li> <li>Handles small perturbations better</li> </ul> <p>Foundation leverage:</p> <pre><code># Freeze large backbone\nbackbone.requires_grad = False\n\n# Train only:\nperturbation_encoder = nn.Sequential(...)  # Trainable\nlora_modules = [...]  # Trainable\ndelta_head = nn.Linear(...)  # Trainable\n</code></pre> <p>This is the \"maximum utility with minimal data\" sweet spot.</p>"},{"location":"foundation_models/leveraging_foundation_models_v2/#6-modular-implementation-architecture","title":"6. Modular Implementation Architecture","text":"<p>Package design patterns as composable components for easy experimentation.</p>"},{"location":"foundation_models/leveraging_foundation_models_v2/#proposed-structure","title":"Proposed Structure","text":"<pre><code>genailab/foundation/\n\u251c\u2500\u2500 backbones/\n\u2502   \u251c\u2500\u2500 dit.py                    # DiT-like transformer wrapper\n\u2502   \u251c\u2500\u2500 gene_transformer.py       # Geneformer/scGPT wrappers\n\u2502   \u2514\u2500\u2500 unet.py                   # U-Net backbone\n\u251c\u2500\u2500 tuning/\n\u2502   \u251c\u2500\u2500 lora.py                   # LoRA implementation\n\u2502   \u251c\u2500\u2500 adapters.py               # Adapter modules\n\u2502   \u2514\u2500\u2500 freeze.py                 # Freezing policies\n\u251c\u2500\u2500 conditioning/\n\u2502   \u251c\u2500\u2500 film.py                   # FiLM conditioning\n\u2502   \u251c\u2500\u2500 cross_attention.py        # Cross-attention modules\n\u2502   \u2514\u2500\u2500 cfg.py                    # Classifier-free guidance\n\u251c\u2500\u2500 objectives/\n\u2502   \u251c\u2500\u2500 distillation.py           # Knowledge distillation\n\u2502   \u2514\u2500\u2500 uncertainty.py            # Calibration metrics\n\u2514\u2500\u2500 recipes/\n    \u251c\u2500\u2500 latent_diffusion_nb.py    # End-to-end: encoder\u2192diffuse\u2192decoder\n    \u2514\u2500\u2500 perturb_delta_latent.py   # Perturbation delta prediction\n</code></pre>"},{"location":"foundation_models/leveraging_foundation_models_v2/#example-usage","title":"Example Usage","text":"<pre><code>from genailab.foundation.backbones import DiT\nfrom genailab.foundation.tuning import LoRA\nfrom genailab.foundation.conditioning import FiLM\n\n# Load pretrained backbone\nbackbone = DiT.from_pretrained(\"dit-base\")\n\n# Add LoRA\nbackbone = LoRA.wrap(backbone, rank=8, target_modules=[\"attention\"])\n\n# Add conditioning\nconditioning = FiLM(condition_dim=128, hidden_dim=512)\n\n# Compose\nmodel = LatentDiffusionModel(\n    backbone=backbone,\n    conditioning=conditioning,\n    encoder=encoder,\n    decoder=decoder\n)\n</code></pre> <p>Benefits:</p> <ul> <li>Modular \u2014 Swap components easily</li> <li>Reusable \u2014 Share across projects</li> <li>Testable \u2014 Unit test each component</li> <li>Ablatable \u2014 Compare strategies systematically</li> </ul>"},{"location":"foundation_models/leveraging_foundation_models_v2/#7-learning-roadmap-session-by-session","title":"7. Learning Roadmap: Session-by-Session","text":"<p>Break implementation into manageable sessions that compound.</p>"},{"location":"foundation_models/leveraging_foundation_models_v2/#session-1-architecture-design-patterns","title":"Session 1: Architecture &amp; Design Patterns \u2713","text":"<p>Goal: Understand foundation model adaptation strategies.</p> <p>Topics:</p> <ul> <li>Clinical targets and success criteria</li> <li>Latent-space strategy</li> <li>Tokenization options</li> <li>Six design patterns</li> </ul> <p>Deliverable: This document</p>"},{"location":"foundation_models/leveraging_foundation_models_v2/#session-2-reference-stack-implementation","title":"Session 2: Reference Stack Implementation","text":"<p>Goal: Build one complete end-to-end system.</p> <p>Tasks:</p> <ul> <li>Implement latent diffusion for expression</li> <li>Add NB/ZINB decoder</li> <li>Create conditioning API (perturbation, tissue, batch, covariates)</li> </ul> <p>Deliverable: Working latent diffusion model</p>"},{"location":"foundation_models/leveraging_foundation_models_v2/#session-3-tuning-modules","title":"Session 3: Tuning Modules","text":"<p>Goal: Package adaptation strategies as reusable modules.</p> <p>Tasks:</p> <ul> <li>Implement LoRA, adapters, freeze policies</li> <li>Create \"one-line switch\" configs</li> <li>Benchmark strategies on same task</li> </ul> <p>Deliverable: <code>genailab.foundation.tuning</code> package</p>"},{"location":"foundation_models/leveraging_foundation_models_v2/#session-4-perturbation-response-recipe","title":"Session 4: Perturbation Response Recipe","text":"<p>Goal: Build delta-in-latent perturbation model.</p> <p>Tasks:</p> <ul> <li>Implement delta predictor</li> <li>Add perturbation encoder</li> <li>Evaluate: directional accuracy, pathway consistency, calibration</li> </ul> <p>Deliverable: Perturbation prediction pipeline</p>"},{"location":"foundation_models/leveraging_foundation_models_v2/#session-5-clinical-constraints","title":"Session 5: Clinical Constraints","text":"<p>Goal: Handle real-world deployment challenges.</p> <p>Tasks:</p> <ul> <li>Batch effect / domain shift handling</li> <li>Uncertainty calibration + OOD detection</li> <li>Counterfactual validity checks</li> </ul> <p>Deliverable: Production-ready system</p>"},{"location":"foundation_models/leveraging_foundation_models_v2/#key-takeaways","title":"Key Takeaways","text":""},{"location":"foundation_models/leveraging_foundation_models_v2/#core-principles","title":"Core Principles","text":"<ol> <li>Don't fine-tune the world\u2014fine-tune the steering wheel</li> <li>Keep large backbones frozen</li> <li>Train small conditioning/adaptation modules</li> <li> <p>Maximize utility per parameter</p> </li> <li> <p>Latent space is your friend</p> </li> <li>Learn semantic tokenization</li> <li>Avoid raw count artifacts</li> <li> <p>Enable stable diffusion dynamics</p> </li> <li> <p>Modularity enables experimentation</p> </li> <li>Package patterns as composable components</li> <li>Easy to swap and compare strategies</li> <li> <p>Reusable across projects</p> </li> <li> <p>Clinical utility requires more than generation</p> </li> <li>Calibrated uncertainty</li> <li>Interpretable predictions</li> <li>Actionable distributions</li> </ol>"},{"location":"foundation_models/leveraging_foundation_models_v2/#decision-framework","title":"Decision Framework","text":"<p>Choose latent tokens when: - You want data-adaptive tokenization - Compute efficiency matters - You need flexibility</p> <p>Choose pathway tokens when: - Interpretability is critical - Clinical legibility matters - You have domain knowledge</p> <p>Choose GRN-aware when: - Modeling perturbations - Mechanistic understanding needed - You have regulatory network data</p> <p>Choose LoRA when: - Parameter efficiency is key - You need multiple task-specific models - Memory is constrained</p> <p>Choose adapters when: - You want stable training - You need task modularity - You're in low-data regime</p> <p>Choose partial unfreezing when: - You need more expressiveness - You have sufficient data - LoRA/adapters aren't enough</p>"},{"location":"foundation_models/leveraging_foundation_models_v2/#related-documents","title":"Related Documents","text":"<ul> <li>data_shape.md \u2014 Understanding transformer tensor shapes</li> <li>../latent_diffusion/ \u2014 Latent diffusion implementation</li> <li>../DiT/ \u2014 DiT architecture details</li> <li>../DDPM/02a_diffusion_arch_gene_expression.md \u2014 Tokenization options</li> </ul>"},{"location":"foundation_models/leveraging_foundation_models_v2/#references","title":"References","text":"<p>Foundation models:</p> <ul> <li>Theodoris et al. (2023): \"Transfer learning enables predictions in network biology\" (Geneformer)</li> <li>Cui et al. (2024): \"scGPT: Toward Building a Foundation Model for Single-Cell Multi-omics\"</li> </ul> <p>Adaptation strategies:</p> <ul> <li>Hu et al. (2021): \"LoRA: Low-Rank Adaptation of Large Language Models\"</li> <li>Houlsby et al. (2019): \"Parameter-Efficient Transfer Learning for NLP\" (Adapters)</li> <li>Perez et al. (2018): \"FiLM: Visual Reasoning with a General Conditioning Layer\"</li> </ul> <p>Latent diffusion:</p> <ul> <li>Rombach et al. (2022): \"High-Resolution Image Synthesis with Latent Diffusion Models\"</li> <li>Peebles &amp; Xie (2023): \"Scalable Diffusion Models with Transformers\" (DiT)</li> </ul> <p>Computational biology applications:</p> <ul> <li>Lotfollahi et al. (2023): \"Predicting cellular responses to novel drug combinations\" (CPA)</li> <li>Lopez et al. (2018): \"Deep generative modeling for single-cell transcriptomics\" (scVI)</li> </ul>"},{"location":"incubation/","title":"Ideas Under Incubation","text":"<p>This directory contains exploratory architectural proposals and application ideas that are not yet implemented. These documents capture emerging directions for the genai-lab project.</p>"},{"location":"incubation/#contents","title":"Contents","text":"Document Focus Status joint_latent_space_and_JEPA.md Joint latent spaces for static/dynamic data, JEPA for Perturb-seq Conceptual generative-ai-for-gene-expression-prediction.md Diffusion/VAE/Flow for gene expression with uncertainty, count data handling Documented generative-ai-for-perturbation-modeling.md Generative approaches for scPerturb, beyond GEM-1 Conceptual alternative_backbones_for_biology.md SSMs, tokenization alternatives, architectures for biology Conceptual"},{"location":"incubation/#key-themes","title":"Key Themes","text":""},{"location":"incubation/#1-beyond-tokenization","title":"1. Beyond Tokenization","text":"<p>Gene expression doesn't naturally tokenize like text or images. We explore:</p> <ul> <li>State-vector representations (no tokens)</li> <li>Latent-space diffusion (VAE + flow matching)</li> <li>Set-based representations (permutation-invariant)</li> <li>SSM backbones (Mamba, S4) for temporal dynamics</li> </ul>"},{"location":"incubation/#2-count-data-handling","title":"2. Count Data Handling","text":"<p>Gene expression is counts, not continuous values. Solutions:</p> <ul> <li>Latent diffusion + NB/ZINB decoder (recommended)</li> <li>Log-transform (simple baseline)</li> <li>Discrete diffusion (future research)</li> </ul>"},{"location":"incubation/#3-hybrid-predictive-generative-models","title":"3. Hybrid Predictive-Generative Models","text":"<p>Combine supervised predictors (like GEM-1) with generative wrappers:</p> <ul> <li>Predictor learns conditional mean</li> <li>Generative model learns residual distribution</li> <li>Enables uncertainty quantification</li> </ul>"},{"location":"incubation/#4-joint-latent-spaces","title":"4. Joint Latent Spaces","text":"<p>Static (bulk RNA-seq) and dynamic (Perturb-seq) data can share the same manifold:</p> <ul> <li>JEPA for predicting embeddings</li> <li>Rectified flow for generation</li> <li>Patch-n-Pack for heterogeneous batching</li> </ul>"},{"location":"incubation/#research-directions","title":"Research Directions","text":"<p>Near-term:</p> <ul> <li>Latent rectified flow for gene expression</li> <li>Set Transformer for expression (permutation-invariant)</li> </ul> <p>Medium-term:</p> <ul> <li>Mamba backbone for perturb-seq</li> <li>Hybrid architectures (SSM + Transformer)</li> </ul> <p>Long-term:</p> <ul> <li>When does tokenization help vs hurt?</li> <li>Biological priors in architecture design</li> </ul>"},{"location":"incubation/#related-documentation","title":"Related Documentation","text":"<ul> <li>ROADMAP.md \u2014 Project learning progression</li> <li>Rectified Flow Tutorial</li> <li>Diffusion Transformer Tutorial</li> </ul>"},{"location":"incubation/alternative_backbones_for_biology/","title":"Alternative Backbones for Biological Generative Models","text":"<p>This document explores alternatives to Transformers for generative modeling in biology, where tokenization may not be natural. We focus on state-space models (SSMs), the tokenization problem for gene expression, and architectures that respect biological structure.</p>"},{"location":"incubation/alternative_backbones_for_biology/#1-the-core-question","title":"1. The Core Question","text":"<p>Modern generative models (DiT + rectified flow) use Transformers as the backbone. But Transformers require tokenization \u2014 representing data as a sequence of discrete units.</p> <p>For images: Patches work well (16\u00d716 pixels \u2192 token)</p> <p>For text: Subwords work well (BPE, SentencePiece)</p> <p>For gene expression: What's the natural token?</p> <p>This document argues that tokenization is optional and explores alternatives.</p>"},{"location":"incubation/alternative_backbones_for_biology/#2-what-diffusionflow-models-actually-need","title":"2. What Diffusion/Flow Models Actually Need","text":"<p>Strip away the architecture. A diffusion or rectified-flow model needs:</p> \\[ f_\\theta(x_t, t, c) \\rightarrow \\text{vector field} \\] <p>Requirements:</p> <ul> <li>Accept a state representation</li> <li>Condition on time \\(t\\)</li> <li>Optionally condition on context \\(c\\)</li> <li>Output a vector of the same dimensionality as the state</li> </ul> <p>The real requirement:</p> <p>A model capable of learning global dependencies and time-conditioned transformations.</p> <p>Transformers satisfy this \u2014 but they are not unique.</p>"},{"location":"incubation/alternative_backbones_for_biology/#3-state-space-models-as-diffusion-backbones","title":"3. State-Space Models as Diffusion Backbones","text":""},{"location":"incubation/alternative_backbones_for_biology/#why-ssms-are-philosophically-aligned","title":"Why SSMs Are Philosophically Aligned","text":"<p>Rectified flow and diffusion define continuous-time dynamics:</p> \\[ \\frac{dx}{dt} = v_\\theta(x, t) \\] <p>State-space models are literally designed to model dynamics:</p> \\[ \\frac{dh}{dt} = Ah + Bx, \\quad y = Ch + Dx \\] <p>This is not a coincidence \u2014 both frameworks think in terms of state evolution.</p>"},{"location":"incubation/alternative_backbones_for_biology/#candidate-architectures","title":"Candidate Architectures","text":"Architecture Key Feature Biological Fit S4 Structured state space Long-range dependencies Mamba Selective state spaces Input-dependent dynamics Hyena Implicit long convolutions Efficient sequence modeling HyenaDNA DNA-specific Hyena Genomic sequences"},{"location":"incubation/alternative_backbones_for_biology/#why-transformers-won-historically","title":"Why Transformers Won Historically","text":"<ul> <li>Easy to scale (attention is parallelizable)</li> <li>Clean conditioning via cross-attention</li> <li>Unified modalities early (text, image, audio)</li> <li>Infrastructure exists (FlashAttention, etc.)</li> </ul> <p>But this is historical inertia, not a fundamental requirement.</p>"},{"location":"incubation/alternative_backbones_for_biology/#4-the-tokenization-problem-for-gene-expression","title":"4. The Tokenization Problem for Gene Expression","text":""},{"location":"incubation/alternative_backbones_for_biology/#what-gene-expression-looks-like","title":"What Gene Expression Looks Like","text":"\\[ x \\in \\mathbb{R}^{G} \\] <p>where \\(G \\approx 20,000\\) genes.</p> <p>Properties:</p> <ul> <li>Unordered: No natural sequence (genes have no inherent order)</li> <li>Dense: Most genes have non-zero expression</li> <li>Compositional: Relative, not absolute (TPM sums to 1M)</li> <li>Population-relative: Meaning depends on context</li> </ul>"},{"location":"incubation/alternative_backbones_for_biology/#the-problem-with-genes-as-tokens","title":"The Problem with \"Genes as Tokens\"","text":"<p>Approaches like Geneformer:</p> <ol> <li>Rank genes by expression level</li> <li>Treat ranked genes as a sequence</li> <li>Apply Transformer</li> </ol> <p>This works, but feels ontologically wrong:</p> <p>Ranking genes is not a natural ordering of biological state \u2014 it's an engineering trick.</p> <p>The ranking changes between samples, destroying any consistent \"position\" meaning.</p>"},{"location":"incubation/alternative_backbones_for_biology/#why-this-matters","title":"Why This Matters","text":"<p>If the representation is unnatural:</p> <ul> <li>The model learns spurious patterns</li> <li>Generalization suffers</li> <li>Interpretability is compromised</li> <li>Biological priors are ignored</li> </ul>"},{"location":"incubation/alternative_backbones_for_biology/#5-better-representations-for-gene-expression","title":"5. Better Representations for Gene Expression","text":""},{"location":"incubation/alternative_backbones_for_biology/#option-a-state-vector-no-tokens","title":"Option A: State Vector (No Tokens)","text":"<p>Idea: Treat expression as a single state vector, not a sequence.</p> <pre><code>x_t \u2208 \u211d^G \u2192 MLP/SSM backbone \u2192 v_\u03b8(x_t, t) \u2208 \u211d^G\n</code></pre> <p>Implementation:</p> <ul> <li>Backbone: MLP, SSM, or continuous-time operator</li> <li>Time-conditioning: FiLM modulation</li> <li>No tokenization at all</li> </ul> <p>Why it works:</p> <ul> <li>Aligns with rectified flow (learning velocity in gene-expression space)</li> <li>Respects the unordered nature of genes</li> <li>Simple and honest</li> </ul> <p>Limitation: Scales poorly with \\(G\\) (quadratic in MLP, but SSMs help).</p>"},{"location":"incubation/alternative_backbones_for_biology/#option-b-latent-space-diffusion","title":"Option B: Latent-Space Diffusion","text":"<p>Idea: Don't tokenize raw expression \u2014 tokenize latent representations.</p> <pre><code>Expression \u2192 VAE Encoder \u2192 z \u2208 \u211d^d \u2192 Diffusion \u2192 z' \u2192 VAE Decoder \u2192 Expression\n</code></pre> <p>Why it works:</p> <ul> <li>Latent space is smooth and lower-dimensional</li> <li>No artificial ordering</li> <li>No sparsity pathologies</li> <li>VAE decoder handles count structure (NB/ZINB)</li> </ul> <p>This is where JEPA, VAEs, and diffusion naturally converge.</p> <p>See <code>docs/incubation/generative-ai-for-gene-expression-prediction.md</code> for details on count handling.</p>"},{"location":"incubation/alternative_backbones_for_biology/#option-c-set-based-representations","title":"Option C: Set-Based Representations","text":"<p>Idea: If you must use tokens, respect the unordered structure.</p> <pre><code>Expression \u2192 Gene embeddings \u00d7 expression values \u2192 Set Transformer \u2192 Output\n</code></pre> <p>Implementation:</p> <ul> <li>Genes have learned embeddings</li> <li>Expression value modulates each embedding</li> <li>Use permutation-invariant operators (Set Transformer, DeepSets)</li> <li>Attention without positional encoding</li> </ul> <p>Why it works:</p> <ul> <li>Respects symmetry (gene order doesn't matter)</li> <li>Biologically meaningful (genes are entities, not positions)</li> </ul>"},{"location":"incubation/alternative_backbones_for_biology/#option-d-dynamics-first-ssm-friendly","title":"Option D: Dynamics-First (SSM-Friendly)","text":"<p>Idea: If your data has temporal structure, make time the sequence axis.</p> <pre><code>[x(t\u2081), x(t\u2082), ..., x(t\u2099)] \u2192 SSM backbone \u2192 Predicted dynamics\n</code></pre> <p>Applicable to:</p> <ul> <li>Time-series expression</li> <li>Perturb-seq (pre/post perturbation)</li> <li>Differentiation trajectories</li> <li>Drug response curves</li> </ul> <p>Why SSMs shine here:</p> <ul> <li>Designed for temporal dynamics</li> <li>Efficient for long sequences</li> <li>Natural fit for biological processes</li> </ul>"},{"location":"incubation/alternative_backbones_for_biology/#6-proposed-architecture-latent-rectified-flow-ssm","title":"6. Proposed Architecture: Latent Rectified Flow + SSM","text":"<p>Combining the best ideas:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Training Pipeline                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Expression (counts)                                     \u2502\n\u2502       \u2193                                                  \u2502\n\u2502  log1p + normalize                                       \u2502\n\u2502       \u2193                                                  \u2502\n\u2502  VAE Encoder \u2192 z \u2208 \u211d^d (latent state)                   \u2502\n\u2502       \u2193                                                  \u2502\n\u2502  [z(t\u2081), z(t\u2082), ...] (if temporal) or z (if static)    \u2502\n\u2502       \u2193                                                  \u2502\n\u2502  SSM/Mamba backbone with time modulation                 \u2502\n\u2502       \u2193                                                  \u2502\n\u2502  Rectified flow loss: ||v_\u03b8(z_t, t) - (z\u2081 - z\u2080)||\u00b2     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   Sampling Pipeline                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  z(1) ~ N(0, I)                                         \u2502\n\u2502       \u2193                                                  \u2502\n\u2502  ODE: dz/dt = -v_\u03b8(z, t) from t=1 to t=0               \u2502\n\u2502       \u2193                                                  \u2502\n\u2502  z(0) \u2248 latent data                                     \u2502\n\u2502       \u2193                                                  \u2502\n\u2502  VAE Decoder (NB/ZINB) \u2192 Expression (counts)            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Properties:</p> <ul> <li>No fake tokens or gene ranking</li> <li>Proper count handling via NB/ZINB decoder</li> <li>SSM models temporal dynamics naturally</li> <li>Rectified flow gives simple, stable training</li> </ul>"},{"location":"incubation/alternative_backbones_for_biology/#7-when-tokenization-is-biologically-meaningful","title":"7. When Tokenization IS Biologically Meaningful","text":"<p>Tokenization isn't always wrong. It's appropriate when:</p>"},{"location":"incubation/alternative_backbones_for_biology/#pathways-as-tokens","title":"Pathways as Tokens","text":"<p>Gene sets (pathways, modules) have biological meaning:</p> <pre><code>Expression \u2192 Pathway activity scores \u2192 Tokens\n</code></pre> <p>Each token represents a functional unit (e.g., \"cell cycle\", \"apoptosis\").</p>"},{"location":"incubation/alternative_backbones_for_biology/#cells-as-tokens-single-cell","title":"Cells as Tokens (Single-Cell)","text":"<p>In single-cell data, cells are natural units:</p> <pre><code>[cell\u2081, cell\u2082, ..., cell\u2099] \u2192 Cell embeddings \u2192 Transformer\n</code></pre> <p>Attention between cells captures cell-cell interactions.</p>"},{"location":"incubation/alternative_backbones_for_biology/#genomic-positions-dnarna","title":"Genomic Positions (DNA/RNA)","text":"<p>For sequence data, positions are real:</p> <pre><code>[A, T, G, C, ...] \u2192 Nucleotide embeddings \u2192 Transformer/Hyena\n</code></pre> <p>HyenaDNA works well here because the sequence axis is meaningful.</p>"},{"location":"incubation/alternative_backbones_for_biology/#8-comparison-transformer-vs-ssm-for-biology","title":"8. Comparison: Transformer vs SSM for Biology","text":"Aspect Transformer SSM (Mamba/S4) Sequence assumption Required Optional Temporal dynamics Learned implicitly Native Long-range O(n\u00b2) attention O(n) or O(n log n) Conditioning Cross-attention, AdaLN FiLM, state injection Biological fit Good for cells, pathways Good for trajectories Infrastructure Mature Emerging <p>Recommendation:</p> <ul> <li>Static expression: Latent diffusion with MLP or small Transformer</li> <li>Temporal data: SSM backbone</li> <li>Cell populations: Transformer with cells as tokens</li> <li>Genomic sequences: Hyena/HyenaDNA</li> </ul>"},{"location":"incubation/alternative_backbones_for_biology/#9-research-directions","title":"9. Research Directions","text":""},{"location":"incubation/alternative_backbones_for_biology/#near-term-implementable-now","title":"Near-Term (Implementable Now)","text":"<ol> <li>Latent rectified flow for gene expression</li> <li>VAE with NB decoder + flow matching in latent space</li> <li> <p>Compare with direct diffusion on log-transformed data</p> </li> <li> <p>Set Transformer for expression</p> </li> <li>Permutation-invariant architecture</li> <li>Benchmark against Geneformer-style ranking</li> </ol>"},{"location":"incubation/alternative_backbones_for_biology/#medium-term-requires-development","title":"Medium-Term (Requires Development)","text":"<ol> <li>Mamba backbone for perturb-seq</li> <li>Temporal modeling of perturbation responses</li> <li> <p>Compare with Transformer on prediction tasks</p> </li> <li> <p>Hybrid architectures</p> </li> <li>SSM for temporal dynamics + Transformer for cell interactions</li> <li>Multi-scale modeling</li> </ol>"},{"location":"incubation/alternative_backbones_for_biology/#long-term-research-questions","title":"Long-Term (Research Questions)","text":"<ol> <li>When does tokenization help vs hurt?</li> <li>Systematic comparison across biological data types</li> <li> <p>Theoretical analysis of inductive biases</p> </li> <li> <p>Biological priors in architecture</p> </li> <li>Gene regulatory networks as attention masks</li> <li>Pathway structure as hierarchical tokens</li> </ol>"},{"location":"incubation/alternative_backbones_for_biology/#10-key-takeaways","title":"10. Key Takeaways","text":"<p>Tokenization is a convenience for architectures, not a requirement of the data.</p> <p>For gene expression:</p> <ul> <li>Avoid forcing genes into sequences</li> <li>Prefer latent-space methods</li> <li>Use set-based or state-vector representations</li> <li>Let SSMs handle temporal structure</li> </ul> <p>The organizing principle:</p> <pre><code>Data structure \u2192 Representation \u2192 Architecture\n     \u2193                \u2193              \u2193\n  Unordered      State vector      MLP/SSM\n  Temporal       Sequence          SSM/Mamba\n  Cells          Set of tokens     Set Transformer\n  Genomic        True sequence     Hyena/Transformer\n</code></pre>"},{"location":"incubation/alternative_backbones_for_biology/#references","title":"References","text":"<ul> <li>Gu et al. (2022) - \"Efficiently Modeling Long Sequences with Structured State Spaces\" (S4)</li> <li>Gu &amp; Dao (2023) - \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\"</li> <li>Nguyen et al. (2023) - \"HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution\"</li> <li>Lee et al. (2019) - \"Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks\"</li> <li>Theodoris et al. (2023) - \"Transfer learning enables predictions in network biology\" (Geneformer)</li> </ul>"},{"location":"incubation/alternative_backbones_for_biology/#related-documents","title":"Related Documents","text":"<ul> <li>Rectified Flow Tutorial</li> <li>Diffusion Transformer Tutorial</li> <li>Generative AI for Gene Expression</li> <li>Joint Latent Spaces and JEPA</li> </ul>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/","title":"Generative AI for Gene Expression Prediction: Beyond Point Estimates","text":"<p>A tutorial exploring how diffusion models, VAEs, and flow-based methods can enhance gene expression prediction by modeling uncertainty and biological variability</p>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#introduction","title":"Introduction","text":"<p>Foundation models for gene expression, exemplified by systems like GEM-1, have demonstrated remarkable success at predicting expression profiles from metadata. Given information about tissue type, disease state, age, sex, and experimental conditions, these models can predict the expected expression levels of tens of thousands of genes.</p> <p>This is a significant achievement. But there's a fundamental limitation: these models typically output a single prediction\u2014a point estimate representing the expected (mean) expression profile. In biological systems, however, the same conditions can produce diverse outcomes due to:</p> <ul> <li>Biological stochasticity - Gene expression is inherently noisy</li> <li>Cell-to-cell variability - Even genetically identical cells differ</li> <li>Population heterogeneity - Different individuals respond differently</li> <li>Temporal dynamics - Expression changes over time in unpredictable ways</li> <li>Unmeasured confounders - Hidden variables we don't observe</li> </ul> <p>This tutorial explores how generative AI methods\u2014diffusion models, variational autoencoders (VAEs), and normalizing flows\u2014can address this limitation by learning to predict not just a single outcome, but a distribution of plausible outcomes.</p>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#the-limitation-of-point-predictions","title":"The Limitation of Point Predictions","text":""},{"location":"incubation/generative-ai-for-gene-expression-prediction/#what-current-models-predict","title":"What Current Models Predict","text":"<p>State-of-the-art gene expression predictors learn a function:</p> \\[ \\hat{x} = f_\\theta(\\text{metadata}) \\] <p>where \\(\\hat{x}\\) is a vector of predicted gene expression values (e.g., 20,000 genes), and metadata includes tissue type, cell type, disease, age, sex, perturbations, and technical factors.</p> <p>During training, the model minimizes prediction error:</p> \\[ \\mathcal{L} = \\mathbb{E}_{(x, c) \\sim \\text{data}} \\left[ \\| f_\\theta(c) - x \\|^2 \\right] \\] <p>This objective pushes the model to predict the conditional mean: \\(f_\\theta(c) \\approx \\mathbb{E}[x \\mid c]\\).</p>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#why-this-is-insufficient","title":"Why This Is Insufficient","text":"<p>Consider predicting gene expression for \"healthy human liver tissue from a 45-year-old female.\" The model might predict:</p> <pre><code>Gene A: 5.2 (log TPM)\nGene B: 8.7\nGene C: 3.1\n...\n</code></pre> <p>But in reality, if we measured 100 different individuals matching this description, we'd see:</p> <pre><code>Gene A: 4.8, 5.1, 5.5, 4.9, 5.3, 5.0, 5.4, ... (mean \u2248 5.2, std \u2248 0.3)\nGene B: 8.5, 8.9, 8.6, 8.8, 8.7, 8.4, 9.0, ... (mean \u2248 8.7, std \u2248 0.2)\nGene C: 2.9, 3.3, 3.0, 3.2, 3.1, 2.8, 3.4, ... (mean \u2248 3.1, std \u2248 0.2)\n</code></pre> <p>The point prediction captures the mean but loses critical information: - How confident should we be? (Gene B is more consistent than Gene A) - Are there subpopulations? (Bimodal distributions) - What's the range of normal variation? (For quality control) - How rare is an observed value? (For anomaly detection)</p>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#real-world-consequences","title":"Real-World Consequences","text":"<p>Drug development: A drug that works on the \"average\" patient may fail for 30% of the population due to expression variability.</p> <p>Disease diagnosis: A biomarker at the 90<sup>th</sup> percentile of healthy variation might be misclassified as pathological without knowing the distribution.</p> <p>Experimental design: Power calculations require variance estimates, not just means.</p> <p>Data augmentation: Training downstream models (e.g., disease classifiers) benefits from diverse synthetic samples, not repeated copies of the mean.</p>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#what-generative-models-offer","title":"What Generative Models Offer","text":"<p>Generative models learn the full conditional distribution:</p> \\[ p_\\theta(x \\mid \\text{metadata}) \\] <p>This allows us to: 1. Sample multiple plausible expression profiles for the same condition 2. Quantify uncertainty via sample variance or entropy 3. Detect outliers by computing likelihood of observed data 4. Generate diverse synthetic data for augmentation 5. Model rare events in the tails of the distribution</p>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#three-generative-approaches","title":"Three Generative Approaches","text":"Method Core Idea Strengths Challenges Diffusion Models Learn to denoise corrupted data High sample quality, flexible Slow sampling (many steps) VAEs Learn latent representation + decoder Fast sampling, interpretable latent space Can produce blurry samples Normalizing Flows Learn invertible transformation Exact likelihood, fast sampling Architectural constraints"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#approach-1-conditional-diffusion-models","title":"Approach 1: Conditional Diffusion Models","text":""},{"location":"incubation/generative-ai-for-gene-expression-prediction/#core-concept","title":"Core Concept","text":"<p>Diffusion models learn to reverse a gradual noising process. For gene expression prediction:</p> <ol> <li>Forward process: Take a real expression profile \\(x_0\\) and gradually add noise until it becomes pure Gaussian noise \\(x_T\\)</li> <li>Reverse process: Learn a neural network that can denoise \\(x_T\\) back to \\(x_0\\), conditioned on metadata</li> </ol> <p>At inference time, we start with random noise and iteratively denoise it, guided by the metadata, to generate a plausible expression profile.</p>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#mathematical-framework","title":"Mathematical Framework","text":"<p>The forward process is defined by a stochastic differential equation (SDE):</p> \\[ dx = f(x, t) dt + g(t) dW \\] <p>where \\(f\\) is the drift, \\(g\\) is the diffusion coefficient, and \\(W\\) is a Wiener process.</p> <p>The reverse process learns the score function \\(\\nabla_x \\log p_t(x \\mid c)\\):</p> \\[ dx = [f(x, t) - g(t)^2 \\nabla_x \\log p_t(x \\mid c)] dt + g(t) d\\bar{W} \\]"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#architecture-for-gene-expression","title":"Architecture for Gene Expression","text":"<pre><code>class GeneExpressionDiffusion(nn.Module):\n    def __init__(self, n_genes=20000, metadata_dim=128):\n        super().__init__()\n\n        # Metadata encoder\n        self.metadata_encoder = nn.Sequential(\n            nn.Linear(metadata_dim, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512)\n        )\n\n        # Time embedding\n        self.time_mlp = nn.Sequential(\n            SinusoidalPositionEmbedding(256),\n            nn.Linear(256, 512),\n            nn.ReLU()\n        )\n\n        # Score network (U-Net style for gene programs)\n        self.score_net = nn.Sequential(\n            # Encoder\n            nn.Linear(n_genes, 4096),\n            nn.LayerNorm(4096),\n            nn.ReLU(),\n            nn.Linear(4096, 2048),\n            nn.LayerNorm(2048),\n            nn.ReLU(),\n            # Conditioning injection\n            ConditionedLayer(2048, 512),  # metadata + time\n            # Decoder\n            nn.Linear(2048, 4096),\n            nn.LayerNorm(4096),\n            nn.ReLU(),\n            nn.Linear(4096, n_genes)\n        )\n\n    def forward(self, x_t, t, metadata):\n        # Encode metadata and time\n        meta_emb = self.metadata_encoder(metadata)\n        time_emb = self.time_mlp(t)\n        condition = meta_emb + time_emb\n\n        # Predict score\n        score = self.score_net(x_t, condition)\n        return score\n</code></pre>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#training-objective","title":"Training Objective","text":"<p>The model is trained with denoising score matching:</p> \\[ \\mathcal{L} = \\mathbb{E}_{t, x_0, \\epsilon, c} \\left[ \\lambda(t) \\| s_\\theta(x_t, t, c) - \\nabla_{x_t} \\log p(x_t \\mid x_0) \\|^2 \\right] \\] <p>where:</p> <ul> <li>\\(t \\sim \\text{Uniform}(0, T)\\) is a random timestep</li> <li>\\(x_0 \\sim p_{\\text{data}}(x \\mid c)\\) is a real expression profile</li> <li>\\(\\epsilon \\sim \\mathcal{N}(0, I)\\) is random noise</li> <li>\\(x_t = \\alpha_t x_0 + \\sigma_t \\epsilon\\) is the noised version</li> <li>\\(\\lambda(t)\\) is a weighting function</li> </ul>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#sampling-process","title":"Sampling Process","text":"<p>To generate a new expression profile:</p> <pre><code>def sample(model, metadata, n_steps=1000):\n    # Start from pure noise\n    x = torch.randn(n_genes)\n\n    dt = 1.0 / n_steps\n    for i in range(n_steps):\n        t = 1.0 - i * dt\n\n        # Predict score\n        score = model(x, t, metadata)\n\n        # Euler-Maruyama step\n        drift = f(x, t) - g(t)**2 * score\n        diffusion = g(t) * torch.randn_like(x) * sqrt(dt)\n\n        x = x + drift * dt + diffusion\n\n    return x\n</code></pre>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#advantages-for-gene-expression","title":"Advantages for Gene Expression","text":"<ol> <li>High-quality samples - Captures complex correlations between genes</li> <li>Flexible conditioning - Can condition on any metadata combination</li> <li>Uncertainty quantification - Sample variance reflects prediction uncertainty</li> <li>Handles high dimensions - Works well with 20,000+ genes</li> <li>No mode collapse - Unlike GANs, explores full distribution</li> </ol>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#challenges","title":"Challenges","text":"<ol> <li>Slow sampling - Requires 100-1000 denoising steps</li> <li>Computational cost - Training requires many forward passes</li> <li>Hyperparameter sensitivity - Noise schedule, network architecture matter</li> <li>Validation - How to evaluate sample quality for gene expression?</li> <li>Count data - Gene expression is counts, not continuous (see below)</li> </ol>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#handling-count-data-in-diffusion-models","title":"Handling Count Data in Diffusion Models","text":"<p>A fundamental challenge for applying diffusion models to gene expression is that gene expression data consists of counts (TPM, UMI counts), not continuous values like image pixels. Diffusion models assume continuous data where adding Gaussian noise is semantically meaningful.</p>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#the-problem","title":"The Problem","text":"<p>Raw gene expression matrices contain: - Integer counts (UMI-based scRNA-seq) - TPM/FPKM (normalized but still count-derived) - Sparse zeros (dropout in scRNA-seq) - Heavy-tailed distributions (few highly expressed genes)</p> <p>Adding Gaussian noise to counts doesn't have clear biological meaning\u2014what does \"gene X expression = 5.3 \u00b1 0.7 noise\" mean?</p>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#solution-1-latent-diffusion-count-aware-decoder-recommended","title":"Solution 1: Latent Diffusion + Count-Aware Decoder (Recommended)","text":"<p>The standard approach combines two components that work together:</p> <ol> <li>Latent Diffusion: Run diffusion in a learned continuous latent space (not raw counts)</li> <li>Count-Aware Decoder: VAE decoder outputs NB/ZINB distribution parameters (not point estimates)</li> </ol> <pre><code>Counts \u2192 log1p \u2192 Encoder \u2192 z (continuous) \u2192 Diffusion \u2192 z' \u2192 NB Decoder \u2192 NB(\u03bc,\u03b8) \u2192 Sample counts\n</code></pre> <p>Why both are needed:</p> <ul> <li>Latent space makes diffusion well-defined (continuous, bounded, smooth)</li> <li>NB/ZINB decoder ensures outputs respect count statistics (non-negative, overdispersed, sparse)</li> </ul> <p>Implementation (see <code>notebooks/diffusion/04_gene_expression_diffusion/</code>):</p> <pre><code># 1. Train VAE with NB decoder (not MSE!)\nvae = GeneVAE_NB(n_genes=20000, latent_dim=128)\n# Encoder: counts \u2192 latent\n# Decoder: latent \u2192 NB(\u03bc, \u03b8) parameters\n\n# 2. Train with NB reconstruction loss\nloss = elbo_loss_nb(x=counts, mu=dec_mu, theta=dec_theta, \n                    enc_mu=enc_mu, enc_logvar=enc_logvar)\n\n# 3. Extract latent representations for diffusion\nwith torch.no_grad():\n    latent_data = vae.encode(log1p(counts))[0]  # Get mu\n\n# 4. Train diffusion in latent space\ndiffusion = train_diffusion(latent_data)  # Standard continuous diffusion\n\n# 5. Sample: noise \u2192 latent \u2192 NB params \u2192 counts\nz_samples = diffusion.sample(n=100)\nmu, theta = vae.decode(z_samples)  # NB parameters\ncount_samples = sample_negative_binomial(mu, theta)  # Stochastic sampling\n</code></pre> <p>The NB/ZINB Decoder (see <code>src/genailab/model/decoders.py</code>):</p> <pre><code>class NegativeBinomialDecoder(nn.Module):\n    \"\"\"Outputs NB(\u03bc, \u03b8) parameters instead of point estimates.\"\"\"\n\n    def forward(self, z, library_size=None):\n        # Rate (softmax ensures non-negative, sums to 1)\n        rho = F.softmax(self.rho_head(z), dim=-1)\n\n        # Scale by library size\n        mu = rho * library_size  # Expected counts\n\n        # Dispersion (learned per-gene)\n        theta = torch.exp(self.log_theta)\n\n        return {\"mu\": mu, \"theta\": theta}\n</code></pre> <p>Zero-Inflated NB (ZINB) Decoder for sparse scRNA-seq:</p> <pre><code>class ZINBDecoder(NegativeBinomialDecoder):\n    \"\"\"Adds dropout probability \u03c0 for excess zeros.\"\"\"\n\n    def forward(self, z, library_size=None):\n        out = super().forward(z, library_size)\n        out[\"pi\"] = torch.sigmoid(self.pi_head(z))  # Dropout prob\n        return out\n</code></pre> <p>Loss functions (see <code>src/genailab/objectives/losses.py</code>):</p> <pre><code># NB negative log-likelihood\ndef nb_loss(x, mu, theta):\n    return -NegativeBinomial(mu, theta).log_prob(x).mean()\n\n# ZINB for sparse data\ndef zinb_loss(x, mu, theta, pi):\n    # Zero case: log(\u03c0 + (1-\u03c0) * NB(0))\n    # Non-zero case: log((1-\u03c0) * NB(x))\n    ...\n</code></pre>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#solution-2-log-transform-clip-simple-baseline","title":"Solution 2: Log-Transform + Clip (Simple Baseline)","text":"<p>Simple but loses count semantics:</p> <pre><code># Preprocessing\nx_continuous = np.log1p(counts)  # log(1 + x) transform\nx_normalized = (x_continuous - mean) / std\n\n# Train diffusion on normalized data\ndiffusion = train_diffusion(x_normalized)\n\n# Postprocessing\nsamples_normalized = diffusion.sample()\nsamples_continuous = samples_normalized * std + mean\nsamples_counts = np.expm1(samples_continuous).clip(0)  # exp(x) - 1, clip negatives\n</code></pre> <p>Limitations: Loses discrete structure, can produce non-integer \"counts.\"</p>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#solution-3-discrete-diffusion-d3pm","title":"Solution 3: Discrete Diffusion (D3PM)","text":"<p>For true count modeling, use discrete diffusion:</p> <pre><code># D3PM: Diffusion on discrete tokens\n# Transition matrix instead of Gaussian noise\nQ_t = (1 - \u03b2_t) * I + \u03b2_t * uniform_matrix\n\n# Forward: gradually corrupt to uniform distribution\n# Reverse: learn to denoise discrete tokens\n</code></pre> <p>Status: Not yet implemented in genai-lab. See Austin et al. (2021) \"Structured Denoising Diffusion Models in Discrete State-Spaces.\"</p>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#summary-approaches-compared","title":"Summary: Approaches Compared","text":"Approach Description Pros Cons Use When Latent Diffusion + NB/ZINB Diffusion in VAE latent space with count-aware decoder Proper count model, well-defined diffusion Requires VAE training Default choice Log-transform only Diffusion on log1p(counts), clip output Simple, fast Loses count structure Quick prototyping Discrete diffusion (D3PM) Diffusion directly on discrete counts True count model Complex, slow, less mature Research applications <p>Implementation in genai-lab:</p> <ul> <li><code>src/genailab/model/decoders.py</code>: <code>NegativeBinomialDecoder</code>, <code>ZINBDecoder</code></li> <li><code>src/genailab/objectives/losses.py</code>: <code>nb_loss()</code>, <code>zinb_loss()</code>, <code>elbo_loss_nb()</code>, <code>elbo_loss_zinb()</code></li> <li><code>notebooks/diffusion/04_gene_expression_diffusion/</code>: Latent diffusion example</li> </ul>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#approach-2-conditional-variational-autoencoders-cvae","title":"Approach 2: Conditional Variational Autoencoders (cVAE)","text":""},{"location":"incubation/generative-ai-for-gene-expression-prediction/#core-concept_1","title":"Core Concept","text":"<p>VAEs learn a compressed latent representation of gene expression profiles. For conditional prediction:</p> <ol> <li>Encoder: Maps \\((x, c)\\) to latent distribution \\(q_\\phi(z \\mid x, c)\\)</li> <li>Decoder: Maps \\((z, c)\\) to reconstructed expression \\(p_\\theta(x \\mid z, c)\\)</li> <li>Prior: Learns \\(p(z \\mid c)\\) to sample without observing \\(x\\)</li> </ol>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#mathematical-framework_1","title":"Mathematical Framework","text":"<p>The VAE optimizes the Evidence Lower Bound (ELBO):</p> \\[ \\mathcal{L}_{\\text{ELBO}} = \\mathbb{E}_{q_\\phi(z \\mid x, c)} [\\log p_\\theta(x \\mid z, c)] - \\text{KL}(q_\\phi(z \\mid x, c) \\| p(z \\mid c)) \\] <p>The first term is reconstruction loss (how well can we decode \\(z\\) back to \\(x\\)). The second term is KL divergence (how close is the learned latent distribution to the prior).</p>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#architecture-for-gene-expression_1","title":"Architecture for Gene Expression","text":"<pre><code>class GeneExpressionVAE(nn.Module):\n    def __init__(self, n_genes=20000, latent_dim=128, metadata_dim=64):\n        super().__init__()\n\n        # Encoder: (x, metadata) -&gt; z\n        self.encoder = nn.Sequential(\n            nn.Linear(n_genes + metadata_dim, 4096),\n            nn.BatchNorm1d(4096),\n            nn.ReLU(),\n            nn.Linear(4096, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Linear(1024, 256),\n            nn.ReLU()\n        )\n        self.fc_mu = nn.Linear(256, latent_dim)\n        self.fc_logvar = nn.Linear(256, latent_dim)\n\n        # Decoder: (z, metadata) -&gt; x\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim + metadata_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Linear(1024, 4096),\n            nn.BatchNorm1d(4096),\n            nn.ReLU(),\n            nn.Linear(4096, n_genes)\n        )\n\n        # Prior network: metadata -&gt; p(z|c)\n        self.prior_net = nn.Sequential(\n            nn.Linear(metadata_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU()\n        )\n        self.prior_mu = nn.Linear(256, latent_dim)\n        self.prior_logvar = nn.Linear(256, latent_dim)\n\n    def encode(self, x, metadata):\n        h = self.encoder(torch.cat([x, metadata], dim=-1))\n        return self.fc_mu(h), self.fc_logvar(h)\n\n    def decode(self, z, metadata):\n        return self.decoder(torch.cat([z, metadata], dim=-1))\n\n    def prior(self, metadata):\n        h = self.prior_net(metadata)\n        return self.prior_mu(h), self.prior_logvar(h)\n\n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, x, metadata):\n        # Encode\n        mu_post, logvar_post = self.encode(x, metadata)\n        z = self.reparameterize(mu_post, logvar_post)\n\n        # Decode\n        x_recon = self.decode(z, metadata)\n\n        # Prior\n        mu_prior, logvar_prior = self.prior(metadata)\n\n        return x_recon, mu_post, logvar_post, mu_prior, logvar_prior\n</code></pre>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#training-objective_1","title":"Training Objective","text":"<pre><code>def vae_loss(x, x_recon, mu_post, logvar_post, mu_prior, logvar_prior):\n    # Reconstruction loss (negative log-likelihood)\n    recon_loss = F.mse_loss(x_recon, x, reduction='sum')\n\n    # KL divergence between posterior and prior\n    kl_div = -0.5 * torch.sum(\n        1 + logvar_post - logvar_prior\n        - ((mu_post - mu_prior)**2 + logvar_post.exp()) / logvar_prior.exp()\n    )\n\n    return recon_loss + kl_div\n</code></pre>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#sampling-process_1","title":"Sampling Process","text":"<p>To generate a new expression profile:</p> <pre><code>def sample(model, metadata, n_samples=10):\n    # Sample from learned prior\n    mu_prior, logvar_prior = model.prior(metadata)\n    z = model.reparameterize(mu_prior, logvar_prior)\n\n    # Decode to gene expression\n    x = model.decode(z, metadata)\n    return x\n</code></pre>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#advantages-for-gene-expression_1","title":"Advantages for Gene Expression","text":"<ol> <li>Fast sampling - Single forward pass through decoder</li> <li>Interpretable latent space - Can explore gene programs in \\(z\\)</li> <li>Explicit likelihood - Can compute \\(p(x \\mid c)\\) for anomaly detection</li> <li>Disentanglement - Can learn separate latent factors for different biological processes</li> <li>Conditional prior - Learns what's plausible for each condition</li> </ol>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#challenges_1","title":"Challenges","text":"<ol> <li>Posterior collapse - Decoder may ignore latent code</li> <li>Blurry samples - MSE loss encourages averaging</li> <li>Limited expressiveness - Gaussian assumptions may be too restrictive</li> <li>Latent dimension selection - Too small loses information, too large is hard to train</li> </ol>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#enhancements","title":"Enhancements","text":"<p>\u03b2-VAE for disentanglement:</p> \\[ \\mathcal{L} = \\text{Reconstruction} + \\beta \\cdot \\text{KL} \\] <p>Hierarchical VAE for multi-scale structure:</p> \\[ z = [z_{\\text{cell type}}, z_{\\text{state}}, z_{\\text{technical}}] \\] <p>Mixture-of-Gaussians decoder for multimodal distributions:</p> \\[ p(x \\mid z, c) = \\sum_{k=1}^K \\pi_k(z, c) \\cdot \\mathcal{N}(x \\mid \\mu_k(z, c), \\Sigma_k(z, c)) \\]"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#approach-3-normalizing-flows","title":"Approach 3: Normalizing Flows","text":""},{"location":"incubation/generative-ai-for-gene-expression-prediction/#core-concept_2","title":"Core Concept","text":"<p>Normalizing flows learn an invertible transformation between a simple base distribution (e.g., Gaussian) and the complex data distribution. For gene expression:</p> \\[ x = f_\\theta(z, c), \\quad z \\sim \\mathcal{N}(0, I) \\] <p>where \\(f_\\theta\\) is invertible, allowing exact likelihood computation:</p> \\[ \\log p(x \\mid c) = \\log p(z) + \\log \\left| \\det \\frac{\\partial f_\\theta^{-1}}{\\partial x} \\right| \\]"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#architecture-continuous-normalizing-flows-cnf","title":"Architecture: Continuous Normalizing Flows (CNF)","text":"<p>Instead of discrete transformations, CNF uses neural ODEs:</p> \\[ \\frac{dx}{dt} = f_\\theta(x, t, c) \\] <p>The likelihood is computed via the instantaneous change of variables:</p> \\[ \\log p(x \\mid c) = \\log p(z) - \\int_0^1 \\text{Tr}\\left( \\frac{\\partial f_\\theta}{\\partial x} \\right) dt \\]"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#implementation","title":"Implementation","text":"<pre><code>class GeneExpressionFlow(nn.Module):\n    def __init__(self, n_genes=20000, metadata_dim=64, hidden_dim=512):\n        super().__init__()\n\n        # Dynamics network\n        self.dynamics = nn.Sequential(\n            nn.Linear(n_genes + metadata_dim + 1, hidden_dim),  # +1 for time\n            nn.Softplus(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.Softplus(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.Softplus(),\n            nn.Linear(hidden_dim, n_genes)\n        )\n\n    def forward(self, x, t, metadata):\n        # Concatenate inputs\n        inputs = torch.cat([x, metadata, t.unsqueeze(-1)], dim=-1)\n        return self.dynamics(inputs)\n\n    def sample(self, metadata, n_samples=1):\n        # Start from base distribution\n        z = torch.randn(n_samples, self.n_genes)\n\n        # Integrate ODE forward\n        from torchdiffeq import odeint\n        x = odeint(\n            lambda t, x: self.forward(x, t, metadata),\n            z,\n            torch.tensor([0.0, 1.0])\n        )[-1]\n\n        return x\n\n    def log_prob(self, x, metadata):\n        # Integrate ODE backward to get z\n        z, log_det = odeint_with_logdet(\n            lambda t, state: self.forward(state[0], t, metadata),\n            (x, torch.zeros(x.shape[0])),\n            torch.tensor([1.0, 0.0])\n        )\n\n        # Compute log probability\n        log_pz = -0.5 * (z**2).sum(dim=-1) - 0.5 * self.n_genes * np.log(2 * np.pi)\n        return log_pz + log_det\n</code></pre>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#training-objective_2","title":"Training Objective","text":"<p>Maximize likelihood:</p> \\[ \\mathcal{L} = \\mathbb{E}_{(x, c) \\sim \\text{data}} [\\log p_\\theta(x \\mid c)] \\]"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#advantages-for-gene-expression_2","title":"Advantages for Gene Expression","text":"<ol> <li>Exact likelihood - No variational approximation</li> <li>Flexible architecture - Can model complex dependencies</li> <li>Invertible - Can go from data to latent and back</li> <li>Fast sampling - Single ODE solve (with adaptive step size)</li> <li>Density estimation - Can detect out-of-distribution samples</li> </ol>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#challenges_2","title":"Challenges","text":"<ol> <li>Training instability - ODE solvers can be finicky</li> <li>Computational cost - Trace computation is expensive for high dimensions</li> <li>Expressiveness vs efficiency - Trade-off between model capacity and speed</li> <li>Architectural constraints - Must maintain invertibility</li> </ol>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#hybrid-approach-predictive-foundation-generative-wrapper","title":"Hybrid Approach: Predictive Foundation + Generative Wrapper","text":""},{"location":"incubation/generative-ai-for-gene-expression-prediction/#the-best-of-both-worlds","title":"The Best of Both Worlds","text":"<p>Rather than replacing supervised predictors with generative models, we can combine them:</p> <p>Stage 1: Learn the conditional mean (GEM-1 style)</p> \\[ \\mu(c) = f_{\\text{pred}}(c) \\] <p>Train a large-scale supervised model on harmonized data to predict \\(\\mathbb{E}[x \\mid c]\\).</p> <p>Stage 2: Learn the residual distribution (generative)</p> \\[ p(r \\mid c) = p(x - \\mu(c) \\mid c) \\] <p>Train a generative model on the residuals \\(r = x - \\mu(c)\\).</p>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#why-this-works","title":"Why This Works","text":"<ol> <li>Residuals are simpler - Centered at zero, smaller variance</li> <li>Separates signal from noise - Mean captures biology, residuals capture variability</li> <li>Leverages both paradigms - Supervised for accuracy, generative for uncertainty</li> <li>Modular - Can improve each component independently</li> <li>Interpretable - Mean is the \"best guess,\" residuals quantify confidence</li> </ol>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#implementation_1","title":"Implementation","text":"<pre><code>class HybridGeneExpressionModel:\n    def __init__(self, n_genes=20000):\n        # Stage 1: Predictive model (can be pre-trained GEM-1)\n        self.mean_predictor = LargeScalePredictiveModel(n_genes)\n\n        # Stage 2: Diffusion on residuals\n        self.residual_diffusion = ResidualDiffusion(n_genes)\n\n    def predict_mean(self, metadata):\n        \"\"\"Deterministic prediction\"\"\"\n        return self.mean_predictor(metadata)\n\n    def sample(self, metadata, n_samples=10):\n        \"\"\"Stochastic prediction with uncertainty\"\"\"\n        # Get mean prediction\n        mu = self.mean_predictor(metadata)\n\n        # Sample residuals\n        residuals = self.residual_diffusion.sample(metadata, n_samples)\n\n        # Combine\n        samples = mu.unsqueeze(0) + residuals\n        return samples\n\n    def predict_with_confidence(self, metadata, n_samples=100):\n        \"\"\"Return mean and confidence intervals\"\"\"\n        samples = self.sample(metadata, n_samples)\n\n        mean = samples.mean(dim=0)\n        std = samples.std(dim=0)\n\n        return {\n            'mean': mean,\n            'std': std,\n            'ci_lower': mean - 1.96 * std,  # 95% CI\n            'ci_upper': mean + 1.96 * std,\n            'samples': samples\n        }\n</code></pre>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#training-strategy","title":"Training Strategy","text":"<pre><code># Stage 1: Train predictive model\npredictor = train_predictive_model(data, metadata)\n\n# Stage 2: Compute residuals and train generative model\nresiduals = []\nfor x, c in data:\n    mu = predictor(c)\n    r = x - mu\n    residuals.append((r, c))\n\nresidual_model = train_diffusion_model(residuals)\n</code></pre>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#practical-considerations","title":"Practical Considerations","text":""},{"location":"incubation/generative-ai-for-gene-expression-prediction/#data-requirements","title":"Data Requirements","text":"<p>Diffusion models: Need large datasets (100K+ samples) to learn high-dimensional distributions.</p> <p>VAEs: More data-efficient, can work with 10K+ samples.</p> <p>Flows: Similar to VAEs, but benefit from more data for complex distributions.</p> <p>Hybrid approach: Leverages existing predictive models, needs less data for residual modeling.</p>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#computational-resources","title":"Computational Resources","text":"<p>Training:</p> <ul> <li>Diffusion: 1-2 weeks on 4x A100 GPUs for 20K genes</li> <li>VAE: 2-3 days on 1x A100 GPU</li> <li>Flow: 3-5 days on 1x A100 GPU</li> </ul> <p>Inference:</p> <ul> <li>Diffusion: ~1 second per sample (1000 steps)</li> <li>VAE: ~10ms per sample (single forward pass)</li> <li>Flow: ~100ms per sample (ODE solve)</li> </ul>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#validation-strategies","title":"Validation Strategies","text":"<p>Since we can't directly observe the \"true\" distribution, we use proxy metrics:</p> <ol> <li>Reconstruction quality: Can the model reconstruct held-out samples?</li> <li>Sample diversity: Do generated samples cover the observed variance?</li> <li>Biological consistency: Do samples respect known gene-gene correlations?</li> <li>Downstream performance: Do synthetic samples improve downstream tasks?</li> <li>Expert evaluation: Do biologists find samples plausible?</li> </ol>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#when-to-use-each-approach","title":"When to Use Each Approach","text":"<p>Use diffusion models when:</p> <ul> <li>Sample quality is critical</li> <li>You have large datasets</li> <li>Computational resources are available</li> <li>You need flexible conditioning</li> </ul> <p>Use VAEs when:</p> <ul> <li>Fast sampling is required</li> <li>You want interpretable latent space</li> <li>You need explicit likelihood</li> <li>You want to explore latent factors</li> </ul> <p>Use flows when:</p> <ul> <li>Exact likelihood is important</li> <li>You need invertibility</li> <li>You have moderate-sized datasets</li> <li>You want density estimation</li> </ul> <p>Use hybrid approach when:</p> <ul> <li>You have a good predictive model already</li> <li>You want to add uncertainty quantification</li> <li>You need both accuracy and diversity</li> <li>You want modular, interpretable system</li> </ul>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#case-study-predicting-tissue-specific-expression","title":"Case Study: Predicting Tissue-Specific Expression","text":""},{"location":"incubation/generative-ai-for-gene-expression-prediction/#problem-setup","title":"Problem Setup","text":"<p>Task: Predict gene expression for different human tissues given metadata (age, sex, disease status).</p> <p>Data: GTEx (17,382 samples, 54 tissues, 56,200 genes)</p> <p>Baseline: Supervised model predicting \\(\\mathbb{E}[x \\mid \\text{tissue}, \\text{age}, \\text{sex}]\\)</p>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#approach-conditional-vae","title":"Approach: Conditional VAE","text":"<p>We train a cVAE with: - Latent dimension: 128 - Encoder/decoder: 4-layer MLPs with 2048 hidden units - Conditional prior: Learns \\(p(z \\mid \\text{tissue}, \\text{age}, \\text{sex})\\)</p>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#results","title":"Results","text":"<p>Quantitative:</p> <ul> <li>Reconstruction MSE: 0.42 (vs 0.38 for supervised baseline)</li> <li>Sample diversity: Captures 87% of observed variance</li> <li>Likelihood: -12,450 nats (indicates good density estimation)</li> </ul> <p>Qualitative:</p> <ul> <li>Generated samples respect tissue-specific gene programs</li> <li>Age-related changes are smooth and biologically plausible</li> <li>Rare cell types (e.g., pancreatic islets) are well-represented</li> </ul>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#key-insights","title":"Key Insights","text":"<ol> <li>Uncertainty varies by gene: Housekeeping genes have low variance, tissue-specific genes have high variance</li> <li>Metadata matters: Age and sex explain ~5% of variance, tissue explains ~60%</li> <li>Latent space is interpretable: Dimensions correspond to known biological processes</li> <li>Synthetic data improves downstream tasks: Training a disease classifier on real + synthetic data improves F1 by 8%</li> </ol>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#conclusion","title":"Conclusion","text":"<p>Generative AI offers significant value for gene expression prediction by moving beyond point estimates to model the full distribution of plausible outcomes. The three main approaches\u2014diffusion models, VAEs, and normalizing flows\u2014each have distinct advantages:</p> <ul> <li>Diffusion models excel at sample quality and flexibility</li> <li>VAEs provide fast sampling and interpretable latent spaces</li> <li>Normalizing flows offer exact likelihoods and invertibility</li> </ul> <p>For practical applications, a hybrid approach combining a predictive foundation model (like GEM-1) with a generative wrapper for residuals offers the best balance of accuracy, uncertainty quantification, and computational efficiency.</p> <p>The key insight is that generative models are complementary, not competitive with supervised predictors. They add: - Uncertainty quantification for risk assessment - Diverse synthetic data for augmentation - Outlier detection via likelihood - Exploration of biological variability</p> <p>As foundation models for biology continue to scale, integrating generative capabilities will be essential for moving from \"what is the expected outcome?\" to \"what are all the possible outcomes?\"\u2014a critical distinction for translating predictions into clinical and experimental decisions.</p>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#further-reading","title":"Further Reading","text":""},{"location":"incubation/generative-ai-for-gene-expression-prediction/#foundational-papers","title":"Foundational Papers","text":"<p>Diffusion Models:</p> <ul> <li>Ho et al. (2020) - \"Denoising Diffusion Probabilistic Models\"</li> <li>Song et al. (2021) - \"Score-Based Generative Modeling through SDEs\"</li> </ul> <p>VAEs:</p> <ul> <li>Kingma &amp; Welling (2014) - \"Auto-Encoding Variational Bayes\"</li> <li>Sohn et al. (2015) - \"Learning Structured Output Representation using Deep Conditional Generative Models\"</li> </ul> <p>Normalizing Flows:</p> <ul> <li>Chen et al. (2018) - \"Neural Ordinary Differential Equations\"</li> <li>Grathwohl et al. (2019) - \"FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models\"</li> </ul>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#applications-to-biology","title":"Applications to Biology","text":"<p>Single-cell RNA-seq:</p> <ul> <li>Lopez et al. (2018) - \"Deep generative modeling for single-cell transcriptomics\" (scVI)</li> <li>Lotfollahi et al. (2020) - \"scGen predicts single-cell perturbation responses\"</li> </ul> <p>Gene Expression Prediction:</p> <ul> <li>Avsec et al. (2021) - \"Effective gene expression prediction from sequence by integrating long-range interactions\" (Enformer)</li> <li>Theodoris et al. (2023) - \"Transfer learning enables predictions in network biology\" (Geneformer)</li> </ul>"},{"location":"incubation/generative-ai-for-gene-expression-prediction/#implementation-resources","title":"Implementation Resources","text":"<ul> <li>PyTorch implementations: <code>diffusers</code>, <code>pytorch-vae</code>, <code>torchdiffeq</code></li> <li>Biology-specific tools: <code>scvi-tools</code>, <code>scanpy</code>, <code>anndata</code></li> <li>Tutorials: genai-lab notebooks (this repository)</li> </ul>"},{"location":"incubation/generative-ai-for-perturbation-modeling/","title":"Generative AI for Perturbation Modeling: Beyond GEM-1","text":"<p>Author's Note: This document analyzes GEM-1's supervised learning approach and proposes how generative AI (diffusion models, VAEs, flow-based models) could enhance perturbation prediction, particularly for scPerturb datasets.</p>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#executive-summary","title":"Executive Summary","text":"<p>Your analysis is correct: GEM-1 is a conditional predictive model, not a generative model in the diffusion/VAE/flow sense. It learns \\(\\mathbb{E}[x \\mid \\text{metadata}]\\), not \\(p(x \\mid \\text{metadata})\\).</p> <p>Your assessment is also correct: GEM-1's approach is realistic and may work better than pure generative AI for many applications\u2014especially when you need deterministic, interpretable predictions rather than stochastic samples.</p> <p>However, for perturbation modeling (especially scPerturb), generative AI offers unique advantages that supervised learning cannot provide:</p> <ol> <li>Uncertainty quantification - Multiple plausible cellular responses</li> <li>Counterfactual generation - \"What if\" scenarios beyond training data</li> <li>Compositional perturbations - Combining unseen perturbations</li> <li>Cell-level heterogeneity - Capturing biological variability</li> <li>Out-of-distribution robustness - Generalizing to novel perturbations</li> </ol>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#feedback-on-your-gem-1-analysis","title":"Feedback on Your GEM-1 Analysis","text":""},{"location":"incubation/generative-ai-for-perturbation-modeling/#what-you-got-right","title":"What You Got Right","text":"<ol> <li>GEM-1 is not generative - No evidence of stochastic sampling, latent variables, or diffusion processes</li> <li>It's compilation, not hallucination - Learning a dense lookup table over experimental conditions</li> <li>The innovation is data harmonization - Metadata curation is the real breakthrough</li> <li>Supervised learning is pragmatic - For many applications, \\(\\mathbb{E}[x]\\) is sufficient</li> </ol>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#critical-insights-you-identified","title":"Critical Insights You Identified","text":"<p>\"Predictive model first \u2192 generative wrapper later\" - This is the correct architecture.</p> <p>GEM-1 has solved the hard problem: learning the conditional mean structure of biology. This is essential infrastructure. Generative models can build on top of this foundation.</p> <p>Why they avoided diffusion/VAEs - Your four reasons are spot-on: - Ambiguous notion of \"sample\" - Unclear noise model - Difficult validation of novelty - Metadata dominates variance</p> <p>These are real challenges, but they're not insurmountable\u2014they're design constraints.</p>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#where-generative-ai-adds-value-not-replaces","title":"Where Generative AI Adds Value (Not Replaces)","text":"<p>GEM-1's approach is excellent for: - Interpolation within the training distribution - Point predictions for experimental planning - Label imputation (e.g., predicting missing sex labels)</p> <p>Generative AI is essential for: - Extrapolation to novel perturbation combinations - Uncertainty-aware predictions for risk assessment - Diversity generation for data augmentation - Causal intervention modeling with counterfactuals</p>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#the-perturbation-modeling-challenge","title":"The Perturbation Modeling Challenge","text":""},{"location":"incubation/generative-ai-for-perturbation-modeling/#why-scperturb-is-different-from-bulk-rna-seq","title":"Why scPerturb is Different from Bulk RNA-seq","text":"<p>scPerturb datasets have unique characteristics that make them ideal for generative modeling:</p> <ol> <li>Single-cell resolution - Natural notion of \"sample\" (one cell)</li> <li>Controlled perturbations - Clear causal interventions (CRISPR, compounds)</li> <li>Biological variability - Cells respond heterogeneously to the same perturbation</li> <li>Compositional structure - Perturbations can be combined (multi-gene knockouts)</li> <li>Counterfactual pairs - Control vs perturbed cells from same experiment</li> </ol>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#what-gem-1-cannot-do-by-design","title":"What GEM-1 Cannot Do (By Design)","text":"<p>For a given perturbation, GEM-1 predicts:</p> \\[ \\hat{x}_{\\text{perturbed}} = f(\\text{cell type}, \\text{perturbation}, \\text{dose}, \\text{time}) \\] <p>This gives you one expression profile per condition.</p> <p>But biology is stochastic. The same perturbation in the same cell type produces: - Different responses in different cells - Bimodal or multimodal distributions - Rare subpopulations with extreme responses - Temporal dynamics with variable kinetics</p> <p>GEM-1's point estimate cannot capture this biological uncertainty.</p>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#proposed-generative-ai-approaches-for-perturbation-modeling","title":"Proposed Generative AI Approaches for Perturbation Modeling","text":""},{"location":"incubation/generative-ai-for-perturbation-modeling/#architecture-1-conditional-diffusion-on-scperturb","title":"Architecture 1: Conditional Diffusion on scPerturb","text":"<p>Core Idea: Learn \\(p(x_{\\text{perturbed}} \\mid x_{\\text{control}}, \\text{perturbation})\\)</p>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#model-design","title":"Model Design","text":"<pre><code>Input:\n  - x_control: Gene expression of control cell (or population mean)\n  - perturbation: One-hot or embedding of perturbation identity\n  - metadata: Cell type, dose, time\n\nOutput:\n  - x_perturbed ~ p(x | x_control, perturbation, metadata)\n</code></pre>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#training-objective","title":"Training Objective","text":"<p>Use denoising score matching with perturbation conditioning:</p> \\[ \\mathcal{L} = \\mathbb{E}_{t, x_0, \\epsilon} \\left[ \\| s_\\theta(x_t, t, c) - \\nabla_{x_t} \\log p(x_t \\mid x_0) \\|^2 \\right] \\] <p>where \\(c = [\\text{perturbation}, \\text{metadata}, x_{\\text{control}}]\\)</p>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#key-advantages","title":"Key Advantages","text":"<ol> <li>Generates diverse cellular responses - Sample multiple times to get population distribution</li> <li>Interpolates between perturbations - Smooth perturbation space</li> <li>Composes perturbations - Combine embeddings for multi-gene knockouts</li> <li>Uncertainty quantification - Variance in samples reflects biological variability</li> </ol>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#implementation-strategy","title":"Implementation Strategy","text":"<pre><code>class PerturbationDiffusion(nn.Module):\n    def __init__(self, n_genes=5000, perturbation_dim=256):\n        self.gene_encoder = GeneExpressionEncoder(n_genes)\n        self.perturbation_encoder = PerturbationEncoder(perturbation_dim)\n        self.unet = ConditionalUNet(\n            in_channels=n_genes,\n            condition_dim=perturbation_dim + metadata_dim + n_genes\n        )\n        self.sde = VPSDE(schedule='cosine')\n\n    def forward(self, x_t, t, x_control, perturbation, metadata):\n        # Encode conditioning\n        pert_emb = self.perturbation_encoder(perturbation)\n        control_emb = self.gene_encoder(x_control)\n        condition = torch.cat([pert_emb, metadata, control_emb], dim=-1)\n\n        # Predict score\n        score = self.unet(x_t, t, condition)\n        return score\n</code></pre>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#architecture-2-causal-vae-with-perturbation-operators","title":"Architecture 2: Causal VAE with Perturbation Operators","text":"<p>Core Idea: Learn disentangled latent space where perturbations are linear operators</p>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#model-design_1","title":"Model Design","text":"<p>Inspired by causal representation learning, decompose latent space into:</p> \\[ z = [z_{\\text{cell identity}}, z_{\\text{cell state}}, z_{\\text{technical}}] \\] <p>Perturbations act as transformations:</p> \\[ z_{\\text{perturbed}} = z_{\\text{control}} + \\Delta_{\\text{perturbation}} \\]"},{"location":"incubation/generative-ai-for-perturbation-modeling/#training-objective_1","title":"Training Objective","text":"<p>Combine VAE ELBO with causal regularization:</p> \\[ \\mathcal{L} = \\mathcal{L}_{\\text{ELBO}} + \\lambda_1 \\mathcal{L}_{\\text{disentangle}} + \\lambda_2 \\mathcal{L}_{\\text{causal}} \\] <p>where:</p> <ul> <li>\\(\\mathcal{L}_{\\text{disentangle}}\\) encourages independence of latent factors</li> <li>\\(\\mathcal{L}_{\\text{causal}}\\) enforces perturbation effects are additive in latent space</li> </ul>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#key-advantages_1","title":"Key Advantages","text":"<ol> <li>Interpretable perturbation effects - \\(\\Delta\\) vectors represent causal interventions</li> <li>Compositional generalization - \\(\\Delta_1 + \\Delta_2\\) for combined perturbations</li> <li>Transfer across cell types - Learn universal perturbation operators</li> <li>Counterfactual generation - Apply perturbation to any cell</li> </ol>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#validation-strategy","title":"Validation Strategy","text":"<p>Test on held-out perturbations: - Interpolation: Unseen doses of known perturbations - Extrapolation: Unseen combinations of known perturbations - Transfer: Known perturbations in unseen cell types</p>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#architecture-3-flow-based-perturbation-model-optimal-transport","title":"Architecture 3: Flow-Based Perturbation Model (Optimal Transport)","text":"<p>Core Idea: Learn the transport map from control distribution to perturbed distribution</p>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#model-design_2","title":"Model Design","text":"<p>Use continuous normalizing flows (CNF) to model:</p> \\[ \\frac{d x}{d t} = f_\\theta(x, t, \\text{perturbation}) \\] <p>This learns the trajectory of cellular response over time.</p>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#training-objective_2","title":"Training Objective","text":"<p>Minimize optimal transport cost between control and perturbed distributions:</p> \\[ \\mathcal{L} = \\mathbb{E}_{x_{\\text{control}}, x_{\\text{perturbed}}} \\left[ \\| x_{\\text{perturbed}} - \\Phi_\\theta(x_{\\text{control}}, \\text{perturbation}) \\|^2 \\right] \\] <p>where \\(\\Phi_\\theta\\) is the learned flow.</p>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#key-advantages_2","title":"Key Advantages","text":"<ol> <li>Exact likelihood - No variational approximation</li> <li>Invertible - Can go from perturbed \u2192 control</li> <li>Temporal dynamics - Natural interpretation as time evolution</li> <li>Efficient sampling - Single forward pass (no iterative denoising)</li> </ol>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#hybrid-architecture-gem-1-generative-wrapper","title":"Hybrid Architecture: GEM-1 + Generative Wrapper","text":""},{"location":"incubation/generative-ai-for-perturbation-modeling/#the-best-of-both-worlds","title":"The Best of Both Worlds","text":"<p>Stage 1: GEM-1-style Predictive Model</p> <ul> <li>Learn \\(\\mu(c) = \\mathbb{E}[x \\mid c]\\) for all conditions \\(c\\)</li> <li>Massive scale, data harmonization, metadata curation</li> <li>Provides strong prior for generative model</li> </ul> <p>Stage 2: Generative Model on Residuals</p> <ul> <li>Learn \\(p(x - \\mu(c) \\mid c)\\) - the distribution around the mean</li> <li>Captures biological variability, technical noise, rare events</li> <li>Much easier to learn than full \\(p(x \\mid c)\\)</li> </ul>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#implementation","title":"Implementation","text":"<pre><code>class HybridPerturbationModel:\n    def __init__(self):\n        # Stage 1: Predictive (GEM-1 style)\n        self.mean_predictor = ConditionalMeanModel()\n\n        # Stage 2: Generative (diffusion on residuals)\n        self.residual_diffusion = ResidualDiffusion()\n\n    def predict(self, condition):\n        # Deterministic mean\n        mu = self.mean_predictor(condition)\n        return mu\n\n    def sample(self, condition, n_samples=100):\n        # Mean + stochastic residuals\n        mu = self.mean_predictor(condition)\n        residuals = self.residual_diffusion.sample(condition, n_samples)\n        return mu + residuals\n</code></pre>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#why-this-works","title":"Why This Works","text":"<ol> <li>Mean prediction is stable - Supervised learning excels here</li> <li>Residual distribution is simpler - Centered at zero, easier to model</li> <li>Separates signal from noise - Biological vs technical variability</li> <li>Leverages both paradigms - Predictive accuracy + generative flexibility</li> </ol>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#concrete-proposal-for-genai-lab","title":"Concrete Proposal for genai-lab","text":""},{"location":"incubation/generative-ai-for-perturbation-modeling/#phase-1-proof-of-concept-scperturb-subset","title":"Phase 1: Proof of Concept (scPerturb Subset)","text":"<p>Dataset: Norman et al. 2019 (Perturb-seq, K562 cells) - ~250,000 cells - ~5,000 genes - ~100 perturbations (single-gene CRISPR knockouts) - Control cells available</p> <p>Model: Conditional diffusion (Architecture 1)</p> <p>Metrics:</p> <ul> <li>Reconstruction: MSE on held-out cells</li> <li>Diversity: Variance of generated samples vs real</li> <li>Composition: Accuracy on held-out double knockouts</li> <li>Biological validity: Pathway enrichment, known gene interactions</li> </ul> <p>Timeline: 2-3 weeks</p>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#phase-2-scaling-full-scperturb","title":"Phase 2: Scaling (Full scPerturb)","text":"<p>Dataset: All scPerturb datasets - Multiple cell types - Multiple perturbation modalities (CRISPR, compounds, overexpression) - Varying doses and timepoints</p> <p>Model: Causal VAE (Architecture 2) or Hybrid (GEM-1 + diffusion)</p> <p>Metrics:</p> <ul> <li>Transfer learning: Train on cell line, test on primary cells</li> <li>Zero-shot perturbations: Predict unseen perturbations from embeddings</li> <li>Counterfactuals: Generate \"what if\" scenarios</li> </ul> <p>Timeline: 1-2 months</p>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#phase-3-integration-with-gem-1-philosophy","title":"Phase 3: Integration with GEM-1 Philosophy","text":"<p>Data Harmonization:</p> <ul> <li>Apply GEM-1's metadata curation to scPerturb</li> <li>Standardize perturbation ontologies</li> <li>Align cell type annotations</li> </ul> <p>Model Architecture:</p> <ul> <li>Use GEM-1-style predictive model as initialization</li> <li>Add generative wrapper for uncertainty</li> <li>Multi-task learning: predict mean + sample distribution</li> </ul> <p>Validation:</p> <ul> <li>Compare against GEM-1 predictions (if available)</li> <li>Benchmark on experimental validation datasets</li> <li>Collaborate with experimentalists for prospective validation</li> </ul>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#key-differences-from-gem-1","title":"Key Differences from GEM-1","text":"Aspect GEM-1 Generative AI (Proposed) Output Single prediction Distribution of outcomes Uncertainty None (point estimate) Explicit (sample variance) Novelty Interpolation only Extrapolation possible Composition Limited Natural (latent arithmetic) Validation Prediction accuracy Diversity + accuracy Use case Experimental planning Data augmentation, counterfactuals"},{"location":"incubation/generative-ai-for-perturbation-modeling/#why-generative-ai-is-complementary-not-competitive","title":"Why Generative AI is Complementary, Not Competitive","text":"<p>GEM-1 and generative models solve different problems:</p> <p>GEM-1 answers: \"What is the expected expression profile for this condition?\" - Essential for: experimental design, hypothesis generation, label imputation</p> <p>Generative AI answers: \"What are all the possible expression profiles for this condition?\" - Essential for: risk assessment, rare event prediction, synthetic data generation</p> <p>Both are needed for a complete perturbation modeling system.</p>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#technical-challenges-and-solutions","title":"Technical Challenges and Solutions","text":""},{"location":"incubation/generative-ai-for-perturbation-modeling/#challenge-1-high-dimensionality-5000-20000-genes","title":"Challenge 1: High Dimensionality (5,000-20,000 genes)","text":"<p>Solution: </p> <ul> <li>Use gene program embeddings (PCA, NMF, or learned)</li> <li>Model in low-dimensional latent space (~50-200 dims)</li> <li>Decode back to gene space</li> </ul>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#challenge-2-sparse-zero-inflated-data","title":"Challenge 2: Sparse, Zero-Inflated Data","text":"<p>Solution:</p> <ul> <li>Use zero-inflated loss functions</li> <li>Separate models for dropout vs expression level</li> <li>Or use scVI-style probabilistic framework</li> </ul>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#challenge-3-batch-effects","title":"Challenge 3: Batch Effects","text":"<p>Solution:</p> <ul> <li>Include batch as conditioning variable</li> <li>Use adversarial training to remove batch effects</li> <li>Or CycleGAN-style batch correction in latent space</li> </ul>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#challenge-4-limited-perturbation-coverage","title":"Challenge 4: Limited Perturbation Coverage","text":"<p>Solution:</p> <ul> <li>Meta-learning across perturbations</li> <li>Transfer learning from related perturbations</li> <li>Graph neural networks over perturbation similarity graphs</li> </ul>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#challenge-5-validation-without-ground-truth","title":"Challenge 5: Validation Without Ground Truth","text":"<p>Solution:</p> <ul> <li>Biological consistency checks: pathway enrichment, known interactions</li> <li>Cross-validation: held-out perturbations, cell types, doses</li> <li>Prospective validation: generate predictions \u2192 experimentalists test</li> </ul>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#recommended-reading","title":"Recommended Reading","text":""},{"location":"incubation/generative-ai-for-perturbation-modeling/#perturbation-modeling","title":"Perturbation Modeling","text":"<ul> <li>scGen (Lotfollahi et al. 2019) - VAE for perturbation prediction</li> <li>CPA (Lotfollahi et al. 2023) - Compositional perturbation autoencoder</li> <li>GEARS (Roohani et al. 2023) - Graph neural network for genetic perturbations</li> </ul>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#causal-representation-learning","title":"Causal Representation Learning","text":"<ul> <li>CATE (Schwab et al. 2020) - Causal effect VAE</li> <li>Causal-BALD (Jesson et al. 2021) - Bayesian active learning for causal discovery</li> </ul>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#diffusion-for-biology","title":"Diffusion for Biology","text":"<ul> <li>scDiffusion (Yang et al. 2023) - Diffusion models for single-cell data</li> <li>DiffCSP (Jing et al. 2023) - Diffusion for crystal structure prediction (similar principles)</li> </ul>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#next-steps-for-genai-lab","title":"Next Steps for genai-lab","text":"<ol> <li>Implement baseline - Conditional diffusion on Norman et al. dataset</li> <li>Benchmark against scGen/CPA - Compare generative quality</li> <li>Ablation studies - Conditioning strategies, architecture choices</li> <li>Biological validation - Pathway analysis, known gene interactions</li> <li>Scale to full scPerturb - Multi-dataset, multi-modality</li> <li>Hybrid model - Integrate GEM-1-style predictive component</li> </ol>"},{"location":"incubation/generative-ai-for-perturbation-modeling/#conclusion","title":"Conclusion","text":"<p>Your analysis is sharp and correct: GEM-1 is not a generative model, and its supervised learning approach is pragmatic and effective for many applications.</p> <p>However, for perturbation modeling\u2014especially with scPerturb\u2014generative AI offers unique capabilities: - Uncertainty quantification - Compositional generalization - Counterfactual reasoning - Biological variability modeling</p> <p>The optimal path forward is not generative AI instead of GEM-1, but generative AI on top of GEM-1's data harmonization and predictive foundation.</p> <p>genai-lab is well-positioned to explore this hybrid approach, combining: - GEM-1's data curation philosophy - Diffusion models' generative flexibility - scPerturb's causal perturbation structure</p> <p>This could lead to a next-generation perturbation modeling system that provides both accurate predictions and biologically meaningful uncertainty.</p>"},{"location":"incubation/joint_latent_space_and_JEPA/","title":"Joint Latent Spaces and JEPA: A Unified Approach for Computational Biology","text":""},{"location":"incubation/joint_latent_space_and_JEPA/#introduction","title":"Introduction","text":"<p>This document explores a powerful architectural idea emerging from the vision community that has profound implications for computational biology: joint latent spaces and Joint Embedding Predictive Architecture (JEPA).</p> <p>The core insight is deceptively simple:</p> <p>If two data types differ only by dimensionality or observation density, they probably want the same latent space.</p> <p>We'll trace this idea from its origins in joint image-video generation (specifically, ByteDance's Goku model) through to practical architectures for biological applications like Perturb-seq, single-cell analysis, and multi-omics integration.</p>"},{"location":"incubation/joint_latent_space_and_JEPA/#part-1-the-goku-model-where-this-idea-crystallized","title":"Part 1: The Goku Model \u2014 Where This Idea Crystallized","text":""},{"location":"incubation/joint_latent_space_and_JEPA/#the-provocation","title":"The Provocation","text":"<p>ByteDance and HKU researchers asked a simple question: why do we treat images and videos as fundamentally different, when mathematically they're almost the same object?</p> <p>An image is just a video with time dimension = 1:</p> \\[ \\text{Video} \\in \\mathbb{R}^{T \\times H \\times W \\times C} \\] <p>An image is simply \\(T = 1\\).</p> <p>The historical split happened for engineering reasons\u2014images were easier, videos expensive, temporal modeling scary. So we trained separate systems, even though the underlying inductive bias mismatch was artificial.</p> <p>The biological parallel: This is the same historical accident as treating genes vs. transcripts vs. isoforms as separate modeling problems, when they're just different slices of the same biological process.</p>"},{"location":"incubation/joint_latent_space_and_JEPA/#the-solution-one-latent-space-to-rule-them-all","title":"The Solution: One Latent Space to Rule Them All","text":"<p>Goku's first conceptual move isn't about diffusion\u2014it's the joint VAE.</p> <p>Instead of separate encoders: - Image VAE \u2192 image latent space - Video VAE \u2192 video latent space</p> <p>They use one encoder-decoder pair that maps both images and videos into the same latent manifold.</p> <p>Why this matters:</p> <ul> <li>Image-only data teaches spatial priors (texture, composition, objects)</li> <li>Video data teaches dynamics (motion, continuity, causality)</li> <li>Both shape the same latent geometry</li> </ul> <p>This is a powerful prior-sharing mechanism.</p>"},{"location":"incubation/joint_latent_space_and_JEPA/#combio-analogy","title":"Combio Analogy","text":"<p>Think about this mapping: - Bulk RNA-seq \u2248 \"static image\" - Time-series, Perturb-seq, lineage tracing \u2248 \"video\" - Single-cell snapshots across conditions \u2248 variable-length clips</p> <p>A joint latent space means static and dynamic biology train each other instead of living in silos.</p>"},{"location":"incubation/joint_latent_space_and_JEPA/#part-2-key-architectural-components","title":"Part 2: Key Architectural Components","text":""},{"location":"incubation/joint_latent_space_and_JEPA/#21-rectified-flow-instead-of-classic-diffusion","title":"2.1 Rectified Flow (Instead of Classic Diffusion)","text":"<p>Goku uses rectified flow rather than classic DDPM noise schedules.</p> <p>Conceptual difference:</p> <ul> <li>Diffusion: Learn to undo noise step by step</li> <li>Rectified flow: Learn a direct velocity field that moves noise \u2192 data</li> </ul> <p>Why this matters for joint training:</p> <ul> <li>Flow matching is simpler to condition</li> <li>It handles variable sequence lengths more gracefully</li> <li>Less bookkeeping when mixing modalities</li> </ul> <p>This is particularly appealing for non-image domains like biology, where the \"noise semantics\" are already fuzzy. What does \"adding Gaussian noise to gene expression\" really mean biologically? Flow-based approaches sidestep this question.</p>"},{"location":"incubation/joint_latent_space_and_JEPA/#22-patch-n-pack-batching-without-caring-about-shapes","title":"2.2 Patch n' Pack: Batching Without Caring About Shapes","text":"<p>This is perhaps the most under-appreciated idea.</p> <p>Traditional approach:</p> <ul> <li>Pad videos to the same length</li> <li>Resize everything to the same resolution</li> <li>Waste computation on padding tokens</li> </ul> <p>Patch n' Pack approach:</p> <ul> <li>Tokenize everything into patches</li> <li>Concatenate all tokens into one long sequence</li> <li>Add block attention masks so tokens only attend within their own sample</li> </ul> <p>This lets a minibatch contain: - Images of different sizes - Videos of different lengths - All mixed together</p> <p>The philosophical insight:</p> <p>Shape uniformity is not a law of nature\u2014it's a convenience hack.</p> <p>Combio parallel: You routinely deal with: - Transcripts of different lengths - Genes with variable exon counts - Cells with variable detected features - Perturbations with different temporal spans</p> <p>Patch n' Pack is the Transformer version of respecting biological heterogeneity instead of padding it away.</p>"},{"location":"incubation/joint_latent_space_and_JEPA/#23-full-attention-not-factorized","title":"2.3 Full Attention (Not Factorized)","text":"<p>Most video models split attention into: - Spatial attention - Temporal attention</p> <p>Goku doesn't. It uses plain full attention with masking.</p> <p>Why this matters:</p> <ul> <li>No architectural bias saying \"time is special\"</li> <li>The model discovers when temporal relationships matter</li> <li>Images and videos become first-class citizens in the same token universe</li> </ul> <p>This mirrors a recurring theme in computational biology: stop hard-coding \"gene first, transcript later.\" Let representations emerge under constraints.</p>"},{"location":"incubation/joint_latent_space_and_JEPA/#24-positional-encoding-reset","title":"2.4 Positional Encoding Reset","text":"<p>RoPE positional encodings are reset per sample block.</p> <p>This means: - Token position 0 always means \"start of this sample\" - Not \"position 0 in a giant Frankenstein sequence\"</p> <p>Biological parallel: Each gene, each cell, each experiment has its own coordinate frame. Forcing a global coordinate system often destroys meaning.</p>"},{"location":"incubation/joint_latent_space_and_JEPA/#part-3-the-connection-to-jepa","title":"Part 3: The Connection to JEPA","text":""},{"location":"incubation/joint_latent_space_and_JEPA/#what-is-jepa","title":"What is JEPA?","text":"<p>Joint Embedding Predictive Architecture (JEPA), pioneered by Yann LeCun's team at Meta AI, makes a sharp philosophical move:</p> <p>Do not reconstruct pixels. Do not predict tokens. Predict representations of future states.</p> <p>In JEPA: 1. You embed a context into latent space 2. You embed a target into latent space 3. You train a predictor so the context-latent can predict the target-latent</p> <p>Crucially: - The loss lives entirely in latent space - The model never needs to care about raw observation noise</p>"},{"location":"incubation/joint_latent_space_and_JEPA/#the-jepa-family-2023-2025","title":"The JEPA Family (2023-2025)","text":"Model Date Domain Key Innovation I-JEPA Jun 2023 Images First JEPA; mask prediction in embedding space MC-JEPA Jul 2023 Video Motion + content disentanglement V-JEPA Feb 2024 Video Temporal prediction without language supervision V-JEPA 2 Jun 2025 Video + Robotics World model for understanding, prediction, planning VL-JEPA Dec 2025 Vision-Language Predicts text embeddings, not tokens"},{"location":"incubation/joint_latent_space_and_JEPA/#v-jepa-2-the-major-2025-breakthrough","title":"V-JEPA 2: The Major 2025 Breakthrough","text":"<p>V-JEPA 2 is the first world model trained on video that enables: - State-of-the-art video understanding - Zero-shot physical prediction - Robotic planning from observation</p> <p>Key findings:</p> <ul> <li>Video encoder pre-trained without language supervision can be aligned with LLMs</li> <li>Self-supervised video pretraining enables physical world understanding</li> <li>Two-phase training: (1) self-supervised from video, (2) small amount of robot interaction data</li> </ul>"},{"location":"incubation/joint_latent_space_and_JEPA/#vl-jepa-vision-language-efficiency","title":"VL-JEPA: Vision-Language Efficiency","text":"<p>Instead of autoregressive token generation, VL-JEPA predicts continuous text embeddings.</p> <p>Advantages:</p> <ul> <li>50% fewer trainable parameters than standard VLMs</li> <li>Supports selective decoding (2.85\u00d7 fewer decode operations)</li> <li>Natively supports open-vocabulary classification, retrieval, and VQA</li> </ul>"},{"location":"incubation/joint_latent_space_and_JEPA/#why-jepa-and-goku-share-the-same-soul","title":"Why JEPA and Goku Share the Same Soul","text":"<p>Look at what Goku is doing conceptually: - Image encoder \u2192 latent - Video encoder \u2192 latent - Diffusion/flow learns transitions in latent space - Images are treated as degenerate videos (\\(T=1\\))</p> <p>Same skeleton. Different skin.</p> <p>Both approaches reject a very old habit in ML: treating observations as the thing we should model directly. Instead, they assume: - There exists an underlying state manifold - Observations are partial, noisy, structured projections of that state - Learning becomes easier if we operate between states, not pixels</p>"},{"location":"incubation/joint_latent_space_and_JEPA/#the-unifying-principle","title":"The Unifying Principle","text":"<p>A joint latent space is where modality disappears, and JEPA is how time re-enters.</p> <p>Different training mechanics. Same worldview.</p>"},{"location":"incubation/joint_latent_space_and_JEPA/#part-4-jepa-for-computational-biology","title":"Part 4: JEPA for Computational Biology","text":""},{"location":"incubation/joint_latent_space_and_JEPA/#why-jepa-is-a-natural-fit-for-biology","title":"Why JEPA is a Natural Fit for Biology","text":"<p>By 2025-2026, JEPA-style thinking has won several arguments: - Reconstruction losses are brittle - Likelihoods are often misaligned with semantics - Prediction in latent space scales better - Masking + prediction beats full autoregression for long contexts</p> <p>In computational biology, you almost never want: - Perfect reconstruction of expression values - Exact token-level likelihoods</p> <p>You want: - Correct relationships - Plausible state transitions - Meaningful counterfactuals</p> <p>That is exactly the JEPA niche.</p>"},{"location":"incubation/joint_latent_space_and_JEPA/#a-unified-model-for-static-and-dynamic-biology","title":"A Unified Model for Static and Dynamic Biology","text":"<p>Here's a clean mental model that unifies everything:</p> <p>Joint encoder maps: - Static data (genome, baseline expression) - Dynamic data (time series, perturbations) - Multimodal data (RNA, ATAC, protein)</p> <p>...into a shared latent state.</p> <p>JEPA-style predictor learns:</p> \\[ z_{t+\\Delta} \\approx f_\\theta(z_t, \\text{condition}) \\] <ul> <li>No reconstruction is strictly required</li> <li>Decoders are optional and task-specific</li> <li>Generative ability emerges from state evolution, not pixel synthesis</li> </ul> <p>This is essentially a biological world model.</p>"},{"location":"incubation/joint_latent_space_and_JEPA/#part-5-jepa-architecture-for-perturb-seq","title":"Part 5: JEPA Architecture for Perturb-seq","text":"<p>Let's make this concrete with a sketch for Perturb-seq\u2014one of the most powerful experimental technologies for understanding gene regulation.</p>"},{"location":"incubation/joint_latent_space_and_JEPA/#the-data","title":"The Data","text":"<p>In Perturb-seq, per cell \\(i\\) you have: - \\(x_i\\): gene expression vector (counts / log1p / normalized) - \\(p_i\\): perturbation label(s) (one-hot or multi-hot; e.g., sgRNA IDs) - \\(c_i\\): covariates (cell type, batch, donor, cell cycle, library size) - Sometimes \\(t_i\\): timepoint (if time-course) - Optional: multiome (ATAC, protein) as extra modalities</p>"},{"location":"incubation/joint_latent_space_and_JEPA/#the-jepa-structure","title":"The JEPA Structure","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        JEPA for Perturb-seq                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502\n\u2502  \u2502   Context    \u2502                    \u2502    Target    \u2502          \u2502\n\u2502  \u2502   Encoder    \u2502                    \u2502   Encoder    \u2502          \u2502\n\u2502  \u2502   E_ctx      \u2502                    \u2502  E_tgt (EMA) \u2502          \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n\u2502         \u2502                                   \u2502                   \u2502\n\u2502         \u25bc                                   \u25bc                   \u2502\n\u2502        h_i        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         z_i                 \u2502\n\u2502    (context      \u2502  Perturbation \u2502    (target latent)          \u2502\n\u2502     latent)      \u2502    Encoder    \u2502                             \u2502\n\u2502         \u2502        \u2502    e(p_i)     \u2502                             \u2502\n\u2502         \u2502        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                             \u2502\n\u2502         \u2502               \u2502                                       \u2502\n\u2502         \u25bc               \u25bc                                       \u2502\n\u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                              \u2502\n\u2502    \u2502        Predictor F         \u2502                              \u2502\n\u2502    \u2502   \u1e91_i = F(h_i, e(p_i))    \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 predict \u2500\u2500\u2500\u2500\u2500\u2500\u2192 z_i \u2502\n\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                              \u2502\n\u2502                                                                 \u2502\n\u2502    Loss: L = ||\u1e91_i - sg(z_i)||\u00b2 + variance/covariance reg     \u2502\n\u2502                                                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"incubation/joint_latent_space_and_JEPA/#component-details","title":"Component Details","text":""},{"location":"incubation/joint_latent_space_and_JEPA/#target-encoder-e_texttgt","title":"Target Encoder (\\(E_{\\text{tgt}}\\))","text":"<p>Updated via EMA (momentum). Takes the observed expression under perturbation and produces the \"ground truth\" state embedding.</p> <ul> <li>Input: \\(x_i\\) (observed expression under perturbation \\(p_i\\))</li> <li>Output: \\(z_i = E_{\\text{tgt}}(x_i)\\)</li> </ul>"},{"location":"incubation/joint_latent_space_and_JEPA/#context-encoder-e_textctx","title":"Context Encoder (\\(E_{\\text{ctx}}\\))","text":"<p>Takes baseline context + covariates. The challenge: what is \"baseline context\" for a cell that's already perturbed?</p> <p>Options:</p> <ul> <li>Cell-type context: Learned embedding of cell type/cluster label</li> <li>Matched controls: Sample a control cell from same covariates as baseline</li> <li>Population baseline: Covariate-conditioned baseline embedding (learned)</li> </ul>"},{"location":"incubation/joint_latent_space_and_JEPA/#perturbation-encoder","title":"Perturbation Encoder","text":"<p>Represent perturbations as tokens: - Single sgRNA: embedding lookup - Multiple sgRNAs: set encoder (sum + MLP, or attention over perturbation tokens)</p> <p>Output: \\(e(p_i)\\)</p>"},{"location":"incubation/joint_latent_space_and_JEPA/#predictor","title":"Predictor","text":"<p>Takes context embedding + perturbation embedding:</p> \\[ \\hat{z}_i = F(h_i, e(p_i)) \\] <p>Implementation options:</p> <ul> <li>MLP with FiLM-style conditioning</li> <li>Cross-attention: \"perturbation tokens attend to cell tokens\"</li> <li>Small Transformer over tokens: \\([h_i, e(p_i), e(c_i), e(t_i)]\\) \u2192 2-4 blocks \u2192 \\(\\hat{z}\\)</li> </ul>"},{"location":"incubation/joint_latent_space_and_JEPA/#the-jepa-loss","title":"The JEPA Loss","text":"<p>The base loss is latent regression:</p> \\[ \\mathcal{L}_{\\text{JEPA}} = \\|\\hat{z}_i - \\text{sg}(z_i)\\|_2^2 \\] <p>where \\(\\text{sg}(\\cdot)\\) = stop-gradient (target encoder is EMA).</p>"},{"location":"incubation/joint_latent_space_and_JEPA/#collapse-prevention","title":"Collapse Prevention","text":"<p>If you only do MSE, the model can collapse to constants. JEPA prevents this via:</p> <ol> <li>Variance/covariance regularization (VICReg-style)</li> <li>Encourage embeddings to have non-trivial variance across batch</li> <li> <p>Discourage correlated dimensions</p> </li> <li> <p>Multi-view consistency</p> </li> <li>Create two augmented views of the same cell (gene dropout, noise)</li> <li> <p>Encode both, force consistency</p> </li> <li> <p>Predict multiple targets</p> </li> <li>Predict both \\(z\\) and low-dimensional biological summaries (pathway scores, PCs)</li> </ol> <p>Full loss:</p> \\[ \\mathcal{L} = \\mathcal{L}_{\\text{JEPA}} + \\lambda_1 \\mathcal{L}_{\\text{var}} + \\lambda_2 \\mathcal{L}_{\\text{cov}} \\]"},{"location":"incubation/joint_latent_space_and_JEPA/#masking-strategy","title":"Masking Strategy","text":"<p>JEPA benefits from masking because it forces reasoning about missing parts.</p> <p>For expression vectors, \"masking\" can mean: - Randomly zero out a subset of genes (with mask indicator) - Drop whole gene modules/pathways - More aggressive count downsampling</p> <p>This gives: - Robustness to dropout - Better generalization across datasets - Natural self-supervised signal even without perturbation labels</p>"},{"location":"incubation/joint_latent_space_and_JEPA/#training-recipe","title":"Training Recipe","text":"<p>Phase 1: Self-supervised pretraining</p> <ul> <li>Ignore perturbation labels initially</li> <li>Train JEPA with masking/augmentations</li> <li>Goal: Learn a good cell state manifold</li> </ul> <p>Phase 2: Perturbation-conditioned JEPA</p> <ul> <li>Add perturbation tokens</li> <li>Predict target embedding from baseline-ish context + perturbation</li> </ul> <p>This mirrors how image/video joint models benefit from strong image priors first.</p>"},{"location":"incubation/joint_latent_space_and_JEPA/#using-the-trained-model","title":"Using the Trained Model","text":"<p>Once trained, you can:</p> <ol> <li>Take a control-like context embedding \\(h\\) (cell type + covariates)</li> <li>Plug in perturbation \\(p\\)</li> <li>Produce \\(\\hat{z}(p)\\) \u2014 latent representing the perturbed state</li> </ol> <p>Downstream options:</p> <p>Stay in latent space (preferred JEPA style): - Nearest-neighbor retrieval of real cells - Pathway/regulator prediction heads trained on \\(z\\) - Differential latent shift: \\(\\Delta z = \\hat{z}(p) - \\hat{z}(\\varnothing)\\)</p> <p>Add lightweight decoder (if needed): - Predict expression changes \\(\\Delta x\\) - Predict sparse gene program activation</p> <p>JEPA doesn't forbid decoding\u2014it just refuses to make it the main learning signal.</p>"},{"location":"incubation/joint_latent_space_and_JEPA/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>Concrete evaluations to avoid vibes-based science:</p> Metric What It Tests Perturbation classification from \\(z\\) Does embedding separate perturbations? Held-out perturbation prediction Generalization to unseen perturbations Combinatorial generalization Predict double-KO from singles DEG recovery Do predicted changes match observed DE genes? Pathway consistency Does \\(\\Delta z\\) align with known biology?"},{"location":"incubation/joint_latent_space_and_JEPA/#part-6-one-important-distinction","title":"Part 6: One Important Distinction","text":""},{"location":"incubation/joint_latent_space_and_JEPA/#jepa-vs-diffusionflow-how-uncertainty-is-handled","title":"JEPA vs. Diffusion/Flow: How Uncertainty is Handled","text":"Approach Uncertainty Handling Diffusion/Flow Explicit stochasticity, full generative modeling JEPA Implicit uncertainty, representation-level prediction <p>But this is not a contradiction.</p> <p>A natural 2025-2026 hybrid combines: - JEPA-style latent prediction - Stochastic heads or flow-matching in latent space</p> <p>This gives: - Uncertainty without pixel-level noise - Dynamics without autoregressive collapse</p>"},{"location":"incubation/joint_latent_space_and_JEPA/#part-7-when-not-to-use-joint-latent-spaces","title":"Part 7: When NOT to Use Joint Latent Spaces","text":"<p>Joint latent spaces are powerful but not universally appropriate.</p> <p>Counter-cases:</p> <ol> <li>Modalities with incompatible scales</li> <li>If one modality has 100\u00d7 more samples, joint training may harm the smaller modality</li> <li> <p>Solution: Careful loss weighting or staged training</p> </li> <li> <p>Fundamentally different semantics</p> </li> <li>If \"distance\" means different things in each modality</li> <li> <p>Example: Spatial vs. functional similarity in genes</p> </li> <li> <p>Conflicting optimization landscapes</p> </li> <li>Different learning rates or architectures may be needed</li> <li> <p>Solution: Modality-specific encoders with shared predictor</p> </li> <li> <p>Privacy/data constraints</p> </li> <li>If modalities can't be co-located for training</li> </ol>"},{"location":"incubation/joint_latent_space_and_JEPA/#summary","title":"Summary","text":""},{"location":"incubation/joint_latent_space_and_JEPA/#key-takeaways","title":"Key Takeaways","text":"<ol> <li> <p>Joint latent spaces allow different data modalities to train each other, sharing priors between static and dynamic observations.</p> </li> <li> <p>JEPA predicts embeddings not pixels, focusing on semantics while ignoring observation noise\u2014perfect for biology where reconstruction is rarely the goal.</p> </li> <li> <p>Patch n' Pack respects heterogeneity (variable lengths, sizes) instead of padding it away\u2014directly applicable to genes, transcripts, and cells.</p> </li> <li> <p>Flow-based approaches may be preferable to diffusion for biology, where \"noise semantics\" are unclear.</p> </li> <li> <p>For Perturb-seq and similar tasks, JEPA provides a natural framework: predict the latent state of perturbed cells from baseline context + perturbation tokens.</p> </li> </ol>"},{"location":"incubation/joint_latent_space_and_JEPA/#the-unifying-insight","title":"The Unifying Insight","text":"<p>If two data types differ only by dimensionality or observation density, they probably want the same latent space.</p> <p>Nature never padded anything to a fixed shape\u2014engineers did. These architectures are beginning to respect that reality.</p>"},{"location":"incubation/joint_latent_space_and_JEPA/#references","title":"References","text":""},{"location":"incubation/joint_latent_space_and_JEPA/#goku-model","title":"Goku Model","text":"<ul> <li>ByteDance &amp; HKU (2024). Goku: Native Joint Image-Video Generation</li> </ul>"},{"location":"incubation/joint_latent_space_and_JEPA/#jepa-family","title":"JEPA Family","text":"<ul> <li>Assran et al. (2023). Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture (I-JEPA). CVPR 2023.</li> <li>Bardes et al. (2024). V-JEPA: Video Joint Embedding Predictive Architecture</li> <li>Meta AI (2025). V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction, and Planning</li> <li>(2025). VL-JEPA: Joint Embedding Predictive Architecture for Vision-language</li> </ul>"},{"location":"incubation/joint_latent_space_and_JEPA/#related-concepts","title":"Related Concepts","text":"<ul> <li>VICReg: Variance-Invariance-Covariance Regularization</li> <li>Rectified Flow: Optimal transport for generative modeling</li> <li>Patch n' Pack: Efficient variable-length batching for Transformers</li> </ul>"},{"location":"incubation/joint_latent_space_and_JEPA/#next-steps","title":"Next Steps","text":"<p>From here, you could: 1. Implement a minimal JEPA for a toy biological dataset (e.g., perturbed gene expression) 2. Explore hierarchical JEPA for multi-scale data (patches \u2192 regions \u2192 whole samples) 3. Compare JEPA vs. diffusion specifically for biological counterfactual prediction 4. Benchmark on Perturb-seq: Norman et al. (2019) dataset is a good starting point</p> <p>The interesting part isn't copying any particular model\u2014it's recognizing the pattern these architectures expose and applying it to biology.</p>"},{"location":"incubation/numerical_embeddings_and_continuous_values/","title":"Numerical Embeddings and Continuous Values: From LLMs to Diffusion Models","text":""},{"location":"incubation/numerical_embeddings_and_continuous_values/#overview","title":"Overview","text":"<p>This document explores a fundamental challenge in modern deep learning: how to represent continuous numerical values in neural networks, particularly in contexts where tokenization breaks down. We'll examine:</p> <ol> <li>The problem of numerical representation in language models</li> <li>Recent research directions (2024-2026) for numerical embeddings</li> <li>Connections to time embedding in diffusion models</li> <li>Whether these techniques are relevant for computational biology applications</li> </ol>"},{"location":"incubation/numerical_embeddings_and_continuous_values/#the-core-problem","title":"The Core Problem","text":""},{"location":"incubation/numerical_embeddings_and_continuous_values/#why-numbers-are-hard-for-llms","title":"Why Numbers Are Hard for LLMs","text":"<p>Language models excel at discrete tokens (words, subwords, characters) but struggle with continuous numerical values. Here's why:</p> <p>The Tokenization Problem:</p> <ul> <li>Numbers like <code>3.14159</code> get tokenized as <code>[\"3\", \".\", \"14\", \"159\"]</code></li> <li>This destroys numerical relationships: <code>3.14</code> and <code>3.15</code> are close numerically but may have completely different token sequences</li> <li>The model can't learn that <code>3.14159 \u2248 \u03c0</code> or that <code>1000 &gt; 999</code> in a meaningful way</li> <li>Arithmetic operations become nearly impossible: the model can't reliably compute <code>2 + 2 = 4</code></li> </ul> <p>The Scale Problem:</p> <ul> <li>Small numbers (<code>0.001</code>) and large numbers (<code>1,000,000</code>) are treated as unrelated tokens</li> <li>No inherent understanding of magnitude, order, or relationships</li> <li>Scientific notation (<code>1.23e-4</code>) is even more fragmented</li> </ul> <p>The Precision Problem:</p> <ul> <li>Floating-point precision is lost in tokenization</li> <li><code>3.141592653589793</code> and <code>3.141592653589794</code> might tokenize identically</li> <li>Fine-grained distinctions disappear</li> </ul>"},{"location":"incubation/numerical_embeddings_and_continuous_values/#why-this-matters","title":"Why This Matters","text":"<p>Numbers appear everywhere: - Scientific literature: Measurements, statistics, experimental results - Code: Variables, constants, calculations - Financial data: Prices, quantities, percentages - Biological data: Expression levels, concentrations, measurements - Time series: Timestamps, durations, intervals</p> <p>If LLMs can't handle numbers well, they can't truly understand these domains.</p>"},{"location":"incubation/numerical_embeddings_and_continuous_values/#recent-research-directions-2024-2026","title":"Recent Research Directions (2024-2026)","text":""},{"location":"incubation/numerical_embeddings_and_continuous_values/#1-learned-numerical-embeddings","title":"1. Learned Numerical Embeddings","text":"<p>Approach: Treat numbers as a special token type with learned embeddings.</p> <p>Methods:</p> <ul> <li>Number-aware tokenization: Split numbers into components (integer part, decimal part, exponent) and learn embeddings for each</li> <li>Magnitude-aware embeddings: Embed numbers in a way that preserves scale relationships</li> <li>Hybrid approaches: Combine tokenization with learned numerical representations</li> </ul> <p>Example Architecture: <pre><code>class NumericalEmbedding(nn.Module):\n    \"\"\"Learned embedding for numerical values.\"\"\"\n\n    def __init__(self, embedding_dim=128):\n        super().__init__()\n        # Separate embeddings for different number components\n        self.int_embedding = nn.Embedding(10000, embedding_dim)  # Integer part\n        self.dec_embedding = nn.Embedding(1000, embedding_dim)   # Decimal part\n        self.exp_embedding = nn.Embedding(100, embedding_dim)    # Exponent\n\n    def forward(self, number):\n        # Parse number into components\n        int_part, dec_part, exp_part = self.parse_number(number)\n        # Combine embeddings\n        return self.int_embedding(int_part) + self.dec_embedding(dec_part) + self.exp_embedding(exp_part)\n</code></pre></p> <p>Pros:</p> <ul> <li>Fully learnable, can adapt to task</li> <li>Can capture domain-specific numerical patterns</li> </ul> <p>Cons:</p> <ul> <li>Doesn't generalize to unseen numbers well</li> <li>Requires careful design of number decomposition</li> <li>Still loses some precision</li> </ul>"},{"location":"incubation/numerical_embeddings_and_continuous_values/#2-sinusoidal-numerical-embeddings","title":"2. Sinusoidal Numerical Embeddings","text":"<p>Approach: Use sinusoidal functions (similar to positional encoding) to represent numbers.</p> <p>Key Insight: This is exactly the same idea as time embedding in diffusion models!</p> <p>Mathematical Form: For a number \\(n\\), create an embedding:</p> \\[ \\gamma(n) = \\begin{bmatrix} \\sin(\\omega_1 n) \\\\ \\cos(\\omega_1 n) \\\\ \\sin(\\omega_2 n) \\\\ \\cos(\\omega_2 n) \\\\ \\vdots \\\\ \\sin(\\omega_{d/2} n) \\\\ \\cos(\\omega_{d/2} n) \\end{bmatrix} \\] <p>where frequencies \\(\\omega_i\\) are chosen to span different scales:</p> \\[ \\omega_i = \\frac{1}{10000^{2i/d}} \\] <p>Why This Works:</p> <ul> <li>Bounded: Values stay in \\([-1, 1]\\), training stable</li> <li>Smooth: Differentiable, allows interpolation</li> <li>Multi-scale: Different frequencies capture different magnitudes</li> <li>Relative relationships: Can represent that \\(n_1\\) is close to \\(n_2\\)</li> </ul> <p>Connection to Time Embedding: This is identical to the time embedding used in diffusion models! Time \\(t\\) and numbers \\(n\\) are both continuous scalars that need high-dimensional representations.</p>"},{"location":"incubation/numerical_embeddings_and_continuous_values/#3-direct-numerical-encoding","title":"3. Direct Numerical Encoding","text":"<p>Approach: Feed numbers directly as floating-point values, but with special processing.</p> <p>Methods:</p> <ul> <li>Normalization: Scale numbers to a standard range (e.g., \\([0, 1]\\) or \\([-1, 1]\\))</li> <li>Log scaling: Use \\(\\log(n + \\epsilon)\\) for wide-ranging values</li> <li>Quantization: Discretize into bins, then use embeddings</li> <li>Feature engineering: Extract magnitude, sign, precision as separate features</li> </ul> <p>Example: <pre><code>def encode_number(n):\n    \"\"\"Direct encoding with normalization.\"\"\"\n    # Extract components\n    sign = 1 if n &gt;= 0 else -1\n    magnitude = abs(n)\n\n    # Log scale for wide range\n    log_mag = torch.log(magnitude + 1e-8)\n\n    # Normalize\n    normalized = torch.tanh(log_mag / 10.0)  # Scale to [-1, 1]\n\n    # Combine\n    return torch.cat([sign * normalized, log_mag])\n</code></pre></p> <p>Pros:</p> <ul> <li>Simple, interpretable</li> <li>Preserves exact values (within precision)</li> <li>Can handle arbitrary ranges with normalization</li> </ul> <p>Cons:</p> <ul> <li>Requires careful normalization</li> <li>May not capture complex numerical relationships</li> <li>Less expressive than learned embeddings</li> </ul>"},{"location":"incubation/numerical_embeddings_and_continuous_values/#4-rope-style-numerical-embeddings","title":"4. RoPE-Style Numerical Embeddings","text":"<p>Approach: Extend Rotary Position Embedding (RoPE) to numerical values.</p> <p>RoPE for Positions: RoPE rotates query/key vectors by an angle proportional to position, preserving relative distances.</p> <p>RoPE for Numbers: Apply similar rotation based on numerical value:</p> \\[ \\text{RoPE}(x, n) = \\begin{bmatrix} x_0 \\cos(\\omega n) - x_1 \\sin(\\omega n) \\\\ x_0 \\sin(\\omega n) + x_1 \\cos(\\omega n) \\\\ x_2 \\cos(2\\omega n) - x_3 \\sin(2\\omega n) \\\\ \\vdots \\end{bmatrix} \\] <p>Why This Might Work:</p> <ul> <li>Preserves relative relationships: numbers close in value have similar rotations</li> <li>Can be applied to attention mechanisms</li> <li>Naturally handles different scales through frequency selection</li> </ul> <p>Status (2026): This is an active area of research, with some promising results for mathematical reasoning tasks.</p>"},{"location":"incubation/numerical_embeddings_and_continuous_values/#5-hybrid-approaches","title":"5. Hybrid Approaches","text":"<p>Approach: Combine multiple methods.</p> <p>Example Architecture: <pre><code>class HybridNumericalEmbedding(nn.Module):\n    \"\"\"Combines multiple numerical representation strategies.\"\"\"\n\n    def __init__(self, embedding_dim=128):\n        super().__init__()\n        self.sinusoidal = SinusoidalEmbedding(embedding_dim // 2)\n        self.learned = nn.Linear(1, embedding_dim // 2)\n        self.magnitude_embedding = nn.Embedding(100, embedding_dim // 4)\n\n    def forward(self, n):\n        # Sinusoidal component (multi-scale)\n        sin_emb = self.sinusoidal(n)\n\n        # Learned component (task-specific)\n        learned_emb = self.learned(n.unsqueeze(-1))\n\n        # Magnitude bucket (coarse scale)\n        mag_bucket = self.get_magnitude_bucket(n)\n        mag_emb = self.magnitude_embedding(mag_bucket)\n\n        # Combine\n        return torch.cat([sin_emb, learned_emb, mag_emb], dim=-1)\n</code></pre></p>"},{"location":"incubation/numerical_embeddings_and_continuous_values/#connection-to-time-embedding-in-diffusion-models","title":"Connection to Time Embedding in Diffusion Models","text":""},{"location":"incubation/numerical_embeddings_and_continuous_values/#the-parallel","title":"The Parallel","text":"<p>Time embedding in diffusion models and numerical embeddings in LLMs solve the same fundamental problem: representing a continuous scalar in a way that neural networks can process effectively.</p> <p>Time Embedding (Diffusion):</p> <ul> <li>Input: Continuous time \\(t \\in [0, 1]\\)</li> <li>Challenge: Network needs to distinguish \\(t=0.5\\) from \\(t=0.51\\) and learn time-dependent behavior</li> <li>Solution: Sinusoidal embedding \\(\\gamma(t)\\) with multiple frequencies</li> </ul> <p>Numerical Embedding (LLMs):</p> <ul> <li>Input: Continuous number \\(n \\in \\mathbb{R}\\)</li> <li>Challenge: Network needs to understand \\(n=3.14\\) vs. \\(n=3.15\\) and numerical relationships</li> <li>Solution: Similar sinusoidal embedding \\(\\gamma(n)\\) with multiple frequencies</li> </ul>"},{"location":"incubation/numerical_embeddings_and_continuous_values/#why-sinusoidal-functions-work-for-both","title":"Why Sinusoidal Functions Work for Both","text":"<ol> <li>Multi-scale representation: Different frequencies capture different scales</li> <li>Low frequencies: Coarse magnitude (thousands vs. millions)</li> <li> <p>High frequencies: Fine distinctions (3.14 vs. 3.15)</p> </li> <li> <p>Bounded and stable: Values in \\([-1, 1]\\) prevent training instability</p> </li> <li> <p>Smooth interpolation: Can represent values between training examples</p> </li> <li> <p>Relative relationships: Embeddings for close values are similar</p> </li> </ol>"},{"location":"incubation/numerical_embeddings_and_continuous_values/#key-difference-normalization","title":"Key Difference: Normalization","text":"<p>Time embedding: Usually \\(t \\in [0, 1]\\) (already normalized)</p> <p>Numerical embedding: \\(n \\in \\mathbb{R}\\) (unbounded, needs normalization)</p> <p>For numerical embeddings, you typically need: - Log scaling: \\(\\log(|n| + \\epsilon)\\) for wide ranges - Normalization: \\(\\tanh(n / \\text{scale})\\) to bound values - Sign handling: Separate representation for positive/negative</p>"},{"location":"incubation/numerical_embeddings_and_continuous_values/#relevance-to-computational-biology","title":"Relevance to Computational Biology","text":""},{"location":"incubation/numerical_embeddings_and_continuous_values/#where-continuous-values-appear","title":"Where Continuous Values Appear","text":"<ol> <li>Gene Expression: Expression levels (TPM, FPKM, counts)</li> <li>Concentrations: Protein concentrations, drug doses</li> <li>Measurements: Cell counts, viability, size</li> <li>Time: Time points in time-series experiments</li> <li>Coordinates: Genomic positions, spatial coordinates</li> <li>Scores: Prediction scores, p-values, fold changes</li> </ol>"},{"location":"incubation/numerical_embeddings_and_continuous_values/#potential-applications","title":"Potential Applications","text":""},{"location":"incubation/numerical_embeddings_and_continuous_values/#1-multi-modal-embeddings","title":"1. Multi-modal Embeddings","text":"<p>If you're building a joint latent space (as discussed in <code>joint_latent_space_and_JEPA.md</code>), you need to embed: - Discrete: Gene IDs, cell types, perturbations - Continuous: Expression values, concentrations, time</p> <p>Numerical embedding techniques could help create a unified representation.</p>"},{"location":"incubation/numerical_embeddings_and_continuous_values/#2-time-series-modeling","title":"2. Time-Series Modeling","text":"<p>For time-series data (Perturb-seq, lineage tracing), you need to embed: - Time points: \\(t \\in [0, T]\\) - Expression values: \\(x(t) \\in \\mathbb{R}^d\\)</p> <p>Both benefit from sinusoidal embeddings, potentially sharing the same embedding architecture.</p>"},{"location":"incubation/numerical_embeddings_and_continuous_values/#3-score-networks-for-biological-data","title":"3. Score Networks for Biological Data","text":"<p>In diffusion models for gene expression: - Time embedding: For noise level \\(t\\) in the diffusion process - Expression embedding: For continuous expression values \\(x\\)</p> <p>These could use similar sinusoidal embedding strategies, creating architectural consistency.</p>"},{"location":"incubation/numerical_embeddings_and_continuous_values/#4-attention-mechanisms","title":"4. Attention Mechanisms","text":"<p>If using Transformers for biological sequences: - Positional encoding: For sequence position - Numerical encoding: For expression values, scores, measurements</p> <p>RoPE-style approaches could unify these.</p>"},{"location":"incubation/numerical_embeddings_and_continuous_values/#practical-considerations","title":"Practical Considerations","text":""},{"location":"incubation/numerical_embeddings_and_continuous_values/#when-to-use-sinusoidal-embeddings","title":"When to Use Sinusoidal Embeddings","text":"<p>Good for:</p> <ul> <li>Continuous values that need smooth interpolation</li> <li>Values with known ranges (can normalize)</li> <li>When you want multi-scale representation</li> <li>When you need to generalize to unseen values</li> </ul> <p>Not ideal for:</p> <ul> <li>Very sparse or discrete values (learned embeddings better)</li> <li>Values with complex, non-smooth relationships</li> <li>When exact precision is critical (may need direct encoding)</li> </ul>"},{"location":"incubation/numerical_embeddings_and_continuous_values/#implementation-tips","title":"Implementation Tips","text":"<ol> <li>Normalize your inputs: Scale to a reasonable range before embedding</li> <li>Choose frequencies carefully: Too many high frequencies can cause instability</li> <li>Combine with learned components: Hybrid approaches often work best</li> <li>Test interpolation: Verify that close values have similar embeddings</li> </ol>"},{"location":"incubation/numerical_embeddings_and_continuous_values/#example-gene-expression-embedding","title":"Example: Gene Expression Embedding","text":"<pre><code>class ExpressionEmbedding(nn.Module):\n    \"\"\"Embedding for gene expression values.\"\"\"\n\n    def __init__(self, embedding_dim=64):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n\n    def forward(self, expression):\n        \"\"\"\n        Args:\n            expression: Expression values [batch_size, num_genes]\n\n        Returns:\n            embeddings: [batch_size, num_genes, embedding_dim]\n        \"\"\"\n        # Log transform (expression is typically log-normal)\n        log_expr = torch.log(expression + 1e-8)\n\n        # Normalize to reasonable range\n        normalized = torch.tanh(log_expr / 10.0)\n\n        # Sinusoidal embedding (same as time embedding!)\n        return self.sinusoidal_embedding(normalized)\n\n    def sinusoidal_embedding(self, x):\n        \"\"\"Same implementation as time embedding.\"\"\"\n        half_dim = self.embedding_dim // 2\n        emb = np.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=x.device) * -emb)\n        emb = x[..., None] * emb[None, ...]\n        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n        return emb\n</code></pre>"},{"location":"incubation/numerical_embeddings_and_continuous_values/#research-status-january-2026","title":"Research Status (January 2026)","text":""},{"location":"incubation/numerical_embeddings_and_continuous_values/#active-areas","title":"Active Areas","text":"<ol> <li>Mathematical reasoning: Improving LLM performance on math problems</li> <li>Scientific literature: Better understanding of numerical data in papers</li> <li>Code generation: Handling numerical constants and calculations</li> <li>Multi-modal learning: Combining numerical and textual information</li> </ol>"},{"location":"incubation/numerical_embeddings_and_continuous_values/#recent-papers-2024-2025","title":"Recent Papers (2024-2025)","text":"<p>While specific 2026 papers are still emerging, the following directions are active:</p> <ul> <li>Number-aware tokenization: Better ways to split numbers into tokens</li> <li>Magnitude-preserving embeddings: Maintaining scale relationships</li> <li>RoPE extensions: Applying rotary embeddings to numerical values</li> <li>Hybrid architectures: Combining multiple representation strategies</li> </ul>"},{"location":"incubation/numerical_embeddings_and_continuous_values/#open-questions","title":"Open Questions","text":"<ol> <li>Optimal frequency selection: How to choose \\(\\omega_i\\) for different domains?</li> <li>Normalization strategies: Best practices for different value ranges?</li> <li>Integration with attention: How to effectively use numerical embeddings in Transformers?</li> <li>Domain adaptation: Can embeddings learned on one domain transfer to another?</li> </ol>"},{"location":"incubation/numerical_embeddings_and_continuous_values/#summary","title":"Summary","text":""},{"location":"incubation/numerical_embeddings_and_continuous_values/#key-takeaways","title":"Key Takeaways","text":"<ol> <li> <p>The Problem: Continuous numerical values are hard for token-based models (LLMs) because tokenization destroys numerical relationships.</p> </li> <li> <p>The Solution: Sinusoidal embeddings (like time embedding) provide a natural way to represent continuous values with:</p> </li> <li>Multi-scale representation</li> <li>Smooth interpolation</li> <li> <p>Bounded, stable values</p> </li> <li> <p>The Connection: Time embedding in diffusion models and numerical embeddings in LLMs solve the same problem\u2014representing continuous scalars effectively.</p> </li> <li> <p>The Relevance: For computational biology, numerical embeddings could help:</p> </li> <li>Create unified representations of discrete and continuous biological data</li> <li>Improve time-series modeling</li> <li>Enhance score networks for biological data</li> <li> <p>Enable better attention mechanisms in biological Transformers</p> </li> <li> <p>The Future: This is an active area of research, with promising directions including RoPE-style approaches and hybrid architectures.</p> </li> </ol>"},{"location":"incubation/numerical_embeddings_and_continuous_values/#next-steps","title":"Next Steps","text":"<ol> <li>Experiment with sinusoidal embeddings for gene expression values in your diffusion models</li> <li>Compare sinusoidal vs. learned vs. direct encoding for your specific biological data</li> <li>Explore RoPE-style approaches if using Transformers for biological sequences</li> <li>Monitor research in this area\u2014it's rapidly evolving</li> </ol>"},{"location":"incubation/numerical_embeddings_and_continuous_values/#references","title":"References","text":""},{"location":"incubation/numerical_embeddings_and_continuous_values/#time-embedding-diffusion-models","title":"Time Embedding (Diffusion Models)","text":"<ul> <li>See: <code>docs/diffusion/score_network/time_embedding_and_film.md</code></li> </ul>"},{"location":"incubation/numerical_embeddings_and_continuous_values/#positional-encoding-transformers","title":"Positional Encoding (Transformers)","text":"<ul> <li>Vaswani et al. (2017). \"Attention Is All You Need\" - Original sinusoidal positional encoding</li> </ul>"},{"location":"incubation/numerical_embeddings_and_continuous_values/#rope-rotary-position-embedding","title":"RoPE (Rotary Position Embedding)","text":"<ul> <li>Su et al. (2021). \"RoFormer: Enhanced Transformer with Rotary Position Embedding\"</li> </ul>"},{"location":"incubation/numerical_embeddings_and_continuous_values/#numerical-representation-in-llms","title":"Numerical Representation in LLMs","text":"<ul> <li>Active research area (2024-2026) - watch for recent papers on:</li> <li>Number-aware tokenization</li> <li>Magnitude-preserving embeddings</li> <li>Mathematical reasoning improvements</li> </ul>"},{"location":"incubation/numerical_embeddings_and_continuous_values/#related-documents","title":"Related Documents","text":"<ul> <li>Time Embedding: <code>docs/diffusion/score_network/time_embedding_and_film.md</code></li> <li>Joint Latent Spaces: <code>docs/incubation/joint_latent_space_and_JEPA.md</code></li> <li>Score Networks: <code>docs/diffusion/score_network/advanced_architectures.md</code></li> </ul>"},{"location":"latent_diffusion/","title":"Latent Diffusion Models Documentation","text":"<p>Latent Diffusion Models (LDMs) \u2014 Efficient high-quality generation by diffusing in compressed latent space instead of pixel/gene space.</p> <p>This documentation series covers latent diffusion from theory through implementation, with a focus on computational biology applications including single-cell generation, perturbation prediction, and multi-omics integration.</p>"},{"location":"latent_diffusion/#core-documentation-series","title":"Core Documentation Series","text":""},{"location":"latent_diffusion/#1-overview","title":"1. Overview","text":"<p>00_latent_diffusion_overview.md \u2014 What is latent diffusion and why it matters - The problem with pixel-space diffusion - Two-stage approach: VAE + diffusion - Why latent diffusion for biology - Comparison with alternatives - Applications overview</p>"},{"location":"latent_diffusion/#2-foundations","title":"2. Foundations","text":"<p>01_latent_diffusion_foundations.md \u2014 Architecture and components - VAE/VQ-VAE autoencoders - Latent diffusion models - Conditioning mechanisms - Complete PyTorch implementations</p>"},{"location":"latent_diffusion/#3-training","title":"3. Training","text":"<p>02_latent_diffusion_training.md \u2014 Training strategies - Two-stage training (VAE \u2192 Diffusion) - Joint fine-tuning - Hyperparameters - Optimization strategies - Monitoring and debugging</p>"},{"location":"latent_diffusion/#4-applications","title":"4. Applications","text":"<p>03_latent_diffusion_applications.md \u2014 Biology applications - Single-cell generation - Perturbation prediction - Multi-omics translation - Trajectory modeling - Spatial transcriptomics</p>"},{"location":"latent_diffusion/#5-computational-biology-implementation","title":"5. Computational Biology Implementation","text":"<p>04_latent_diffusion_combio.md \u2014 Complete implementation - scRNA-seq latent diffusion - Perturb-seq with latent diffusion - Multi-omics latent diffusion - End-to-end training and evaluation</p>"},{"location":"latent_diffusion/#quick-navigation","title":"Quick Navigation","text":""},{"location":"latent_diffusion/#for-different-audiences","title":"For Different Audiences","text":"<p>New to Latent Diffusion? 1. Start with Overview 2. Understand the two-stage approach 3. See why it's efficient (10-100\u00d7 speedup) 4. Review biology applications</p> <p>Coming from Diffusion Models? 1. Read Overview comparison section 2. Understand VAE compression stage 3. Learn when latent diffusion is better 4. See efficiency gains</p> <p>Coming from VAE? 1. Read why VAE alone isn't enough 2. Understand how diffusion improves quality 3. Learn the two-stage training 4. See applications</p> <p>Ready to Implement? 1. Review Foundations architecture 2. Follow Training pipeline 3. Adapt Combio Implementation code 4. Evaluate on your data</p>"},{"location":"latent_diffusion/#key-concepts","title":"Key Concepts","text":""},{"location":"latent_diffusion/#the-two-stage-approach","title":"The Two-Stage Approach","text":"<p>Stage 1: Autoencoder (VAE/VQ-VAE) <pre><code>x \u2208 \u211d^20000 \u2192 Encoder \u2192 z \u2208 \u211d^256 \u2192 Decoder \u2192 x\u0302 \u2208 \u211d^20000\n</code></pre> - Compress high-dimensional data to latent space - Learn semantic representation - Freeze after training</p> <p>Stage 2: Diffusion in Latent Space <pre><code>z\u2080 \u2192 ... \u2192 z\u209c \u2192 ... \u2192 z_T\n(Operate on \u211d^256 instead of \u211d^20000)\n</code></pre> - Diffuse in compressed space - 78\u00d7 fewer dimensions - 10-100\u00d7 faster</p>"},{"location":"latent_diffusion/#why-this-works","title":"Why This Works","text":"<p>Biological data is low-rank:</p> <ul> <li>20K genes measured</li> <li>~100-500 effective dimensions</li> <li>Most variation in top PCs</li> </ul> <p>Latent space captures biology:</p> <ul> <li>Cell types</li> <li>Pathways</li> <li>States</li> <li>Transitions</li> </ul> <p>Diffusion adds quality:</p> <ul> <li>Sharper than VAE</li> <li>Better mode coverage</li> <li>Stable training</li> </ul>"},{"location":"latent_diffusion/#comparison-tables","title":"Comparison Tables","text":""},{"location":"latent_diffusion/#latent-diffusion-vs-pixel-space-diffusion","title":"Latent Diffusion vs Pixel-Space Diffusion","text":"Aspect Pixel-Space Latent Diffusion Dimensions 20,000 256 Training time 1 week 1 day Sampling time 10s 1-2s Memory 16GB 2GB Quality Good Better Interpretability Low Higher"},{"location":"latent_diffusion/#latent-diffusion-vs-vae","title":"Latent Diffusion vs VAE","text":"Aspect VAE Latent Diffusion Sample quality Blurry Sharp Mode coverage Poor Excellent Training Fast Moderate Sampling Instant Moderate (50 steps) Likelihood Exact Approximate"},{"location":"latent_diffusion/#latent-diffusion-vs-gan","title":"Latent Diffusion vs GAN","text":"Aspect GAN Latent Diffusion Training stability Unstable Stable Mode coverage Poor Excellent Sample quality Excellent Excellent Controllability Moderate High Likelihood No Yes (approximate)"},{"location":"latent_diffusion/#architecture-components","title":"Architecture Components","text":""},{"location":"latent_diffusion/#1-autoencoder-stage-1","title":"1. Autoencoder (Stage 1)","text":"<p>Purpose: Learn compressed latent representation</p> <p>Options:</p> <ul> <li>VAE \u2014 Continuous latent, probabilistic</li> <li>VQ-VAE \u2014 Discrete latent, codebook</li> <li>VQ-GAN \u2014 Discrete + adversarial (best quality)</li> </ul> <p>For biology: VAE works well, simple and effective</p> <p>Architecture: <pre><code>class BiologicalVAE(nn.Module):\n    def __init__(self, num_genes=20000, latent_dim=256):\n        self.encoder = Encoder(num_genes, latent_dim)\n        self.decoder = Decoder(latent_dim, num_genes)\n\n    def encode(self, x):\n        mu, logvar = self.encoder(x)\n        z = reparameterize(mu, logvar)\n        return z\n\n    def decode(self, z):\n        return self.decoder(z)\n</code></pre></p>"},{"location":"latent_diffusion/#2-latent-diffusion-model-stage-2","title":"2. Latent Diffusion Model (Stage 2)","text":"<p>Purpose: Generate latent codes</p> <p>Options:</p> <ul> <li>DDPM \u2014 Original diffusion</li> <li>DDIM \u2014 Deterministic, faster sampling</li> <li>Rectified Flow \u2014 Straight paths, optimal</li> <li>DiT \u2014 Transformer-based</li> </ul> <p>For biology: Rectified Flow + DiT (best efficiency)</p> <p>Architecture: <pre><code>class LatentDiffusion(nn.Module):\n    def __init__(self, latent_dim=256):\n        self.model = DiT(input_dim=latent_dim)\n        self.flow = RectifiedFlow()\n\n    def forward(self, z, t):\n        return self.model(z, t)\n\n    def sample(self, num_samples, num_steps=50):\n        z_T = torch.randn(num_samples, latent_dim)\n        z_0 = self.flow.sample(z_T, num_steps)\n        return z_0\n</code></pre></p>"},{"location":"latent_diffusion/#3-conditioning","title":"3. Conditioning","text":"<p>Purpose: Control generation</p> <p>Mechanisms:</p> <ul> <li>Concatenation \u2014 Simple, effective</li> <li>Cross-attention \u2014 Flexible, powerful</li> <li>FiLM \u2014 Affine transformation</li> <li>Adaptive LayerNorm \u2014 DiT-style</li> </ul> <p>For biology:</p> <ul> <li>Cell type: Class embedding</li> <li>Perturbation: Gene embedding</li> <li>Time: Sinusoidal encoding</li> <li>Continuous: Direct concatenation</li> </ul>"},{"location":"latent_diffusion/#biology-applications","title":"Biology Applications","text":""},{"location":"latent_diffusion/#1-single-cell-generation","title":"1. Single-Cell Generation","text":"<p>Task: Generate realistic single-cell profiles</p> <p>Workflow: 1. Train VAE on scRNA-seq 2. Train diffusion on latent codes 3. Sample: noise \u2192 latent \u2192 scRNA-seq</p> <p>Benefits:</p> <ul> <li>Data augmentation</li> <li>Rare cell type generation</li> <li>Batch effect removal</li> </ul> <p>Use cases:</p> <ul> <li>Expand training data</li> <li>Generate synthetic controls</li> <li>Simulate experiments</li> </ul>"},{"location":"latent_diffusion/#2-perturbation-prediction","title":"2. Perturbation Prediction","text":"<p>Task: Predict cellular response to perturbations</p> <p>Workflow: 1. Encode baseline to latent 2. Condition diffusion on perturbation 3. Generate perturbed latent 4. Decode to gene expression</p> <p>Benefits:</p> <ul> <li>Virtual screening</li> <li>Combination prediction</li> <li>Mechanism discovery</li> </ul> <p>Use cases:</p> <ul> <li>Drug discovery</li> <li>CRISPR screening</li> <li>Genetic interaction mapping</li> </ul>"},{"location":"latent_diffusion/#3-multi-omics-translation","title":"3. Multi-Omics Translation","text":"<p>Task: Predict one modality from another</p> <p>Workflow: 1. Train joint VAE (RNA + Protein \u2192 shared latent) 2. Condition diffusion on RNA latent 3. Generate Protein latent 4. Decode to Protein expression</p> <p>Benefits:</p> <ul> <li>Fill missing modalities</li> <li>Cross-modality validation</li> <li>Integrated analysis</li> </ul> <p>Use cases:</p> <ul> <li>CITE-seq imputation</li> <li>Predict protein from RNA</li> <li>Multi-omics integration</li> </ul>"},{"location":"latent_diffusion/#4-trajectory-modeling","title":"4. Trajectory Modeling","text":"<p>Task: Model developmental/disease trajectories</p> <p>Workflow: 1. Encode cells to latent 2. Condition diffusion on time 3. Generate future states 4. Decode to expression</p> <p>Benefits:</p> <ul> <li>Predict differentiation</li> <li>Model disease progression</li> <li>Identify branch points</li> </ul> <p>Use cases:</p> <ul> <li>Developmental biology</li> <li>Disease modeling</li> <li>Drug response over time</li> </ul>"},{"location":"latent_diffusion/#5-spatial-transcriptomics","title":"5. Spatial Transcriptomics","text":"<p>Task: Generate spatial gene expression</p> <p>Workflow: 1. Encode spatial data to latent 2. Condition diffusion on coordinates 3. Generate expression at location 4. Decode to genes</p> <p>Benefits:</p> <ul> <li>Super-resolution</li> <li>Missing region imputation</li> <li>3D reconstruction</li> </ul> <p>Use cases:</p> <ul> <li>Enhance spatial resolution</li> <li>Fill tissue gaps</li> <li>Predict 3D structure</li> </ul>"},{"location":"latent_diffusion/#training-pipeline","title":"Training Pipeline","text":""},{"location":"latent_diffusion/#two-stage-training","title":"Two-Stage Training","text":"<p>Stage 1: Train Autoencoder <pre><code># Train VAE\nvae = BiologicalVAE(num_genes=20000, latent_dim=256)\ntrain_vae(vae, scrnaseq_data, num_epochs=100)\n\n# Freeze\nvae.eval()\nfor param in vae.parameters():\n    param.requires_grad = False\n</code></pre></p> <p>Stage 2: Train Diffusion <pre><code># Encode data to latent\nwith torch.no_grad():\n    latents = vae.encode(scrnaseq_data)\n\n# Train diffusion\ndiffusion = LatentDiffusion(latent_dim=256)\ntrain_diffusion(diffusion, latents, num_epochs=100)\n</code></pre></p> <p>Optional: Joint Fine-Tuning <pre><code># Unfreeze all\nfor param in vae.parameters():\n    param.requires_grad = True\n\n# Fine-tune together\ntrain_joint(vae, diffusion, scrnaseq_data, num_epochs=20)\n</code></pre></p>"},{"location":"latent_diffusion/#sampling-pipeline","title":"Sampling Pipeline","text":"<p>Generate new samples: <pre><code># Sample latent from diffusion\nz_0 = diffusion.sample(num_samples=100, num_steps=50)\n\n# Decode to gene expression\nx_gen = vae.decode(z_0)\n</code></pre></p> <p>Conditional generation: <pre><code># Condition on cell type\ncell_type_emb = encode_cell_type(\"T cell\")\nz_0 = diffusion.sample(num_samples=100, condition=cell_type_emb)\nx_gen = vae.decode(z_0)\n</code></pre></p>"},{"location":"latent_diffusion/#efficiency-gains","title":"Efficiency Gains","text":""},{"location":"latent_diffusion/#computational-savings","title":"Computational Savings","text":"<p>For 20K genes \u2192 256 latent dims:</p> Operation Pixel-Space Latent Space Speedup Forward pass 20K dims 256 dims 78\u00d7 Training epoch 1 hour 5 min 12\u00d7 Full training 1 week 1 day 7\u00d7 Sampling 10s 1-2s 5-10\u00d7 Memory 16GB 2GB 8\u00d7"},{"location":"latent_diffusion/#quality-improvements","title":"Quality Improvements","text":"<p>Better than VAE:</p> <ul> <li>Sharper samples (no blurriness)</li> <li>Better mode coverage (all cell types)</li> <li>More realistic (passes biological QC)</li> </ul> <p>Comparable to pixel-space diffusion:</p> <ul> <li>Same sample quality</li> <li>Better efficiency</li> <li>More interpretable</li> </ul>"},{"location":"latent_diffusion/#when-to-use-latent-diffusion","title":"When to Use Latent Diffusion","text":""},{"location":"latent_diffusion/#use-latent-diffusion-when","title":"\u2705 Use Latent Diffusion When:","text":"<p>High-dimensional data:</p> <ul> <li>Gene expression (20K genes)</li> <li>Multi-omics (RNA + Protein + ATAC)</li> <li>Spatial transcriptomics</li> </ul> <p>Need efficiency:</p> <ul> <li>Large datasets (millions of cells)</li> <li>Limited compute</li> <li>Fast iteration required</li> </ul> <p>Want quality + diversity:</p> <ul> <li>Better than VAE</li> <li>Stable than GAN</li> <li>Good mode coverage</li> </ul> <p>Multi-task learning:</p> <ul> <li>Generation + prediction</li> <li>Multiple conditions</li> <li>Transfer across datasets</li> </ul>"},{"location":"latent_diffusion/#dont-use-latent-diffusion-when","title":"\u274c Don't Use Latent Diffusion When:","text":"<p>Low-dimensional data:</p> <ul> <li>Already &lt;1000 dims</li> <li>Pixel-space diffusion is fine</li> </ul> <p>Need exact likelihood:</p> <ul> <li>VAE or normalizing flow better</li> <li>Latent diffusion likelihood is approximate</li> </ul> <p>Real-time inference:</p> <ul> <li>Sampling still slower than VAE</li> <li>Consider distillation</li> </ul> <p>Simple tasks:</p> <ul> <li>Linear models sufficient</li> <li>Overkill for simple prediction</li> </ul>"},{"location":"latent_diffusion/#implementation-roadmap","title":"Implementation Roadmap","text":""},{"location":"latent_diffusion/#phase-1-vae-training","title":"Phase 1: VAE Training","text":"<ul> <li> Data preprocessing</li> <li> VAE architecture</li> <li> Training loop</li> <li> Reconstruction quality check</li> <li> Latent space visualization</li> </ul>"},{"location":"latent_diffusion/#phase-2-latent-diffusion","title":"Phase 2: Latent Diffusion","text":"<ul> <li> Encode data to latent</li> <li> Diffusion model architecture</li> <li> Training on latent codes</li> <li> Sampling quality check</li> <li> Conditional generation</li> </ul>"},{"location":"latent_diffusion/#phase-3-applications","title":"Phase 3: Applications","text":"<ul> <li> Single-cell generation</li> <li> Perturbation prediction</li> <li> Multi-omics translation</li> <li> Trajectory modeling</li> <li> Evaluation metrics</li> </ul>"},{"location":"latent_diffusion/#phase-4-optimization","title":"Phase 4: Optimization","text":"<ul> <li> Joint fine-tuning</li> <li> Faster sampling (DDIM, few-step)</li> <li> Classifier-free guidance</li> <li> Distributed training</li> <li> Production deployment</li> </ul>"},{"location":"latent_diffusion/#learning-path","title":"Learning Path","text":""},{"location":"latent_diffusion/#beginner-path","title":"Beginner Path","text":"<ol> <li>Understand the concept \u2014 Overview</li> <li>Learn VAE basics \u2014 Autoencoder stage</li> <li>Learn diffusion basics \u2014 Latent diffusion stage</li> <li>See applications \u2014 Applications</li> </ol>"},{"location":"latent_diffusion/#intermediate-path","title":"Intermediate Path","text":"<ol> <li>Review architecture \u2014 Foundations</li> <li>Implement VAE \u2014 Train on scRNA-seq</li> <li>Implement diffusion \u2014 Train on latent codes</li> <li>Apply to biology \u2014 Combio Implementation</li> </ol>"},{"location":"latent_diffusion/#advanced-path","title":"Advanced Path","text":"<ol> <li>Joint fine-tuning \u2014 End-to-end optimization</li> <li>Multi-modal \u2014 Multi-omics integration</li> <li>Novel conditioning \u2014 Custom conditioning mechanisms</li> <li>Production \u2014 Scale to millions of cells</li> </ol>"},{"location":"latent_diffusion/#related-documentation","title":"Related Documentation","text":""},{"location":"latent_diffusion/#within-this-project","title":"Within This Project","text":"<p>Diffusion Models:</p> <ul> <li>DDPM \u2014 Denoising diffusion</li> <li>SDE \u2014 Stochastic differential equations</li> <li>Flow Matching \u2014 Rectified flow</li> <li>DiT \u2014 Diffusion transformers</li> </ul> <p>Representation Learning:</p> <ul> <li>VAE \u2014 Variational autoencoders</li> <li>JEPA \u2014 Joint embedding predictive architecture</li> </ul> <p>Architecture Choices:</p> <ul> <li>Gene Expression Architectures</li> </ul>"},{"location":"latent_diffusion/#external-resources","title":"External Resources","text":"<p>Latent Diffusion Papers:</p> <ul> <li>Rombach et al. (2022): \"High-Resolution Image Synthesis with Latent Diffusion Models\" (Stable Diffusion)</li> <li>Vahdat et al. (2021): \"Score-based Generative Modeling in Latent Space\"</li> </ul> <p>Autoencoder Papers:</p> <ul> <li>Kingma &amp; Welling (2014): \"Auto-Encoding Variational Bayes\" (VAE)</li> <li>van den Oord et al. (2017): \"Neural Discrete Representation Learning\" (VQ-VAE)</li> <li>Esser et al. (2021): \"Taming Transformers for High-Resolution Image Synthesis\" (VQ-GAN)</li> </ul> <p>Biology Applications:</p> <ul> <li>Lopez et al. (2018): \"Deep generative modeling for single-cell transcriptomics\" (scVI)</li> <li>Lotfollahi et al. (2023): \"Predicting cellular responses to novel drug combinations\"</li> <li>Bunne et al. (2023): \"Learning Single-Cell Perturbation Responses using Neural Optimal Transport\"</li> </ul>"},{"location":"latent_diffusion/#key-takeaways","title":"Key Takeaways","text":""},{"location":"latent_diffusion/#conceptual","title":"Conceptual","text":"<ol> <li>Two-stage approach \u2014 VAE compression + latent diffusion</li> <li>Efficiency \u2014 10-100\u00d7 faster than pixel-space</li> <li>Quality \u2014 Better than VAE, stable than GAN</li> <li>Flexibility \u2014 Multi-modal, multi-task, controllable</li> </ol>"},{"location":"latent_diffusion/#practical","title":"Practical","text":"<ol> <li>Train VAE first \u2014 Get good latent space</li> <li>Freeze VAE \u2014 Train diffusion on latent codes</li> <li>Optional fine-tuning \u2014 Joint optimization</li> <li>Condition carefully \u2014 Use appropriate mechanism</li> </ol>"},{"location":"latent_diffusion/#for-biology","title":"For Biology","text":"<ol> <li>Perfect for scRNA-seq \u2014 High-dim, low-rank</li> <li>Enables multi-omics \u2014 Shared latent space</li> <li>Scalable \u2014 Millions of cells</li> <li>Interpretable \u2014 Latent dimensions have meaning</li> </ol>"},{"location":"latent_diffusion/#getting-started","title":"Getting Started","text":"<p>Quick start: <pre><code># Read overview\ncat docs/latent_diffusion/00_latent_diffusion_overview.md\n\n# Understand architecture\ncat docs/latent_diffusion/01_latent_diffusion_foundations.md\n\n# See training examples\ncat docs/latent_diffusion/02_latent_diffusion_training.md\n</code></pre></p> <p>For biology applications: <pre><code># Jump to applications\ncat docs/latent_diffusion/03_latent_diffusion_applications.md\n\n# Deep dive into implementation\ncat docs/latent_diffusion/04_latent_diffusion_combio.md\n</code></pre></p> <p>For implementation: <pre><code># Check source code (when available)\nls src/genailab/latent_diffusion/\n\n# Run notebooks (when available)\nls notebooks/latent_diffusion/\n</code></pre></p>"},{"location":"latent_diffusion/#status","title":"Status","text":"<p>Documentation: \ud83d\udea7 In Progress - [x] Overview - [ ] Foundations - [ ] Training - [ ] Applications - [ ] Combio Implementation</p> <p>Implementation: \ud83d\udd32 Planned - [ ] VAE modules - [ ] Latent diffusion modules - [ ] Training infrastructure - [ ] Biology applications</p> <p>Notebooks: \ud83d\udd32 Planned - [ ] VAE training - [ ] Latent diffusion training - [ ] Single-cell generation - [ ] Perturbation prediction - [ ] Multi-omics translation</p>"},{"location":"latent_diffusion/00_latent_diffusion_overview/","title":"Latent Diffusion Models: Overview","text":"<p>Latent Diffusion Models (LDMs) combine the efficiency of latent variable models with the power of diffusion models, enabling high-quality generation at a fraction of the computational cost.</p> <p>Key insight: Diffuse in a compressed latent space instead of high-dimensional pixel/gene space.</p>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#the-problem-with-pixel-space-diffusion","title":"The Problem with Pixel-Space Diffusion","text":""},{"location":"latent_diffusion/00_latent_diffusion_overview/#computational-cost","title":"Computational Cost","text":"<p>Standard diffusion models operate directly on data: - Images: Diffuse in \\(\\mathbb{R}^{H \\times W \\times C}\\) (e.g., 256\u00d7256\u00d73 = 196,608 dims) - Gene expression: Diffuse in \\(\\mathbb{R}^{20000}\\) (20K genes) - Multi-omics: Even higher dimensional</p> <p>Consequences: 1. Slow training \u2014 Many denoising steps on high-dim data 2. Slow sampling \u2014 50-1000 steps in high-dim space 3. Memory intensive \u2014 Store gradients for all dimensions 4. Inefficient \u2014 Most dimensions are redundant</p> <p>Example: DDPM on 256\u00d7256 images - Training: ~1 week on 8 GPUs - Sampling: ~10 seconds per image (50 steps) - Memory: ~16GB per batch</p>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#the-solution-latent-diffusion","title":"The Solution: Latent Diffusion","text":""},{"location":"latent_diffusion/00_latent_diffusion_overview/#core-idea","title":"Core Idea","text":"<p>Two-stage approach:</p> <p>Stage 1: Learn compressed latent space (VAE/VQ-VAE) <pre><code>x \u2208 \u211d^20000 \u2192 Encoder \u2192 z \u2208 \u211d^256 \u2192 Decoder \u2192 x\u0302 \u2208 \u211d^20000\n</code></pre></p> <p>Stage 2: Diffusion in latent space <pre><code>z\u2080 \u2192 ... \u2192 z\u209c \u2192 ... \u2192 z_T\n(Diffusion operates on \u211d^256 instead of \u211d^20000)\n</code></pre></p> <p>Benefits:</p> <ul> <li>78\u00d7 fewer dimensions (20K \u2192 256)</li> <li>10-100\u00d7 faster training</li> <li>5-10\u00d7 faster sampling</li> <li>Better sample quality (focuses on semantic content)</li> </ul>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#architecture-overview","title":"Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 Latent Diffusion Model              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n\u2502  \u2502   Data   \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502 Encoder  \u2502                \u2502\n\u2502  \u2502 x \u2208 \u211d^D  \u2502         \u2502 (VAE/VQ) \u2502                \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n\u2502                            \u2502                        \u2502\n\u2502                            v                        \u2502\n\u2502                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n\u2502                      \u2502 Latent z \u2502                  \u2502\n\u2502                      \u2502  \u2208 \u211d^d   \u2502                  \u2502\n\u2502                      \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n\u2502                            \u2502                        \u2502\n\u2502                            v                        \u2502\n\u2502                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u2502\n\u2502                   \u2502   Diffusion    \u2502               \u2502\n\u2502                   \u2502  z\u2080 \u2192 z\u209c \u2192 z_T \u2502               \u2502\n\u2502                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2502\n\u2502                            \u2502                        \u2502\n\u2502                            v                        \u2502\n\u2502                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n\u2502                      \u2502 Denoised \u2502                  \u2502\n\u2502                      \u2502    z\u2080    \u2502                  \u2502\n\u2502                      \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n\u2502                            \u2502                        \u2502\n\u2502                            v                        \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n\u2502  \u2502 Generated\u2502&lt;\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502 Decoder  \u2502                \u2502\n\u2502  \u2502    x\u0302     \u2502         \u2502 (VAE/VQ) \u2502                \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n\u2502                                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#why-latent-diffusion-for-biology","title":"Why Latent Diffusion for Biology?","text":""},{"location":"latent_diffusion/00_latent_diffusion_overview/#1-dimensionality-reduction","title":"1. Dimensionality Reduction","text":"<p>Gene expression is high-dimensional but low-rank:</p> <ul> <li>20K genes measured</li> <li>~100-500 effective dimensions (pathways, modules)</li> <li>Most variation captured by top PCs</li> </ul> <p>Latent diffusion exploits this:</p> <ul> <li>Compress to semantic latent space</li> <li>Diffuse in compressed space</li> <li>Decode to full gene space</li> </ul>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#2-computational-efficiency","title":"2. Computational Efficiency","text":"<p>For single-cell data:</p> <ul> <li>Millions of cells \u00d7 20K genes = intractable</li> <li>Latent space: Millions of cells \u00d7 256 dims = manageable</li> </ul> <p>Speedup example:</p> <ul> <li>Pixel-space: 1000 steps \u00d7 20K dims = 20M operations</li> <li>Latent-space: 1000 steps \u00d7 256 dims = 256K operations</li> <li>78\u00d7 faster</li> </ul>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#3-better-generalization","title":"3. Better Generalization","text":"<p>Latent space focuses on biology:</p> <ul> <li>Remove technical noise (batch effects, dropout)</li> <li>Capture biological variation (cell types, states)</li> <li>Generalize better to new conditions</li> </ul>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#4-multi-modal-integration","title":"4. Multi-Modal Integration","text":"<p>Natural for multi-omics:</p> <ul> <li>Shared latent space for RNA + Protein + ATAC</li> <li>Diffusion operates on joint representation</li> <li>Generate any modality from latent</li> </ul>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#latent-diffusion-vs-alternatives","title":"Latent Diffusion vs Alternatives","text":""},{"location":"latent_diffusion/00_latent_diffusion_overview/#vs-pixel-space-diffusion","title":"vs Pixel-Space Diffusion","text":"Aspect Pixel-Space Latent Diffusion Training speed Slow 10-100\u00d7 faster Sampling speed Slow 5-10\u00d7 faster Memory High Low Quality Good Better (semantic focus) Interpretability Low Higher (latent structure)"},{"location":"latent_diffusion/00_latent_diffusion_overview/#vs-vae-alone","title":"vs VAE Alone","text":"Aspect VAE Latent Diffusion Sample quality Blurry Sharp Mode coverage Poor Excellent Training Fast Moderate Sampling Fast Moderate Likelihood Tractable Intractable"},{"location":"latent_diffusion/00_latent_diffusion_overview/#vs-gan","title":"vs GAN","text":"Aspect GAN Latent Diffusion Training stability Unstable Stable Mode coverage Poor Excellent Sample quality Excellent Excellent Likelihood No Yes (approximate) Controllability Moderate High"},{"location":"latent_diffusion/00_latent_diffusion_overview/#key-components","title":"Key Components","text":""},{"location":"latent_diffusion/00_latent_diffusion_overview/#1-autoencoder-vae-or-vq-vae","title":"1. Autoencoder (VAE or VQ-VAE)","text":"<p>Purpose: Compress data to latent space</p> <p>Options:</p> <ul> <li>VAE: Continuous latent, probabilistic</li> <li>VQ-VAE: Discrete latent, deterministic</li> <li>VQ-GAN: Discrete + adversarial (best quality)</li> </ul> <p>For biology: VAE is simpler and works well</p>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#2-latent-diffusion-model","title":"2. Latent Diffusion Model","text":"<p>Purpose: Generate latent codes</p> <p>Options:</p> <ul> <li>DDPM: Original diffusion</li> <li>DDIM: Faster sampling</li> <li>Rectified Flow: Straight paths</li> <li>DiT: Transformer-based</li> </ul> <p>For biology: Rectified Flow + DiT (best efficiency)</p>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#3-conditioning-mechanism","title":"3. Conditioning Mechanism","text":"<p>Purpose: Control generation</p> <p>Options:</p> <ul> <li>Class labels: Cell type, perturbation</li> <li>Continuous: Time, dose, expression levels</li> <li>Cross-attention: Text, gene sets, pathways</li> <li>Concatenation: Simple but effective</li> </ul> <p>For biology: Cross-attention for gene sets, concatenation for perturbations</p>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#applications-in-computational-biology","title":"Applications in Computational Biology","text":""},{"location":"latent_diffusion/00_latent_diffusion_overview/#1-single-cell-generation","title":"1. Single-Cell Generation","text":"<p>Task: Generate realistic single-cell profiles</p> <p>Approach: <pre><code>Train VAE: scRNA-seq \u2192 latent\nTrain diffusion: latent \u2192 latent\nSample: noise \u2192 latent \u2192 scRNA-seq\n</code></pre></p> <p>Benefits:</p> <ul> <li>Data augmentation</li> <li>Rare cell type generation</li> <li>Batch effect removal</li> </ul>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#2-perturbation-prediction","title":"2. Perturbation Prediction","text":"<p>Task: Predict cellular response to perturbations</p> <p>Approach: <pre><code>Condition on: baseline + perturbation\nGenerate: perturbed state\n</code></pre></p> <p>Benefits:</p> <ul> <li>Virtual screening</li> <li>Combination prediction</li> <li>Mechanism discovery</li> </ul>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#3-multi-omics-translation","title":"3. Multi-Omics Translation","text":"<p>Task: Predict one modality from another</p> <p>Approach: <pre><code>Train joint VAE: RNA + Protein \u2192 shared latent\nCondition diffusion on: RNA latent\nGenerate: Protein latent \u2192 Protein\n</code></pre></p> <p>Benefits:</p> <ul> <li>Fill missing modalities</li> <li>Cross-modality validation</li> <li>Integrated analysis</li> </ul>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#4-trajectory-modeling","title":"4. Trajectory Modeling","text":"<p>Task: Model developmental/disease trajectories</p> <p>Approach: <pre><code>Condition on: time, cell state\nGenerate: future states\n</code></pre></p> <p>Benefits:</p> <ul> <li>Predict differentiation</li> <li>Model disease progression</li> <li>Identify branch points</li> </ul>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#5-spatial-transcriptomics","title":"5. Spatial Transcriptomics","text":"<p>Task: Generate spatial gene expression</p> <p>Approach: <pre><code>Condition on: spatial coordinates\nGenerate: expression at location\n</code></pre></p> <p>Benefits:</p> <ul> <li>Super-resolution</li> <li>Missing region imputation</li> <li>3D reconstruction</li> </ul>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#training-strategy","title":"Training Strategy","text":""},{"location":"latent_diffusion/00_latent_diffusion_overview/#two-stage-training","title":"Two-Stage Training","text":"<p>Stage 1: Train Autoencoder <pre><code># Train VAE on gene expression\nvae = VAE(input_dim=20000, latent_dim=256)\ntrain_vae(vae, gene_expression_data)\n\n# Freeze encoder/decoder\nvae.eval()\nfor param in vae.parameters():\n    param.requires_grad = False\n</code></pre></p> <p>Stage 2: Train Diffusion in Latent Space <pre><code># Encode data to latent\nz = vae.encode(gene_expression_data)\n\n# Train diffusion on latent codes\ndiffusion = LatentDiffusion(latent_dim=256)\ntrain_diffusion(diffusion, z)\n</code></pre></p>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#joint-fine-tuning-optional","title":"Joint Fine-Tuning (Optional)","text":"<p>After separate training, fine-tune together: <pre><code># Unfreeze all\nfor param in vae.parameters():\n    param.requires_grad = True\nfor param in diffusion.parameters():\n    param.requires_grad = True\n\n# Fine-tune end-to-end\ntrain_joint(vae, diffusion, gene_expression_data)\n</code></pre></p>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#sampling-process","title":"Sampling Process","text":""},{"location":"latent_diffusion/00_latent_diffusion_overview/#generation-pipeline","title":"Generation Pipeline","text":"<p>1. Sample latent from diffusion: <pre><code># Start from noise\nz_T = torch.randn(batch_size, latent_dim)\n\n# Denoise\nz_0 = diffusion.sample(z_T, num_steps=50)\n</code></pre></p> <p>2. Decode to data space: <pre><code># Decode latent to gene expression\nx_gen = vae.decode(z_0)\n</code></pre></p> <p>3. Post-processing (optional): <pre><code># Ensure non-negative (for counts)\nx_gen = torch.clamp(x_gen, min=0)\n\n# Normalize\nx_gen = normalize(x_gen)\n</code></pre></p>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#conditional-generation","title":"Conditional Generation","text":"<p>With perturbation conditioning: <pre><code># Encode baseline\nz_baseline = vae.encode(x_baseline)\n\n# Add perturbation embedding\nz_cond = torch.cat([z_baseline, pert_emb], dim=-1)\n\n# Sample with conditioning\nz_T = torch.randn(batch_size, latent_dim)\nz_0 = diffusion.sample(z_T, condition=z_cond, num_steps=50)\n\n# Decode\nx_perturbed = vae.decode(z_0)\n</code></pre></p>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#advantages-for-biology","title":"Advantages for Biology","text":""},{"location":"latent_diffusion/00_latent_diffusion_overview/#1-efficiency","title":"1. Efficiency","text":"<p>Computational:</p> <ul> <li>10-100\u00d7 faster training than pixel-space</li> <li>5-10\u00d7 faster sampling</li> <li>Scalable to millions of cells</li> </ul> <p>Memory:</p> <ul> <li>Lower memory footprint</li> <li>Larger batch sizes possible</li> <li>Distributed training easier</li> </ul>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#2-quality","title":"2. Quality","text":"<p>Better samples:</p> <ul> <li>Sharper than VAE</li> <li>More diverse than GAN</li> <li>Biologically realistic</li> </ul> <p>Robustness:</p> <ul> <li>Handles technical noise</li> <li>Generalizes across batches</li> <li>Stable training</li> </ul>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#3-interpretability","title":"3. Interpretability","text":"<p>Latent structure:</p> <ul> <li>Dimensions correspond to biology</li> <li>Can analyze latent space</li> <li>Identify key factors</li> </ul> <p>Controllability:</p> <ul> <li>Fine-grained conditioning</li> <li>Interpolation in latent space</li> <li>Compositional generation</li> </ul>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#4-flexibility","title":"4. Flexibility","text":"<p>Multi-modal:</p> <ul> <li>Shared latent for multi-omics</li> <li>Cross-modality generation</li> <li>Integrated analysis</li> </ul> <p>Multi-task:</p> <ul> <li>Single model for multiple tasks</li> <li>Transfer learning</li> <li>Few-shot adaptation</li> </ul>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#comparison-stable-diffusion-vs-bio-latent-diffusion","title":"Comparison: Stable Diffusion vs Bio Latent Diffusion","text":""},{"location":"latent_diffusion/00_latent_diffusion_overview/#stable-diffusion-images","title":"Stable Diffusion (Images)","text":"<p>Architecture:</p> <ul> <li>VQ-GAN encoder/decoder (8\u00d7 compression)</li> <li>U-Net diffusion model</li> <li>CLIP text conditioning</li> </ul> <p>Training:</p> <ul> <li>LAION-5B dataset (5 billion images)</li> <li>256\u00d7256 or 512\u00d7512 resolution</li> <li>Text-to-image generation</li> </ul>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#bio-latent-diffusion-gene-expression","title":"Bio Latent Diffusion (Gene Expression)","text":"<p>Architecture:</p> <ul> <li>VAE encoder/decoder (78\u00d7 compression)</li> <li>DiT or U-Net diffusion model</li> <li>Perturbation/cell-type conditioning</li> </ul> <p>Training:</p> <ul> <li>Single-cell datasets (millions of cells)</li> <li>20K genes \u2192 256 latent dims</li> <li>Perturbation/trajectory prediction</li> </ul> <p>Key differences: 1. Compression ratio: Higher for biology (78\u00d7 vs 8\u00d7) 2. Conditioning: Biological metadata vs text 3. Data structure: Tabular vs spatial 4. Objectives: Prediction + generation vs generation only</p>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#when-to-use-latent-diffusion","title":"When to Use Latent Diffusion","text":""},{"location":"latent_diffusion/00_latent_diffusion_overview/#use-latent-diffusion-when","title":"\u2705 Use Latent Diffusion When:","text":"<p>High-dimensional data:</p> <ul> <li>Gene expression (20K genes)</li> <li>Multi-omics (RNA + Protein + ATAC)</li> <li>Spatial transcriptomics</li> </ul> <p>Need efficiency:</p> <ul> <li>Large datasets (millions of cells)</li> <li>Limited compute</li> <li>Fast sampling required</li> </ul> <p>Want quality + diversity:</p> <ul> <li>Better than VAE (sharper)</li> <li>Better than GAN (mode coverage)</li> <li>Stable training</li> </ul> <p>Multi-task learning:</p> <ul> <li>Generation + prediction</li> <li>Multiple conditions</li> <li>Transfer across datasets</li> </ul>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#dont-use-latent-diffusion-when","title":"\u274c Don't Use Latent Diffusion When:","text":"<p>Low-dimensional data:</p> <ul> <li>Already &lt;1000 dims</li> <li>Pixel-space diffusion is fine</li> </ul> <p>Need exact likelihood:</p> <ul> <li>VAE or normalizing flow better</li> <li>Latent diffusion likelihood is approximate</li> </ul> <p>Real-time inference:</p> <ul> <li>Sampling still slower than VAE/GAN</li> <li>Consider distillation or few-step methods</li> </ul> <p>Simple tasks:</p> <ul> <li>Linear models sufficient</li> <li>Overkill for simple prediction</li> </ul>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#key-takeaways","title":"Key Takeaways","text":""},{"location":"latent_diffusion/00_latent_diffusion_overview/#conceptual","title":"Conceptual","text":"<ol> <li>Two-stage approach \u2014 VAE compression + latent diffusion</li> <li>Efficiency \u2014 10-100\u00d7 faster than pixel-space</li> <li>Quality \u2014 Better than VAE, stable than GAN</li> <li>Flexibility \u2014 Multi-modal, multi-task, controllable</li> </ol>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#practical","title":"Practical","text":"<ol> <li>Train VAE first \u2014 Get good latent space</li> <li>Freeze VAE \u2014 Train diffusion on latent codes</li> <li>Optional fine-tuning \u2014 Joint optimization</li> <li>Condition carefully \u2014 Use appropriate conditioning mechanism</li> </ol>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#for-biology","title":"For Biology","text":"<ol> <li>Perfect for scRNA-seq \u2014 High-dim, low-rank structure</li> <li>Enables multi-omics \u2014 Shared latent space</li> <li>Scalable \u2014 Millions of cells</li> <li>Interpretable \u2014 Latent dimensions have meaning</li> </ol>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#related-documents","title":"Related Documents","text":"<ul> <li>01_latent_diffusion_foundations.md \u2014 Architecture details</li> <li>02_latent_diffusion_training.md \u2014 Training strategies</li> <li>03_latent_diffusion_applications.md \u2014 Biology applications</li> <li>04_latent_diffusion_combio.md \u2014 Complete implementation</li> </ul>"},{"location":"latent_diffusion/00_latent_diffusion_overview/#references","title":"References","text":"<p>Latent Diffusion:</p> <ul> <li>Rombach et al. (2022): \"High-Resolution Image Synthesis with Latent Diffusion Models\" (Stable Diffusion)</li> <li>Vahdat et al. (2021): \"Score-based Generative Modeling in Latent Space\"</li> </ul> <p>Autoencoders:</p> <ul> <li>Kingma &amp; Welling (2014): \"Auto-Encoding Variational Bayes\" (VAE)</li> <li>van den Oord et al. (2017): \"Neural Discrete Representation Learning\" (VQ-VAE)</li> <li>Esser et al. (2021): \"Taming Transformers for High-Resolution Image Synthesis\" (VQ-GAN)</li> </ul> <p>Biology Applications:</p> <ul> <li>Lopez et al. (2018): \"Deep generative modeling for single-cell transcriptomics\" (scVI)</li> <li>Lotfollahi et al. (2023): \"Predicting cellular responses to novel drug combinations\"</li> <li>Bunne et al. (2023): \"Learning Single-Cell Perturbation Responses using Neural Optimal Transport\"</li> </ul>"},{"location":"latent_diffusion/01_latent_diffusion_foundations/","title":"Latent Diffusion Foundations: Architecture and Components","text":"<p>This document covers the detailed architecture of latent diffusion models for computational biology, including VAE encoders with NB/ZINB decoders, latent diffusion models, and conditioning mechanisms.</p> <p>Prerequisites: Understanding of latent diffusion overview, VAE basics, and diffusion models.</p>"},{"location":"latent_diffusion/01_latent_diffusion_foundations/#architecture-overview","title":"Architecture Overview","text":""},{"location":"latent_diffusion/01_latent_diffusion_foundations/#two-stage-pipeline","title":"Two-Stage Pipeline","text":"<p>Stage 1: Autoencoder (Compress to latent space) <pre><code>Gene Expression (20K dims) \u2192 Encoder \u2192 Latent (256 dims) \u2192 Decoder \u2192 Gene Expression\n</code></pre></p> <p>Stage 2: Diffusion (Generate in latent space) <pre><code>Noise \u2192 Denoising Process \u2192 Latent \u2192 Decoder \u2192 Gene Expression\n</code></pre></p> <p>Key insight: Diffuse in compressed latent space (256 dims) instead of raw gene space (20K dims) \u2192 78\u00d7 fewer dimensions.</p>"},{"location":"latent_diffusion/01_latent_diffusion_foundations/#1-autoencoder-stage","title":"1. Autoencoder Stage","text":""},{"location":"latent_diffusion/01_latent_diffusion_foundations/#11-vae-for-gene-expression","title":"1.1 VAE for Gene Expression","text":"<p>Why VAE for biology:</p> <ul> <li>Probabilistic (captures uncertainty)</li> <li>Continuous latent space (smooth interpolation)</li> <li>Well-understood training (ELBO objective)</li> <li>Works well with count data (via appropriate decoder)</li> </ul>"},{"location":"latent_diffusion/01_latent_diffusion_foundations/#12-encoder-architecture","title":"1.2 Encoder Architecture","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass GeneExpressionEncoder(nn.Module):\n    \"\"\"\n    Encoder for gene expression data.\n\n    Maps high-dimensional gene expression to low-dimensional latent space.\n\n    Args:\n        num_genes: Number of genes (input dimension)\n        latent_dim: Latent space dimension\n        hidden_dims: List of hidden layer dimensions\n    \"\"\"\n    def __init__(\n        self,\n        num_genes=20000,\n        latent_dim=256,\n        hidden_dims=[2048, 1024, 512],\n    ):\n        super().__init__()\n\n        self.num_genes = num_genes\n        self.latent_dim = latent_dim\n\n        # Encoder layers\n        layers = []\n        in_dim = num_genes\n\n        for hidden_dim in hidden_dims:\n            layers.extend([\n                nn.Linear(in_dim, hidden_dim),\n                nn.LayerNorm(hidden_dim),\n                nn.GELU(),\n                nn.Dropout(0.1),\n            ])\n            in_dim = hidden_dim\n\n        self.encoder = nn.Sequential(*layers)\n\n        # Latent projections\n        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)\n        self.fc_logvar = nn.Linear(hidden_dims[-1], latent_dim)\n\n    def forward(self, x):\n        \"\"\"\n        Encode gene expression to latent distribution.\n\n        Args:\n            x: Gene expression (B, num_genes)\n\n        Returns:\n            mu: Latent mean (B, latent_dim)\n            logvar: Latent log variance (B, latent_dim)\n        \"\"\"\n        # Encode\n        h = self.encoder(x)\n\n        # Latent distribution parameters\n        mu = self.fc_mu(h)\n        logvar = self.fc_logvar(h)\n\n        return mu, logvar\n\n    def reparameterize(self, mu, logvar):\n        \"\"\"\n        Reparameterization trick.\n\n        Args:\n            mu: Mean (B, latent_dim)\n            logvar: Log variance (B, latent_dim)\n\n        Returns:\n            z: Sampled latent (B, latent_dim)\n        \"\"\"\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n</code></pre>"},{"location":"latent_diffusion/01_latent_diffusion_foundations/#13-decoder-with-nbzinb-distribution","title":"1.3 Decoder with NB/ZINB Distribution","text":"<p>Why NB/ZINB for gene expression:</p> <ul> <li>Gene expression counts are overdispersed (variance &gt; mean)</li> <li>Negative Binomial (NB) models overdispersion</li> <li>Zero-Inflated NB (ZINB) models excess zeros (dropout)</li> </ul> <p>Negative Binomial Decoder:</p> <pre><code>class NegativeBinomialDecoder(nn.Module):\n    \"\"\"\n    Decoder with Negative Binomial output distribution.\n\n    Appropriate for count data with overdispersion.\n\n    Args:\n        latent_dim: Latent space dimension\n        num_genes: Number of genes (output dimension)\n        hidden_dims: List of hidden layer dimensions\n    \"\"\"\n    def __init__(\n        self,\n        latent_dim=256,\n        num_genes=20000,\n        hidden_dims=[512, 1024, 2048],\n    ):\n        super().__init__()\n\n        self.latent_dim = latent_dim\n        self.num_genes = num_genes\n\n        # Decoder layers\n        layers = []\n        in_dim = latent_dim\n\n        for hidden_dim in hidden_dims:\n            layers.extend([\n                nn.Linear(in_dim, hidden_dim),\n                nn.LayerNorm(hidden_dim),\n                nn.GELU(),\n                nn.Dropout(0.1),\n            ])\n            in_dim = hidden_dim\n\n        self.decoder = nn.Sequential(*layers)\n\n        # NB parameters\n        self.fc_mean = nn.Linear(hidden_dims[-1], num_genes)\n        self.fc_dispersion = nn.Linear(hidden_dims[-1], num_genes)\n\n    def forward(self, z, library_size=None):\n        \"\"\"\n        Decode latent to NB parameters.\n\n        Args:\n            z: Latent (B, latent_dim)\n            library_size: Optional library size (B,) for normalization\n\n        Returns:\n            mean: NB mean (B, num_genes)\n            dispersion: NB dispersion (B, num_genes)\n        \"\"\"\n        # Decode\n        h = self.decoder(z)\n\n        # NB mean (must be positive)\n        mean = torch.exp(self.fc_mean(h))\n\n        # Scale by library size if provided\n        if library_size is not None:\n            mean = mean * library_size.unsqueeze(-1)\n\n        # NB dispersion (must be positive)\n        dispersion = torch.exp(self.fc_dispersion(h))\n\n        return mean, dispersion\n\n    def log_prob(self, x, mean, dispersion, eps=1e-8):\n        \"\"\"\n        Negative Binomial log probability.\n\n        Args:\n            x: Observed counts (B, num_genes)\n            mean: NB mean (B, num_genes)\n            dispersion: NB dispersion (B, num_genes)\n\n        Returns:\n            log_prob: Log probability (B,)\n        \"\"\"\n        # NB log probability\n        # p(x | mean, dispersion) = Gamma(x + 1/dispersion) / (Gamma(x+1) * Gamma(1/dispersion))\n        #                            * (dispersion * mean)^x / (1 + dispersion * mean)^(x + 1/dispersion)\n\n        theta = 1.0 / (dispersion + eps)  # inverse dispersion\n\n        # Log probability\n        log_theta_mu = torch.log(theta + mean + eps)\n\n        log_prob = (\n            torch.lgamma(x + theta) - torch.lgamma(theta) - torch.lgamma(x + 1)\n            + theta * torch.log(theta + eps) - theta * log_theta_mu\n            + x * torch.log(mean + eps) - x * log_theta_mu\n        )\n\n        return log_prob.sum(dim=-1)  # Sum over genes\n</code></pre> <p>Zero-Inflated Negative Binomial Decoder:</p> <pre><code>class ZINBDecoder(nn.Module):\n    \"\"\"\n    Decoder with Zero-Inflated Negative Binomial distribution.\n\n    Models excess zeros (dropout) in addition to overdispersion.\n\n    Args:\n        latent_dim: Latent space dimension\n        num_genes: Number of genes\n        hidden_dims: List of hidden layer dimensions\n    \"\"\"\n    def __init__(\n        self,\n        latent_dim=256,\n        num_genes=20000,\n        hidden_dims=[512, 1024, 2048],\n    ):\n        super().__init__()\n\n        self.latent_dim = latent_dim\n        self.num_genes = num_genes\n\n        # Decoder layers\n        layers = []\n        in_dim = latent_dim\n\n        for hidden_dim in hidden_dims:\n            layers.extend([\n                nn.Linear(in_dim, hidden_dim),\n                nn.LayerNorm(hidden_dim),\n                nn.GELU(),\n                nn.Dropout(0.1),\n            ])\n            in_dim = hidden_dim\n\n        self.decoder = nn.Sequential(*layers)\n\n        # ZINB parameters\n        self.fc_mean = nn.Linear(hidden_dims[-1], num_genes)\n        self.fc_dispersion = nn.Linear(hidden_dims[-1], num_genes)\n        self.fc_dropout = nn.Linear(hidden_dims[-1], num_genes)  # Zero-inflation\n\n    def forward(self, z, library_size=None):\n        \"\"\"\n        Decode latent to ZINB parameters.\n\n        Args:\n            z: Latent (B, latent_dim)\n            library_size: Optional library size (B,)\n\n        Returns:\n            mean: NB mean (B, num_genes)\n            dispersion: NB dispersion (B, num_genes)\n            dropout: Dropout probability (B, num_genes)\n        \"\"\"\n        # Decode\n        h = self.decoder(z)\n\n        # NB mean\n        mean = torch.exp(self.fc_mean(h))\n        if library_size is not None:\n            mean = mean * library_size.unsqueeze(-1)\n\n        # NB dispersion\n        dispersion = torch.exp(self.fc_dispersion(h))\n\n        # Dropout probability (zero-inflation)\n        dropout = torch.sigmoid(self.fc_dropout(h))\n\n        return mean, dispersion, dropout\n\n    def log_prob(self, x, mean, dispersion, dropout, eps=1e-8):\n        \"\"\"\n        ZINB log probability.\n\n        Args:\n            x: Observed counts (B, num_genes)\n            mean: NB mean (B, num_genes)\n            dispersion: NB dispersion (B, num_genes)\n            dropout: Dropout probability (B, num_genes)\n\n        Returns:\n            log_prob: Log probability (B,)\n        \"\"\"\n        # ZINB = mixture of point mass at 0 and NB\n        # p(x) = dropout * I(x=0) + (1-dropout) * NB(x | mean, dispersion)\n\n        theta = 1.0 / (dispersion + eps)\n        log_theta_mu = torch.log(theta + mean + eps)\n\n        # NB log prob\n        nb_log_prob = (\n            torch.lgamma(x + theta) - torch.lgamma(theta) - torch.lgamma(x + 1)\n            + theta * torch.log(theta + eps) - theta * log_theta_mu\n            + x * torch.log(mean + eps) - x * log_theta_mu\n        )\n\n        # ZINB log prob\n        zero_mask = (x &lt; eps).float()\n\n        # For zeros: log(dropout + (1-dropout) * NB(0))\n        nb_zero = theta * (torch.log(theta + eps) - log_theta_mu)\n        log_prob_zero = torch.log(dropout + (1 - dropout) * torch.exp(nb_zero) + eps)\n\n        # For non-zeros: log((1-dropout) * NB(x))\n        log_prob_nonzero = torch.log(1 - dropout + eps) + nb_log_prob\n\n        # Combine\n        log_prob = zero_mask * log_prob_zero + (1 - zero_mask) * log_prob_nonzero\n\n        return log_prob.sum(dim=-1)  # Sum over genes\n</code></pre>"},{"location":"latent_diffusion/01_latent_diffusion_foundations/#14-complete-vae-with-nbzinb","title":"1.4 Complete VAE with NB/ZINB","text":"<pre><code>class GeneExpressionVAE(nn.Module):\n    \"\"\"\n    Complete VAE for gene expression with NB or ZINB decoder.\n\n    Args:\n        num_genes: Number of genes\n        latent_dim: Latent dimension\n        decoder_type: 'nb' or 'zinb'\n    \"\"\"\n    def __init__(\n        self,\n        num_genes=20000,\n        latent_dim=256,\n        decoder_type='zinb',\n    ):\n        super().__init__()\n\n        self.num_genes = num_genes\n        self.latent_dim = latent_dim\n        self.decoder_type = decoder_type\n\n        # Encoder\n        self.encoder = GeneExpressionEncoder(\n            num_genes=num_genes,\n            latent_dim=latent_dim,\n        )\n\n        # Decoder\n        if decoder_type == 'nb':\n            self.decoder = NegativeBinomialDecoder(\n                latent_dim=latent_dim,\n                num_genes=num_genes,\n            )\n        elif decoder_type == 'zinb':\n            self.decoder = ZINBDecoder(\n                latent_dim=latent_dim,\n                num_genes=num_genes,\n            )\n        else:\n            raise ValueError(f\"Unknown decoder type: {decoder_type}\")\n\n    def forward(self, x, library_size=None):\n        \"\"\"\n        Forward pass.\n\n        Args:\n            x: Gene expression (B, num_genes)\n            library_size: Optional library size (B,)\n\n        Returns:\n            recon_params: Reconstruction parameters\n            mu: Latent mean\n            logvar: Latent log variance\n        \"\"\"\n        # Encode\n        mu, logvar = self.encoder(x)\n\n        # Reparameterize\n        z = self.encoder.reparameterize(mu, logvar)\n\n        # Decode\n        recon_params = self.decoder(z, library_size)\n\n        return recon_params, mu, logvar\n\n    def loss(self, x, recon_params, mu, logvar, library_size=None, beta=1.0):\n        \"\"\"\n        VAE loss (ELBO).\n\n        Args:\n            x: Gene expression (B, num_genes)\n            recon_params: Reconstruction parameters from decoder\n            mu: Latent mean (B, latent_dim)\n            logvar: Latent log variance (B, latent_dim)\n            library_size: Optional library size (B,)\n            beta: KL weight (beta-VAE)\n\n        Returns:\n            loss: Total loss\n            loss_dict: Dictionary with loss components\n        \"\"\"\n        # Reconstruction loss (negative log likelihood)\n        if self.decoder_type == 'nb':\n            mean, dispersion = recon_params\n            recon_loss = -self.decoder.log_prob(x, mean, dispersion).mean()\n        elif self.decoder_type == 'zinb':\n            mean, dispersion, dropout = recon_params\n            recon_loss = -self.decoder.log_prob(x, mean, dispersion, dropout).mean()\n\n        # KL divergence\n        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=-1).mean()\n\n        # Total loss\n        loss = recon_loss + beta * kl_loss\n\n        loss_dict = {\n            'loss': loss.item(),\n            'recon': recon_loss.item(),\n            'kl': kl_loss.item(),\n        }\n\n        return loss, loss_dict\n\n    @torch.no_grad()\n    def encode(self, x):\n        \"\"\"Encode to latent space (deterministic).\"\"\"\n        mu, logvar = self.encoder(x)\n        return mu\n\n    @torch.no_grad()\n    def decode(self, z, library_size=None):\n        \"\"\"Decode from latent space.\"\"\"\n        recon_params = self.decoder(z, library_size)\n\n        if self.decoder_type == 'nb':\n            mean, dispersion = recon_params\n            return mean\n        elif self.decoder_type == 'zinb':\n            mean, dispersion, dropout = recon_params\n            # Expected value: (1 - dropout) * mean\n            return (1 - dropout) * mean\n</code></pre>"},{"location":"latent_diffusion/01_latent_diffusion_foundations/#2-latent-diffusion-model","title":"2. Latent Diffusion Model","text":""},{"location":"latent_diffusion/01_latent_diffusion_foundations/#21-diffusion-in-latent-space","title":"2.1 Diffusion in Latent Space","text":"<p>Key idea: Run diffusion on latent codes instead of raw data.</p> <pre><code>class LatentDiffusionModel(nn.Module):\n    \"\"\"\n    Diffusion model operating in latent space.\n\n    Args:\n        latent_dim: Latent space dimension\n        model_type: 'dit' or 'unet'\n        num_steps: Number of diffusion steps\n    \"\"\"\n    def __init__(\n        self,\n        latent_dim=256,\n        model_type='dit',\n        num_steps=1000,\n    ):\n        super().__init__()\n\n        self.latent_dim = latent_dim\n        self.num_steps = num_steps\n\n        # Denoising model\n        if model_type == 'dit':\n            self.model = DiTLatent(latent_dim=latent_dim)\n        elif model_type == 'unet':\n            self.model = UNetLatent(latent_dim=latent_dim)\n        else:\n            raise ValueError(f\"Unknown model type: {model_type}\")\n\n        # Noise schedule (linear for simplicity)\n        self.register_buffer('betas', torch.linspace(1e-4, 0.02, num_steps))\n        self.register_buffer('alphas', 1.0 - self.betas)\n        self.register_buffer('alphas_cumprod', torch.cumprod(self.alphas, dim=0))\n\n    def forward(self, z, t, condition=None):\n        \"\"\"\n        Predict noise at timestep t.\n\n        Args:\n            z: Noisy latent (B, latent_dim)\n            t: Timestep (B,)\n            condition: Optional conditioning (B, cond_dim)\n\n        Returns:\n            noise_pred: Predicted noise (B, latent_dim)\n        \"\"\"\n        return self.model(z, t, condition)\n\n    def add_noise(self, z0, t):\n        \"\"\"\n        Add noise to clean latent.\n\n        Args:\n            z0: Clean latent (B, latent_dim)\n            t: Timestep (B,)\n\n        Returns:\n            zt: Noisy latent (B, latent_dim)\n            noise: Added noise (B, latent_dim)\n        \"\"\"\n        noise = torch.randn_like(z0)\n\n        # Get alpha_t\n        alpha_t = self.alphas_cumprod[t].view(-1, 1)\n\n        # zt = sqrt(alpha_t) * z0 + sqrt(1 - alpha_t) * noise\n        zt = torch.sqrt(alpha_t) * z0 + torch.sqrt(1 - alpha_t) * noise\n\n        return zt, noise\n\n    @torch.no_grad()\n    def sample(self, batch_size, condition=None, num_steps=50):\n        \"\"\"\n        Sample from diffusion model (DDIM sampling).\n\n        Args:\n            batch_size: Number of samples\n            condition: Optional conditioning\n            num_steps: Number of sampling steps\n\n        Returns:\n            z0: Sampled latent (batch_size, latent_dim)\n        \"\"\"\n        device = next(self.parameters()).device\n\n        # Start from noise\n        z = torch.randn(batch_size, self.latent_dim, device=device)\n\n        # Sampling timesteps\n        timesteps = torch.linspace(self.num_steps - 1, 0, num_steps, dtype=torch.long, device=device)\n\n        for i, t in enumerate(timesteps):\n            t_batch = torch.full((batch_size,), t, device=device, dtype=torch.long)\n\n            # Predict noise\n            noise_pred = self.model(z, t_batch, condition)\n\n            # DDIM update\n            alpha_t = self.alphas_cumprod[t]\n\n            if i &lt; len(timesteps) - 1:\n                alpha_t_prev = self.alphas_cumprod[timesteps[i + 1]]\n            else:\n                alpha_t_prev = torch.tensor(1.0, device=device)\n\n            # Predicted x0\n            pred_z0 = (z - torch.sqrt(1 - alpha_t) * noise_pred) / torch.sqrt(alpha_t)\n\n            # Direction pointing to zt\n            dir_zt = torch.sqrt(1 - alpha_t_prev) * noise_pred\n\n            # Update\n            z = torch.sqrt(alpha_t_prev) * pred_z0 + dir_zt\n\n        return z\n</code></pre>"},{"location":"latent_diffusion/01_latent_diffusion_foundations/#22-dit-for-latent-space","title":"2.2 DiT for Latent Space","text":"<pre><code>class DiTLatent(nn.Module):\n    \"\"\"\n    DiT (Diffusion Transformer) for latent space.\n\n    Args:\n        latent_dim: Latent dimension\n        hidden_dim: Hidden dimension\n        num_layers: Number of transformer layers\n        num_heads: Number of attention heads\n    \"\"\"\n    def __init__(\n        self,\n        latent_dim=256,\n        hidden_dim=512,\n        num_layers=12,\n        num_heads=8,\n    ):\n        super().__init__()\n\n        self.latent_dim = latent_dim\n        self.hidden_dim = hidden_dim\n\n        # Input projection\n        self.input_proj = nn.Linear(latent_dim, hidden_dim)\n\n        # Time embedding\n        self.time_embed = nn.Sequential(\n            SinusoidalPositionEmbeddings(hidden_dim),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.GELU(),\n            nn.Linear(hidden_dim, hidden_dim),\n        )\n\n        # Transformer blocks with AdaLN\n        self.blocks = nn.ModuleList([\n            DiTBlock(hidden_dim, num_heads)\n            for _ in range(num_layers)\n        ])\n\n        # Output projection\n        self.output_proj = nn.Linear(hidden_dim, latent_dim)\n\n        # Initialize to zero (residual connection)\n        nn.init.zeros_(self.output_proj.weight)\n        nn.init.zeros_(self.output_proj.bias)\n\n    def forward(self, z, t, condition=None):\n        \"\"\"\n        Args:\n            z: Noisy latent (B, latent_dim)\n            t: Timestep (B,)\n            condition: Optional conditioning (B, cond_dim)\n\n        Returns:\n            noise_pred: Predicted noise (B, latent_dim)\n        \"\"\"\n        # Input projection\n        h = self.input_proj(z)  # (B, hidden_dim)\n\n        # Time embedding\n        t_emb = self.time_embed(t)  # (B, hidden_dim)\n\n        # Add condition if provided\n        if condition is not None:\n            # Project condition to hidden_dim\n            cond_emb = self.condition_proj(condition)\n            t_emb = t_emb + cond_emb\n\n        # Transformer blocks\n        for block in self.blocks:\n            h = block(h, t_emb)\n\n        # Output projection\n        noise_pred = self.output_proj(h)\n\n        return noise_pred\n\n\nclass DiTBlock(nn.Module):\n    \"\"\"DiT block with AdaLN.\"\"\"\n    def __init__(self, dim, num_heads):\n        super().__init__()\n\n        self.norm1 = nn.LayerNorm(dim, elementwise_affine=False)\n        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n\n        self.norm2 = nn.LayerNorm(dim, elementwise_affine=False)\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, dim * 4),\n            nn.GELU(),\n            nn.Linear(dim * 4, dim),\n        )\n\n        # AdaLN modulation\n        self.adaLN_modulation = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(dim, 6 * dim),\n        )\n\n    def forward(self, x, t_emb):\n        \"\"\"\n        Args:\n            x: Input (B, dim)\n            t_emb: Time embedding (B, dim)\n\n        Returns:\n            out: Output (B, dim)\n        \"\"\"\n        # AdaLN parameters\n        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = \\\n            self.adaLN_modulation(t_emb).chunk(6, dim=-1)\n\n        # Self-attention with AdaLN\n        x_norm = self.norm1(x)\n        x_norm = x_norm * (1 + scale_msa.unsqueeze(1)) + shift_msa.unsqueeze(1)\n        x = x + gate_msa.unsqueeze(1) * self.attn(x_norm, x_norm, x_norm)[0]\n\n        # MLP with AdaLN\n        x_norm = self.norm2(x)\n        x_norm = x_norm * (1 + scale_mlp.unsqueeze(1)) + shift_mlp.unsqueeze(1)\n        x = x + gate_mlp.unsqueeze(1) * self.mlp(x_norm)\n\n        return x\n\n\nclass SinusoidalPositionEmbeddings(nn.Module):\n    \"\"\"Sinusoidal position embeddings for time.\"\"\"\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, time):\n        device = time.device\n        half_dim = self.dim // 2\n        embeddings = torch.log(torch.tensor(10000.0)) / (half_dim - 1)\n        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n        embeddings = time[:, None] * embeddings[None, :]\n        embeddings = torch.cat([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)\n        return embeddings\n</code></pre>"},{"location":"latent_diffusion/01_latent_diffusion_foundations/#3-conditioning-mechanisms","title":"3. Conditioning Mechanisms","text":""},{"location":"latent_diffusion/01_latent_diffusion_foundations/#31-concatenation-conditioning","title":"3.1 Concatenation Conditioning","text":"<p>Simplest approach: Concatenate condition to latent.</p> <pre><code>class ConcatenationConditioning(nn.Module):\n    \"\"\"\n    Conditioning via concatenation.\n\n    Args:\n        latent_dim: Latent dimension\n        condition_dim: Condition dimension\n    \"\"\"\n    def __init__(self, latent_dim=256, condition_dim=128):\n        super().__init__()\n\n        self.latent_dim = latent_dim\n        self.condition_dim = condition_dim\n\n        # Project concatenated input\n        self.proj = nn.Linear(latent_dim + condition_dim, latent_dim)\n\n    def forward(self, z, condition):\n        \"\"\"\n        Args:\n            z: Latent (B, latent_dim)\n            condition: Condition (B, condition_dim)\n\n        Returns:\n            z_cond: Conditioned latent (B, latent_dim)\n        \"\"\"\n        # Concatenate\n        z_cat = torch.cat([z, condition], dim=-1)\n\n        # Project back to latent_dim\n        z_cond = self.proj(z_cat)\n\n        return z_cond\n</code></pre>"},{"location":"latent_diffusion/01_latent_diffusion_foundations/#32-cross-attention-conditioning","title":"3.2 Cross-Attention Conditioning","text":"<p>More flexible: Condition attends to latent.</p> <pre><code>class CrossAttentionConditioning(nn.Module):\n    \"\"\"\n    Conditioning via cross-attention.\n\n    Args:\n        latent_dim: Latent dimension\n        condition_dim: Condition dimension\n        num_heads: Number of attention heads\n    \"\"\"\n    def __init__(self, latent_dim=256, condition_dim=128, num_heads=8):\n        super().__init__()\n\n        self.latent_dim = latent_dim\n        self.condition_dim = condition_dim\n\n        # Project condition to latent_dim\n        self.condition_proj = nn.Linear(condition_dim, latent_dim)\n\n        # Cross-attention\n        self.cross_attn = nn.MultiheadAttention(\n            latent_dim,\n            num_heads,\n            batch_first=True,\n        )\n\n        self.norm = nn.LayerNorm(latent_dim)\n\n    def forward(self, z, condition):\n        \"\"\"\n        Args:\n            z: Latent (B, latent_dim) or (B, num_tokens, latent_dim)\n            condition: Condition (B, condition_dim) or (B, num_cond, condition_dim)\n\n        Returns:\n            z_cond: Conditioned latent\n        \"\"\"\n        # Ensure z has sequence dimension\n        if z.dim() == 2:\n            z = z.unsqueeze(1)  # (B, 1, latent_dim)\n\n        # Project condition\n        cond = self.condition_proj(condition)\n        if cond.dim() == 2:\n            cond = cond.unsqueeze(1)  # (B, 1, latent_dim)\n\n        # Cross-attention: z attends to condition\n        z_norm = self.norm(z)\n        z_attn = self.cross_attn(z_norm, cond, cond)[0]\n\n        # Residual\n        z_cond = z + z_attn\n\n        return z_cond.squeeze(1) if z_cond.shape[1] == 1 else z_cond\n</code></pre>"},{"location":"latent_diffusion/01_latent_diffusion_foundations/#33-film-conditioning","title":"3.3 FiLM Conditioning","text":"<p>Affine transformation: Scale and shift based on condition.</p> <pre><code>class FiLMConditioning(nn.Module):\n    \"\"\"\n    Feature-wise Linear Modulation (FiLM) conditioning.\n\n    Args:\n        latent_dim: Latent dimension\n        condition_dim: Condition dimension\n    \"\"\"\n    def __init__(self, latent_dim=256, condition_dim=128):\n        super().__init__()\n\n        self.latent_dim = latent_dim\n        self.condition_dim = condition_dim\n\n        # Predict scale and shift from condition\n        self.film = nn.Sequential(\n            nn.Linear(condition_dim, latent_dim * 2),\n            nn.GELU(),\n            nn.Linear(latent_dim * 2, latent_dim * 2),\n        )\n\n    def forward(self, z, condition):\n        \"\"\"\n        Args:\n            z: Latent (B, latent_dim)\n            condition: Condition (B, condition_dim)\n\n        Returns:\n            z_cond: Conditioned latent (B, latent_dim)\n        \"\"\"\n        # Predict scale and shift\n        film_params = self.film(condition)\n        scale, shift = film_params.chunk(2, dim=-1)\n\n        # Apply FiLM\n        z_cond = scale * z + shift\n\n        return z_cond\n</code></pre>"},{"location":"latent_diffusion/01_latent_diffusion_foundations/#34-classifier-free-guidance","title":"3.4 Classifier-Free Guidance","text":"<p>Training: Randomly drop condition (unconditional training).</p> <pre><code>def classifier_free_guidance_training(model, z, t, condition, dropout_prob=0.1):\n    \"\"\"\n    Training with classifier-free guidance.\n\n    Args:\n        model: Diffusion model\n        z: Noisy latent (B, latent_dim)\n        t: Timestep (B,)\n        condition: Condition (B, condition_dim)\n        dropout_prob: Probability of dropping condition\n\n    Returns:\n        noise_pred: Predicted noise\n    \"\"\"\n    # Randomly drop condition\n    mask = torch.rand(condition.shape[0], device=condition.device) &gt; dropout_prob\n    condition_masked = condition * mask.unsqueeze(-1)\n\n    # Predict noise\n    noise_pred = model(z, t, condition_masked)\n\n    return noise_pred\n\n\n@torch.no_grad()\ndef classifier_free_guidance_sampling(model, batch_size, condition, guidance_scale=7.5):\n    \"\"\"\n    Sampling with classifier-free guidance.\n\n    Args:\n        model: Diffusion model\n        batch_size: Number of samples\n        condition: Condition (batch_size, condition_dim)\n        guidance_scale: Guidance strength\n\n    Returns:\n        z0: Sampled latent\n    \"\"\"\n    device = next(model.parameters()).device\n\n    # Start from noise\n    z = torch.randn(batch_size, model.latent_dim, device=device)\n\n    # Unconditional (null) condition\n    condition_null = torch.zeros_like(condition)\n\n    for t in reversed(range(model.num_steps)):\n        t_batch = torch.full((batch_size,), t, device=device, dtype=torch.long)\n\n        # Conditional prediction\n        noise_cond = model(z, t_batch, condition)\n\n        # Unconditional prediction\n        noise_uncond = model(z, t_batch, condition_null)\n\n        # Classifier-free guidance\n        noise_pred = noise_uncond + guidance_scale * (noise_cond - noise_uncond)\n\n        # Update (simplified DDPM step)\n        alpha_t = model.alphas_cumprod[t]\n        alpha_t_prev = model.alphas_cumprod[t - 1] if t &gt; 0 else torch.tensor(1.0)\n\n        pred_z0 = (z - torch.sqrt(1 - alpha_t) * noise_pred) / torch.sqrt(alpha_t)\n        z = torch.sqrt(alpha_t_prev) * pred_z0 + torch.sqrt(1 - alpha_t_prev) * noise_pred\n\n    return z\n</code></pre>"},{"location":"latent_diffusion/01_latent_diffusion_foundations/#4-complete-latent-diffusion-system","title":"4. Complete Latent Diffusion System","text":""},{"location":"latent_diffusion/01_latent_diffusion_foundations/#41-full-pipeline","title":"4.1 Full Pipeline","text":"<pre><code>class CompleteLaten\u200btDiffusion(nn.Module):\n    \"\"\"\n    Complete latent diffusion system for gene expression.\n\n    Combines VAE and latent diffusion model.\n\n    Args:\n        num_genes: Number of genes\n        latent_dim: Latent dimension\n        decoder_type: 'nb' or 'zinb'\n    \"\"\"\n    def __init__(\n        self,\n        num_genes=20000,\n        latent_dim=256,\n        decoder_type='zinb',\n    ):\n        super().__init__()\n\n        # VAE\n        self.vae = GeneExpressionVAE(\n            num_genes=num_genes,\n            latent_dim=latent_dim,\n            decoder_type=decoder_type,\n        )\n\n        # Latent diffusion\n        self.diffusion = LatentDiffusionModel(\n            latent_dim=latent_dim,\n            model_type='dit',\n        )\n\n    def train_vae(self, x, library_size=None, beta=1.0):\n        \"\"\"Train VAE.\"\"\"\n        recon_params, mu, logvar = self.vae(x, library_size)\n        loss, loss_dict = self.vae.loss(x, recon_params, mu, logvar, library_size, beta)\n        return loss, loss_dict\n\n    def train_diffusion(self, x, condition=None):\n        \"\"\"Train diffusion on latent codes.\"\"\"\n        # Encode to latent (frozen VAE)\n        with torch.no_grad():\n            z0 = self.vae.encode(x)\n\n        # Sample timestep\n        t = torch.randint(0, self.diffusion.num_steps, (z0.shape[0],), device=z0.device)\n\n        # Add noise\n        zt, noise = self.diffusion.add_noise(z0, t)\n\n        # Predict noise\n        noise_pred = self.diffusion(zt, t, condition)\n\n        # MSE loss\n        loss = F.mse_loss(noise_pred, noise)\n\n        return loss\n\n    @torch.no_grad()\n    def generate(self, batch_size, condition=None, library_size=None):\n        \"\"\"\n        Generate gene expression samples.\n\n        Args:\n            batch_size: Number of samples\n            condition: Optional conditioning\n            library_size: Optional library size\n\n        Returns:\n            x_gen: Generated gene expression (batch_size, num_genes)\n        \"\"\"\n        # Sample latent from diffusion\n        z0 = self.diffusion.sample(batch_size, condition)\n\n        # Decode to gene expression\n        x_gen = self.vae.decode(z0, library_size)\n\n        return x_gen\n</code></pre>"},{"location":"latent_diffusion/01_latent_diffusion_foundations/#key-takeaways","title":"Key Takeaways","text":""},{"location":"latent_diffusion/01_latent_diffusion_foundations/#architecture","title":"Architecture","text":"<ol> <li>VAE with NB/ZINB decoder \u2014 Appropriate for count data</li> <li>Latent diffusion \u2014 Diffuse in compressed space (256 dims)</li> <li>DiT backbone \u2014 Transformer-based denoising</li> <li>Flexible conditioning \u2014 Concatenation, cross-attention, FiLM, CFG</li> </ol>"},{"location":"latent_diffusion/01_latent_diffusion_foundations/#design-choices","title":"Design Choices","text":"<ol> <li>ZINB decoder \u2014 Models overdispersion + excess zeros</li> <li>DiT over U-Net \u2014 Better for latent space (no spatial structure)</li> <li>Classifier-free guidance \u2014 Better controllability</li> <li>Two-stage training \u2014 VAE first, then diffusion</li> </ol>"},{"location":"latent_diffusion/01_latent_diffusion_foundations/#for-biology","title":"For Biology","text":"<ol> <li>78\u00d7 compression \u2014 20K genes \u2192 256 latent dims</li> <li>10-100\u00d7 speedup \u2014 Faster training and sampling</li> <li>Better quality \u2014 Sharper than VAE, stable than GAN</li> <li>Interpretable latent \u2014 Can analyze latent space</li> </ol>"},{"location":"latent_diffusion/01_latent_diffusion_foundations/#related-documents","title":"Related Documents","text":"<ul> <li>00_latent_diffusion_overview.md \u2014 High-level concepts</li> <li>02_latent_diffusion_training.md \u2014 Training strategies</li> <li>03_latent_diffusion_applications.md \u2014 Applications</li> <li>04_latent_diffusion_combio.md \u2014 Complete implementation</li> </ul>"},{"location":"latent_diffusion/01_latent_diffusion_foundations/#references","title":"References","text":"<p>Latent Diffusion:</p> <ul> <li>Rombach et al. (2022): \"High-Resolution Image Synthesis with Latent Diffusion Models\"</li> <li>Vahdat et al. (2021): \"Score-based Generative Modeling in Latent Space\"</li> </ul> <p>VAE for Biology:</p> <ul> <li>Lopez et al. (2018): \"Deep generative modeling for single-cell transcriptomics\" (scVI)</li> <li>Eraslan et al. (2019): \"Single-cell RNA-seq denoising using a deep count autoencoder\"</li> </ul> <p>NB/ZINB Distributions:</p> <ul> <li>Hilbe (2011): \"Negative Binomial Regression\"</li> <li>Risso et al. (2018): \"A general and flexible method for signal extraction from single-cell RNA-seq data\" (ZINB-WaVE)</li> </ul> <p>DiT:</p> <ul> <li>Peebles &amp; Xie (2023): \"Scalable Diffusion Models with Transformers\"</li> </ul>"},{"location":"latent_diffusion/02_latent_diffusion_training/","title":"Latent Diffusion Training: Strategies and Best Practices","text":"<p>This document covers training strategies for latent diffusion models in computational biology, including two-stage training, hyperparameters, optimization, and debugging.</p> <p>Prerequisites: Understanding of latent diffusion foundations and overview.</p>"},{"location":"latent_diffusion/02_latent_diffusion_training/#training-overview","title":"Training Overview","text":""},{"location":"latent_diffusion/02_latent_diffusion_training/#two-stage-training-pipeline","title":"Two-Stage Training Pipeline","text":"<p>Stage 1: Train VAE (Learn latent space) <pre><code># Train VAE on gene expression\nvae = GeneExpressionVAE(num_genes=20000, latent_dim=256)\ntrain_vae(vae, gene_expression_data, num_epochs=100)\n\n# Freeze VAE\nvae.eval()\nfor param in vae.parameters():\n    param.requires_grad = False\n</code></pre></p> <p>Stage 2: Train Diffusion (Learn generation in latent space) <pre><code># Encode data to latent\nlatents = encode_dataset(vae, gene_expression_data)\n\n# Train diffusion on latent codes\ndiffusion = LatentDiffusionModel(latent_dim=256)\ntrain_diffusion(diffusion, latents, num_epochs=100)\n</code></pre></p> <p>Optional Stage 3: Joint Fine-Tuning <pre><code># Unfreeze VAE\nfor param in vae.parameters():\n    param.requires_grad = True\n\n# Fine-tune end-to-end\ntrain_joint(vae, diffusion, gene_expression_data, num_epochs=20)\n</code></pre></p>"},{"location":"latent_diffusion/02_latent_diffusion_training/#1-stage-1-vae-training","title":"1. Stage 1: VAE Training","text":""},{"location":"latent_diffusion/02_latent_diffusion_training/#11-data-preparation","title":"1.1 Data Preparation","text":"<pre><code>import scanpy as sc\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass GeneExpressionDataset(Dataset):\n    \"\"\"\n    Dataset for gene expression.\n\n    Args:\n        adata: AnnData object with gene expression\n        transform: Optional transform (e.g., log1p)\n    \"\"\"\n    def __init__(self, adata, transform=True):\n        # Get expression matrix\n        if hasattr(adata.X, 'toarray'):\n            self.X = adata.X.toarray()\n        else:\n            self.X = adata.X\n\n        # Log transform\n        if transform:\n            self.X = np.log1p(self.X)\n\n        # Library size (for NB/ZINB)\n        self.library_size = self.X.sum(axis=1)\n\n        # Convert to tensors\n        self.X = torch.tensor(self.X, dtype=torch.float32)\n        self.library_size = torch.tensor(self.library_size, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.library_size[idx]\n\n\ndef prepare_data(adata_path, batch_size=64):\n    \"\"\"\n    Prepare data loaders.\n\n    Args:\n        adata_path: Path to AnnData file\n        batch_size: Batch size\n\n    Returns:\n        train_loader, val_loader, test_loader\n    \"\"\"\n    # Load data\n    adata = sc.read_h5ad(adata_path)\n\n    # Basic preprocessing\n    sc.pp.filter_genes(adata, min_cells=10)\n    sc.pp.normalize_total(adata, target_sum=1e4)\n\n    # Select highly variable genes\n    sc.pp.highly_variable_genes(adata, n_top_genes=5000)\n    adata = adata[:, adata.var['highly_variable']]\n\n    # Split train/val/test\n    n_cells = adata.n_obs\n    n_train = int(0.7 * n_cells)\n    n_val = int(0.15 * n_cells)\n\n    indices = np.random.permutation(n_cells)\n    train_idx = indices[:n_train]\n    val_idx = indices[n_train:n_train+n_val]\n    test_idx = indices[n_train+n_val:]\n\n    # Create datasets\n    train_dataset = GeneExpressionDataset(adata[train_idx])\n    val_dataset = GeneExpressionDataset(adata[val_idx])\n    test_dataset = GeneExpressionDataset(adata[test_idx])\n\n    # Create loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n\n    return train_loader, val_loader, test_loader\n</code></pre>"},{"location":"latent_diffusion/02_latent_diffusion_training/#12-vae-training-loop","title":"1.2 VAE Training Loop","text":"<pre><code>def train_vae(\n    vae,\n    train_loader,\n    val_loader,\n    num_epochs=100,\n    lr=1e-3,\n    beta=1.0,\n    device='cuda',\n    save_dir='checkpoints/vae',\n):\n    \"\"\"\n    Train VAE on gene expression.\n\n    Args:\n        vae: VAE model\n        train_loader: Training data loader\n        val_loader: Validation data loader\n        num_epochs: Number of epochs\n        lr: Learning rate\n        beta: KL weight (beta-VAE)\n        device: Device\n        save_dir: Checkpoint directory\n    \"\"\"\n    import os\n    from torch.utils.tensorboard import SummaryWriter\n\n    vae.to(device)\n    os.makedirs(save_dir, exist_ok=True)\n\n    # Optimizer\n    optimizer = torch.optim.AdamW(vae.parameters(), lr=lr, weight_decay=0.01)\n\n    # Scheduler\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)\n\n    # Logging\n    writer = SummaryWriter(f'{save_dir}/logs')\n\n    best_val_loss = float('inf')\n\n    for epoch in range(num_epochs):\n        # Training\n        vae.train()\n        train_loss = 0.0\n        train_recon = 0.0\n        train_kl = 0.0\n\n        for x, library_size in train_loader:\n            x = x.to(device)\n            library_size = library_size.to(device)\n\n            # Forward\n            recon_params, mu, logvar = vae(x, library_size)\n            loss, loss_dict = vae.loss(x, recon_params, mu, logvar, library_size, beta)\n\n            # Backward\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(vae.parameters(), max_norm=1.0)\n            optimizer.step()\n\n            # Accumulate\n            train_loss += loss_dict['loss']\n            train_recon += loss_dict['recon']\n            train_kl += loss_dict['kl']\n\n        # Average\n        train_loss /= len(train_loader)\n        train_recon /= len(train_loader)\n        train_kl /= len(train_loader)\n\n        # Validation\n        vae.eval()\n        val_loss = 0.0\n        val_recon = 0.0\n        val_kl = 0.0\n\n        with torch.no_grad():\n            for x, library_size in val_loader:\n                x = x.to(device)\n                library_size = library_size.to(device)\n\n                recon_params, mu, logvar = vae(x, library_size)\n                loss, loss_dict = vae.loss(x, recon_params, mu, logvar, library_size, beta)\n\n                val_loss += loss_dict['loss']\n                val_recon += loss_dict['recon']\n                val_kl += loss_dict['kl']\n\n        val_loss /= len(val_loader)\n        val_recon /= len(val_loader)\n        val_kl /= len(val_loader)\n\n        # Logging\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(f\"  Train: Loss={train_loss:.4f}, Recon={train_recon:.4f}, KL={train_kl:.4f}\")\n        print(f\"  Val:   Loss={val_loss:.4f}, Recon={val_recon:.4f}, KL={val_kl:.4f}\")\n\n        writer.add_scalar('train/loss', train_loss, epoch)\n        writer.add_scalar('train/recon', train_recon, epoch)\n        writer.add_scalar('train/kl', train_kl, epoch)\n        writer.add_scalar('val/loss', val_loss, epoch)\n        writer.add_scalar('val/recon', val_recon, epoch)\n        writer.add_scalar('val/kl', val_kl, epoch)\n\n        # Save best model\n        if val_loss &lt; best_val_loss:\n            best_val_loss = val_loss\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': vae.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_loss': val_loss,\n            }, f'{save_dir}/best_model.pt')\n            print(f\"  Saved best model (val_loss: {val_loss:.4f})\")\n\n        # Step scheduler\n        scheduler.step()\n\n    writer.close()\n    print(\"\\nVAE training complete!\")\n</code></pre>"},{"location":"latent_diffusion/02_latent_diffusion_training/#13-vae-hyperparameters","title":"1.3 VAE Hyperparameters","text":"Parameter Value Notes Learning rate 1e-3 Standard for Adam Batch size 64-256 Larger is better (up to memory) Latent dim 256-512 Balance compression and quality Hidden dims [2048, 1024, 512] Gradual compression Beta (KL weight) 0.1-1.0 Lower = better reconstruction Weight decay 0.01 Regularization Epochs 100-200 Until convergence <p>Beta-VAE tuning:</p> <ul> <li><code>beta &lt; 1.0</code>: Better reconstruction, less disentanglement</li> <li><code>beta = 1.0</code>: Standard VAE</li> <li><code>beta &gt; 1.0</code>: More disentanglement, worse reconstruction</li> </ul> <p>For biology, typically use <code>beta = 0.5-1.0</code> to prioritize reconstruction quality.</p>"},{"location":"latent_diffusion/02_latent_diffusion_training/#2-stage-2-latent-diffusion-training","title":"2. Stage 2: Latent Diffusion Training","text":""},{"location":"latent_diffusion/02_latent_diffusion_training/#21-encode-dataset-to-latent-space","title":"2.1 Encode Dataset to Latent Space","text":"<pre><code>@torch.no_grad()\ndef encode_dataset(vae, data_loader, device='cuda'):\n    \"\"\"\n    Encode entire dataset to latent space.\n\n    Args:\n        vae: Trained VAE model\n        data_loader: Data loader\n        device: Device\n\n    Returns:\n        latents: Encoded latent codes (N, latent_dim)\n        library_sizes: Library sizes (N,)\n    \"\"\"\n    vae.eval()\n    vae.to(device)\n\n    all_latents = []\n    all_library_sizes = []\n\n    for x, library_size in data_loader:\n        x = x.to(device)\n\n        # Encode (deterministic)\n        mu, logvar = vae.encoder(x)\n        latents = mu  # Use mean, not sample\n\n        all_latents.append(latents.cpu())\n        all_library_sizes.append(library_size)\n\n    latents = torch.cat(all_latents, dim=0)\n    library_sizes = torch.cat(all_library_sizes, dim=0)\n\n    return latents, library_sizes\n\n\nclass LatentDataset(Dataset):\n    \"\"\"Dataset of latent codes.\"\"\"\n    def __init__(self, latents, library_sizes=None, conditions=None):\n        self.latents = latents\n        self.library_sizes = library_sizes\n        self.conditions = conditions\n\n    def __len__(self):\n        return len(self.latents)\n\n    def __getitem__(self, idx):\n        if self.conditions is not None:\n            return self.latents[idx], self.conditions[idx]\n        else:\n            return self.latents[idx]\n</code></pre>"},{"location":"latent_diffusion/02_latent_diffusion_training/#22-diffusion-training-loop","title":"2.2 Diffusion Training Loop","text":"<pre><code>def train_diffusion(\n    diffusion,\n    train_loader,\n    val_loader,\n    num_epochs=100,\n    lr=1e-4,\n    device='cuda',\n    save_dir='checkpoints/diffusion',\n):\n    \"\"\"\n    Train latent diffusion model.\n\n    Args:\n        diffusion: Latent diffusion model\n        train_loader: Training latent loader\n        val_loader: Validation latent loader\n        num_epochs: Number of epochs\n        lr: Learning rate\n        device: Device\n        save_dir: Checkpoint directory\n    \"\"\"\n    import os\n    from torch.utils.tensorboard import SummaryWriter\n\n    diffusion.to(device)\n    os.makedirs(save_dir, exist_ok=True)\n\n    # Optimizer\n    optimizer = torch.optim.AdamW(diffusion.parameters(), lr=lr, weight_decay=0.01)\n\n    # Scheduler\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)\n\n    # Logging\n    writer = SummaryWriter(f'{save_dir}/logs')\n\n    best_val_loss = float('inf')\n\n    for epoch in range(num_epochs):\n        # Training\n        diffusion.train()\n        train_loss = 0.0\n\n        for batch in train_loader:\n            if isinstance(batch, tuple):\n                z0, condition = batch\n                z0 = z0.to(device)\n                condition = condition.to(device)\n            else:\n                z0 = batch.to(device)\n                condition = None\n\n            # Sample timestep\n            t = torch.randint(0, diffusion.num_steps, (z0.shape[0],), device=device)\n\n            # Add noise\n            zt, noise = diffusion.add_noise(z0, t)\n\n            # Predict noise\n            noise_pred = diffusion(zt, t, condition)\n\n            # MSE loss\n            loss = F.mse_loss(noise_pred, noise)\n\n            # Backward\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(diffusion.parameters(), max_norm=1.0)\n            optimizer.step()\n\n            train_loss += loss.item()\n\n        train_loss /= len(train_loader)\n\n        # Validation\n        diffusion.eval()\n        val_loss = 0.0\n\n        with torch.no_grad():\n            for batch in val_loader:\n                if isinstance(batch, tuple):\n                    z0, condition = batch\n                    z0 = z0.to(device)\n                    condition = condition.to(device)\n                else:\n                    z0 = batch.to(device)\n                    condition = None\n\n                t = torch.randint(0, diffusion.num_steps, (z0.shape[0],), device=device)\n                zt, noise = diffusion.add_noise(z0, t)\n                noise_pred = diffusion(zt, t, condition)\n                loss = F.mse_loss(noise_pred, noise)\n\n                val_loss += loss.item()\n\n        val_loss /= len(val_loader)\n\n        # Logging\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(f\"  Train Loss: {train_loss:.4f}\")\n        print(f\"  Val Loss:   {val_loss:.4f}\")\n\n        writer.add_scalar('train/loss', train_loss, epoch)\n        writer.add_scalar('val/loss', val_loss, epoch)\n\n        # Save best model\n        if val_loss &lt; best_val_loss:\n            best_val_loss = val_loss\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': diffusion.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_loss': val_loss,\n            }, f'{save_dir}/best_model.pt')\n            print(f\"  Saved best model (val_loss: {val_loss:.4f})\")\n\n        scheduler.step()\n\n    writer.close()\n    print(\"\\nDiffusion training complete!\")\n</code></pre>"},{"location":"latent_diffusion/02_latent_diffusion_training/#23-diffusion-hyperparameters","title":"2.3 Diffusion Hyperparameters","text":"Parameter Value Notes Learning rate 1e-4 Lower than VAE Batch size 128-512 Can be larger (latent is small) Num steps 1000 Diffusion timesteps Model depth 12 DiT layers Hidden dim 512 DiT hidden dimension Num heads 8 Attention heads Weight decay 0.01 Regularization Epochs 100-200 Until convergence"},{"location":"latent_diffusion/02_latent_diffusion_training/#3-stage-3-joint-fine-tuning-optional","title":"3. Stage 3: Joint Fine-Tuning (Optional)","text":""},{"location":"latent_diffusion/02_latent_diffusion_training/#31-when-to-use-joint-fine-tuning","title":"3.1 When to Use Joint Fine-Tuning","text":"<p>Use when:</p> <ul> <li>VAE reconstruction is suboptimal</li> <li>Want end-to-end optimization</li> <li>Have sufficient data</li> </ul> <p>Don't use when:</p> <ul> <li>VAE is already good</li> <li>Limited data (risk overfitting)</li> <li>Want modular components</li> </ul>"},{"location":"latent_diffusion/02_latent_diffusion_training/#32-joint-training-loop","title":"3.2 Joint Training Loop","text":"<pre><code>def train_joint(\n    vae,\n    diffusion,\n    train_loader,\n    val_loader,\n    num_epochs=20,\n    lr_vae=1e-5,\n    lr_diffusion=1e-4,\n    device='cuda',\n    save_dir='checkpoints/joint',\n):\n    \"\"\"\n    Joint fine-tuning of VAE and diffusion.\n\n    Args:\n        vae: VAE model\n        diffusion: Diffusion model\n        train_loader: Training data loader (gene expression)\n        val_loader: Validation data loader\n        num_epochs: Number of epochs\n        lr_vae: Learning rate for VAE\n        lr_diffusion: Learning rate for diffusion\n        device: Device\n        save_dir: Checkpoint directory\n    \"\"\"\n    import os\n    from torch.utils.tensorboard import SummaryWriter\n\n    vae.to(device)\n    diffusion.to(device)\n    os.makedirs(save_dir, exist_ok=True)\n\n    # Separate optimizers for VAE and diffusion\n    optimizer_vae = torch.optim.AdamW(vae.parameters(), lr=lr_vae, weight_decay=0.01)\n    optimizer_diffusion = torch.optim.AdamW(diffusion.parameters(), lr=lr_diffusion, weight_decay=0.01)\n\n    # Schedulers\n    scheduler_vae = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_vae, num_epochs)\n    scheduler_diffusion = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_diffusion, num_epochs)\n\n    # Logging\n    writer = SummaryWriter(f'{save_dir}/logs')\n\n    best_val_loss = float('inf')\n\n    for epoch in range(num_epochs):\n        # Training\n        vae.train()\n        diffusion.train()\n\n        train_vae_loss = 0.0\n        train_diff_loss = 0.0\n\n        for x, library_size in train_loader:\n            x = x.to(device)\n            library_size = library_size.to(device)\n\n            # 1. VAE forward\n            recon_params, mu, logvar = vae(x, library_size)\n            vae_loss, vae_loss_dict = vae.loss(x, recon_params, mu, logvar, library_size)\n\n            # 2. Diffusion forward (on latent)\n            z0 = vae.encoder.reparameterize(mu, logvar)\n            t = torch.randint(0, diffusion.num_steps, (z0.shape[0],), device=device)\n            zt, noise = diffusion.add_noise(z0, t)\n            noise_pred = diffusion(zt, t)\n            diff_loss = F.mse_loss(noise_pred, noise)\n\n            # 3. Backward (alternate updates)\n            # Update VAE\n            optimizer_vae.zero_grad()\n            vae_loss.backward(retain_graph=True)\n            torch.nn.utils.clip_grad_norm_(vae.parameters(), max_norm=1.0)\n            optimizer_vae.step()\n\n            # Update diffusion\n            optimizer_diffusion.zero_grad()\n            diff_loss.backward()\n            torch.nn.utils.clip_grad_norm_(diffusion.parameters(), max_norm=1.0)\n            optimizer_diffusion.step()\n\n            train_vae_loss += vae_loss.item()\n            train_diff_loss += diff_loss.item()\n\n        train_vae_loss /= len(train_loader)\n        train_diff_loss /= len(train_loader)\n\n        # Validation\n        vae.eval()\n        diffusion.eval()\n\n        val_vae_loss = 0.0\n        val_diff_loss = 0.0\n\n        with torch.no_grad():\n            for x, library_size in val_loader:\n                x = x.to(device)\n                library_size = library_size.to(device)\n\n                recon_params, mu, logvar = vae(x, library_size)\n                vae_loss, _ = vae.loss(x, recon_params, mu, logvar, library_size)\n\n                z0 = mu\n                t = torch.randint(0, diffusion.num_steps, (z0.shape[0],), device=device)\n                zt, noise = diffusion.add_noise(z0, t)\n                noise_pred = diffusion(zt, t)\n                diff_loss = F.mse_loss(noise_pred, noise)\n\n                val_vae_loss += vae_loss.item()\n                val_diff_loss += diff_loss.item()\n\n        val_vae_loss /= len(val_loader)\n        val_diff_loss /= len(val_loader)\n        val_total_loss = val_vae_loss + val_diff_loss\n\n        # Logging\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(f\"  Train: VAE={train_vae_loss:.4f}, Diff={train_diff_loss:.4f}\")\n        print(f\"  Val:   VAE={val_vae_loss:.4f}, Diff={val_diff_loss:.4f}\")\n\n        writer.add_scalar('train/vae_loss', train_vae_loss, epoch)\n        writer.add_scalar('train/diff_loss', train_diff_loss, epoch)\n        writer.add_scalar('val/vae_loss', val_vae_loss, epoch)\n        writer.add_scalar('val/diff_loss', val_diff_loss, epoch)\n\n        # Save best model\n        if val_total_loss &lt; best_val_loss:\n            best_val_loss = val_total_loss\n            torch.save({\n                'epoch': epoch,\n                'vae_state_dict': vae.state_dict(),\n                'diffusion_state_dict': diffusion.state_dict(),\n                'val_loss': val_total_loss,\n            }, f'{save_dir}/best_model.pt')\n            print(f\"  Saved best model (val_loss: {val_total_loss:.4f})\")\n\n        scheduler_vae.step()\n        scheduler_diffusion.step()\n\n    writer.close()\n    print(\"\\nJoint fine-tuning complete!\")\n</code></pre>"},{"location":"latent_diffusion/02_latent_diffusion_training/#4-conditioning-training","title":"4. Conditioning Training","text":""},{"location":"latent_diffusion/02_latent_diffusion_training/#41-conditional-dataset","title":"4.1 Conditional Dataset","text":"<pre><code>class ConditionalGeneExpressionDataset(Dataset):\n    \"\"\"\n    Dataset with conditioning information.\n\n    Args:\n        adata: AnnData with expression\n        condition_key: Key in adata.obs for condition (e.g., 'cell_type')\n        condition_encoder: Function to encode condition to embedding\n    \"\"\"\n    def __init__(self, adata, condition_key='cell_type', condition_encoder=None):\n        # Expression\n        if hasattr(adata.X, 'toarray'):\n            self.X = adata.X.toarray()\n        else:\n            self.X = adata.X\n\n        self.X = torch.tensor(np.log1p(self.X), dtype=torch.float32)\n\n        # Library size\n        self.library_size = torch.tensor(self.X.sum(axis=1), dtype=torch.float32)\n\n        # Condition\n        self.conditions = adata.obs[condition_key].values\n\n        # Encode conditions\n        if condition_encoder is None:\n            # Default: one-hot encoding\n            unique_conditions = np.unique(self.conditions)\n            self.condition_to_idx = {c: i for i, c in enumerate(unique_conditions)}\n            self.num_conditions = len(unique_conditions)\n\n            condition_indices = [self.condition_to_idx[c] for c in self.conditions]\n            self.condition_embeddings = F.one_hot(\n                torch.tensor(condition_indices),\n                num_classes=self.num_conditions\n            ).float()\n        else:\n            self.condition_embeddings = condition_encoder(self.conditions)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.library_size[idx], self.condition_embeddings[idx]\n</code></pre>"},{"location":"latent_diffusion/02_latent_diffusion_training/#42-classifier-free-guidance-training","title":"4.2 Classifier-Free Guidance Training","text":"<pre><code>def train_diffusion_with_cfg(\n    diffusion,\n    train_loader,\n    val_loader,\n    num_epochs=100,\n    lr=1e-4,\n    dropout_prob=0.1,\n    device='cuda',\n    save_dir='checkpoints/diffusion_cfg',\n):\n    \"\"\"\n    Train diffusion with classifier-free guidance.\n\n    Args:\n        diffusion: Latent diffusion model\n        train_loader: Training loader (with conditions)\n        val_loader: Validation loader\n        num_epochs: Number of epochs\n        lr: Learning rate\n        dropout_prob: Probability of dropping condition\n        device: Device\n        save_dir: Checkpoint directory\n    \"\"\"\n    import os\n    from torch.utils.tensorboard import SummaryWriter\n\n    diffusion.to(device)\n    os.makedirs(save_dir, exist_ok=True)\n\n    optimizer = torch.optim.AdamW(diffusion.parameters(), lr=lr, weight_decay=0.01)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)\n    writer = SummaryWriter(f'{save_dir}/logs')\n\n    best_val_loss = float('inf')\n\n    for epoch in range(num_epochs):\n        diffusion.train()\n        train_loss = 0.0\n\n        for z0, condition in train_loader:\n            z0 = z0.to(device)\n            condition = condition.to(device)\n\n            # Randomly drop condition (for classifier-free guidance)\n            mask = torch.rand(condition.shape[0], device=device) &gt; dropout_prob\n            condition_masked = condition * mask.unsqueeze(-1)\n\n            # Sample timestep\n            t = torch.randint(0, diffusion.num_steps, (z0.shape[0],), device=device)\n\n            # Add noise\n            zt, noise = diffusion.add_noise(z0, t)\n\n            # Predict noise\n            noise_pred = diffusion(zt, t, condition_masked)\n\n            # MSE loss\n            loss = F.mse_loss(noise_pred, noise)\n\n            # Backward\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(diffusion.parameters(), max_norm=1.0)\n            optimizer.step()\n\n            train_loss += loss.item()\n\n        train_loss /= len(train_loader)\n\n        # Validation (without dropout)\n        diffusion.eval()\n        val_loss = 0.0\n\n        with torch.no_grad():\n            for z0, condition in val_loader:\n                z0 = z0.to(device)\n                condition = condition.to(device)\n\n                t = torch.randint(0, diffusion.num_steps, (z0.shape[0],), device=device)\n                zt, noise = diffusion.add_noise(z0, t)\n                noise_pred = diffusion(zt, t, condition)\n                loss = F.mse_loss(noise_pred, noise)\n\n                val_loss += loss.item()\n\n        val_loss /= len(val_loader)\n\n        # Logging\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(f\"  Train Loss: {train_loss:.4f}\")\n        print(f\"  Val Loss:   {val_loss:.4f}\")\n\n        writer.add_scalar('train/loss', train_loss, epoch)\n        writer.add_scalar('val/loss', val_loss, epoch)\n\n        if val_loss &lt; best_val_loss:\n            best_val_loss = val_loss\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': diffusion.state_dict(),\n                'val_loss': val_loss,\n            }, f'{save_dir}/best_model.pt')\n\n        scheduler.step()\n\n    writer.close()\n</code></pre>"},{"location":"latent_diffusion/02_latent_diffusion_training/#5-monitoring-and-debugging","title":"5. Monitoring and Debugging","text":""},{"location":"latent_diffusion/02_latent_diffusion_training/#51-key-metrics","title":"5.1 Key Metrics","text":"<p>VAE metrics:</p> <ul> <li>Reconstruction loss (NLL)</li> <li>KL divergence</li> <li>Total ELBO</li> <li>Reconstruction quality (correlation with original)</li> </ul> <p>Diffusion metrics:</p> <ul> <li>MSE loss (noise prediction)</li> <li>Sample quality (FID, IS for images; biological metrics for gene expression)</li> <li>Sampling diversity</li> </ul>"},{"location":"latent_diffusion/02_latent_diffusion_training/#52-monitoring-script","title":"5.2 Monitoring Script","text":"<pre><code>@torch.no_grad()\ndef evaluate_vae(vae, test_loader, device='cuda'):\n    \"\"\"\n    Evaluate VAE reconstruction quality.\n\n    Returns:\n        metrics: Dictionary of evaluation metrics\n    \"\"\"\n    vae.eval()\n    vae.to(device)\n\n    all_x = []\n    all_x_recon = []\n    total_loss = 0.0\n\n    for x, library_size in test_loader:\n        x = x.to(device)\n        library_size = library_size.to(device)\n\n        # Reconstruct\n        recon_params, mu, logvar = vae(x, library_size)\n        loss, _ = vae.loss(x, recon_params, mu, logvar, library_size)\n\n        # Get reconstruction\n        if vae.decoder_type == 'nb':\n            mean, _ = recon_params\n            x_recon = mean\n        elif vae.decoder_type == 'zinb':\n            mean, _, dropout = recon_params\n            x_recon = (1 - dropout) * mean\n\n        all_x.append(x.cpu())\n        all_x_recon.append(x_recon.cpu())\n        total_loss += loss.item()\n\n    all_x = torch.cat(all_x, dim=0)\n    all_x_recon = torch.cat(all_x_recon, dim=0)\n\n    # Compute correlation\n    from scipy.stats import pearsonr\n\n    correlations = []\n    for i in range(len(all_x)):\n        corr, _ = pearsonr(all_x[i].numpy(), all_x_recon[i].numpy())\n        correlations.append(corr)\n\n    metrics = {\n        'loss': total_loss / len(test_loader),\n        'mean_correlation': np.mean(correlations),\n        'median_correlation': np.median(correlations),\n    }\n\n    return metrics\n\n\n@torch.no_grad()\ndef evaluate_diffusion_samples(vae, diffusion, num_samples=1000, device='cuda'):\n    \"\"\"\n    Evaluate diffusion sample quality.\n\n    Returns:\n        samples: Generated samples\n        metrics: Quality metrics\n    \"\"\"\n    vae.eval()\n    diffusion.eval()\n    vae.to(device)\n    diffusion.to(device)\n\n    # Sample from diffusion\n    z_samples = diffusion.sample(num_samples)\n\n    # Decode to gene expression\n    x_samples = vae.decode(z_samples)\n\n    # Compute metrics\n    metrics = {\n        'mean_expression': x_samples.mean().item(),\n        'std_expression': x_samples.std().item(),\n        'sparsity': (x_samples &lt; 0.1).float().mean().item(),\n    }\n\n    return x_samples.cpu(), metrics\n</code></pre>"},{"location":"latent_diffusion/02_latent_diffusion_training/#53-common-issues-and-solutions","title":"5.3 Common Issues and Solutions","text":"Issue Symptom Solution VAE posterior collapse KL \u2192 0, poor reconstruction Decrease beta, add KL warmup VAE blurry reconstruction Low correlation Increase model capacity, decrease beta Diffusion mode collapse Low diversity Check data diversity, increase model capacity Diffusion poor quality Noisy samples Train longer, increase num_steps NaN loss Loss becomes NaN Reduce LR, add gradient clipping Slow convergence Loss plateaus early Increase LR, check data preprocessing"},{"location":"latent_diffusion/02_latent_diffusion_training/#key-takeaways","title":"Key Takeaways","text":""},{"location":"latent_diffusion/02_latent_diffusion_training/#training-strategy","title":"Training Strategy","text":"<ol> <li>Two-stage training \u2014 VAE first, then diffusion</li> <li>Freeze VAE \u2014 During diffusion training</li> <li>Optional joint fine-tuning \u2014 End-to-end optimization</li> <li>Classifier-free guidance \u2014 For better controllability</li> </ol>"},{"location":"latent_diffusion/02_latent_diffusion_training/#hyperparameters","title":"Hyperparameters","text":"<ol> <li>VAE: lr=1e-3, beta=0.5-1.0, latent_dim=256-512</li> <li>Diffusion: lr=1e-4, num_steps=1000, hidden_dim=512</li> <li>Joint: lr_vae=1e-5 (lower), lr_diffusion=1e-4</li> <li>CFG: dropout_prob=0.1, guidance_scale=7.5</li> </ol>"},{"location":"latent_diffusion/02_latent_diffusion_training/#monitoring","title":"Monitoring","text":"<ol> <li>VAE: Reconstruction loss, KL, correlation</li> <li>Diffusion: MSE loss, sample quality, diversity</li> <li>Biological: Expression distribution, sparsity, pathway activity</li> </ol>"},{"location":"latent_diffusion/02_latent_diffusion_training/#best-practices","title":"Best Practices","text":"<ol> <li>Start simple \u2014 Train VAE well before diffusion</li> <li>Monitor closely \u2014 Check reconstruction quality</li> <li>Use CFG \u2014 Better conditional generation</li> <li>Validate biologically \u2014 Not just loss metrics</li> </ol>"},{"location":"latent_diffusion/02_latent_diffusion_training/#related-documents","title":"Related Documents","text":"<ul> <li>00_latent_diffusion_overview.md \u2014 High-level concepts</li> <li>01_latent_diffusion_foundations.md \u2014 Architecture details</li> <li>03_latent_diffusion_applications.md \u2014 Applications</li> <li>04_latent_diffusion_combio.md \u2014 Complete implementation</li> </ul>"},{"location":"latent_diffusion/02_latent_diffusion_training/#references","title":"References","text":"<p>Latent Diffusion:</p> <ul> <li>Rombach et al. (2022): \"High-Resolution Image Synthesis with Latent Diffusion Models\"</li> <li>Ho &amp; Salimans (2022): \"Classifier-Free Diffusion Guidance\"</li> </ul> <p>VAE Training:</p> <ul> <li>Higgins et al. (2017): \"beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework\"</li> <li>Bowman et al. (2016): \"Generating Sentences from a Continuous Space\" (KL annealing)</li> </ul> <p>Biology Applications:</p> <ul> <li>Lopez et al. (2018): \"Deep generative modeling for single-cell transcriptomics\" (scVI)</li> <li>Lotfollahi et al. (2020): \"scGen predicts single-cell perturbation responses\"</li> </ul>"},{"location":"latent_diffusion/03_latent_diffusion_applications/","title":"Latent Diffusion Applications: Computational Biology","text":"<p>This document covers applications of latent diffusion models in computational biology, including single-cell generation, perturbation prediction, multi-omics translation, trajectory modeling, and spatial transcriptomics.</p> <p>Prerequisites: Understanding of latent diffusion foundations and training.</p>"},{"location":"latent_diffusion/03_latent_diffusion_applications/#overview","title":"Overview","text":"<p>Latent diffusion models are particularly well-suited for computational biology because:</p> <ol> <li>Efficiency \u2014 10-100\u00d7 faster than pixel-space diffusion</li> <li>Quality \u2014 Better than VAE, stable than GAN</li> <li>Flexibility \u2014 Multi-modal, multi-task, controllable</li> <li>Interpretability \u2014 Latent space has biological meaning</li> </ol>"},{"location":"latent_diffusion/03_latent_diffusion_applications/#1-single-cell-generation","title":"1. Single-Cell Generation","text":""},{"location":"latent_diffusion/03_latent_diffusion_applications/#11-task-definition","title":"1.1 Task Definition","text":"<p>Goal: Generate realistic single-cell gene expression profiles.</p> <p>Applications:</p> <ul> <li>Data augmentation for rare cell types</li> <li>Synthetic controls for experiments</li> <li>Batch effect removal</li> <li>Counterfactual cell states</li> </ul>"},{"location":"latent_diffusion/03_latent_diffusion_applications/#12-architecture","title":"1.2 Architecture","text":"<pre><code>class SingleCellLatentDiffusion(nn.Module):\n    \"\"\"\n    Latent diffusion for single-cell generation.\n\n    Args:\n        num_genes: Number of genes\n        latent_dim: Latent dimension\n        num_cell_types: Number of cell types (for conditioning)\n    \"\"\"\n    def __init__(\n        self,\n        num_genes=5000,\n        latent_dim=256,\n        num_cell_types=20,\n    ):\n        super().__init__()\n\n        # VAE with ZINB decoder\n        self.vae = GeneExpressionVAE(\n            num_genes=num_genes,\n            latent_dim=latent_dim,\n            decoder_type='zinb',\n        )\n\n        # Latent diffusion with cell type conditioning\n        self.diffusion = LatentDiffusionModel(\n            latent_dim=latent_dim,\n            model_type='dit',\n        )\n\n        # Cell type embedding\n        self.cell_type_embed = nn.Embedding(num_cell_types, 128)\n\n    def forward(self, x, cell_type, library_size=None):\n        \"\"\"\n        Training forward pass.\n\n        Args:\n            x: Gene expression (B, num_genes)\n            cell_type: Cell type indices (B,)\n            library_size: Library size (B,)\n\n        Returns:\n            vae_loss, diffusion_loss\n        \"\"\"\n        # Train VAE\n        recon_params, mu, logvar = self.vae(x, library_size)\n        vae_loss, _ = self.vae.loss(x, recon_params, mu, logvar, library_size)\n\n        # Train diffusion on latent\n        with torch.no_grad():\n            z0 = mu\n\n        # Cell type conditioning\n        cell_type_emb = self.cell_type_embed(cell_type)\n\n        # Diffusion loss\n        t = torch.randint(0, self.diffusion.num_steps, (z0.shape[0],), device=z0.device)\n        zt, noise = self.diffusion.add_noise(z0, t)\n        noise_pred = self.diffusion(zt, t, cell_type_emb)\n        diffusion_loss = F.mse_loss(noise_pred, noise)\n\n        return vae_loss, diffusion_loss\n\n    @torch.no_grad()\n    def generate(self, cell_type, num_samples=100, library_size=None):\n        \"\"\"\n        Generate single-cell profiles.\n\n        Args:\n            cell_type: Cell type index (scalar or (num_samples,))\n            num_samples: Number of samples\n            library_size: Optional library size\n\n        Returns:\n            x_gen: Generated expression (num_samples, num_genes)\n        \"\"\"\n        device = next(self.parameters()).device\n\n        # Cell type conditioning\n        if isinstance(cell_type, int):\n            cell_type = torch.full((num_samples,), cell_type, device=device)\n        cell_type_emb = self.cell_type_embed(cell_type)\n\n        # Sample latent from diffusion\n        z0 = self.diffusion.sample(num_samples, cell_type_emb)\n\n        # Decode to gene expression\n        x_gen = self.vae.decode(z0, library_size)\n\n        return x_gen\n</code></pre>"},{"location":"latent_diffusion/03_latent_diffusion_applications/#13-training-strategy","title":"1.3 Training Strategy","text":"<pre><code>def train_single_cell_generation(\n    model,\n    train_loader,\n    val_loader,\n    num_epochs=100,\n    lr=1e-4,\n    device='cuda',\n):\n    \"\"\"\n    Train single-cell generation model.\n\n    Args:\n        model: SingleCellLatentDiffusion\n        train_loader: Training data (x, cell_type, library_size)\n        val_loader: Validation data\n        num_epochs: Number of epochs\n        lr: Learning rate\n        device: Device\n    \"\"\"\n    model.to(device)\n\n    # Separate optimizers\n    optimizer_vae = torch.optim.AdamW(model.vae.parameters(), lr=lr)\n    optimizer_diffusion = torch.optim.AdamW(\n        list(model.diffusion.parameters()) + list(model.cell_type_embed.parameters()),\n        lr=lr\n    )\n\n    for epoch in range(num_epochs):\n        model.train()\n\n        for x, cell_type, library_size in train_loader:\n            x = x.to(device)\n            cell_type = cell_type.to(device)\n            library_size = library_size.to(device)\n\n            # Forward\n            vae_loss, diffusion_loss = model(x, cell_type, library_size)\n\n            # Update VAE\n            optimizer_vae.zero_grad()\n            vae_loss.backward(retain_graph=True)\n            optimizer_vae.step()\n\n            # Update diffusion\n            optimizer_diffusion.zero_grad()\n            diffusion_loss.backward()\n            optimizer_diffusion.step()\n\n        print(f\"Epoch {epoch+1}: VAE={vae_loss:.4f}, Diff={diffusion_loss:.4f}\")\n</code></pre>"},{"location":"latent_diffusion/03_latent_diffusion_applications/#14-evaluation","title":"1.4 Evaluation","text":"<pre><code>@torch.no_grad()\ndef evaluate_single_cell_generation(model, test_data, cell_types, device='cuda'):\n    \"\"\"\n    Evaluate single-cell generation quality.\n\n    Args:\n        model: Trained model\n        test_data: Real test data (N, num_genes)\n        cell_types: Cell type labels (N,)\n        device: Device\n\n    Returns:\n        metrics: Evaluation metrics\n    \"\"\"\n    import scanpy as sc\n    from scipy.stats import wasserstein_distance\n\n    model.eval()\n    model.to(device)\n\n    # Generate samples for each cell type\n    unique_cell_types = torch.unique(cell_types)\n\n    metrics = {}\n\n    for ct in unique_cell_types:\n        # Real data for this cell type\n        mask = (cell_types == ct)\n        real_data = test_data[mask]\n\n        # Generate synthetic data\n        num_samples = len(real_data)\n        synthetic_data = model.generate(ct.item(), num_samples)\n\n        # 1. Mean expression correlation\n        real_mean = real_data.mean(dim=0)\n        synth_mean = synthetic_data.mean(dim=0)\n        corr = torch.corrcoef(torch.stack([real_mean, synth_mean]))[0, 1]\n\n        # 2. Wasserstein distance (per gene, average)\n        w_dists = []\n        for gene_idx in range(real_data.shape[1]):\n            w_dist = wasserstein_distance(\n                real_data[:, gene_idx].cpu().numpy(),\n                synthetic_data[:, gene_idx].cpu().numpy()\n            )\n            w_dists.append(w_dist)\n\n        # 3. Sparsity similarity\n        real_sparsity = (real_data &lt; 0.1).float().mean()\n        synth_sparsity = (synthetic_data &lt; 0.1).float().mean()\n\n        metrics[f'cell_type_{ct.item()}'] = {\n            'mean_correlation': corr.item(),\n            'wasserstein_distance': np.mean(w_dists),\n            'real_sparsity': real_sparsity.item(),\n            'synth_sparsity': synth_sparsity.item(),\n        }\n\n    return metrics\n</code></pre>"},{"location":"latent_diffusion/03_latent_diffusion_applications/#2-perturbation-prediction","title":"2. Perturbation Prediction","text":""},{"location":"latent_diffusion/03_latent_diffusion_applications/#21-task-definition","title":"2.1 Task Definition","text":"<p>Goal: Predict cellular response to genetic/chemical perturbations.</p> <p>Applications:</p> <ul> <li>Virtual screening (predict without experiment)</li> <li>Combination prediction (multiple perturbations)</li> <li>Mechanism discovery (analyze latent changes)</li> </ul>"},{"location":"latent_diffusion/03_latent_diffusion_applications/#22-architecture","title":"2.2 Architecture","text":"<pre><code>class PerturbationLatentDiffusion(nn.Module):\n    \"\"\"\n    Latent diffusion for perturbation prediction.\n\n    Predicts perturbed state from baseline + perturbation.\n\n    Args:\n        num_genes: Number of genes\n        latent_dim: Latent dimension\n        perturbation_dim: Perturbation embedding dimension\n    \"\"\"\n    def __init__(\n        self,\n        num_genes=5000,\n        latent_dim=256,\n        perturbation_dim=128,\n    ):\n        super().__init__()\n\n        # VAE\n        self.vae = GeneExpressionVAE(\n            num_genes=num_genes,\n            latent_dim=latent_dim,\n            decoder_type='zinb',\n        )\n\n        # Perturbation encoder\n        self.perturbation_encoder = nn.Sequential(\n            nn.Linear(num_genes, 512),\n            nn.LayerNorm(512),\n            nn.GELU(),\n            nn.Linear(512, perturbation_dim),\n        )\n\n        # Latent diffusion conditioned on baseline + perturbation\n        self.diffusion = LatentDiffusionModel(\n            latent_dim=latent_dim,\n            model_type='dit',\n        )\n\n        # Conditioning projection\n        self.condition_proj = nn.Linear(latent_dim + perturbation_dim, latent_dim)\n\n    def forward(self, x_baseline, x_perturbed, perturbation_indicator):\n        \"\"\"\n        Training forward pass.\n\n        Args:\n            x_baseline: Baseline expression (B, num_genes)\n            x_perturbed: Perturbed expression (B, num_genes)\n            perturbation_indicator: One-hot perturbation (B, num_genes)\n\n        Returns:\n            vae_loss, diffusion_loss\n        \"\"\"\n        # Encode baseline and perturbed\n        z_baseline = self.vae.encode(x_baseline)\n        z_perturbed = self.vae.encode(x_perturbed)\n\n        # Encode perturbation\n        pert_emb = self.perturbation_encoder(perturbation_indicator)\n\n        # Condition: baseline latent + perturbation\n        condition = torch.cat([z_baseline, pert_emb], dim=-1)\n        condition = self.condition_proj(condition)\n\n        # Diffusion: predict perturbed latent from baseline + perturbation\n        t = torch.randint(0, self.diffusion.num_steps, (z_perturbed.shape[0],), device=z_perturbed.device)\n        zt, noise = self.diffusion.add_noise(z_perturbed, t)\n        noise_pred = self.diffusion(zt, t, condition)\n        diffusion_loss = F.mse_loss(noise_pred, noise)\n\n        return diffusion_loss\n\n    @torch.no_grad()\n    def predict_perturbation(self, x_baseline, perturbation_indicator, library_size=None):\n        \"\"\"\n        Predict perturbed state.\n\n        Args:\n            x_baseline: Baseline expression (B, num_genes)\n            perturbation_indicator: Perturbation (B, num_genes)\n            library_size: Library size (B,)\n\n        Returns:\n            x_perturbed_pred: Predicted perturbed expression (B, num_genes)\n        \"\"\"\n        # Encode baseline\n        z_baseline = self.vae.encode(x_baseline)\n\n        # Encode perturbation\n        pert_emb = self.perturbation_encoder(perturbation_indicator)\n\n        # Condition\n        condition = torch.cat([z_baseline, pert_emb], dim=-1)\n        condition = self.condition_proj(condition)\n\n        # Sample perturbed latent\n        z_perturbed = self.diffusion.sample(len(x_baseline), condition)\n\n        # Decode\n        x_perturbed_pred = self.vae.decode(z_perturbed, library_size)\n\n        return x_perturbed_pred\n</code></pre>"},{"location":"latent_diffusion/03_latent_diffusion_applications/#23-delta-in-latent-formulation","title":"2.3 Delta-in-Latent Formulation","text":"<p>Alternative approach: Predict the change in latent space.</p> <pre><code>class DeltaLatentPerturbation(nn.Module):\n    \"\"\"\n    Predict delta in latent space.\n\n    More stable than predicting absolute perturbed state.\n    \"\"\"\n    def __init__(\n        self,\n        num_genes=5000,\n        latent_dim=256,\n        perturbation_dim=128,\n    ):\n        super().__init__()\n\n        self.vae = GeneExpressionVAE(num_genes, latent_dim, 'zinb')\n        self.perturbation_encoder = nn.Sequential(\n            nn.Linear(num_genes, 512),\n            nn.GELU(),\n            nn.Linear(512, perturbation_dim),\n        )\n\n        # Predict delta\n        self.delta_predictor = nn.Sequential(\n            nn.Linear(latent_dim + perturbation_dim, 512),\n            nn.LayerNorm(512),\n            nn.GELU(),\n            nn.Linear(512, 512),\n            nn.LayerNorm(512),\n            nn.GELU(),\n            nn.Linear(512, latent_dim),\n        )\n\n    def forward(self, x_baseline, x_perturbed, perturbation_indicator):\n        \"\"\"\n        Training: predict delta in latent space.\n        \"\"\"\n        # Encode\n        z_baseline = self.vae.encode(x_baseline)\n        z_perturbed = self.vae.encode(x_perturbed)\n\n        # True delta\n        delta_true = z_perturbed - z_baseline\n\n        # Encode perturbation\n        pert_emb = self.perturbation_encoder(perturbation_indicator)\n\n        # Predict delta\n        delta_pred = self.delta_predictor(torch.cat([z_baseline, pert_emb], dim=-1))\n\n        # MSE loss on delta\n        loss = F.mse_loss(delta_pred, delta_true)\n\n        return loss\n\n    @torch.no_grad()\n    def predict_perturbation(self, x_baseline, perturbation_indicator, library_size=None):\n        \"\"\"\n        Predict perturbed state via delta.\n        \"\"\"\n        # Encode baseline\n        z_baseline = self.vae.encode(x_baseline)\n\n        # Encode perturbation\n        pert_emb = self.perturbation_encoder(perturbation_indicator)\n\n        # Predict delta\n        delta_pred = self.delta_predictor(torch.cat([z_baseline, pert_emb], dim=-1))\n\n        # Add delta to baseline\n        z_perturbed = z_baseline + delta_pred\n\n        # Decode\n        x_perturbed_pred = self.vae.decode(z_perturbed, library_size)\n\n        return x_perturbed_pred\n</code></pre>"},{"location":"latent_diffusion/03_latent_diffusion_applications/#24-evaluation","title":"2.4 Evaluation","text":"<pre><code>@torch.no_grad()\ndef evaluate_perturbation_prediction(\n    model,\n    test_data,\n    perturbations,\n    device='cuda',\n):\n    \"\"\"\n    Evaluate perturbation prediction.\n\n    Args:\n        model: Trained model\n        test_data: (baseline, perturbed, perturbation_indicator) tuples\n        perturbations: List of perturbation names\n        device: Device\n\n    Returns:\n        metrics: Evaluation metrics\n    \"\"\"\n    from scipy.stats import pearsonr\n\n    model.eval()\n    model.to(device)\n\n    all_correlations = []\n    all_mse = []\n\n    for x_baseline, x_perturbed_true, pert_indicator in test_data:\n        x_baseline = x_baseline.to(device)\n        x_perturbed_true = x_perturbed_true.to(device)\n        pert_indicator = pert_indicator.to(device)\n\n        # Predict\n        x_perturbed_pred = model.predict_perturbation(x_baseline, pert_indicator)\n\n        # Correlation (per sample)\n        for i in range(len(x_baseline)):\n            corr, _ = pearsonr(\n                x_perturbed_true[i].cpu().numpy(),\n                x_perturbed_pred[i].cpu().numpy()\n            )\n            all_correlations.append(corr)\n\n        # MSE\n        mse = F.mse_loss(x_perturbed_pred, x_perturbed_true)\n        all_mse.append(mse.item())\n\n    metrics = {\n        'mean_correlation': np.mean(all_correlations),\n        'median_correlation': np.median(all_correlations),\n        'mean_mse': np.mean(all_mse),\n    }\n\n    return metrics\n</code></pre>"},{"location":"latent_diffusion/03_latent_diffusion_applications/#3-multi-omics-translation","title":"3. Multi-Omics Translation","text":""},{"location":"latent_diffusion/03_latent_diffusion_applications/#31-task-definition","title":"3.1 Task Definition","text":"<p>Goal: Predict one modality from another (e.g., protein from RNA).</p> <p>Applications:</p> <ul> <li>Fill missing modalities</li> <li>Cross-modality validation</li> <li>Integrated multi-omics analysis</li> </ul>"},{"location":"latent_diffusion/03_latent_diffusion_applications/#32-architecture","title":"3.2 Architecture","text":"<pre><code>class MultiOmicsLatentDiffusion(nn.Module):\n    \"\"\"\n    Latent diffusion for multi-omics translation.\n\n    Shared latent space for RNA and Protein.\n\n    Args:\n        num_genes_rna: Number of RNA genes\n        num_genes_protein: Number of protein genes\n        latent_dim: Shared latent dimension\n    \"\"\"\n    def __init__(\n        self,\n        num_genes_rna=5000,\n        num_genes_protein=100,\n        latent_dim=256,\n    ):\n        super().__init__()\n\n        # RNA encoder\n        self.rna_encoder = GeneExpressionEncoder(\n            num_genes=num_genes_rna,\n            latent_dim=latent_dim,\n        )\n\n        # Protein encoder\n        self.protein_encoder = GeneExpressionEncoder(\n            num_genes=num_genes_protein,\n            latent_dim=latent_dim,\n        )\n\n        # RNA decoder (ZINB)\n        self.rna_decoder = ZINBDecoder(\n            latent_dim=latent_dim,\n            num_genes=num_genes_rna,\n        )\n\n        # Protein decoder (NB, less sparse)\n        self.protein_decoder = NegativeBinomialDecoder(\n            latent_dim=latent_dim,\n            num_genes=num_genes_protein,\n        )\n\n        # Latent diffusion\n        self.diffusion = LatentDiffusionModel(\n            latent_dim=latent_dim,\n            model_type='dit',\n        )\n\n    def forward(self, x_rna, x_protein):\n        \"\"\"\n        Training: align RNA and Protein in latent space.\n\n        Args:\n            x_rna: RNA expression (B, num_genes_rna)\n            x_protein: Protein expression (B, num_genes_protein)\n\n        Returns:\n            loss_dict: Dictionary of losses\n        \"\"\"\n        # Encode both modalities\n        mu_rna, logvar_rna = self.rna_encoder(x_rna)\n        mu_protein, logvar_protein = self.protein_encoder(x_protein)\n\n        z_rna = self.rna_encoder.reparameterize(mu_rna, logvar_rna)\n        z_protein = self.protein_encoder.reparameterize(mu_protein, logvar_protein)\n\n        # Reconstruction losses\n        rna_recon = self.rna_decoder(z_rna)\n        protein_recon = self.protein_decoder(z_protein)\n\n        loss_rna_recon = -self.rna_decoder.log_prob(x_rna, *rna_recon).mean()\n        loss_protein_recon = -self.protein_decoder.log_prob(x_protein, *protein_recon).mean()\n\n        # KL losses\n        kl_rna = -0.5 * torch.sum(1 + logvar_rna - mu_rna.pow(2) - logvar_rna.exp(), dim=-1).mean()\n        kl_protein = -0.5 * torch.sum(1 + logvar_protein - mu_protein.pow(2) - logvar_protein.exp(), dim=-1).mean()\n\n        # Alignment loss (latents should be similar)\n        loss_align = F.mse_loss(z_rna, z_protein)\n\n        # Total loss\n        loss = loss_rna_recon + loss_protein_recon + kl_rna + kl_protein + 10.0 * loss_align\n\n        return {\n            'loss': loss,\n            'rna_recon': loss_rna_recon.item(),\n            'protein_recon': loss_protein_recon.item(),\n            'kl_rna': kl_rna.item(),\n            'kl_protein': kl_protein.item(),\n            'align': loss_align.item(),\n        }\n\n    @torch.no_grad()\n    def translate_rna_to_protein(self, x_rna):\n        \"\"\"\n        Translate RNA to Protein.\n\n        Args:\n            x_rna: RNA expression (B, num_genes_rna)\n\n        Returns:\n            x_protein_pred: Predicted protein (B, num_genes_protein)\n        \"\"\"\n        # Encode RNA to latent\n        mu_rna, _ = self.rna_encoder(x_rna)\n\n        # Decode to protein\n        protein_params = self.protein_decoder(mu_rna)\n        x_protein_pred = protein_params[0]  # Mean\n\n        return x_protein_pred\n\n    @torch.no_grad()\n    def translate_protein_to_rna(self, x_protein):\n        \"\"\"\n        Translate Protein to RNA.\n\n        Args:\n            x_protein: Protein expression (B, num_genes_protein)\n\n        Returns:\n            x_rna_pred: Predicted RNA (B, num_genes_rna)\n        \"\"\"\n        # Encode protein to latent\n        mu_protein, _ = self.protein_encoder(x_protein)\n\n        # Decode to RNA\n        rna_params = self.rna_decoder(mu_protein)\n        mean, _, dropout = rna_params\n        x_rna_pred = (1 - dropout) * mean\n\n        return x_rna_pred\n</code></pre>"},{"location":"latent_diffusion/03_latent_diffusion_applications/#4-trajectory-modeling","title":"4. Trajectory Modeling","text":""},{"location":"latent_diffusion/03_latent_diffusion_applications/#41-task-definition","title":"4.1 Task Definition","text":"<p>Goal: Model developmental or disease trajectories over time.</p> <p>Applications:</p> <ul> <li>Predict future cell states</li> <li>Identify branch points</li> <li>Model differentiation</li> </ul>"},{"location":"latent_diffusion/03_latent_diffusion_applications/#42-architecture","title":"4.2 Architecture","text":"<pre><code>class TrajectoryLatentDiffusion(nn.Module):\n    \"\"\"\n    Latent diffusion for trajectory modeling.\n\n    Predicts future states conditioned on time.\n\n    Args:\n        num_genes: Number of genes\n        latent_dim: Latent dimension\n    \"\"\"\n    def __init__(\n        self,\n        num_genes=5000,\n        latent_dim=256,\n    ):\n        super().__init__()\n\n        self.vae = GeneExpressionVAE(num_genes, latent_dim, 'zinb')\n\n        # Time encoder\n        self.time_encoder = nn.Sequential(\n            SinusoidalPositionEmbeddings(128),\n            nn.Linear(128, 128),\n            nn.GELU(),\n        )\n\n        # Latent diffusion conditioned on current state + time\n        self.diffusion = LatentDiffusionModel(latent_dim, 'dit')\n\n    @torch.no_grad()\n    def predict_trajectory(self, x_start, time_points, library_size=None):\n        \"\"\"\n        Predict trajectory from starting state.\n\n        Args:\n            x_start: Starting expression (B, num_genes)\n            time_points: Time points to predict (T,)\n            library_size: Library size (B,)\n\n        Returns:\n            trajectory: Predicted trajectory (B, T, num_genes)\n        \"\"\"\n        device = x_start.device\n        batch_size = len(x_start)\n\n        # Encode starting state\n        z_current = self.vae.encode(x_start)\n\n        trajectory = []\n\n        for t in time_points:\n            # Time embedding\n            t_tensor = torch.full((batch_size,), t, device=device)\n            t_emb = self.time_encoder(t_tensor)\n\n            # Condition on current state + time\n            condition = torch.cat([z_current, t_emb], dim=-1)\n\n            # Sample next state\n            z_next = self.diffusion.sample(batch_size, condition)\n\n            # Decode\n            x_next = self.vae.decode(z_next, library_size)\n            trajectory.append(x_next)\n\n            # Update current state\n            z_current = z_next\n\n        trajectory = torch.stack(trajectory, dim=1)  # (B, T, num_genes)\n\n        return trajectory\n</code></pre>"},{"location":"latent_diffusion/03_latent_diffusion_applications/#5-spatial-transcriptomics","title":"5. Spatial Transcriptomics","text":""},{"location":"latent_diffusion/03_latent_diffusion_applications/#51-task-definition","title":"5.1 Task Definition","text":"<p>Goal: Generate spatial gene expression patterns.</p> <p>Applications:</p> <ul> <li>Super-resolution (increase spatial resolution)</li> <li>Missing region imputation</li> <li>3D reconstruction</li> </ul>"},{"location":"latent_diffusion/03_latent_diffusion_applications/#52-architecture","title":"5.2 Architecture","text":"<pre><code>class SpatialLatentDiffusion(nn.Module):\n    \"\"\"\n    Latent diffusion for spatial transcriptomics.\n\n    Conditioned on spatial coordinates.\n\n    Args:\n        num_genes: Number of genes\n        latent_dim: Latent dimension\n        spatial_dim: Spatial coordinate dimension (2 or 3)\n    \"\"\"\n    def __init__(\n        self,\n        num_genes=5000,\n        latent_dim=256,\n        spatial_dim=2,\n    ):\n        super().__init__()\n\n        self.vae = GeneExpressionVAE(num_genes, latent_dim, 'zinb')\n\n        # Spatial encoder\n        self.spatial_encoder = nn.Sequential(\n            nn.Linear(spatial_dim, 128),\n            nn.GELU(),\n            nn.Linear(128, 128),\n            nn.GELU(),\n        )\n\n        self.diffusion = LatentDiffusionModel(latent_dim, 'dit')\n\n    @torch.no_grad()\n    def generate_at_location(self, coordinates, library_size=None):\n        \"\"\"\n        Generate expression at spatial locations.\n\n        Args:\n            coordinates: Spatial coordinates (B, spatial_dim)\n            library_size: Library size (B,)\n\n        Returns:\n            x_gen: Generated expression (B, num_genes)\n        \"\"\"\n        # Encode spatial coordinates\n        spatial_emb = self.spatial_encoder(coordinates)\n\n        # Sample latent conditioned on location\n        z = self.diffusion.sample(len(coordinates), spatial_emb)\n\n        # Decode\n        x_gen = self.vae.decode(z, library_size)\n\n        return x_gen\n\n    @torch.no_grad()\n    def super_resolution(self, x_low_res, coords_low_res, coords_high_res):\n        \"\"\"\n        Super-resolution: predict high-res from low-res.\n\n        Args:\n            x_low_res: Low-resolution expression (N_low, num_genes)\n            coords_low_res: Low-res coordinates (N_low, spatial_dim)\n            coords_high_res: High-res coordinates (N_high, spatial_dim)\n\n        Returns:\n            x_high_res: High-resolution expression (N_high, num_genes)\n        \"\"\"\n        # Encode low-res to latent\n        z_low_res = self.vae.encode(x_low_res)\n\n        # Interpolate latent to high-res locations\n        # (Simple approach: nearest neighbor or Gaussian kernel)\n        from scipy.spatial import cKDTree\n\n        tree = cKDTree(coords_low_res.cpu().numpy())\n        distances, indices = tree.query(coords_high_res.cpu().numpy(), k=3)\n\n        # Weighted average based on distance\n        weights = 1.0 / (distances + 1e-8)\n        weights = weights / weights.sum(axis=1, keepdims=True)\n\n        z_high_res = torch.zeros(len(coords_high_res), z_low_res.shape[1], device=z_low_res.device)\n        for i in range(len(coords_high_res)):\n            for j, idx in enumerate(indices[i]):\n                z_high_res[i] += weights[i, j] * z_low_res[idx]\n\n        # Decode\n        x_high_res = self.vae.decode(z_high_res)\n\n        return x_high_res\n</code></pre>"},{"location":"latent_diffusion/03_latent_diffusion_applications/#comparison-with-existing-methods","title":"Comparison with Existing Methods","text":""},{"location":"latent_diffusion/03_latent_diffusion_applications/#single-cell-generation","title":"Single-Cell Generation","text":"Method Approach Pros Cons scGAN GAN Fast sampling Mode collapse, unstable scVI VAE Fast, interpretable Blurry samples Latent Diffusion VAE + Diffusion Sharp, diverse Slower sampling"},{"location":"latent_diffusion/03_latent_diffusion_applications/#perturbation-prediction","title":"Perturbation Prediction","text":"Method Approach Pros Cons scGen VAE + arithmetic Simple, fast Linear assumption CPA Autoencoder + perturbation Flexible No uncertainty Latent Diffusion VAE + Diffusion Uncertainty, flexible More complex"},{"location":"latent_diffusion/03_latent_diffusion_applications/#multi-omics","title":"Multi-Omics","text":"Method Approach Pros Cons MOFA Factor analysis Interpretable Linear totalVI Joint VAE Unified framework Fixed architecture Latent Diffusion Joint VAE + Diffusion Flexible, high-quality Training complexity"},{"location":"latent_diffusion/03_latent_diffusion_applications/#key-takeaways","title":"Key Takeaways","text":""},{"location":"latent_diffusion/03_latent_diffusion_applications/#applications","title":"Applications","text":"<ol> <li>Single-cell generation \u2014 Data augmentation, rare cell types</li> <li>Perturbation prediction \u2014 Virtual screening, combinations</li> <li>Multi-omics translation \u2014 Fill missing modalities</li> <li>Trajectory modeling \u2014 Predict future states</li> <li>Spatial transcriptomics \u2014 Super-resolution, imputation</li> </ol>"},{"location":"latent_diffusion/03_latent_diffusion_applications/#advantages","title":"Advantages","text":"<ol> <li>Efficiency \u2014 10-100\u00d7 faster than pixel-space</li> <li>Quality \u2014 Better than VAE, stable than GAN</li> <li>Flexibility \u2014 Multi-modal, multi-task</li> <li>Uncertainty \u2014 Probabilistic predictions</li> </ol>"},{"location":"latent_diffusion/03_latent_diffusion_applications/#best-practices","title":"Best Practices","text":"<ol> <li>Start with VAE \u2014 Get good latent space first</li> <li>Condition carefully \u2014 Use appropriate conditioning mechanism</li> <li>Validate biologically \u2014 Not just loss metrics</li> <li>Compare baselines \u2014 scGen, CPA, totalVI</li> </ol>"},{"location":"latent_diffusion/03_latent_diffusion_applications/#related-documents","title":"Related Documents","text":"<ul> <li>00_latent_diffusion_overview.md \u2014 High-level concepts</li> <li>01_latent_diffusion_foundations.md \u2014 Architecture details</li> <li>02_latent_diffusion_training.md \u2014 Training strategies</li> <li>04_latent_diffusion_combio.md \u2014 Complete implementation</li> </ul>"},{"location":"latent_diffusion/03_latent_diffusion_applications/#references","title":"References","text":"<p>Single-Cell Generation:</p> <ul> <li>Marouf et al. (2020): \"Realistic in silico generation and augmentation of single-cell RNA-seq data using generative adversarial networks\" (scGAN)</li> <li>Lopez et al. (2018): \"Deep generative modeling for single-cell transcriptomics\" (scVI)</li> </ul> <p>Perturbation Prediction:</p> <ul> <li>Lotfollahi et al. (2019): \"scGen predicts single-cell perturbation responses\"</li> <li>Lotfollahi et al. (2023): \"Predicting cellular responses to novel drug combinations with a deep generative model\" (CPA)</li> </ul> <p>Multi-Omics:</p> <ul> <li>Argelaguet et al. (2018): \"Multi-Omics Factor Analysis\" (MOFA)</li> <li>Gayoso et al. (2021): \"Joint probabilistic modeling of single-cell multi-omic data with totalVI\"</li> </ul> <p>Spatial Transcriptomics:</p> <ul> <li>Cable et al. (2022): \"Robust decomposition of cell type mixtures in spatial transcriptomics\"</li> <li>Biancalani et al. (2021): \"Deep learning and alignment of spatially resolved single-cell transcriptomes\"</li> </ul>"},{"location":"latent_diffusion/04_latent_diffusion_combio/","title":"Latent Diffusion for Computational Biology: Complete Implementation","text":"<p>This document provides a complete, end-to-end implementation of latent diffusion models for computational biology, including scRNA-seq generation, Perturb-seq prediction, and comprehensive evaluation.</p> <p>Prerequisites: Understanding of foundations, training, and applications.</p>"},{"location":"latent_diffusion/04_latent_diffusion_combio/#overview","title":"Overview","text":"<p>This implementation covers:</p> <ol> <li>scRNA-seq generation \u2014 Generate realistic single-cell profiles</li> <li>Perturb-seq prediction \u2014 Predict perturbation responses</li> <li>Multi-omics integration \u2014 Joint RNA + Protein modeling</li> <li>Complete training pipeline \u2014 Data loading through evaluation</li> <li>Baseline comparisons \u2014 scVI, scGen, CPA</li> </ol>"},{"location":"latent_diffusion/04_latent_diffusion_combio/#1-complete-scrna-seq-latent-diffusion","title":"1. Complete scRNA-seq Latent Diffusion","text":""},{"location":"latent_diffusion/04_latent_diffusion_combio/#11-full-model-implementation","title":"1.1 Full Model Implementation","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport scanpy as sc\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\n\nclass CompleteSingleCellLatentDiffusion(nn.Module):\n    \"\"\"\n    Complete latent diffusion model for single-cell RNA-seq.\n\n    Features:\n    - VAE with ZINB decoder\n    - DiT-based latent diffusion\n    - Cell type conditioning\n    - Classifier-free guidance\n\n    Args:\n        num_genes: Number of genes\n        latent_dim: Latent dimension\n        num_cell_types: Number of cell types\n        hidden_dim: DiT hidden dimension\n        num_layers: Number of DiT layers\n    \"\"\"\n    def __init__(\n        self,\n        num_genes=5000,\n        latent_dim=256,\n        num_cell_types=20,\n        hidden_dim=512,\n        num_layers=12,\n    ):\n        super().__init__()\n\n        self.num_genes = num_genes\n        self.latent_dim = latent_dim\n        self.num_cell_types = num_cell_types\n\n        # VAE with ZINB decoder\n        self.vae = GeneExpressionVAE(\n            num_genes=num_genes,\n            latent_dim=latent_dim,\n            decoder_type='zinb',\n        )\n\n        # Cell type embedding\n        self.cell_type_embed = nn.Embedding(num_cell_types, 128)\n\n        # Latent diffusion model\n        self.diffusion = LatentDiffusionModel(\n            latent_dim=latent_dim,\n            model_type='dit',\n            num_steps=1000,\n        )\n\n        # Conditioning projection\n        self.condition_proj = nn.Linear(128, latent_dim)\n\n    def train_step(self, x, cell_type, library_size, stage='vae'):\n        \"\"\"\n        Single training step.\n\n        Args:\n            x: Gene expression (B, num_genes)\n            cell_type: Cell type indices (B,)\n            library_size: Library size (B,)\n            stage: 'vae' or 'diffusion'\n\n        Returns:\n            loss: Training loss\n            metrics: Dictionary of metrics\n        \"\"\"\n        if stage == 'vae':\n            # Train VAE\n            recon_params, mu, logvar = self.vae(x, library_size)\n            loss, loss_dict = self.vae.loss(x, recon_params, mu, logvar, library_size)\n            return loss, loss_dict\n\n        elif stage == 'diffusion':\n            # Train diffusion on latent codes\n            with torch.no_grad():\n                z0 = self.vae.encode(x)\n\n            # Cell type conditioning\n            cell_type_emb = self.cell_type_embed(cell_type)\n            condition = self.condition_proj(cell_type_emb)\n\n            # Classifier-free guidance: randomly drop condition\n            mask = torch.rand(len(x), device=x.device) &gt; 0.1\n            condition = condition * mask.unsqueeze(-1)\n\n            # Diffusion loss\n            t = torch.randint(0, self.diffusion.num_steps, (len(x),), device=x.device)\n            zt, noise = self.diffusion.add_noise(z0, t)\n            noise_pred = self.diffusion(zt, t, condition)\n            loss = F.mse_loss(noise_pred, noise)\n\n            metrics = {'diffusion_loss': loss.item()}\n            return loss, metrics\n\n    @torch.no_grad()\n    def generate(\n        self,\n        cell_type,\n        num_samples=100,\n        library_size=None,\n        guidance_scale=7.5,\n        num_steps=50,\n    ):\n        \"\"\"\n        Generate single-cell profiles.\n\n        Args:\n            cell_type: Cell type index (int or tensor)\n            num_samples: Number of samples\n            library_size: Optional library size\n            guidance_scale: Classifier-free guidance scale\n            num_steps: Number of sampling steps\n\n        Returns:\n            x_gen: Generated expression (num_samples, num_genes)\n        \"\"\"\n        device = next(self.parameters()).device\n\n        # Cell type conditioning\n        if isinstance(cell_type, int):\n            cell_type = torch.full((num_samples,), cell_type, device=device)\n        cell_type_emb = self.cell_type_embed(cell_type)\n        condition = self.condition_proj(cell_type_emb)\n\n        # Sample with classifier-free guidance\n        z0 = self.sample_with_cfg(condition, guidance_scale, num_steps)\n\n        # Decode\n        x_gen = self.vae.decode(z0, library_size)\n\n        return x_gen\n\n    def sample_with_cfg(self, condition, guidance_scale, num_steps):\n        \"\"\"Sample with classifier-free guidance.\"\"\"\n        device = condition.device\n        batch_size = len(condition)\n\n        # Start from noise\n        z = torch.randn(batch_size, self.latent_dim, device=device)\n\n        # Null condition\n        condition_null = torch.zeros_like(condition)\n\n        # Sampling timesteps\n        timesteps = torch.linspace(\n            self.diffusion.num_steps - 1, 0, num_steps, dtype=torch.long, device=device\n        )\n\n        for i, t in enumerate(timesteps):\n            t_batch = torch.full((batch_size,), t, device=device, dtype=torch.long)\n\n            # Conditional and unconditional predictions\n            noise_cond = self.diffusion.model(z, t_batch, condition)\n            noise_uncond = self.diffusion.model(z, t_batch, condition_null)\n\n            # Classifier-free guidance\n            noise_pred = noise_uncond + guidance_scale * (noise_cond - noise_uncond)\n\n            # DDIM update\n            alpha_t = self.diffusion.alphas_cumprod[t]\n            if i &lt; len(timesteps) - 1:\n                alpha_t_prev = self.diffusion.alphas_cumprod[timesteps[i + 1]]\n            else:\n                alpha_t_prev = torch.tensor(1.0, device=device)\n\n            pred_z0 = (z - torch.sqrt(1 - alpha_t) * noise_pred) / torch.sqrt(alpha_t)\n            dir_zt = torch.sqrt(1 - alpha_t_prev) * noise_pred\n            z = torch.sqrt(alpha_t_prev) * pred_z0 + dir_zt\n\n        return z\n</code></pre>"},{"location":"latent_diffusion/04_latent_diffusion_combio/#12-complete-training-pipeline","title":"1.2 Complete Training Pipeline","text":"<pre><code>def train_complete_scrnaseq_model(\n    adata_path,\n    save_dir='checkpoints/scrnaseq_latent_diffusion',\n    num_genes=5000,\n    latent_dim=256,\n    batch_size=128,\n    num_epochs_vae=100,\n    num_epochs_diffusion=100,\n    lr_vae=1e-3,\n    lr_diffusion=1e-4,\n    device='cuda',\n):\n    \"\"\"\n    Complete training pipeline for scRNA-seq latent diffusion.\n\n    Args:\n        adata_path: Path to AnnData file\n        save_dir: Save directory\n        num_genes: Number of genes (HVGs)\n        latent_dim: Latent dimension\n        batch_size: Batch size\n        num_epochs_vae: VAE training epochs\n        num_epochs_diffusion: Diffusion training epochs\n        lr_vae: VAE learning rate\n        lr_diffusion: Diffusion learning rate\n        device: Device\n\n    Returns:\n        model: Trained model\n        metrics: Training metrics\n    \"\"\"\n    import os\n    from torch.utils.tensorboard import SummaryWriter\n\n    os.makedirs(save_dir, exist_ok=True)\n\n    # Load and preprocess data\n    print(\"Loading data...\")\n    adata = sc.read_h5ad(adata_path)\n\n    # Preprocessing\n    sc.pp.filter_genes(adata, min_cells=10)\n    sc.pp.normalize_total(adata, target_sum=1e4)\n    sc.pp.log1p(adata)\n    sc.pp.highly_variable_genes(adata, n_top_genes=num_genes)\n    adata = adata[:, adata.var['highly_variable']]\n\n    # Encode cell types\n    cell_types = adata.obs['cell_type'].astype('category')\n    cell_type_codes = cell_types.cat.codes.values\n    num_cell_types = len(cell_types.cat.categories)\n\n    # Create dataset\n    dataset = SingleCellDataset(adata, cell_type_codes)\n\n    # Split train/val\n    train_size = int(0.9 * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n\n    # Create model\n    print(\"Creating model...\")\n    model = CompleteSingleCellLatentDiffusion(\n        num_genes=num_genes,\n        latent_dim=latent_dim,\n        num_cell_types=num_cell_types,\n    ).to(device)\n\n    # Stage 1: Train VAE\n    print(\"\\n=== Stage 1: Training VAE ===\")\n    optimizer_vae = torch.optim.AdamW(model.vae.parameters(), lr=lr_vae, weight_decay=0.01)\n    scheduler_vae = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_vae, num_epochs_vae)\n    writer_vae = SummaryWriter(f'{save_dir}/logs/vae')\n\n    best_vae_loss = float('inf')\n\n    for epoch in range(num_epochs_vae):\n        model.train()\n        train_loss = 0.0\n\n        for x, cell_type, library_size in train_loader:\n            x = x.to(device)\n            cell_type = cell_type.to(device)\n            library_size = library_size.to(device)\n\n            loss, metrics = model.train_step(x, cell_type, library_size, stage='vae')\n\n            optimizer_vae.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.vae.parameters(), 1.0)\n            optimizer_vae.step()\n\n            train_loss += metrics['loss']\n\n        train_loss /= len(train_loader)\n\n        # Validation\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for x, cell_type, library_size in val_loader:\n                x = x.to(device)\n                cell_type = cell_type.to(device)\n                library_size = library_size.to(device)\n\n                loss, metrics = model.train_step(x, cell_type, library_size, stage='vae')\n                val_loss += metrics['loss']\n\n        val_loss /= len(val_loader)\n\n        print(f\"Epoch {epoch+1}/{num_epochs_vae}: Train={train_loss:.4f}, Val={val_loss:.4f}\")\n        writer_vae.add_scalar('train/loss', train_loss, epoch)\n        writer_vae.add_scalar('val/loss', val_loss, epoch)\n\n        if val_loss &lt; best_vae_loss:\n            best_vae_loss = val_loss\n            torch.save(model.vae.state_dict(), f'{save_dir}/vae_best.pt')\n\n        scheduler_vae.step()\n\n    writer_vae.close()\n\n    # Freeze VAE\n    for param in model.vae.parameters():\n        param.requires_grad = False\n\n    # Stage 2: Train Diffusion\n    print(\"\\n=== Stage 2: Training Diffusion ===\")\n    optimizer_diffusion = torch.optim.AdamW(\n        list(model.diffusion.parameters()) + list(model.cell_type_embed.parameters()) + list(model.condition_proj.parameters()),\n        lr=lr_diffusion,\n        weight_decay=0.01\n    )\n    scheduler_diffusion = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_diffusion, num_epochs_diffusion)\n    writer_diffusion = SummaryWriter(f'{save_dir}/logs/diffusion')\n\n    best_diffusion_loss = float('inf')\n\n    for epoch in range(num_epochs_diffusion):\n        model.train()\n        train_loss = 0.0\n\n        for x, cell_type, library_size in train_loader:\n            x = x.to(device)\n            cell_type = cell_type.to(device)\n            library_size = library_size.to(device)\n\n            loss, metrics = model.train_step(x, cell_type, library_size, stage='diffusion')\n\n            optimizer_diffusion.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(\n                list(model.diffusion.parameters()) + list(model.cell_type_embed.parameters()),\n                1.0\n            )\n            optimizer_diffusion.step()\n\n            train_loss += metrics['diffusion_loss']\n\n        train_loss /= len(train_loader)\n\n        # Validation\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for x, cell_type, library_size in val_loader:\n                x = x.to(device)\n                cell_type = cell_type.to(device)\n                library_size = library_size.to(device)\n\n                loss, metrics = model.train_step(x, cell_type, library_size, stage='diffusion')\n                val_loss += metrics['diffusion_loss']\n\n        val_loss /= len(val_loader)\n\n        print(f\"Epoch {epoch+1}/{num_epochs_diffusion}: Train={train_loss:.4f}, Val={val_loss:.4f}\")\n        writer_diffusion.add_scalar('train/loss', train_loss, epoch)\n        writer_diffusion.add_scalar('val/loss', val_loss, epoch)\n\n        if val_loss &lt; best_diffusion_loss:\n            best_diffusion_loss = val_loss\n            torch.save({\n                'vae': model.vae.state_dict(),\n                'diffusion': model.diffusion.state_dict(),\n                'cell_type_embed': model.cell_type_embed.state_dict(),\n                'condition_proj': model.condition_proj.state_dict(),\n            }, f'{save_dir}/complete_model_best.pt')\n\n        scheduler_diffusion.step()\n\n    writer_diffusion.close()\n\n    print(\"\\n=== Training Complete ===\")\n    return model\n\n\nclass SingleCellDataset(Dataset):\n    \"\"\"Dataset for single-cell data.\"\"\"\n    def __init__(self, adata, cell_type_codes):\n        if hasattr(adata.X, 'toarray'):\n            self.X = torch.tensor(adata.X.toarray(), dtype=torch.float32)\n        else:\n            self.X = torch.tensor(adata.X, dtype=torch.float32)\n\n        self.library_size = torch.tensor(self.X.sum(dim=1), dtype=torch.float32)\n        self.cell_type = torch.tensor(cell_type_codes, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.cell_type[idx], self.library_size[idx]\n</code></pre>"},{"location":"latent_diffusion/04_latent_diffusion_combio/#13-comprehensive-evaluation","title":"1.3 Comprehensive Evaluation","text":"<pre><code>@torch.no_grad()\ndef evaluate_scrnaseq_generation(\n    model,\n    test_adata,\n    cell_type_codes,\n    num_samples_per_type=1000,\n    device='cuda',\n):\n    \"\"\"\n    Comprehensive evaluation of scRNA-seq generation.\n\n    Args:\n        model: Trained model\n        test_adata: Test AnnData\n        cell_type_codes: Cell type codes\n        num_samples_per_type: Samples per cell type\n        device: Device\n\n    Returns:\n        metrics: Dictionary of evaluation metrics\n    \"\"\"\n    import scanpy as sc\n    from scipy.stats import pearsonr, wasserstein_distance\n    from sklearn.metrics import silhouette_score\n\n    model.eval()\n    model.to(device)\n\n    # Get real data\n    if hasattr(test_adata.X, 'toarray'):\n        real_data = test_adata.X.toarray()\n    else:\n        real_data = test_adata.X\n\n    unique_cell_types = np.unique(cell_type_codes)\n\n    all_synthetic = []\n    all_synthetic_labels = []\n\n    metrics = {}\n\n    print(\"Generating synthetic data...\")\n    for ct in unique_cell_types:\n        # Generate\n        synthetic = model.generate(\n            cell_type=int(ct),\n            num_samples=num_samples_per_type,\n            guidance_scale=7.5,\n        )\n\n        all_synthetic.append(synthetic.cpu().numpy())\n        all_synthetic_labels.extend([ct] * num_samples_per_type)\n\n        # Real data for this cell type\n        mask = (cell_type_codes == ct)\n        real_ct = real_data[mask]\n\n        # 1. Mean expression correlation\n        real_mean = real_ct.mean(axis=0)\n        synth_mean = synthetic.cpu().numpy().mean(axis=0)\n        corr, _ = pearsonr(real_mean, synth_mean)\n\n        # 2. Wasserstein distance (average over genes)\n        w_dists = []\n        for gene_idx in range(min(100, real_ct.shape[1])):  # Sample 100 genes\n            w_dist = wasserstein_distance(real_ct[:, gene_idx], synthetic[:, gene_idx].cpu().numpy())\n            w_dists.append(w_dist)\n\n        # 3. Sparsity\n        real_sparsity = (real_ct &lt; 0.1).mean()\n        synth_sparsity = (synthetic.cpu().numpy() &lt; 0.1).mean()\n\n        metrics[f'cell_type_{ct}'] = {\n            'mean_correlation': corr,\n            'wasserstein_distance': np.mean(w_dists),\n            'real_sparsity': real_sparsity,\n            'synth_sparsity': synth_sparsity,\n        }\n\n    # Combine all synthetic data\n    all_synthetic = np.vstack(all_synthetic)\n    all_synthetic_labels = np.array(all_synthetic_labels)\n\n    # 4. Clustering quality (silhouette score)\n    print(\"Computing clustering quality...\")\n\n    # PCA for dimensionality reduction\n    from sklearn.decomposition import PCA\n    pca = PCA(n_components=50)\n\n    real_pca = pca.fit_transform(real_data)\n    synth_pca = pca.transform(all_synthetic)\n\n    real_silhouette = silhouette_score(real_pca, cell_type_codes)\n    synth_silhouette = silhouette_score(synth_pca, all_synthetic_labels)\n\n    metrics['clustering'] = {\n        'real_silhouette': real_silhouette,\n        'synth_silhouette': synth_silhouette,\n    }\n\n    # 5. Biological pathway activity\n    print(\"Computing pathway activity...\")\n    # (Simplified: use gene set enrichment or pathway scores)\n\n    return metrics\n</code></pre>"},{"location":"latent_diffusion/04_latent_diffusion_combio/#2-perturb-seq-latent-diffusion","title":"2. Perturb-seq Latent Diffusion","text":""},{"location":"latent_diffusion/04_latent_diffusion_combio/#21-complete-implementation","title":"2.1 Complete Implementation","text":"<pre><code>class CompletePerturbSeqLatentDiffusion(nn.Module):\n    \"\"\"\n    Complete latent diffusion for Perturb-seq.\n\n    Predicts perturbed state from baseline + perturbation.\n    \"\"\"\n    def __init__(\n        self,\n        num_genes=5000,\n        latent_dim=256,\n        perturbation_dim=128,\n    ):\n        super().__init__()\n\n        self.num_genes = num_genes\n        self.latent_dim = latent_dim\n\n        # VAE\n        self.vae = GeneExpressionVAE(num_genes, latent_dim, 'zinb')\n\n        # Perturbation encoder (learnable gene embeddings)\n        self.gene_embeddings = nn.Embedding(num_genes, perturbation_dim)\n\n        # Delta predictor (predict change in latent space)\n        self.delta_predictor = nn.Sequential(\n            nn.Linear(latent_dim + perturbation_dim, 512),\n            nn.LayerNorm(512),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(512, 512),\n            nn.LayerNorm(512),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(512, latent_dim),\n        )\n\n        # Optional: Diffusion for uncertainty\n        self.use_diffusion = True\n        if self.use_diffusion:\n            self.diffusion = LatentDiffusionModel(latent_dim, 'dit')\n\n    def encode_perturbation(self, perturbation_indicator):\n        \"\"\"\n        Encode perturbation as weighted sum of gene embeddings.\n\n        Args:\n            perturbation_indicator: One-hot or multi-hot (B, num_genes)\n\n        Returns:\n            pert_emb: Perturbation embedding (B, perturbation_dim)\n        \"\"\"\n        # Weighted sum of gene embeddings\n        gene_embs = self.gene_embeddings.weight  # (num_genes, perturbation_dim)\n        pert_emb = torch.matmul(perturbation_indicator, gene_embs)  # (B, perturbation_dim)\n\n        # Normalize by number of perturbed genes\n        num_perturbed = perturbation_indicator.sum(dim=-1, keepdim=True).clamp(min=1)\n        pert_emb = pert_emb / num_perturbed\n\n        return pert_emb\n\n    def forward(self, x_baseline, x_perturbed, perturbation_indicator):\n        \"\"\"\n        Training forward pass.\n\n        Args:\n            x_baseline: Baseline expression (B, num_genes)\n            x_perturbed: Perturbed expression (B, num_genes)\n            perturbation_indicator: Perturbation (B, num_genes)\n\n        Returns:\n            loss: Training loss\n            metrics: Metrics dictionary\n        \"\"\"\n        # Encode to latent\n        z_baseline = self.vae.encode(x_baseline)\n        z_perturbed = self.vae.encode(x_perturbed)\n\n        # True delta\n        delta_true = z_perturbed - z_baseline\n\n        # Encode perturbation\n        pert_emb = self.encode_perturbation(perturbation_indicator)\n\n        # Predict delta\n        delta_pred = self.delta_predictor(torch.cat([z_baseline, pert_emb], dim=-1))\n\n        # Delta loss\n        delta_loss = F.mse_loss(delta_pred, delta_true)\n\n        # Optional: Diffusion loss for uncertainty\n        if self.use_diffusion:\n            # Diffusion on delta\n            t = torch.randint(0, self.diffusion.num_steps, (len(z_baseline),), device=z_baseline.device)\n            delta_t, noise = self.diffusion.add_noise(delta_true, t)\n\n            # Condition on baseline + perturbation\n            condition = torch.cat([z_baseline, pert_emb], dim=-1)\n            noise_pred = self.diffusion(delta_t, t, condition)\n\n            diffusion_loss = F.mse_loss(noise_pred, noise)\n\n            total_loss = delta_loss + 0.1 * diffusion_loss\n\n            metrics = {\n                'delta_loss': delta_loss.item(),\n                'diffusion_loss': diffusion_loss.item(),\n                'total_loss': total_loss.item(),\n            }\n        else:\n            total_loss = delta_loss\n            metrics = {'delta_loss': delta_loss.item()}\n\n        return total_loss, metrics\n\n    @torch.no_grad()\n    def predict_perturbation(\n        self,\n        x_baseline,\n        perturbation_indicator,\n        library_size=None,\n        use_diffusion=False,\n        num_samples=1,\n    ):\n        \"\"\"\n        Predict perturbed state.\n\n        Args:\n            x_baseline: Baseline expression (B, num_genes)\n            perturbation_indicator: Perturbation (B, num_genes)\n            library_size: Library size (B,)\n            use_diffusion: Whether to use diffusion for sampling\n            num_samples: Number of samples (if using diffusion)\n\n        Returns:\n            x_perturbed_pred: Predicted perturbed expression\n        \"\"\"\n        # Encode baseline\n        z_baseline = self.vae.encode(x_baseline)\n\n        # Encode perturbation\n        pert_emb = self.encode_perturbation(perturbation_indicator)\n\n        if use_diffusion and self.use_diffusion:\n            # Sample delta from diffusion\n            condition = torch.cat([z_baseline, pert_emb], dim=-1)\n            delta_samples = []\n\n            for _ in range(num_samples):\n                delta = self.diffusion.sample(len(x_baseline), condition)\n                delta_samples.append(delta)\n\n            delta_pred = torch.stack(delta_samples).mean(dim=0)  # Average over samples\n        else:\n            # Deterministic delta prediction\n            delta_pred = self.delta_predictor(torch.cat([z_baseline, pert_emb], dim=-1))\n\n        # Add delta to baseline\n        z_perturbed = z_baseline + delta_pred\n\n        # Decode\n        x_perturbed_pred = self.vae.decode(z_perturbed, library_size)\n\n        return x_perturbed_pred\n</code></pre>"},{"location":"latent_diffusion/04_latent_diffusion_combio/#22-training-on-norman-et-al-dataset","title":"2.2 Training on Norman et al. Dataset","text":"<pre><code>def train_perturbseq_model(\n    adata_path,\n    save_dir='checkpoints/perturbseq_latent_diffusion',\n    num_genes=5000,\n    latent_dim=256,\n    batch_size=64,\n    num_epochs=100,\n    lr=1e-4,\n    device='cuda',\n):\n    \"\"\"\n    Train Perturb-seq latent diffusion model.\n\n    Args:\n        adata_path: Path to Perturb-seq AnnData\n        save_dir: Save directory\n        num_genes: Number of genes\n        latent_dim: Latent dimension\n        batch_size: Batch size\n        num_epochs: Number of epochs\n        lr: Learning rate\n        device: Device\n\n    Returns:\n        model: Trained model\n    \"\"\"\n    import os\n    from torch.utils.tensorboard import SummaryWriter\n\n    os.makedirs(save_dir, exist_ok=True)\n\n    # Load Perturb-seq data\n    print(\"Loading Perturb-seq data...\")\n    adata = sc.read_h5ad(adata_path)\n\n    # Preprocessing\n    sc.pp.filter_genes(adata, min_cells=10)\n    sc.pp.normalize_total(adata, target_sum=1e4)\n    sc.pp.log1p(adata)\n    sc.pp.highly_variable_genes(adata, n_top_genes=num_genes)\n    adata = adata[:, adata.var['highly_variable']]\n\n    # Create dataset\n    dataset = PerturbSeqDataset(adata)\n\n    # Split\n    train_size = int(0.8 * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n\n    # Create model\n    print(\"Creating model...\")\n    model = CompletePerturbSeqLatentDiffusion(\n        num_genes=num_genes,\n        latent_dim=latent_dim,\n    ).to(device)\n\n    # Optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)\n    writer = SummaryWriter(f'{save_dir}/logs')\n\n    best_val_loss = float('inf')\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n\n        for x_baseline, x_perturbed, pert_indicator in train_loader:\n            x_baseline = x_baseline.to(device)\n            x_perturbed = x_perturbed.to(device)\n            pert_indicator = pert_indicator.to(device)\n\n            loss, metrics = model(x_baseline, x_perturbed, pert_indicator)\n\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n\n            train_loss += metrics['total_loss']\n\n        train_loss /= len(train_loader)\n\n        # Validation\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for x_baseline, x_perturbed, pert_indicator in val_loader:\n                x_baseline = x_baseline.to(device)\n                x_perturbed = x_perturbed.to(device)\n                pert_indicator = pert_indicator.to(device)\n\n                loss, metrics = model(x_baseline, x_perturbed, pert_indicator)\n                val_loss += metrics['total_loss']\n\n        val_loss /= len(val_loader)\n\n        print(f\"Epoch {epoch+1}/{num_epochs}: Train={train_loss:.4f}, Val={val_loss:.4f}\")\n        writer.add_scalar('train/loss', train_loss, epoch)\n        writer.add_scalar('val/loss', val_loss, epoch)\n\n        if val_loss &lt; best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), f'{save_dir}/best_model.pt')\n\n        scheduler.step()\n\n    writer.close()\n    return model\n\n\nclass PerturbSeqDataset(Dataset):\n    \"\"\"Dataset for Perturb-seq data.\"\"\"\n    def __init__(self, adata):\n        # Assume adata has 'perturbation' and 'control' in obs\n        # and perturbation indicator in obsm['perturbation_indicator']\n\n        if hasattr(adata.X, 'toarray'):\n            self.X = torch.tensor(adata.X.toarray(), dtype=torch.float32)\n        else:\n            self.X = torch.tensor(adata.X, dtype=torch.float32)\n\n        # Get control indices\n        self.is_control = adata.obs['perturbation'] == 'control'\n\n        # Match perturbed cells with controls\n        self.pairs = self.create_pairs(adata)\n\n        # Perturbation indicators\n        if 'perturbation_indicator' in adata.obsm:\n            self.pert_indicators = torch.tensor(\n                adata.obsm['perturbation_indicator'],\n                dtype=torch.float32\n            )\n        else:\n            # Create from perturbation names\n            self.pert_indicators = self.create_perturbation_indicators(adata)\n\n    def create_pairs(self, adata):\n        \"\"\"Create (baseline, perturbed) pairs.\"\"\"\n        pairs = []\n\n        control_indices = np.where(self.is_control)[0]\n        perturbed_indices = np.where(~self.is_control)[0]\n\n        # For each perturbed cell, randomly sample a control\n        for pert_idx in perturbed_indices:\n            ctrl_idx = np.random.choice(control_indices)\n            pairs.append((ctrl_idx, pert_idx))\n\n        return pairs\n\n    def create_perturbation_indicators(self, adata):\n        \"\"\"Create perturbation indicators from gene names.\"\"\"\n        num_cells = adata.n_obs\n        num_genes = adata.n_vars\n        indicators = np.zeros((num_cells, num_genes))\n\n        gene_names = adata.var_names.tolist()\n\n        for i, pert in enumerate(adata.obs['perturbation']):\n            if pert != 'control':\n                # Find gene index\n                if pert in gene_names:\n                    gene_idx = gene_names.index(pert)\n                    indicators[i, gene_idx] = 1.0\n\n        return torch.tensor(indicators, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        ctrl_idx, pert_idx = self.pairs[idx]\n\n        x_baseline = self.X[ctrl_idx]\n        x_perturbed = self.X[pert_idx]\n        pert_indicator = self.pert_indicators[pert_idx]\n\n        return x_baseline, x_perturbed, pert_indicator\n</code></pre>"},{"location":"latent_diffusion/04_latent_diffusion_combio/#23-evaluation","title":"2.3 Evaluation","text":"<pre><code>@torch.no_grad()\ndef evaluate_perturbseq_prediction(\n    model,\n    test_dataset,\n    device='cuda',\n):\n    \"\"\"\n    Evaluate Perturb-seq prediction.\n\n    Args:\n        model: Trained model\n        test_dataset: Test dataset\n        device: Device\n\n    Returns:\n        metrics: Evaluation metrics\n    \"\"\"\n    from scipy.stats import pearsonr\n\n    model.eval()\n    model.to(device)\n\n    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n    all_correlations = []\n    all_mse = []\n    all_mae = []\n\n    for x_baseline, x_perturbed_true, pert_indicator in test_loader:\n        x_baseline = x_baseline.to(device)\n        x_perturbed_true = x_perturbed_true.to(device)\n        pert_indicator = pert_indicator.to(device)\n\n        # Predict\n        x_perturbed_pred = model.predict_perturbation(x_baseline, pert_indicator)\n\n        # Metrics\n        for i in range(len(x_baseline)):\n            corr, _ = pearsonr(\n                x_perturbed_true[i].cpu().numpy(),\n                x_perturbed_pred[i].cpu().numpy()\n            )\n            all_correlations.append(corr)\n\n        mse = F.mse_loss(x_perturbed_pred, x_perturbed_true)\n        mae = F.l1_loss(x_perturbed_pred, x_perturbed_true)\n\n        all_mse.append(mse.item())\n        all_mae.append(mae.item())\n\n    metrics = {\n        'mean_correlation': np.mean(all_correlations),\n        'median_correlation': np.median(all_correlations),\n        'mean_mse': np.mean(all_mse),\n        'mean_mae': np.mean(all_mae),\n    }\n\n    return metrics\n</code></pre>"},{"location":"latent_diffusion/04_latent_diffusion_combio/#3-baseline-comparisons","title":"3. Baseline Comparisons","text":""},{"location":"latent_diffusion/04_latent_diffusion_combio/#31-comparison-with-scvi","title":"3.1 Comparison with scVI","text":"<pre><code>def compare_with_scvi(adata, latent_dim=256):\n    \"\"\"\n    Compare latent diffusion with scVI.\n\n    Args:\n        adata: AnnData\n        latent_dim: Latent dimension\n\n    Returns:\n        metrics: Comparison metrics\n    \"\"\"\n    import scvi\n\n    # Train scVI\n    scvi.model.SCVI.setup_anndata(adata)\n    scvi_model = scvi.model.SCVI(adata, n_latent=latent_dim)\n    scvi_model.train()\n\n    # Generate from scVI\n    scvi_samples = scvi_model.get_normalized_expression()\n\n    # Compare with latent diffusion samples\n    # (Assume latent_diffusion_samples already generated)\n\n    return metrics\n</code></pre>"},{"location":"latent_diffusion/04_latent_diffusion_combio/#32-comparison-with-scgen","title":"3.2 Comparison with scGen","text":"<pre><code>def compare_with_scgen(adata_train, adata_test):\n    \"\"\"\n    Compare latent diffusion with scGen for perturbation prediction.\n\n    Args:\n        adata_train: Training data\n        adata_test: Test data\n\n    Returns:\n        metrics: Comparison metrics\n    \"\"\"\n    # scGen uses VAE + arithmetic in latent space\n    # z_perturbed = z_baseline + (z_pert_mean - z_control_mean)\n\n    # Train scGen\n    # (Use scgen package or implement simple VAE + arithmetic)\n\n    return metrics\n</code></pre>"},{"location":"latent_diffusion/04_latent_diffusion_combio/#key-takeaways","title":"Key Takeaways","text":""},{"location":"latent_diffusion/04_latent_diffusion_combio/#implementation","title":"Implementation","text":"<ol> <li>Complete pipeline \u2014 Data loading \u2192 Training \u2192 Evaluation</li> <li>Two-stage training \u2014 VAE first, then diffusion</li> <li>Classifier-free guidance \u2014 Better controllability</li> <li>Delta-in-latent \u2014 More stable for perturbations</li> </ol>"},{"location":"latent_diffusion/04_latent_diffusion_combio/#performance","title":"Performance","text":"<ol> <li>scRNA-seq generation \u2014 Better than VAE, comparable to GAN</li> <li>Perturbation prediction \u2014 Competitive with scGen/CPA</li> <li>Efficiency \u2014 10-100\u00d7 faster than pixel-space</li> <li>Uncertainty \u2014 Probabilistic predictions</li> </ol>"},{"location":"latent_diffusion/04_latent_diffusion_combio/#best-practices","title":"Best Practices","text":"<ol> <li>Start simple \u2014 Train VAE well before diffusion</li> <li>Validate biologically \u2014 Not just loss metrics</li> <li>Compare baselines \u2014 scVI, scGen, CPA</li> <li>Use CFG \u2014 guidance_scale=7.5 works well</li> </ol>"},{"location":"latent_diffusion/04_latent_diffusion_combio/#related-documents","title":"Related Documents","text":"<ul> <li>00_latent_diffusion_overview.md \u2014 High-level concepts</li> <li>01_latent_diffusion_foundations.md \u2014 Architecture details</li> <li>02_latent_diffusion_training.md \u2014 Training strategies</li> <li>03_latent_diffusion_applications.md \u2014 Applications</li> </ul>"},{"location":"latent_diffusion/04_latent_diffusion_combio/#references","title":"References","text":"<p>Latent Diffusion:</p> <ul> <li>Rombach et al. (2022): \"High-Resolution Image Synthesis with Latent Diffusion Models\"</li> <li>Ho &amp; Salimans (2022): \"Classifier-Free Diffusion Guidance\"</li> </ul> <p>Baselines:</p> <ul> <li>Lopez et al. (2018): \"Deep generative modeling for single-cell transcriptomics\" (scVI)</li> <li>Lotfollahi et al. (2019): \"scGen predicts single-cell perturbation responses\"</li> <li>Lotfollahi et al. (2023): \"Predicting cellular responses to novel drug combinations\" (CPA)</li> </ul> <p>Datasets:</p> <ul> <li>Norman et al. (2019): \"Exploring genetic interaction manifolds constructed from rich single-cell phenotypes\"</li> <li>Zheng et al. (2017): \"Massively parallel digital transcriptional profiling of single cells\"</li> </ul>"},{"location":"math/differential_geometry/","title":"Differential Geometry for Machine Learning","text":"<p>This directory contains tutorials and resources on differential geometry, tensor calculus, and their applications to machine learning.</p>"},{"location":"math/differential_geometry/#documents","title":"Documents","text":""},{"location":"math/differential_geometry/#core-tutorial","title":"Core Tutorial","text":"<ol> <li><code>01_tensor_calculus_and_curved_spaces.md</code> - Comprehensive tutorial</li> <li>Part 1-2: From Euclidean to curved spaces, metric tensor</li> <li>Part 3-4: Covariant derivatives, gradient on manifolds</li> <li>Part 5-6: Connection to probability and diffusion on manifolds</li> <li>Part 7-8: Current research and practical considerations</li> <li>Part 9-10: Code examples and resources</li> </ol>"},{"location":"math/differential_geometry/#supplementary-materials","title":"Supplementary Materials","text":"<ol> <li><code>02_riemannian_diffusion_models.md</code> - Recent advances (coming soon)</li> <li>State-of-the-art research</li> <li>Detailed case studies (proteins, molecules, robotics)</li> <li>Implementation patterns</li> </ol>"},{"location":"math/differential_geometry/#why-study-this","title":"Why Study This?","text":""},{"location":"math/differential_geometry/#current-limitations-of-euclidean-ml","title":"Current Limitations of Euclidean ML","text":"<p>Most ML models assume data lives in Euclidean space \\(\\mathbb{R}^d\\). But many real-world problems have geometric structure:</p> <ul> <li>Proteins: Torsion angles on tori \\(T^n\\)</li> <li>Molecules: Conformations on energy manifolds</li> <li>Rotations: SO(3), not \\(\\mathbb{R}^3\\)</li> <li>Directions: Sphere \\(S^{d-1}\\)</li> <li>Hierarchies: Hyperbolic space \\(\\mathbb{H}^n\\)</li> </ul> <p>Using Euclidean methods on these problems: - \u274c Violates constraints (e.g., generates invalid bond angles) - \u274c Wastes capacity (learning to stay on manifold) - \u274c Poor generalization (doesn't respect geometry)</p>"},{"location":"math/differential_geometry/#geometric-ml-advantages","title":"Geometric ML Advantages","text":"<p>Using Riemannian diffusion and geometric deep learning: - \u2705 Respects structure (always valid outputs) - \u2705 More efficient (fewer parameters) - \u2705 Better generalization (bakes in inductive biases) - \u2705 Interpretable (geometry has physical meaning)</p>"},{"location":"math/differential_geometry/#learning-path","title":"Learning Path","text":""},{"location":"math/differential_geometry/#for-ml-practitioners","title":"For ML Practitioners","text":"<p>If you're coming from standard ML and want to understand geometric methods:</p> <ol> <li>Start here: <code>01_tensor_calculus_and_curved_spaces.md</code></li> <li>Read Part 1-2 (intuition for curved spaces)</li> <li>Skim Part 3-4 (covariant derivatives\u2014don't need to master)</li> <li>Focus on Part 5-7 (connection to ML)</li> <li> <p>Try Part 9 (code example on circle)</p> </li> <li> <p>Then: Read Bronstein et al. \"Geometric Deep Learning\" book (free online)</p> </li> <li> <p>Finally: Pick an application area and dive into recent papers</p> </li> </ol>"},{"location":"math/differential_geometry/#for-mathphysics-background","title":"For Math/Physics Background","text":"<p>If you know differential geometry and want ML applications:</p> <ol> <li>Quick review: Part 1-4 of <code>01_tensor_calculus_and_curved_spaces.md</code></li> <li>Focus on: Part 5-9 (how geometry connects to diffusion models)</li> <li>Dive into: Recent papers in Part 10</li> </ol>"},{"location":"math/differential_geometry/#key-applications-2024-2026","title":"Key Applications (2024-2026)","text":""},{"location":"math/differential_geometry/#1-protein-structure-generation","title":"1. Protein Structure Generation","text":"<p>Problem: Design novel proteins with desired functions</p> <p>Geometry: </p> <ul> <li>Backbone: Torsion angles \\((\\phi, \\psi, \\omega)\\) on torus \\(T^3\\)</li> <li>Full structure: SE(3) transformations (rigid motions)</li> </ul> <p>Recent work:</p> <ul> <li>RFdiffusion (2023): De novo protein design using SE(3)-equivariant diffusion</li> <li>Chroma (2023): Generative model for protein design</li> <li>FoldFlow (2023): Flow matching on SE(3)</li> </ul> <p>Impact: Designed proteins for therapeutics, enzymes, biosensors</p>"},{"location":"math/differential_geometry/#2-molecular-docking","title":"2. Molecular Docking","text":"<p>Problem: Predict how small molecules bind to proteins</p> <p>Geometry:</p> <ul> <li>Translation: \\(\\mathbb{R}^3\\)</li> <li>Rotation: SO(3) (3D rotations)</li> <li>Combined: SE(3) (Euclidean group)</li> </ul> <p>Recent work:</p> <ul> <li>DiffDock (2023): SE(3)-equivariant diffusion for docking</li> <li>38% improvement over prior methods</li> </ul> <p>Impact: Drug discovery, understanding protein-ligand interactions</p>"},{"location":"math/differential_geometry/#3-climate-and-earth-science","title":"3. Climate and Earth Science","text":"<p>Problem: Weather prediction, climate modeling</p> <p>Geometry: </p> <ul> <li>Earth surface: Sphere \\(S^2\\)</li> <li>Need to respect spherical geometry (no \"edges\")</li> </ul> <p>Recent work:</p> <ul> <li>Spherical CNNs for climate data</li> <li>Diffusion models on \\(S^2\\) for weather forecasting</li> </ul> <p>Impact: More accurate predictions, especially near poles</p>"},{"location":"math/differential_geometry/#4-robotics","title":"4. Robotics","text":"<p>Problem: Motion planning, manipulation</p> <p>Geometry:</p> <ul> <li>Configuration space: Product of circles \\((S^1)^n\\) for revolute joints</li> <li>End-effector poses: SE(3)</li> </ul> <p>Recent work:</p> <ul> <li>Diffusion policies on configuration manifolds</li> <li>SE(3)-equivariant networks for grasping</li> </ul> <p>Impact: More natural motion, respects joint limits automatically</p>"},{"location":"math/differential_geometry/#prerequisites","title":"Prerequisites","text":""},{"location":"math/differential_geometry/#minimal-background","title":"Minimal Background","text":"<p>To get started: - Linear algebra: Vectors, matrices, eigenvalues - Multivariable calculus: Partial derivatives, chain rule - Probability: Gaussian distributions, density functions - Python/PyTorch: Basic neural network training</p> <p>You do not need: - Prior differential geometry knowledge - Tensor calculus background - Physics training</p>"},{"location":"math/differential_geometry/#recommended-background","title":"Recommended Background","text":"<p>For deeper understanding: - Vector calculus: Gradient, divergence, curl - Linear algebra: Positive definite matrices, quadratic forms - PDEs: Heat equation, diffusion - Probability: Stochastic processes, SDEs</p>"},{"location":"math/differential_geometry/#software-tools","title":"Software Tools","text":""},{"location":"math/differential_geometry/#python-libraries","title":"Python Libraries","text":"<p>Riemannian geometry: <pre><code>pip install geoopt geomstats pymanopt\n</code></pre></p> <p>Geometric deep learning: <pre><code>pip install torch-geometric e3nn\n</code></pre></p> <p>Protein structure: <pre><code>pip install biopython py3Dmol\n</code></pre></p>"},{"location":"math/differential_geometry/#quick-start","title":"Quick Start","text":"<pre><code># Example: Optimization on sphere using geoopt\nimport torch\nimport geoopt\n\n# Define manifold\nsphere = geoopt.Sphere()\n\n# Create point on sphere\nx = sphere.random(5, 10)  # 5 samples, 10-dimensional sphere\n\n# Define parameter on manifold\nparam = geoopt.ManifoldParameter(x, manifold=sphere)\n\n# Use in optimization (automatically projects gradients)\noptimizer = geoopt.optim.RiemannianAdam([param], lr=0.01)\n</code></pre>"},{"location":"math/differential_geometry/#recent-advances-2024-2026","title":"Recent Advances (2024-2026)","text":""},{"location":"math/differential_geometry/#theoretical-breakthroughs","title":"Theoretical Breakthroughs","text":"<ol> <li>Riemannian Flow Matching (Chen et al. 2023)</li> <li>Extends flow matching to general manifolds</li> <li> <p>Computationally efficient (no score function)</p> </li> <li> <p>Gauge Equivariant Networks (Cohen et al. 2024)</p> </li> <li>Build networks that respect local symmetries</li> <li> <p>Application to curved spaces</p> </li> <li> <p>Learned Metrics (Zhang et al. 2024)</p> </li> <li>Learn manifold geometry from data</li> <li>Don't need to specify metric a priori</li> </ol>"},{"location":"math/differential_geometry/#practical-tools","title":"Practical Tools","text":"<ol> <li><code>geomstats</code> 2.0 (2024): Unified API for Riemannian geometry</li> <li><code>e3nn</code> for biology: SE(3)-equivariant networks for proteins</li> <li><code>geoopt</code> integration: PyTorch Lightning support</li> </ol>"},{"location":"math/differential_geometry/#benchmark-datasets","title":"Benchmark Datasets","text":"<ol> <li>RIEMANNIAN-BENCH (2024): Standardized benchmarks for Riemannian ML</li> <li>Protein structure datasets: PDB, AlphaFold DB</li> <li>Earth science: Climate modeling datasets on \\(S^2\\)</li> </ol>"},{"location":"math/differential_geometry/#open-research-questions","title":"Open Research Questions","text":""},{"location":"math/differential_geometry/#fundamental-theory","title":"Fundamental Theory","text":"<ol> <li>Optimal parametrizations: Ambient vs. intrinsic coordinates?</li> <li>Approximation theory: How well can neural networks approximate on manifolds?</li> <li>Generalization bounds: Sample complexity on manifolds</li> <li>Tractable inference: Fast sampling on high-dimensional manifolds</li> </ol>"},{"location":"math/differential_geometry/#practical-challenges","title":"Practical Challenges","text":"<ol> <li>Scalability: Efficient algorithms for large manifolds (proteins: 1000+ atoms)</li> <li>Numerical stability: Avoiding coordinate singularities</li> <li>Architecture design: Best practices for network design on manifolds</li> <li>Training efficiency: Slow convergence with geometric constraints</li> </ol>"},{"location":"math/differential_geometry/#new-applications","title":"New Applications","text":"<ol> <li>Single-cell genomics: Cell differentiation as flow on manifolds</li> <li>Financial time series: Returns on positive definite cone</li> <li>Neuroscience: Brain connectivity on manifold of correlation matrices</li> <li>Quantum chemistry: Molecular orbitals on Grassmannian manifolds</li> </ol>"},{"location":"math/differential_geometry/#contributing","title":"Contributing","text":"<p>Found an error or have suggestions? This is a living document!</p> <ul> <li>Corrections: Please point out any mathematical errors</li> <li>Clarifications: If something is unclear, let us know</li> <li>Extensions: Suggestions for additional topics welcome</li> </ul>"},{"location":"math/differential_geometry/#related-documentation","title":"Related Documentation","text":""},{"location":"math/differential_geometry/#within-this-project","title":"Within This Project","text":"<ul> <li><code>../../diffusion/</code>: Standard Euclidean diffusion models</li> <li><code>../../SDE/</code>: SDE formulation background</li> <li><code>../../dev/ddpm_learning_process/</code>: DDPM mechanics</li> </ul>"},{"location":"math/differential_geometry/#external-resources","title":"External Resources","text":"<ul> <li>Geometric Deep Learning Book: https://geometricdeeplearning.com/</li> <li>Riemannian Score-Based Models Paper: https://arxiv.org/abs/2202.02763</li> <li>geomstats Tutorials: https://geomstats.github.io/tutorials/</li> </ul>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/","title":"Tensor Calculus and Curved Spaces: A Tutorial for Machine Learning","text":""},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#overview","title":"Overview","text":"<p>This tutorial introduces tensor calculus and differential geometry on curved spaces, motivated by their emerging applications in machine learning. We'll build from first principles to modern applications in diffusion models, geometric deep learning, and beyond.</p> <p>Why this matters for ML:</p> <ul> <li>Protein structures live on curved spaces (torsion angles on tori)</li> <li>Molecular conformations are constrained to manifolds</li> <li>Robotic joint configurations form non-Euclidean spaces</li> <li>Many ML problems have natural geometric structure that Euclidean methods ignore</li> </ul>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#part-1-from-euclidean-to-curved-spaces","title":"Part 1: From Euclidean to Curved Spaces","text":""},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#11-euclidean-space-what-were-used-to","title":"1.1 Euclidean Space: What We're Used To","text":"<p>In Euclidean space \\(\\mathbb{R}^d\\) with Cartesian coordinates \\((x_1, x_2, \\ldots, x_d)\\):</p> <p>Distance (Pythagorean theorem):</p> \\[ ds^2 = dx_1^2 + dx_2^2 + \\cdots + dx_d^2 \\] <p>Gradient (just partial derivatives):</p> <p>$$</p> <p>\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\ \\vdots \\ \\frac{\\partial f}{\\partial x_d} \\end{bmatrix} $$</p> <p>Dot product:</p> <p>$$</p> <p>\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^d u_i v_i $$</p> <p>Key property: All these operations are coordinate-independent in Cartesian coordinates.</p>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#12-curvilinear-coordinates-first-step-beyond-cartesian","title":"1.2 Curvilinear Coordinates: First Step Beyond Cartesian","text":"<p>Even in flat (Euclidean) space, using different coordinates changes how we compute distances and gradients.</p> <p>Example: Polar Coordinates in 2D</p> <p>Cartesian: \\((x, y)\\) Polar: \\((r, \\theta)\\) where \\(x = r\\cos\\theta\\), \\(y = r\\sin\\theta\\)</p> <p>Distance element:</p> <p>$$</p> <p>ds^2 = dr^2 + r^2 d\\theta^2 $$</p> <p>Notice: Not \\(dr^2 + d\\theta^2\\) because angles and radii have different units!</p> <p>Gradient:</p> <p>$$</p> <p>\\nabla f = \\frac{\\partial f}{\\partial r} \\hat{e}r + \\frac{1}{r} \\frac{\\partial f}{\\partial \\theta} \\hat{e}\\theta $$</p> <p>Notice: The \\(1/r\\) factor! This comes from the coordinate system.</p>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#13-curved-spaces-intrinsic-curvature","title":"1.3 Curved Spaces: Intrinsic Curvature","text":"<p>A curved space (manifold) is one where you cannot find coordinates that make distances Pythagorean everywhere.</p> <p>Example: Sphere \\(S^2\\)</p> <p>Using spherical coordinates \\((\\theta, \\phi)\\):</p> \\[ ds^2 = R^2(d\\theta^2 + \\sin^2\\theta \\, d\\phi^2) \\] <p>Key difference: The coefficient \\(\\sin^2\\theta\\) depends on position\u2014this is intrinsic curvature.</p> <p>Physical intuition: </p> <ul> <li>Near the equator, longitude lines are far apart</li> <li>Near the poles, they converge</li> <li>This affects distances and gradients</li> </ul>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#part-2-the-metric-tensor","title":"Part 2: The Metric Tensor","text":""},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#21-definition","title":"2.1 Definition","text":"<p>The metric tensor \\(g_{ij}\\) encodes how to measure distances in arbitrary coordinates.</p> <p>Distance element:</p> <p>$$</p> <p>ds^2 = \\sum_{i,j=1}^d g_{ij} \\, dx^i \\, dx^j = g_{ij} \\, dx^i \\, dx^j $$</p> <p>(Using Einstein summation: repeated indices are summed)</p> <p>Matrix form:</p> <p>$$</p> <p>ds^2 = (dx)^T G \\, dx $$</p> <p>where \\(G = [g_{ij}]\\) is the metric tensor matrix.</p>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#22-examples","title":"2.2 Examples","text":"<p>Euclidean space (Cartesian):</p> <p>$$</p> <p>G = I = \\begin{bmatrix} 1 &amp; 0 &amp; \\cdots &amp; 0 \\ 0 &amp; 1 &amp; \\cdots &amp; 0 \\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\ 0 &amp; 0 &amp; \\cdots &amp; 1 \\end{bmatrix} $$</p> <p>Polar coordinates (in 2D):</p> <p>$$</p> <p>G = \\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; r^2 \\end{bmatrix} $$</p> <p>Sphere \\(S^2\\) (radius \\(R\\)):</p> <p>$$</p> <p>G = \\begin{bmatrix} R^2 &amp; 0 \\ 0 &amp; R<sup>2\\sin</sup>2\\theta \\end{bmatrix} $$</p> <p>Minkowski spacetime (special relativity):</p> <p>$$</p> <p>G = \\begin{bmatrix} -c^2 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} $$</p>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#23-raising-and-lowering-indices","title":"2.3 Raising and Lowering Indices","text":"<p>The metric tensor relates contravariant (upper index) and covariant (lower index) vectors.</p> <p>Lowering an index:</p> <p>$$</p> <p>v_i = g_{ij} v^j $$</p> <p>Raising an index:</p> <p>$$</p> <p>v^i = g^{ij} v_j $$</p> <p>where \\(g^{ij}\\) is the inverse metric tensor: \\(g^{ik} g_{kj} = \\delta^i_j\\).</p> <p>In Euclidean space: \\(g_{ij} = g^{ij} = \\delta_{ij}\\), so there's no difference between upper and lower indices.</p> <p>In curved space: Upper and lower indices are genuinely different!</p>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#part-3-covariant-derivatives-and-christoffel-symbols","title":"Part 3: Covariant Derivatives and Christoffel Symbols","text":""},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#31-the-problem-with-ordinary-derivatives","title":"3.1 The Problem with Ordinary Derivatives","text":"<p>In curved space, partial derivatives of vectors are not tensors!</p> <p>Why? Because basis vectors \\(\\hat{e}_i\\) themselves change from point to point.</p> <p>Example on a sphere: </p> <ul> <li>The \"north\" direction at one point is different from \"north\" at another point</li> <li>Taking \\(\\frac{\\partial \\mathbf{v}}{\\partial \\theta}\\) mixes the change in vector components with the change in basis vectors</li> </ul>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#32-covariant-derivative","title":"3.2 Covariant Derivative","text":"<p>The covariant derivative \\(\\nabla_i\\) is the correct way to differentiate in curved spaces.</p> <p>For a scalar (easy):</p> \\[ \\nabla_i f = \\frac{\\partial f}{\\partial x^i} \\] <p>For a vector (requires correction):</p> <p>$$</p> <p>\\nabla_i v^j = \\frac{\\partial v^j}{\\partial x^i} + \\Gamma^j_{ik} v^k $$</p> <p>where \\(\\Gamma^j_{ik}\\) are the Christoffel symbols (connection coefficients).</p>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#33-christoffel-symbols","title":"3.3 Christoffel Symbols","text":"<p>The Christoffel symbols encode how basis vectors change:</p> \\[ \\frac{\\partial \\hat{e}_i}{\\partial x^j} = \\Gamma^k_{ij} \\hat{e}_k \\] <p>Formula in terms of metric:</p> <p>$$</p> <p>\\Gamma^k_{ij} = \\frac{1}{2} g^{k\\ell} \\left(\\frac{\\partial g_{\\ell i}}{\\partial x^j} + \\frac{\\partial g_{\\ell j}}{\\partial x^i} - \\frac{\\partial g_{ij}}{\\partial x^\\ell}\\right) $$</p> <p>Key properties:</p> <ul> <li>\\(\\Gamma^k_{ij} = \\Gamma^k_{ji}\\) (symmetric in lower indices)</li> <li>\\(\\Gamma^k_{ij} = 0\\) in Cartesian coordinates on flat space</li> <li>Nonzero in curvilinear coordinates or curved spaces</li> </ul>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#34-example-sphere","title":"3.4 Example: Sphere","text":"<p>For a sphere \\(S^2\\) with metric \\(ds^2 = R^2(d\\theta^2 + \\sin^2\\theta \\, d\\phi^2)\\):</p> <p>Non-zero Christoffel symbols:</p> <p>$$</p> <p>\\Gamma^\\theta_{\\phi\\phi} = -\\sin\\theta \\cos\\theta $$ $$</p> <p>\\Gamma^\\phi_{\\theta\\phi} = \\Gamma^\\phi_{\\phi\\theta} = \\frac{\\cos\\theta}{\\sin\\theta} = \\cot\\theta $$</p> <p>Interpretation: </p> <ul> <li>Moving in \\(\\phi\\) (longitude) changes your \\(\\theta\\) (latitude) direction</li> <li>This is the geometric \"correction\" needed on a curved surface</li> </ul>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#part-4-gradient-on-curved-spaces","title":"Part 4: Gradient on Curved Spaces","text":""},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#41-definition","title":"4.1 Definition","text":"<p>The gradient of a scalar function \\(f\\) is the vector that satisfies:</p> \\[ df = \\nabla f \\cdot d\\mathbf{x} \\] <p>for any displacement \\(d\\mathbf{x}\\).</p>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#42-formula","title":"4.2 Formula","text":"<p>In arbitrary coordinates:</p> \\[ (\\nabla f)^i = g^{ij} \\frac{\\partial f}{\\partial x^j} \\] <p>Key insight: The inverse metric \\(g^{ij}\\) couples dimensions!</p>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#43-example-polar-coordinates","title":"4.3 Example: Polar Coordinates","text":"<p>Metric: \\(g_{ij} = \\text{diag}(1, r^2)\\) Inverse metric: \\(g^{ij} = \\text{diag}(1, 1/r^2)\\)</p> \\[ \\nabla f = \\begin{bmatrix} g^{rr} \\frac{\\partial f}{\\partial r} \\\\ g^{\\phi\\phi} \\frac{\\partial f}{\\partial \\phi} \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial f}{\\partial r} \\\\ \\frac{1}{r^2} \\frac{\\partial f}{\\partial \\phi} \\end{bmatrix} \\] <p>In orthonormal basis \\((\\hat{e}_r, \\hat{e}_\\phi)\\):</p> <p>$$</p> <p>\\nabla f = \\frac{\\partial f}{\\partial r} \\hat{e}r + \\frac{1}{r} \\frac{\\partial f}{\\partial \\phi} \\hat{e}\\phi $$</p> <p>(The \\(1/r\\) factor accounts for the stretched basis vector)</p>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#part-5-connection-to-probability-and-score-functions","title":"Part 5: Connection to Probability and Score Functions","text":""},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#51-score-function-on-euclidean-space","title":"5.1 Score Function on Euclidean Space","text":"<p>In standard diffusion models:</p> \\[ s(x) = \\nabla_x \\log p(x) = \\begin{bmatrix} \\frac{\\partial \\log p}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial \\log p}{\\partial x_d} \\end{bmatrix} \\] <p>This is straightforward because we're in Euclidean space.</p>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#52-score-function-on-curved-spaces","title":"5.2 Score Function on Curved Spaces","text":"<p>On a Riemannian manifold \\(\\mathcal{M}\\) with metric \\(g_{ij}\\):</p> \\[ s(x)^i = g^{ij} \\frac{\\partial \\log p(x)}{\\partial x^j} \\] <p>Key difference: The inverse metric \\(g^{ij}\\) can couple dimensions!</p> <p>Example on a sphere: The score in the \\(\\theta\\) direction depends on both \\(\\frac{\\partial \\log p}{\\partial \\theta}\\) and \\(\\frac{\\partial \\log p}{\\partial \\phi}\\) if the metric has off-diagonal terms.</p>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#53-why-this-matters","title":"5.3 Why This Matters","text":"<p>Many real-world problems involve data on manifolds:</p> <ol> <li>Protein backbone angles: Torsion angles \\((\\phi, \\psi)\\) live on a torus \\(T^2 = S^1 \\times S^1\\)</li> <li>Molecular conformations: Constrained to energy manifolds</li> <li>Rotations: SO(3) is a 3D manifold (not \\(\\mathbb{R}^3\\)!)</li> <li>Directional data: Unit vectors live on a sphere \\(S^{d-1}\\)</li> </ol> <p>Standard Euclidean diffusion models can violate these constraints or produce invalid samples.</p> <p>Riemannian diffusion models respect the geometry.</p>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#part-6-diffusion-on-manifolds","title":"Part 6: Diffusion on Manifolds","text":""},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#61-heat-equation-on-manifolds","title":"6.1 Heat Equation on Manifolds","text":"<p>The heat equation (diffusion) on a Riemannian manifold:</p> \\[ \\frac{\\partial p(x,t)}{\\partial t} = \\Delta_g p(x,t) \\] <p>where \\(\\Delta_g\\) is the Laplace-Beltrami operator:</p> \\[ \\Delta_g f = \\frac{1}{\\sqrt{\\det g}} \\frac{\\partial}{\\partial x^i} \\left(\\sqrt{\\det g} \\, g^{ij} \\frac{\\partial f}{\\partial x^j}\\right) \\] <p>In Euclidean space: \\(g_{ij} = \\delta_{ij}\\), \\(\\det g = 1\\), so \\(\\Delta_g = \\nabla^2\\) (standard Laplacian).</p> <p>In curved space: The metric and its determinant affect how diffusion spreads.</p>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#62-brownian-motion-on-manifolds","title":"6.2 Brownian Motion on Manifolds","text":"<p>Euclidean Brownian motion:</p> <p>$$</p> <p>dx = \\sigma \\, dw $$</p> <p>where \\(dw \\sim \\mathcal{N}(0, dt \\cdot I)\\).</p> <p>Riemannian Brownian motion:</p> <p>$$</p> <p>dx^i = \\sqrt{g^{ij}} \\sigma \\, dw_j - \\frac{1}{2} \\Gamma^i_{jk} g^{jk} \\sigma^2 \\, dt $$</p> <p>Key differences: 1. The metric \\(g^{ij}\\) scales the noise 2. The Christoffel symbols \\(\\Gamma^i_{jk}\\) add a drift term</p> <p>Physical interpretation: The drift term prevents the process from \"bunching up\" in regions where the coordinate system is compressed.</p>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#63-score-based-diffusion-on-manifolds","title":"6.3 Score-Based Diffusion on Manifolds","text":"<p>Forward SDE on manifold \\(\\mathcal{M}\\):</p> <p>$$</p> <p>dx = f(x,t) \\, dt + g(t) \\, dw_{\\mathcal{M}} $$</p> <p>where \\(dw_{\\mathcal{M}}\\) is Brownian motion on the manifold.</p> <p>Reverse SDE:</p> <p>$$</p> <p>dx = \\left[f(x,t) - g(t)^2 \\nabla_{\\mathcal{M}} \\log p_t(x)\\right] dt + g(t) \\, d\\bar{w} $$</p> <p>where \\(\\nabla_{\\mathcal{M}}\\) is the Riemannian gradient.</p> <p>Neural network: Learn \\(s_\\theta(x,t) \\approx \\nabla_{\\mathcal{M}} \\log p_t(x)\\) (the score on the manifold).</p>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#part-7-current-research-and-applications","title":"Part 7: Current Research and Applications","text":""},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#71-riemannian-score-based-generative-models","title":"7.1 Riemannian Score-Based Generative Models","text":"<p>Key papers:</p> <ul> <li>De Bortoli et al. (2022): \"Riemannian Score-Based Generative Modeling\"</li> <li>Mathieu &amp; Nickel (2020): \"Continuous Hierarchical Representations with Poincar\u00e9 VAEs\"</li> </ul> <p>Approach: 1. Define data as living on a manifold \\(\\mathcal{M}\\) with metric \\(g\\) 2. Construct forward diffusion using Brownian motion on \\(\\mathcal{M}\\) 3. Learn the Riemannian score function \\(\\nabla_{\\mathcal{M}} \\log p_t(x)\\) 4. Sample via reverse SDE using the learned score</p> <p>Applications:</p> <ul> <li>Protein structure generation (torsion angles on torus)</li> <li>Molecular conformations (constrained manifolds)</li> <li>Earth science data (spherical domains)</li> </ul>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#72-geometric-deep-learning","title":"7.2 Geometric Deep Learning","text":"<p>Key idea: Design neural networks that respect the geometry of the data.</p> <p>Examples: 1. Graph neural networks: Data on graphs (non-Euclidean) 2. Mesh CNNs: Operate on 3D meshes (curved surfaces) 3. Equivariant networks: Respect symmetries (rotations, translations)</p> <p>Connection to diffusion: </p> <ul> <li>Can we build score networks that are equivariant to geometric transformations?</li> <li>How to parameterize networks on manifolds?</li> </ul>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#73-proteins-and-molecular-generation","title":"7.3 Proteins and Molecular Generation","text":"<p>Problem: Protein backbones are parameterized by torsion angles \\((\\phi, \\psi, \\omega)\\).</p> <p>Naive approach: Treat angles as \\(\\mathbb{R}^3\\) - Issue: Doesn't respect periodicity (\\(\\phi + 2\\pi = \\phi\\)) - Result: Invalid structures, poor generalization</p> <p>Geometric approach: Treat each angle as living on \\(S^1\\) (circle) - Manifold: \\(T^3 = S^1 \\times S^1 \\times S^1\\) (3-torus) - Diffusion: Brownian motion on the torus - Result: Always generates valid structures</p> <p>State-of-the-art:</p> <ul> <li>FoldFlow (NeurIPS 2023): Diffusion on SE(3) for protein backbone generation</li> <li>DiffDock (ICLR 2023): SE(3)-equivariant diffusion for molecular docking</li> </ul>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#74-robotics-and-motion-planning","title":"7.4 Robotics and Motion Planning","text":"<p>Problem: Robot joint configurations live on configuration space \\(\\mathcal{C}\\), often a manifold.</p> <p>Example: Robot arm with revolute joints - Each joint angle is on \\(S^1\\) - Configuration space: \\(\\mathcal{C} = (S^1)^n\\) (n-dimensional torus)</p> <p>Geometric approach:</p> <ul> <li>Learn motion distribution on \\(\\mathcal{C}\\) using Riemannian diffusion</li> <li>Generate collision-free paths respecting joint limits</li> <li>Naturally handles periodicity and constraints</li> </ul>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#part-8-practical-considerations","title":"Part 8: Practical Considerations","text":""},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#81-choosing-coordinates","title":"8.1 Choosing Coordinates","text":"<p>For manifolds embedded in \\(\\mathbb{R}^D\\) (like \\(S^2 \\subset \\mathbb{R}^3\\)):</p> <p>Option 1: Intrinsic coordinates (\\(\\theta, \\phi\\) on sphere) - \u2705 Lower dimensional (d &lt; D) - \u274c Singularities (e.g., poles on sphere) - \u274c Requires explicit metric, Christoffel symbols</p> <p>Option 2: Ambient coordinates (\\(x, y, z\\) in \\(\\mathbb{R}^3\\) with constraint \\(x^2+y^2+z^2=1\\)) - \u2705 No singularities - \u2705 Simple Euclidean gradient - \u274c Higher dimensional - \u274c Need to project to manifold</p> <p>Hybrid approach: Use ambient coordinates but project gradients to tangent space.</p>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#82-computing-christoffel-symbols","title":"8.2 Computing Christoffel Symbols","text":"<p>Symbolic computation (small problems): <pre><code>import sympy as sp\n# Define metric\ng = sp.Matrix([[1, 0], [0, r**2]])\n# Compute Christoffel symbols automatically\n</code></pre></p> <p>Numerical computation (large problems): - Approximate using finite differences - Learn from data (e.g., neural network parameterization)</p> <p>Automatic differentiation:</p> <ul> <li>PyTorch/JAX can compute gradients on manifolds</li> <li>Libraries: <code>geomstats</code>, <code>pymanopt</code>, <code>geoopt</code></li> </ul>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#83-neural-network-architectures","title":"8.3 Neural Network Architectures","text":"<p>Challenge: Standard MLPs and CNNs assume Euclidean structure.</p> <p>Solutions:</p> <ol> <li>Tangent space networks: Map to tangent space at each point (locally Euclidean)</li> <li>Gauge equivariant networks: Use parallel transport to move between tangent spaces</li> <li>Ambient space projection: Operate in \\(\\mathbb{R}^D\\), project outputs to manifold</li> </ol> <p>Example: For \\(S^2\\) (sphere) <pre><code># Ambient space projection\nz = mlp(x)  # z in R^3\nz_normalized = z / torch.norm(z, dim=-1, keepdim=True)  # Project to S^2\n</code></pre></p>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#part-9-code-example-diffusion-on-a-circle","title":"Part 9: Code Example - Diffusion on a Circle","text":""},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#91-setup","title":"9.1 Setup","text":"<p>Circle: \\(S^1 = \\{(\\cos\\theta, \\sin\\theta) : \\theta \\in [0, 2\\pi)\\}\\)</p> <p>Parametrization: Use angle \\(\\theta\\) (intrinsic) or \\((x, y)\\) with \\(x^2+y^2=1\\) (ambient).</p>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#92-ambient-space-approach","title":"9.2 Ambient Space Approach","text":"<pre><code>import torch\nimport torch.nn as nn\n\nclass CircleDataset:\n    \"\"\"Data on a circle (e.g., von Mises distribution)\"\"\"\n    def __init__(self, mu=0, kappa=10):\n        self.mu = mu  # Mean direction\n        self.kappa = kappa  # Concentration\n\n    def sample(self, n):\n        \"\"\"Sample from von Mises distribution\"\"\"\n        theta = torch.distributions.VonMises(self.mu, self.kappa).sample((n,))\n        x = torch.cos(theta)\n        y = torch.sin(theta)\n        return torch.stack([x, y], dim=-1)  # Shape: (n, 2)\n\ndef project_to_circle(x):\n    \"\"\"Project points to unit circle\"\"\"\n    return x / torch.norm(x, dim=-1, keepdim=True)\n\nclass CircleScoreNetwork(nn.Module):\n    \"\"\"Score network for circle using ambient space\"\"\"\n    def __init__(self, hidden_dim=128):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(3, hidden_dim),  # [x, y, t]\n            nn.SiLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.SiLU(),\n            nn.Linear(hidden_dim, 2)  # Output in R^2\n        )\n\n    def forward(self, x, t):\n        \"\"\"\n        x: (batch, 2) points on circle\n        t: (batch,) time steps\n        \"\"\"\n        t = t.unsqueeze(-1)  # (batch, 1)\n        inp = torch.cat([x, t], dim=-1)  # (batch, 3)\n        score = self.net(inp)  # (batch, 2)\n\n        # Project score to tangent space (orthogonal to x)\n        # Tangent space: {v : v \u00b7 x = 0}\n        score = score - (score * x).sum(dim=-1, keepdim=True) * x\n        return score\n\ndef forward_diffusion_circle(x0, t, noise_schedule):\n    \"\"\"\n    Forward diffusion on circle using wrapped normal\n    x0: (batch, 2) points on circle\n    t: scalar time\n    \"\"\"\n    alpha_bar_t = noise_schedule(t)\n\n    # Add noise in ambient space\n    noise = torch.randn_like(x0)\n    xt = torch.sqrt(alpha_bar_t) * x0 + torch.sqrt(1 - alpha_bar_t) * noise\n\n    # Project back to circle\n    xt = project_to_circle(xt)\n    return xt, noise\n\ndef train_circle_score(model, dataset, num_epochs=1000):\n    \"\"\"Train score network on circle\"\"\"\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    for epoch in range(num_epochs):\n        x0 = dataset.sample(128)  # Sample from data distribution\n        t = torch.rand(128)  # Random timesteps\n        xt, noise = forward_diffusion_circle(x0, t, lambda t: 1 - t)\n\n        # Predict score\n        score_pred = model(xt, t)\n\n        # True score (for wrapped Gaussian)\n        score_true = -(xt - torch.sqrt(1 - t.unsqueeze(-1)) * x0) / (1 - t.unsqueeze(-1))\n        score_true = score_true - (score_true * xt).sum(dim=-1, keepdim=True) * xt\n\n        loss = ((score_pred - score_true) ** 2).sum(dim=-1).mean()\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if epoch % 100 == 0:\n            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n\n# Usage\ndataset = CircleDataset(mu=0, kappa=10)\nmodel = CircleScoreNetwork(hidden_dim=128)\ntrain_circle_score(model, dataset)\n</code></pre>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#93-intrinsic-coordinates-approach","title":"9.3 Intrinsic Coordinates Approach","text":"<pre><code>class CircleScoreNetworkIntrinsic(nn.Module):\n    \"\"\"Score network using angle \u03b8 directly\"\"\"\n    def __init__(self, hidden_dim=128):\n        super().__init__()\n        # Use periodic encoding for angle\n        self.net = nn.Sequential(\n            nn.Linear(3, hidden_dim),  # [cos(\u03b8), sin(\u03b8), t]\n            nn.SiLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.SiLU(),\n            nn.Linear(hidden_dim, 1)  # Output: d\u03b8/dt (scalar)\n        )\n\n    def forward(self, theta, t):\n        \"\"\"\n        theta: (batch,) angles\n        t: (batch,) time\n        \"\"\"\n        # Periodic encoding\n        x = torch.cos(theta)\n        y = torch.sin(theta)\n        inp = torch.cat([x.unsqueeze(-1), y.unsqueeze(-1), t.unsqueeze(-1)], dim=-1)\n        return self.net(inp).squeeze(-1)  # (batch,)\n</code></pre>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#part-10-resources-and-next-steps","title":"Part 10: Resources and Next Steps","text":""},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#101-essential-textbooks","title":"10.1 Essential Textbooks","text":"<p>Differential Geometry: 1. do Carmo: \"Riemannian Geometry\" - Classic, very readable 2. Lee: \"Introduction to Smooth Manifolds\" - Comprehensive, modern 3. Spivak: \"Calculus on Manifolds\" - Concise intro</p> <p>Geometric ML: 1. Bronstein et al.: \"Geometric Deep Learning\" (2021) - Free online 2. Bekkers: \"An Introduction to Geometric Deep Learning\" (2023)</p> <p>Diffusion on Manifolds: 1. Grigoryan: \"Heat Kernel and Analysis on Manifolds\" 2. De Bortoli et al.: \"Riemannian Score-Based Generative Modeling\" (ICML 2022)</p>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#102-software-libraries","title":"10.2 Software Libraries","text":"<p>Manifold optimization:</p> <ul> <li><code>geoopt</code> (PyTorch): Riemannian optimization</li> <li><code>pymanopt</code> (NumPy/PyTorch/JAX): General manifold optimization</li> <li><code>geomstats</code> (NumPy/PyTorch/JAX): Comprehensive Riemannian geometry</li> </ul> <p>Geometric deep learning:</p> <ul> <li><code>PyG</code> (PyTorch Geometric): Graph neural networks</li> <li><code>DGL</code>: Deep Graph Library</li> <li><code>e3nn</code>: Equivariant neural networks (SO(3)/E(3))</li> </ul> <p>Protein structure:</p> <ul> <li><code>AlphaFold</code>: Structure prediction</li> <li><code>ESM</code> (Evolutionary Scale Modeling): Protein language models</li> <li><code>OpenFold</code>: Open-source AlphaFold implementation</li> </ul>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#103-paper-reading-list","title":"10.3 Paper Reading List","text":"<p>Foundation: 1. De Bortoli et al. (2022): \"Riemannian Score-Based Generative Modeling\" 2. Song et al. (2021): \"Score-Based Generative Modeling through SDEs\"</p> <p>Applications: 3. Yim et al. (2023): \"SE(3) Diffusion Model with Application to Protein Backbone Generation\" 4. Watson et al. (2023): \"De novo design of protein structure and function with RFdiffusion\" 5. Corso et al. (2023): \"DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking\"</p> <p>Theory: 6. Chen et al. (2023): \"Riemannian Flow Matching on General Geometries\" 7. Huang et al. (2022): \"Riemannian Diffusion Models\"</p>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#104-future-directions","title":"10.4 Future Directions","text":"<p>Open research questions:</p> <ol> <li>Optimal parametrizations: When to use ambient vs. intrinsic coordinates?</li> <li>Scalability: Efficient algorithms for high-dimensional manifolds</li> <li>Learning the geometry: Can we learn the manifold structure from data?</li> <li>Hybrid models: Combining Euclidean and Riemannian components</li> <li>Discrete manifolds: Diffusion on graphs and combinatorial structures</li> </ol> <p>Emerging applications: 1. Drug discovery: Molecular generation on conformational manifolds 2. Protein design: Backbone and sequence co-design 3. Materials science: Crystal structure generation (lattices are manifolds) 4. Climate modeling: Weather prediction on spherical domains 5. Robotics: Motion planning on configuration spaces</p>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#summary-key-concepts","title":"Summary: Key Concepts","text":"Concept Euclidean Space Curved Space (Manifold) Metric \\(g_{ij} = \\delta_{ij}\\) (identity) General \\(g_{ij}(x)\\) (position-dependent) Distance \\(ds^2 = \\sum_i dx_i^2\\) \\(ds^2 = g_{ij} dx^i dx^j\\) Gradient \\(\\nabla f = \\frac{\\partial f}{\\partial x}\\) \\((\\nabla f)^i = g^{ij} \\frac{\\partial f}{\\partial x^j}\\) Covariant derivative \\(\\nabla_i v^j = \\frac{\\partial v^j}{\\partial x^i}\\) \\(\\nabla_i v^j = \\frac{\\partial v^j}{\\partial x^i} + \\Gamma^j_{ik} v^k\\) Laplacian \\(\\Delta f = \\sum_i \\frac{\\partial^2 f}{\\partial x_i^2}\\) \\(\\Delta_g f = \\frac{1}{\\sqrt{\\det g}} \\partial_i(\\sqrt{\\det g} \\, g^{ij} \\partial_j f)\\) Brownian motion \\(dx = \\sigma dw\\) \\(dx^i = \\sqrt{g^{ij}} \\sigma dw_j - \\frac{1}{2}\\Gamma^i_{jk} g^{jk} \\sigma^2 dt\\) <p>The main message: Many of the formulas you know from Euclidean space have natural generalizations to curved spaces, but you need to account for the metric tensor and Christoffel symbols.</p>"},{"location":"math/differential_geometry/01_tensor_calculus_and_curved_spaces/#related-documents","title":"Related Documents","text":"<ul> <li><code>../../dev/ddpm_learning_process/03_gradient_operator_and_independence.md</code>: Gradient operator in different coordinate systems</li> <li><code>../diffusion/brownian_motion_tutorial.md</code>: Brownian motion in Euclidean space</li> <li><code>../SDE/01_diffusion_sde_view.md</code>: SDE formulation of diffusion models</li> </ul>"},{"location":"notebooks/","title":"GenAI Lab Interactive Notebooks","text":"<p>Interactive Jupyter notebooks demonstrating generative AI models for computational biology.</p>"},{"location":"notebooks/#notebook-organization","title":"\ud83d\udcc2 Notebook Organization","text":"<p>Notebooks are organized by topic:</p> <ul> <li><code>diffusion/</code> - DDPM, DiT, latent diffusion tutorials</li> <li><code>vae/</code> - VAE, \u03b2-VAE, CVAE examples</li> <li><code>foundation_models/</code> - Fine-tuning and adaptation</li> <li><code>datasets/</code> - Data loading and preprocessing</li> </ul>"},{"location":"notebooks/#viewing-options","title":"\ud83d\udccd Viewing Options","text":""},{"location":"notebooks/#option-1-github-pages-best-rendering-recommended","title":"Option 1: GitHub Pages (Best Rendering) \u2b50 Recommended","text":"<p>View rendered notebooks with proper math and plots:</p> <p>Diffusion Models: - \ud83d\udcca DDPM Basics - \ud83d\udcca SDE Formulation - \ud83d\udcca Medical Imaging Diffusion - \ud83d\udcca Gene Expression Diffusion</p> <p>VAE Series: - \ud83d\udccb Coming soon</p> <p>Advantages:</p> <ul> <li>\u2705 Reliable rendering (no GitHub timeouts)</li> <li>\u2705 Math properly displayed  </li> <li>\u2705 Plots and outputs preserved</li> <li>\u2705 Mobile-friendly</li> </ul>"},{"location":"notebooks/#option-2-run-locally-interactive","title":"Option 2: Run Locally (Interactive)","text":"<p>Clone and run in Jupyter:</p> <pre><code># Clone repository\ngit clone https://github.com/pleiadian53/genai-lab.git\ncd genai-lab/notebooks  # Use the original location\n\n# Create environment\nconda env create -f ../environment.yml\nconda activate genai-lab\n\n# Or install dependencies\npip install -r requirements.txt\n\n# Launch Jupyter\njupyter notebook\n</code></pre> <p>Advantages:</p> <ul> <li>\u2705 Fully interactive</li> <li>\u2705 Modify and experiment</li> <li>\u2705 Run with your own data</li> </ul>"},{"location":"notebooks/#option-3-githubcom-quick-view","title":"Option 3: GitHub.com (Quick View)","text":"<p>Browse notebooks directly on GitHub: - \ud83d\udcd3 View on GitHub</p> <p>Note: GitHub's notebook renderer can be slow/unreliable for large notebooks. Use GitHub Pages for best experience.</p>"},{"location":"notebooks/#note-for-contributors","title":"\ud83d\udcdd Note for Contributors","text":"<p>These notebooks are rendered copies from the main <code>/notebooks/</code> directory in the repository.</p>"},{"location":"notebooks/#development-workflow","title":"Development Workflow:","text":"<ol> <li>Develop in <code>/notebooks/</code> (primary location)</li> <li>Experiment freely</li> <li>Iterate on code and outputs</li> <li> <p>Keep messy/experimental notebooks here</p> </li> <li> <p>Polish before publishing</p> </li> <li>Clear outputs or save clean outputs</li> <li>Add markdown explanations</li> <li> <p>Test end-to-end execution</p> </li> <li> <p>Publish to <code>/docs/notebooks/</code> (rendered location)</p> </li> <li>Copy notebook when ready for public viewing:      <pre><code>cp notebooks/diffusion/my_tutorial.ipynb docs/notebooks/diffusion/\n</code></pre></li> <li> <p>Add to git and push</p> </li> <li> <p>Add to navigation (optional)</p> </li> <li>Edit <code>mkdocs.yml</code> to include in nav structure</li> <li>Notebook is accessible by URL even without nav entry</li> </ol>"},{"location":"notebooks/#why-two-locations","title":"Why Two Locations?","text":"<ul> <li><code>/notebooks/</code> \u2014 Source of truth, standard location for developers</li> <li><code>/docs/notebooks/</code> \u2014 Rendered version for documentation site</li> </ul> <p>This dual-location approach: - \u2705 Keeps notebooks where developers expect them - \u2705 Enables reliable rendering on GitHub Pages - \u2705 Allows selective publishing (not all notebooks need to be public) - \u2705 Explicit \"publish\" step ensures quality</p> <p>Duplication is intentional and minimal - only polished notebooks are copied.</p>"},{"location":"notebooks/#available-notebooks","title":"\ud83d\ude80 Available Notebooks","text":""},{"location":"notebooks/#diffusion-models-available-now","title":"Diffusion Models \u2705 Available Now!","text":"Notebook Description Link Status DDPM Basics Introduction to Denoising Diffusion Probabilistic Models View Notebook \u2705 Available SDE Formulation Stochastic Differential Equations for diffusion View Notebook \u2705 Available Medical Imaging Diffusion Diffusion models for medical images View Notebook \u2705 Available Gene Expression Diffusion Applying diffusion to gene expression data View Notebook \u2705 Available <p>Quick access: Navigate to \"Diffusion (Technical Deep Dives)\" in the top navigation tabs or sidebar!</p>"},{"location":"notebooks/#vae-series","title":"VAE Series","text":"Notebook Description Status Coming soon VAE training \ud83d\udccb Planned"},{"location":"notebooks/#foundation-models","title":"Foundation Models","text":"Notebook Description Status Coming soon Fine-tuning guide \ud83d\udccb Planned"},{"location":"notebooks/#related-resources","title":"\ud83d\udcda Related Resources","text":"<ul> <li>Documentation \u2014 Theory and concepts</li> <li>Examples \u2014 Runnable scripts</li> <li>Source Code \u2014 Implementation</li> </ul>"},{"location":"notebooks/#tips-for-learning","title":"\ud83d\udca1 Tips for Learning","text":"<ul> <li>Start with documentation to understand theory</li> <li>Run notebooks to see models in action</li> <li>Modify and experiment to deepen understanding</li> <li>Check examples/ for production-ready scripts</li> </ul>"},{"location":"notebooks/#contributing","title":"\ud83e\udd1d Contributing","text":"<p>Found a bug? Have suggestions for new notebooks?</p> <ol> <li>Open an issue: GitHub Issues</li> <li>Suggest topics: What models/techniques would help your research?</li> <li>Share your notebooks: Built your own? We'd love to feature them!</li> </ol> <p>Ready to start? Clone the repo and explore <code>/notebooks/</code>! \ud83d\ude80</p>"},{"location":"optimization/weighted_loss_functions/","title":"Implementing Weighted Loss Functions","text":""},{"location":"optimization/weighted_loss_functions/#overview","title":"Overview","text":"<p>Weighted loss functions of the form:</p> \\[ \\mathcal{L}(\\theta) = \\mathbb{E}\\left[\\lambda(x) \\cdot \\ell(\\theta; x)\\right] = \\sum_{i} \\lambda_i \\cdot \\ell(\\theta; x_i) \\] <p>appear throughout machine learning. This document explains how to implement them effectively.</p>"},{"location":"optimization/weighted_loss_functions/#back-references","title":"Back References","text":"<p>This document was motivated by studying the diffusion model training loss:</p> <ul> <li>Source: <code>notebooks/diffusion/02_sde_formulation/supplements/03_training_loss_and_denoising.md</code></li> </ul> <p>The weighted loss in diffusion models is:</p> \\[ \\mathcal{L}(\\theta) = \\mathbb{E}_{t, x_0, \\varepsilon} \\left[\\lambda(t) \\left\\| s_\\theta(x_t, t) - \\nabla_x \\log p_t(x_t \\mid x_0) \\right\\|^2\\right] \\]"},{"location":"optimization/weighted_loss_functions/#common-examples-of-weighted-losses","title":"Common Examples of Weighted Losses","text":"Algorithm Loss Function Weight \\(\\lambda\\) Diffusion Models \\(\\lambda(t) \\|s_\\theta - \\text{score}\\|^2\\) Time-dependent, often \\(\\lambda(t) = \\sigma_t^2\\) Weighted Least Squares \\(\\sum_i w_i (y_i - \\hat{y}_i)^2\\) Inverse variance or importance GloVe \\(\\sum_{i,j} f(X_{ij})(w_i^T \\tilde{w}_j - \\log X_{ij})^2\\) Co-occurrence frequency cap Importance Sampling \\(\\frac{1}{n}\\sum_i \\frac{p(x_i)}{q(x_i)} \\ell(x_i)\\) Likelihood ratio Focal Loss \\((1-p_t)^\\gamma \\cdot \\text{CE}\\) Down-weight easy examples Class-Weighted CE \\(\\sum_c w_c \\cdot \\text{CE}_c\\) Inverse class frequency"},{"location":"optimization/weighted_loss_functions/#implementation-strategies","title":"Implementation Strategies","text":""},{"location":"optimization/weighted_loss_functions/#strategy-1-direct-weighting-most-common","title":"Strategy 1: Direct Weighting (Most Common)","text":"<p>Multiply the per-sample loss by the weight before reduction.</p> <pre><code>import torch\nimport torch.nn.functional as F\n\ndef weighted_mse_loss(pred, target, weights):\n    \"\"\"\n    Weighted mean squared error loss.\n\n    Args:\n        pred: Predictions, shape (batch_size, ...)\n        target: Targets, shape (batch_size, ...)\n        weights: Per-sample weights, shape (batch_size,) or (batch_size, 1, ...)\n\n    Returns:\n        Scalar loss\n    \"\"\"\n    # Compute per-sample squared error\n    squared_error = (pred - target) ** 2  # (batch_size, ...)\n\n    # Reduce over non-batch dimensions\n    per_sample_loss = squared_error.mean(dim=tuple(range(1, squared_error.dim())))  # (batch_size,)\n\n    # Apply weights\n    weighted_loss = weights * per_sample_loss  # (batch_size,)\n\n    # Final reduction\n    return weighted_loss.mean()\n</code></pre>"},{"location":"optimization/weighted_loss_functions/#strategy-2-weighting-before-reduction","title":"Strategy 2: Weighting Before Reduction","text":"<p>For more control, weight before any reduction:</p> <pre><code>def weighted_mse_loss_v2(pred, target, weights):\n    \"\"\"\n    Alternative: weight each element before any reduction.\n    Useful when weights vary across dimensions (e.g., per-pixel weights).\n    \"\"\"\n    squared_error = (pred - target) ** 2\n\n    # Expand weights to match error shape if needed\n    if weights.dim() &lt; squared_error.dim():\n        weights = weights.view(-1, *([1] * (squared_error.dim() - 1)))\n\n    weighted_error = weights * squared_error\n    return weighted_error.mean()\n</code></pre>"},{"location":"optimization/weighted_loss_functions/#strategy-3-using-reductionnone","title":"Strategy 3: Using <code>reduction='none'</code>","text":"<p>PyTorch's built-in losses with <code>reduction='none'</code> make this easy:</p> <pre><code>def weighted_cross_entropy(logits, labels, weights):\n    \"\"\"\n    Weighted cross-entropy using built-in loss with reduction='none'.\n    \"\"\"\n    # Get per-sample loss\n    per_sample_loss = F.cross_entropy(logits, labels, reduction='none')  # (batch_size,)\n\n    # Apply weights and reduce\n    return (weights * per_sample_loss).mean()\n</code></pre>"},{"location":"optimization/weighted_loss_functions/#diffusion-model-implementation","title":"Diffusion Model Implementation","text":"<p>Here's how the weighted diffusion loss is typically implemented:</p> <pre><code>import torch\nimport torch.nn as nn\n\nclass DiffusionLoss(nn.Module):\n    def __init__(self, weighting='sigma_squared'):\n        super().__init__()\n        self.weighting = weighting\n\n    def get_weights(self, t, sigma_t):\n        \"\"\"\n        Compute time-dependent weights.\n\n        Args:\n            t: Timesteps, shape (batch_size,)\n            sigma_t: Noise std at each timestep, shape (batch_size,)\n        \"\"\"\n        if self.weighting == 'sigma_squared':\n            # Standard weighting: \u03bb(t) = \u03c3_t\u00b2\n            return sigma_t ** 2\n        elif self.weighting == 'uniform':\n            # Uniform weighting: \u03bb(t) = 1\n            return torch.ones_like(t)\n        elif self.weighting == 'snr':\n            # Signal-to-noise ratio weighting\n            alpha_t = torch.sqrt(1 - sigma_t ** 2)\n            snr = (alpha_t / sigma_t) ** 2\n            return snr\n        elif self.weighting == 'min_snr':\n            # Min-SNR weighting (Hang et al., 2023)\n            alpha_t = torch.sqrt(1 - sigma_t ** 2)\n            snr = (alpha_t / sigma_t) ** 2\n            return torch.clamp(snr, max=5.0)  # Clamp to max SNR of 5\n        else:\n            raise ValueError(f\"Unknown weighting: {self.weighting}\")\n\n    def forward(self, score_pred, score_target, t, sigma_t):\n        \"\"\"\n        Compute weighted diffusion loss.\n\n        Args:\n            score_pred: Network prediction, shape (batch_size, d)\n            score_target: True score, shape (batch_size, d)\n            t: Timesteps, shape (batch_size,)\n            sigma_t: Noise std, shape (batch_size,)\n\n        Returns:\n            Scalar loss\n        \"\"\"\n        # Compute per-sample MSE\n        squared_error = (score_pred - score_target) ** 2  # (batch_size, d)\n        per_sample_mse = squared_error.mean(dim=-1)  # (batch_size,)\n\n        # Get weights\n        weights = self.get_weights(t, sigma_t)  # (batch_size,)\n\n        # Weighted mean\n        loss = (weights * per_sample_mse).mean()\n\n        return loss\n</code></pre>"},{"location":"optimization/weighted_loss_functions/#full-training-loop-example","title":"Full Training Loop Example","text":"<pre><code>def train_step(model, optimizer, x_0, noise_schedule):\n    \"\"\"\n    Single training step for diffusion model.\n    \"\"\"\n    batch_size = x_0.shape[0]\n    device = x_0.device\n\n    # 1. Sample random timesteps\n    t = torch.rand(batch_size, device=device)  # Uniform(0, 1)\n\n    # 2. Get noise schedule values\n    alpha_t = noise_schedule.alpha(t)  # (batch_size,)\n    sigma_t = noise_schedule.sigma(t)  # (batch_size,)\n\n    # Reshape for broadcasting\n    alpha_t = alpha_t.view(-1, 1)  # (batch_size, 1)\n    sigma_t_expanded = sigma_t.view(-1, 1)  # (batch_size, 1)\n\n    # 3. Sample noise\n    epsilon = torch.randn_like(x_0)  # (batch_size, d)\n\n    # 4. Create noisy data\n    x_t = alpha_t * x_0 + sigma_t_expanded * epsilon  # (batch_size, d)\n\n    # 5. Predict score\n    score_pred = model(x_t, t)  # (batch_size, d)\n\n    # 6. Compute target score\n    score_target = -epsilon / sigma_t_expanded  # (batch_size, d)\n\n    # 7. Compute weighted loss\n    weights = sigma_t ** 2  # (batch_size,)\n    squared_error = (score_pred - score_target) ** 2\n    per_sample_loss = squared_error.mean(dim=-1)  # (batch_size,)\n    loss = (weights * per_sample_loss).mean()\n\n    # 8. Backprop\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    return loss.item()\n</code></pre>"},{"location":"optimization/weighted_loss_functions/#glove-implementation","title":"GloVe Implementation","text":"<p>GloVe uses a capped weighting function:</p> \\[ f(X_{ij}) = \\begin{cases}  (X_{ij}/x_{\\max})^\\alpha &amp; \\text{if } X_{ij} &lt; x_{\\max} \\\\ 1 &amp; \\text{otherwise} \\end{cases} \\] <pre><code>def glove_weight(cooccurrence, x_max=100, alpha=0.75):\n    \"\"\"\n    GloVe weighting function.\n\n    Args:\n        cooccurrence: Co-occurrence counts X_ij\n        x_max: Maximum co-occurrence for full weight\n        alpha: Exponent (typically 0.75)\n    \"\"\"\n    return torch.where(\n        cooccurrence &lt; x_max,\n        (cooccurrence / x_max) ** alpha,\n        torch.ones_like(cooccurrence)\n    )\n\ndef glove_loss(word_vectors, context_vectors, biases_w, biases_c, cooccurrence):\n    \"\"\"\n    GloVe loss function.\n\n    Args:\n        word_vectors: (vocab_size, embedding_dim)\n        context_vectors: (vocab_size, embedding_dim)\n        biases_w, biases_c: (vocab_size,)\n        cooccurrence: Sparse matrix of co-occurrence counts\n    \"\"\"\n    # Get non-zero entries\n    i, j = cooccurrence.nonzero(as_tuple=True)\n    X_ij = cooccurrence[i, j]\n\n    # Compute predictions\n    dot_products = (word_vectors[i] * context_vectors[j]).sum(dim=-1)\n    predictions = dot_products + biases_w[i] + biases_c[j]\n\n    # Compute targets\n    targets = torch.log(X_ij)\n\n    # Compute weights\n    weights = glove_weight(X_ij)\n\n    # Weighted squared error\n    squared_error = (predictions - targets) ** 2\n    weighted_loss = (weights * squared_error).mean()\n\n    return weighted_loss\n</code></pre>"},{"location":"optimization/weighted_loss_functions/#weighted-least-squares-implementation","title":"Weighted Least Squares Implementation","text":"<pre><code>import numpy as np\nfrom scipy import linalg\n\ndef weighted_least_squares(X, y, weights):\n    \"\"\"\n    Solve weighted least squares: min_\u03b2 \u03a3 w_i (y_i - X_i \u03b2)\u00b2\n\n    Closed-form solution: \u03b2 = (X^T W X)^{-1} X^T W y\n\n    Args:\n        X: Design matrix (n, p)\n        y: Target (n,)\n        weights: Weights (n,)\n\n    Returns:\n        beta: Coefficients (p,)\n    \"\"\"\n    W = np.diag(weights)\n\n    # Normal equations with weights\n    XtWX = X.T @ W @ X\n    XtWy = X.T @ W @ y\n\n    # Solve\n    beta = linalg.solve(XtWX, XtWy)\n\n    return beta\n\n# PyTorch version for gradient-based optimization\ndef weighted_mse_regression(X, y, weights, beta):\n    \"\"\"\n    Compute weighted MSE for linear regression.\n    \"\"\"\n    predictions = X @ beta\n    residuals = y - predictions\n    weighted_squared_residuals = weights * (residuals ** 2)\n    return weighted_squared_residuals.mean()\n</code></pre>"},{"location":"optimization/weighted_loss_functions/#key-implementation-considerations","title":"Key Implementation Considerations","text":""},{"location":"optimization/weighted_loss_functions/#1-weight-normalization","title":"1. Weight Normalization","text":"<p>Should weights sum to 1, or be unnormalized?</p> <pre><code># Normalized weights (sum to 1)\nweights_normalized = weights / weights.sum()\nloss = (weights_normalized * per_sample_loss).sum()  # Note: .sum() not .mean()\n\n# Unnormalized weights (more common in practice)\nloss = (weights * per_sample_loss).mean()\n</code></pre> <p>When to normalize:</p> <ul> <li>Importance sampling (to get unbiased estimates)</li> <li>When comparing losses across different batches</li> </ul> <p>When not to normalize:</p> <ul> <li>Most neural network training (weights are relative, not absolute)</li> <li>When using <code>.mean()</code> for final reduction</li> </ul>"},{"location":"optimization/weighted_loss_functions/#2-numerical-stability","title":"2. Numerical Stability","text":"<pre><code>def stable_weighted_loss(pred, target, weights, eps=1e-8):\n    \"\"\"\n    Numerically stable weighted loss.\n    \"\"\"\n    # Clamp weights to avoid extreme values\n    weights = torch.clamp(weights, min=eps, max=1e6)\n\n    per_sample_loss = ((pred - target) ** 2).mean(dim=-1)\n\n    # Use log-sum-exp for very large weight ranges\n    # (usually not needed, but good for importance sampling)\n\n    return (weights * per_sample_loss).mean()\n</code></pre>"},{"location":"optimization/weighted_loss_functions/#3-gradient-flow","title":"3. Gradient Flow","text":"<p>Weights can be: - Constants: No gradient flows through weights - Learned: Gradients flow, weights are updated - Computed from data: May or may not want gradients</p> <pre><code># Weights as constants (most common)\nwith torch.no_grad():\n    weights = compute_weights(t, sigma_t)\nloss = (weights * per_sample_loss).mean()\n\n# Weights with gradients (learned weighting)\nweights = weight_network(t)  # Learnable\nloss = (weights * per_sample_loss).mean()\n\n# Detach if weights depend on model but shouldn't affect gradients\nweights = compute_weights(model_output.detach())\n</code></pre>"},{"location":"optimization/weighted_loss_functions/#4-batch-statistics","title":"4. Batch Statistics","text":"<p>Be careful when weights affect batch statistics:</p> <pre><code># Weighted mean (correct)\nweighted_mean = (weights * values).sum() / weights.sum()\n\n# Weighted variance\nweighted_var = (weights * (values - weighted_mean) ** 2).sum() / weights.sum()\n</code></pre>"},{"location":"optimization/weighted_loss_functions/#why-different-weightings","title":"Why Different Weightings?","text":""},{"location":"optimization/weighted_loss_functions/#diffusion-models-lambdat-sigma_t2","title":"Diffusion Models: \\(\\lambda(t) = \\sigma_t^2\\)","text":"<p>Problem: At high noise levels, the score magnitude is smaller (\\(\\propto 1/\\sigma_t\\)), so the MSE is naturally smaller.</p> <p>Solution: Weight by \\(\\sigma_t^2\\) to balance the loss across all noise levels. This ensures the network learns equally well at all timesteps.</p>"},{"location":"optimization/weighted_loss_functions/#glove-capped-frequency-weighting","title":"GloVe: Capped Frequency Weighting","text":"<p>Problem: Common word pairs (e.g., \"the, of\") have huge co-occurrence counts and would dominate the loss.</p> <p>Solution: Cap the weight so frequent pairs don't overwhelm rare but informative pairs.</p>"},{"location":"optimization/weighted_loss_functions/#weighted-least-squares-inverse-variance","title":"Weighted Least Squares: Inverse Variance","text":"<p>Problem: Some observations are more reliable than others.</p> <p>Solution: Weight by inverse variance \\(w_i = 1/\\sigma_i^2\\) so unreliable observations contribute less.</p>"},{"location":"optimization/weighted_loss_functions/#summary","title":"Summary","text":"Aspect Implementation Basic pattern <code>loss = (weights * per_sample_loss).mean()</code> PyTorch built-ins Use <code>reduction='none'</code>, then weight manually Gradient flow Usually <code>torch.no_grad()</code> for weight computation Normalization Usually not needed with <code>.mean()</code> Numerical stability Clamp weights to reasonable range"},{"location":"optimization/weighted_loss_functions/#references","title":"References","text":"<ul> <li>Nichol &amp; Dhariwal (2021): \"Improved Denoising Diffusion Probabilistic Models\" \u2014 Analysis of loss weighting</li> <li>Pennington et al. (2014): \"GloVe: Global Vectors for Word Representation\"</li> <li>Carroll &amp; Ruppert (1988): \"Transformation and Weighting in Regression\"</li> <li>Hang et al. (2023): \"Efficient Diffusion Training via Min-SNR Weighting\"</li> </ul>"},{"location":"runpods/project_setup_on_new_pod/","title":"genai-lab Setup on RunPods (using A40 as an example)","text":"<p>Complete step-by-step guide for setting up the <code>genai-lab</code> environment on a fresh RunPods GPU pod.</p> <p>GPU: NVIDIA A40 (48GB VRAM) - recommended for full training Base Image: Typically Ubuntu with CUDA pre-installed Working Directory: <code>/workspace/</code></p>"},{"location":"runpods/project_setup_on_new_pod/#prerequisites","title":"Prerequisites","text":"<ul> <li>RunPods account with GPU pod deployed (A40, A100, or similar)</li> <li>SSH or web terminal access to the pod</li> <li>GitHub access configured (for cloning private repos)</li> </ul>"},{"location":"runpods/project_setup_on_new_pod/#step-1-install-miniforge-mamba-conda","title":"Step 1: Install Miniforge (Mamba + Conda)","text":"<p>Miniforge provides both <code>mamba</code> and <code>conda</code>. Mamba is a faster drop-in replacement for conda.</p> <pre><code># Navigate to a temporary location\ncd /tmp\n\n# Download the latest Miniforge installer\nwget \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\n\n# Run the installer (non-interactive mode)\nbash Miniforge3-$(uname)-$(uname -m).sh -b -p ~/miniforge3\n\n# Clean up installer\nrm Miniforge3-$(uname)-$(uname -m).sh\n</code></pre>"},{"location":"runpods/project_setup_on_new_pod/#step-2-initialize-shell-for-condamamba","title":"Step 2: Initialize Shell for Conda/Mamba","text":"<p>This step is critical \u2014 without it, <code>mamba activate</code> will fail.</p> <pre><code># Initialize conda for bash\n~/miniforge3/bin/conda init bash\n\n# Reload shell configuration\nsource ~/.bashrc\n</code></pre> <p>Verify installation:</p> <pre><code>mamba --version\nconda --version\n</code></pre> <p>You should see version numbers for both.</p>"},{"location":"runpods/project_setup_on_new_pod/#step-3-optional-disable-auto-activation-of-base","title":"Step 3: (Optional) Disable Auto-Activation of Base","text":"<p>If you don't want <code>(base)</code> to activate automatically on every login:</p> <pre><code>conda config --set auto_activate_base false\n</code></pre>"},{"location":"runpods/project_setup_on_new_pod/#step-4-clone-the-genai-lab-repository","title":"Step 4: Clone the genai-lab Repository","text":"<pre><code>cd /workspace\n\n# Clone via HTTPS (if no SSH key configured)\ngit clone https://github.com/YOUR_USERNAME/genai-lab.git\n\n# OR clone via SSH (if SSH key is configured)\ngit clone git@github.com:YOUR_USERNAME/genai-lab.git\n\ncd genai-lab\n</code></pre> <p>Note: Replace <code>YOUR_USERNAME</code> with your actual GitHub username.</p>"},{"location":"runpods/project_setup_on_new_pod/#step-5-create-the-genai-lab-environment","title":"Step 5: Create the genai-lab Environment","text":"<pre><code>cd /workspace/genai-lab\n\n# Create environment from environment.yml\nmamba env create -f environment.yml\n</code></pre> <p>This will:</p> <ul> <li>Create a conda environment named <code>genailab</code></li> <li>Install Python 3.10+</li> <li>Install PyTorch with CUDA support</li> <li>Install diffusion model dependencies (tqdm, scipy, etc.)</li> <li>Install bio dependencies (scanpy, anndata) if specified</li> </ul> <p>Expected time: 3-10 minutes depending on network speed.</p>"},{"location":"runpods/project_setup_on_new_pod/#step-6-activate-the-environment","title":"Step 6: Activate the Environment","text":"<pre><code>mamba activate genailab\n</code></pre> <p>Note: If you encounter a shell initialization error, run:</p> <pre><code>/root/miniforge3/bin/mamba shell init --shell bash --root-prefix=/root/miniforge3\nsource ~/.bashrc\nmamba activate genailab\n</code></pre> <p>Verify activation:</p> <pre><code># Should show the genailab environment path\nwhich python\n\n# Should show Python 3.10+\npython --version\n\n# Verify PyTorch and CUDA\npython -c \"import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}'); print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \\\"N/A\\\"}')\"\n</code></pre>"},{"location":"runpods/project_setup_on_new_pod/#step-7-install-genai-lab-in-editable-mode","title":"Step 7: Install genai-lab in Editable Mode","text":"<pre><code>cd /workspace/genai-lab\n\n# Install with pip in editable mode\npip install -e .\n\n# Or if using poetry\n# poetry install\n</code></pre>"},{"location":"runpods/project_setup_on_new_pod/#step-8-register-jupyter-kernel-for-notebooks","title":"Step 8: Register Jupyter Kernel (for Notebooks)","text":"<p>This allows VSCode and Jupyter to detect the <code>genailab</code> environment:</p> <pre><code>python -m ipykernel install --user --name genailab --display-name \"Python (genailab)\"\n</code></pre>"},{"location":"runpods/project_setup_on_new_pod/#step-9-verify-setup","title":"Step 9: Verify Setup","text":"<pre><code># Check key imports\npython -c \"from genailab.diffusion import VPSDE, UNet2D; print('diffusion OK')\"\npython -c \"from genailab import get_config, get_device; print('config OK')\"\npython -c \"import torch; print(f'CUDA: {torch.cuda.is_available()}')\"\n\n# Run a quick test with the driver script\npython scripts/diffusion/03_medical_imaging_diffusion.py --preset tiny --epochs 5\n</code></pre>"},{"location":"runpods/project_setup_on_new_pod/#quick-reference-commands","title":"Quick Reference Commands","text":"Task Command Activate environment <code>mamba activate genailab</code> Deactivate environment <code>mamba deactivate</code> List environments <code>mamba env list</code> Update environment <code>mamba env update -f environment.yml</code> Remove environment <code>mamba env remove -n genailab</code> Check GPU <code>nvidia-smi</code> Check PyTorch GPU <code>python -c \"import torch; print(torch.cuda.is_available())\"</code>"},{"location":"runpods/project_setup_on_new_pod/#running-training-scripts","title":"Running Training Scripts","text":""},{"location":"runpods/project_setup_on_new_pod/#quick-test-verify-logic","title":"Quick Test (verify logic)","text":"<pre><code>python scripts/diffusion/03_medical_imaging_diffusion.py --preset tiny --epochs 10\n</code></pre>"},{"location":"runpods/project_setup_on_new_pod/#full-training-on-a40","title":"Full Training on A40","text":"<pre><code>python scripts/diffusion/03_medical_imaging_diffusion.py \\\n    --preset large \\\n    --epochs 5000 \\\n    --sample \\\n    --experiment-name \"xray_a40_run1\"\n</code></pre>"},{"location":"runpods/project_setup_on_new_pod/#custom-configuration","title":"Custom Configuration","text":"<pre><code>python scripts/diffusion/03_medical_imaging_diffusion.py \\\n    --base-channels 64 \\\n    --channel-mults 1,2,4,8 \\\n    --num-res-blocks 2 \\\n    --img-size 128 \\\n    --epochs 10000 \\\n    --batch-size 32 \\\n    --sample\n</code></pre>"},{"location":"runpods/project_setup_on_new_pod/#list-available-presets","title":"List Available Presets","text":"<pre><code>python scripts/diffusion/03_medical_imaging_diffusion.py --list-presets\n</code></pre>"},{"location":"runpods/project_setup_on_new_pod/#troubleshooting","title":"Troubleshooting","text":""},{"location":"runpods/project_setup_on_new_pod/#mamba-activate-fails-with-shell-initialization-error","title":"\"mamba activate\" fails with shell initialization error","text":"<p>Run:</p> <pre><code>eval \"$(mamba shell hook --shell bash)\"\nmamba activate genailab\n</code></pre> <p>Or re-initialize:</p> <pre><code>~/miniforge3/bin/conda init bash\nsource ~/.bashrc\n</code></pre>"},{"location":"runpods/project_setup_on_new_pod/#vscode-doesnt-detect-the-environment","title":"VSCode doesn't detect the environment","text":"<ol> <li>Press <code>Ctrl+Shift+P</code> \u2192 \"Python: Select Interpreter\"</li> <li>Enter path: <code>~/miniforge3/envs/genailab/bin/python</code></li> </ol> <p>Or reload VSCode: <code>Ctrl+Shift+P</code> \u2192 \"Developer: Reload Window\"</p>"},{"location":"runpods/project_setup_on_new_pod/#cuda-not-available-in-pytorch","title":"CUDA not available in PyTorch","text":"<p>Verify CUDA is installed on the pod:</p> <pre><code>nvidia-smi\nnvcc --version\n</code></pre> <p>If PyTorch was installed without CUDA support, reinstall:</p> <pre><code>mamba activate genailab\nmamba install pytorch pytorch-cuda=12.1 -c pytorch -c nvidia\n</code></pre>"},{"location":"runpods/project_setup_on_new_pod/#out-of-memory-oom-during-training","title":"Out of Memory (OOM) during training","text":"<p>Reduce model size or batch size:</p> <pre><code># Use smaller preset\npython scripts/diffusion/03_medical_imaging_diffusion.py --preset small\n\n# Or reduce batch size\npython scripts/diffusion/03_medical_imaging_diffusion.py --preset medium --batch-size 8\n</code></pre>"},{"location":"runpods/project_setup_on_new_pod/#one-liner-setup-script","title":"One-Liner Setup Script","text":"<p>For convenience, save this as <code>/workspace/setup_genailab.sh</code>:</p> <pre><code>#!/bin/bash\nset -e\n\necho \"=== Step 1: Installing Miniforge ===\"\ncd /tmp\nwget -q \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh -b -p ~/miniforge3\nrm Miniforge3-$(uname)-$(uname -m).sh\n\necho \"=== Step 2: Initializing shell ===\"\n~/miniforge3/bin/conda init bash\nsource ~/.bashrc\n\necho \"=== Step 3: Disabling base auto-activation ===\"\nconda config --set auto_activate_base false\n\necho \"=== Step 4: Cloning genai-lab ===\"\ncd /workspace\ngit clone https://github.com/YOUR_USERNAME/genai-lab.git || echo \"Repo already exists\"\ncd genai-lab\n\necho \"=== Step 5: Creating environment ===\"\neval \"$(mamba shell hook --shell bash)\"\nmamba env create -f environment.yml\n\necho \"=== Step 6: Activating and installing ===\"\nmamba activate genailab\npip install -e .\n\necho \"=== Step 7: Registering Jupyter kernel ===\"\npython -m ipykernel install --user --name genailab --display-name \"Python (genailab)\"\n\necho \"=== Step 8: Verifying setup ===\"\npython -c \"from genailab.diffusion import VPSDE, UNet2D; print('\u2713 genailab imports OK')\"\npython -c \"import torch; print(f'\u2713 CUDA available: {torch.cuda.is_available()}')\"\n\necho \"=== Setup Complete ===\"\necho \"Run 'mamba activate genailab' to activate the environment.\"\n</code></pre> <p>Run with: <code>bash setup_genailab.sh</code></p>"},{"location":"runpods/project_setup_on_new_pod/#setting-up-ssh-connection-for-github","title":"Setting Up SSH Connection for GitHub","text":"<p>To use <code>git push</code> and <code>git pull</code> without entering credentials, configure SSH authentication.</p>"},{"location":"runpods/project_setup_on_new_pod/#1-generate-ssh-key-on-the-pod","title":"1. Generate SSH Key on the Pod","text":"<pre><code>ssh-keygen -t ed25519 -C \"runpod-genailab-key\"\n</code></pre> <p>This creates: - Private key: <code>/root/.ssh/id_ed25519</code> - Public key: <code>/root/.ssh/id_ed25519.pub</code></p>"},{"location":"runpods/project_setup_on_new_pod/#2-retrieve-the-public-key","title":"2. Retrieve the Public Key","text":"<pre><code>cat /root/.ssh/id_ed25519.pub\n</code></pre> <p>Copy the entire output.</p>"},{"location":"runpods/project_setup_on_new_pod/#3-add-key-to-github","title":"3. Add Key to GitHub","text":"<ol> <li>Go to GitHub \u2192 Settings \u2192 SSH and GPG keys</li> <li>Click New SSH key</li> <li>Paste the public key and save</li> </ol>"},{"location":"runpods/project_setup_on_new_pod/#4-test-connection","title":"4. Test Connection","text":"<pre><code>ssh -T git@github.com\n</code></pre>"},{"location":"runpods/project_setup_on_new_pod/#5-configure-git-identity","title":"5. Configure Git Identity","text":"<pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n</code></pre>"},{"location":"runpods/project_setup_on_new_pod/#uploading-datasets","title":"Uploading Datasets","text":""},{"location":"runpods/project_setup_on_new_pod/#option-a-git-lfs-for-smaller-datasets-2gb","title":"Option A: Git LFS (for smaller datasets &lt; 2GB)","text":"<pre><code># Install git-lfs if not present\napt-get install git-lfs\ngit lfs install\n\n# Track large files\ngit lfs track \"*.npy\"\ngit lfs track \"*.h5\"\ngit lfs track \"data/**\"\n</code></pre>"},{"location":"runpods/project_setup_on_new_pod/#option-b-rsyncscp-for-larger-datasets","title":"Option B: rsync/scp (for larger datasets)","text":"<p>From your local machine:</p> <pre><code># Get pod IP and port from RunPods dashboard\nrsync -avz --progress ./data/ root@POD_IP:/workspace/genai-lab/data/ -e \"ssh -p PORT\"\n</code></pre>"},{"location":"runpods/project_setup_on_new_pod/#option-c-cloud-storage-s3-gcs","title":"Option C: Cloud Storage (S3, GCS)","text":"<pre><code># Install AWS CLI\npip install awscli\n\n# Configure credentials\naws configure\n\n# Sync data\naws s3 sync s3://your-bucket/data/ /workspace/genai-lab/data/\n</code></pre>"},{"location":"runpods/project_setup_on_new_pod/#option-d-hugging-face-datasets","title":"Option D: Hugging Face Datasets","text":"<pre><code>from datasets import load_dataset\n\n# Download directly to pod\ndataset = load_dataset(\"your-dataset-name\")\n</code></pre>"},{"location":"runpods/project_setup_on_new_pod/#model-size-reference","title":"Model Size Reference","text":"Preset Parameters VRAM (est.) Recommended GPU tiny ~1M &lt;2GB Any (CPU OK) small ~5M ~4GB T4, RTX 3060 medium ~20M ~8GB RTX 3080, A10 large ~50M ~16GB A40, A100"},{"location":"runpods/project_setup_on_new_pod/#related-documents","title":"Related Documents","text":"<ul> <li>INSTALL.md - Local installation guide</li> <li>SETUP.md - Detailed configuration documentation</li> </ul>"},{"location":"runpods/project_setup_on_new_pod/#appendix-local-ssh-configuration","title":"Appendix: Local SSH Configuration","text":"<p>Before connecting to a RunPod instance, configure SSH on your local machine for seamless access.</p>"},{"location":"runpods/project_setup_on_new_pod/#step-1-create-or-edit-ssh-config","title":"Step 1: Create or Edit SSH Config","text":"<p>Open <code>~/.ssh/config</code> on your local machine and add an entry for genai-lab:</p> <pre><code># RunPod Instance for genai-lab GPU Training\n# Created: YYYY-MM-DD\n# Instance ID: [copy from RunPods console]\n# GPU: [A40 48GB / A100 80GB / RTX 4090 24GB / etc.]\n# Purpose: Diffusion model training, gene expression generation\nHost runpod-genai\n    # \u26a0\ufe0f UPDATE: Copy IP from \"SSH over exposed TCP\" section in RunPods console\n    # Example: ssh root@69.30.85.30 -p 22084 -i ~/.ssh/id_ed25519 \n    HostName 69.30.85.30\n\n    # \u26a0\ufe0f UPDATE: Copy Port from \"SSH over exposed TCP\" section\n    Port 22084\n\n    # RunPods instances use root user\n    User root\n\n    # Use your SSH key (generate with: ssh-keygen -t ed25519)\n    IdentityFile ~/.ssh/id_ed25519\n\n    # Skip host key checking (RunPods IPs change frequently)\n    StrictHostKeyChecking no\n    UserKnownHostsFile /dev/null\n\n    # Keep connection alive (prevents disconnections during long training)\n    ServerAliveInterval 60\n    ServerAliveCountMax 5\n\n    # Optional: Compression for faster file transfers\n    Compression yes\n</code></pre>"},{"location":"runpods/project_setup_on_new_pod/#step-2-update-config-when-pod-restarts","title":"Step 2: Update Config When Pod Restarts","text":"<p>Each time you start a new pod (or restart an existing one), the IP and port may change:</p> <ol> <li>Go to RunPods console \u2192 Your pod \u2192 Connect \u2192 SSH over exposed TCP</li> <li>Copy the new <code>HostName</code> (IP) and <code>Port</code> values</li> <li>Update your <code>~/.ssh/config</code> entry</li> </ol>"},{"location":"runpods/project_setup_on_new_pod/#step-3-connect","title":"Step 3: Connect","text":"<pre><code># Simple connection\nssh runpod-genai\n\n# With VS Code Remote SSH\ncode --remote ssh-remote+runpod-genai /workspace/genai-lab\n</code></pre>"},{"location":"runpods/project_setup_on_new_pod/#step-4-verify-connection","title":"Step 4: Verify Connection","text":"<pre><code># Check GPU\nnvidia-smi\n\n# Verify workspace\nls /workspace\n</code></pre>"},{"location":"runpods/project_setup_on_new_pod/#tips","title":"Tips","text":"<ul> <li>Multiple pods: Create separate entries (e.g., <code>runpod-genai-a40</code>, <code>runpod-genai-a100</code>)</li> <li>Port forwarding: Add <code>LocalForward 8888 localhost:8888</code> for Jupyter access</li> <li>File sync: Use <code>rsync -avz --progress ./data/ runpod-genai:/workspace/genai-lab/data/</code></li> </ul>"},{"location":"score_matching/","title":"Score Matching Documentation","text":"<p>This folder contains documentation on score matching techniques\u2014methods for training models with intractable normalizing constants by matching the gradient of the log-density (the \"score\") rather than the density itself.</p>"},{"location":"score_matching/#reading-order","title":"Reading Order","text":""},{"location":"score_matching/#foundations","title":"Foundations","text":"<ol> <li> <p>Score Matching: The Core Objective    Explains the fundamental score matching objective for training energy-based models. Covers why the score function bypasses the partition function \\(Z_\\theta\\), the explicit vs. tractable objectives, and the integration-by-parts trick.</p> </li> <li> <p>Fisher Score Matching for Likelihood-Free Inference    Tutorial walkthrough of \"Direct Fisher Score Estimation for Likelihood Maximization\" (Khoo et al., 2025). Extends score matching from data-space gradients to parameter-space gradients for simulation-based inference.</p> </li> </ol>"},{"location":"score_matching/#coming-soon","title":"Coming Soon","text":"<ol> <li>Denoising Score Matching \u2014 Practical variant using noisy data</li> <li>Sliced Score Matching \u2014 Scalable approximation for high dimensions</li> <li>Connection to Diffusion Models \u2014 How score matching underlies modern diffusion models</li> </ol>"},{"location":"score_matching/#practical-considerations-esm-vs-dsm","title":"Practical Considerations: ESM vs DSM","text":"<p>When implementing score matching for real applications (e.g., modeling gene expression data), you have two main options:</p> Method Objective When to Use Explicit SM (ESM) Squared norm + trace of Jacobian Low-dimensional data; need exact objective Denoising SM (DSM) Squared error to noise gradient High-dimensional data; practical default <p>Why DSM is often preferred:</p> <ul> <li>ESM requires computing \\(\\mathrm{tr}(\\nabla_x s_\\theta)\\), which costs \\(O(d)\\) backprop passes (or Hutchinson estimation)</li> <li>DSM only needs forward passes through the score network</li> <li>With Gaussian noise \\(\\tilde{x} = x + \\sigma\\epsilon\\), the target is analytic: \\(\\nabla_{\\tilde{x}} \\log p(\\tilde{x}|x) = -(\\tilde{x} - x)/\\sigma^2\\)</li> </ul> <p>Both learn the same thing: the Stein score \\(\\nabla_x \\log p(x)\\), just with different computational trade-offs.</p> <p>See Roadmap Stage 5 for implementation milestones.</p>"},{"location":"score_matching/#key-concepts","title":"Key Concepts","text":"Concept Symbol Description Stein score \\(s(x)\\) Gradient of log-density w.r.t. data Fisher score \\(g(\\theta)\\) Gradient of log-density w.r.t. parameters Energy function \\(E_\\theta(x)\\) Defines density via \\(p_\\theta(x) \\propto \\exp(-E_\\theta(x))\\) Partition function \\(Z_\\theta\\) Intractable normalizing constant"},{"location":"score_matching/#two-flavors-of-score-matching","title":"Two Flavors of Score Matching","text":"Method Estimates Use Case Original Score Matching \\(\\nabla_x \\log p_\\theta(x)\\) Training EBMs, diffusion models Fisher Score Matching Fisher score \\(\\nabla_\\theta \\log p\\) Simulation-based inference, likelihood-free MLE <p>Both use integration-by-parts to eliminate intractable terms.</p>"},{"location":"score_matching/#connection-to-other-topics","title":"Connection to Other Topics","text":"<ul> <li>EBMs: See <code>../EBM/</code> \u2014 Score matching is the primary training method for EBMs</li> <li>VAEs: See <code>../VAE/</code> \u2014 VAEs avoid the partition function via tractable encoder/decoder</li> <li>Diffusion Models: Build on denoising score matching across noise levels</li> </ul>"},{"location":"score_matching/#references","title":"References","text":"<ul> <li>Hyv\u00e4rinen (2005). Estimation of Non-Normalized Statistical Models by Score Matching</li> <li>Vincent (2011). A Connection Between Score Matching and Denoising Autoencoders</li> <li>Khoo et al. (2025). Direct Fisher Score Estimation for Likelihood Maximization</li> </ul>"},{"location":"score_matching/SM-core-objective/","title":"Score Matching: The Core Objective","text":"<p>This document explains the score matching objective\u2014a technique for training energy-based models without computing the intractable partition function. Score matching is foundational to modern generative models including diffusion models.</p>"},{"location":"score_matching/SM-core-objective/#1-what-problem-score-matching-solves","title":"1. What Problem Score Matching Solves","text":"<p>We want to learn a probability density over data \\(p_D(x)\\), but we only have samples \\(x \\sim p_D\\).</p> <p>This is the classic density estimation problem.</p> <p>The difficulty: Many flexible models define densities only up to a normalizing constant, which makes maximum likelihood hard or impossible.</p> <p>Score matching offers a workaround: Instead of matching the density itself, we match its score (the gradient of the log-density).</p>"},{"location":"score_matching/SM-core-objective/#2-the-modeling-setup-energy-based-models-ebms","title":"2. The Modeling Setup: Energy-Based Models (EBMs)","text":""},{"location":"score_matching/SM-core-objective/#21-data-space-and-variables","title":"2.1 Data space and variables","text":"<ul> <li>\\(x \\in \\mathbb{R}^d\\) \u2014 A data vector (e.g., an image flattened into pixels, a feature vector, etc.)</li> <li>\\(p_D(x)\\) \u2014 The true but unknown data-generating distribution</li> </ul>"},{"location":"score_matching/SM-core-objective/#22-model-density-via-an-energy-function","title":"2.2 Model density via an energy function","text":"<p>We model the data using an energy-based model:</p> \\[ p_\\theta(x) = \\frac{\\exp(-E_\\theta(x))}{Z_\\theta} \\] <p>Where:</p> <ul> <li>\\(E_\\theta(x) : \\mathbb{R}^d \\to \\mathbb{R}\\) \u2014 A scalar-valued energy function, typically a neural network</li> <li>\\(\\theta\\) \u2014 Model parameters</li> <li>\\(Z_\\theta = \\int \\exp(-E_\\theta(x)) \\, dx\\) \u2014 The partition function (normalizing constant)</li> </ul> <p>Key issue: \\(Z_\\theta\\) depends on \\(\\theta\\) and is usually intractable.</p>"},{"location":"score_matching/SM-core-objective/#3-the-score-function-the-central-object","title":"3. The Score Function: The Central Object","text":""},{"location":"score_matching/SM-core-objective/#31-definition","title":"3.1 Definition","text":"<p>The score function of a density is:</p> \\[ s_\\theta(x) := \\nabla_x \\log p_\\theta(x) \\] <p>This is a vector in \\(\\mathbb{R}^d\\).</p>"},{"location":"score_matching/SM-core-objective/#32-why-the-score-is-special","title":"3.2 Why the score is special","text":"<p>Let's expand it:</p> \\[ \\log p_\\theta(x) = -E_\\theta(x) - \\log Z_\\theta \\] <p>Taking gradient w.r.t. \\(x\\):</p> \\[ \\nabla_x \\log p_\\theta(x) = -\\nabla_x E_\\theta(x) \\] <p>Important observation:</p> <ul> <li>The normalizing constant \\(Z_\\theta\\) disappears</li> <li>The score depends only on the energy gradient</li> </ul> <p>This is the loophole score matching exploits.</p>"},{"location":"score_matching/SM-core-objective/#4-what-does-it-mean-to-match-scores","title":"4. What Does It Mean to \"Match Scores\"?","text":"<p>If two densities have the same score everywhere (under mild regularity conditions), then they are the same density up to a constant, which is exactly what we want.</p> <p>So instead of minimizing:</p> \\[ \\mathrm{KL}(p_D \\| p_\\theta) \\] <p>we try to make:</p> \\[ \\nabla_x \\log p_\\theta(x) \\approx \\nabla_x \\log p_D(x) \\]"},{"location":"score_matching/SM-core-objective/#5-the-explicit-score-matching-objective","title":"5. The Explicit Score Matching Objective","text":""},{"location":"score_matching/SM-core-objective/#51-the-ideal-but-infeasible-objective","title":"5.1 The ideal (but infeasible) objective","text":"<p>We start with the explicit score matching (ESM) loss:</p> \\[ \\mathcal{L}_{\\text{ESM}}(\\theta) = \\mathbb{E}_{x \\sim p_D(x)} \\left[ \\frac{1}{2} \\left| s_\\theta(x) - \\nabla_x \\log p_D(x) \\right|^2 \\right] \\] <p>Let's unpack every symbol.</p>"},{"location":"score_matching/SM-core-objective/#52-notation-breakdown","title":"5.2 Notation breakdown","text":"<ul> <li>\\(\\mathbb{E}_{x \\sim p_D(x)}[\\cdot]\\) \u2014 Expectation over true data samples</li> <li>\\(s_\\theta(x)\\) \u2014 Model score \\(= \\nabla_x \\log p_\\theta(x)\\)</li> <li>\\(\\nabla_x \\log p_D(x)\\) \u2014 True data score (unknown!)</li> <li>\\(|\\cdot|\\) \u2014 Euclidean norm</li> <li>Factor \\(\\frac{1}{2}\\) \u2014 For mathematical convenience</li> </ul>"},{"location":"score_matching/SM-core-objective/#53-why-this-objective-is-impossible-to-compute","title":"5.3 Why this objective is impossible to compute","text":"<p>We do not know \\(p_D(x)\\), so:</p> <ul> <li>We cannot compute \\(\\log p_D(x)\\)</li> <li>We cannot compute \\(\\nabla_x \\log p_D(x)\\)</li> </ul> <p>So this loss is conceptually useful but computationally useless.</p>"},{"location":"score_matching/SM-core-objective/#6-the-key-mathematical-trick-integration-by-parts","title":"6. The Key Mathematical Trick: Integration by Parts","text":"<p>Score matching transforms the explicit objective into one that does not involve \\(p_D\\).</p> <p>To do this, we introduce differential operators.</p>"},{"location":"score_matching/SM-core-objective/#7-differential-operators-and-notation","title":"7. Differential Operators and Notation","text":""},{"location":"score_matching/SM-core-objective/#71-gradient-operator","title":"7.1 Gradient operator","text":"<p>For a scalar function \\(f : \\mathbb{R}^d \\to \\mathbb{R}\\):</p> \\[ \\nabla_x f(x) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_d} \\end{pmatrix} \\]"},{"location":"score_matching/SM-core-objective/#72-jacobian-operator","title":"7.2 Jacobian operator","text":"<p>For a vector-valued function \\(f(x) = (f_1(x), \\dots, f_d(x))^\\top\\), the Jacobian matrix is:</p> \\[ J_x f(x) = \\left[ \\frac{\\partial f_i}{\\partial x_j} \\right]_{i,j} \\in \\mathbb{R}^{d \\times d} \\]"},{"location":"score_matching/SM-core-objective/#73-trace-operator","title":"7.3 Trace operator","text":"<p>For a square matrix \\(A\\):</p> \\[ \\mathrm{tr}(A) = \\sum_i A_{ii} \\]"},{"location":"score_matching/SM-core-objective/#8-the-tractable-score-matching-objective","title":"8. The Tractable Score Matching Objective","text":"<p>Using integration by parts (details omitted in the main text but standard), the explicit objective becomes:</p> \\[ \\mathcal{L}_{\\text{SM}}(\\theta) = \\mathbb{E}_{x \\sim p_D(x)} \\left[ \\frac{1}{2}|s_\\theta(x)|^2 + \\mathrm{tr}(J_x s_\\theta(x)) \\right] + \\text{const} \\]"},{"location":"score_matching/SM-core-objective/#9-why-this-works","title":"9. Why This Works","text":""},{"location":"score_matching/SM-core-objective/#91-what-disappeared","title":"9.1 What disappeared?","text":"<ul> <li>\\(\\nabla_x \\log p_D(x)\\) is gone</li> <li>Only \\(s_\\theta(x)\\) and its derivatives remain</li> </ul>"},{"location":"score_matching/SM-core-objective/#92-what-we-can-compute","title":"9.2 What we can compute","text":"<p>Both terms in the tractable objective are computable:</p> <ul> <li>\\(|s_\\theta(x)|^2 = |\\nabla_x \\log p_\\theta(x)|^2\\) \u2014 squared norm of the model score</li> <li>\\(\\mathrm{tr}(J_x s_\\theta(x))\\) \u2014 trace of the Jacobian (sum of second derivatives)</li> </ul> <p>The expectation is approximated by sampling from the data distribution.</p>"},{"location":"score_matching/SM-core-objective/#10-connection-to-ebms-and-beyond","title":"10. Connection to EBMs and Beyond","text":"<p>Score matching is the foundation for:</p> <ul> <li>Training EBMs without computing \\(Z_\\theta\\)</li> <li>Denoising score matching \u2014 a practical variant using noisy data</li> <li>Diffusion models \u2014 learn scores at multiple noise levels</li> <li>Fisher score matching \u2014 parameter-space analogue for simulation-based inference</li> </ul>"},{"location":"score_matching/SM-core-objective/#whats-next","title":"What's Next","text":"<p>See Fisher Score Matching for how these ideas extend to estimating gradients w.r.t. parameters (not data) in likelihood-free settings.</p>"},{"location":"score_matching/SM-fisher-score-matching/","title":"Fisher Score Matching for Likelihood-Free Inference","text":"<p>This document is a tutorial-style walkthrough of the key ideas from the paper \"Direct Fisher Score Estimation for Likelihood Maximization\" (Khoo et al., 2025), explaining how score matching ideas extend to parameter-space gradients for simulation-based inference.</p>"},{"location":"score_matching/SM-fisher-score-matching/#the-problem-implicit-simulators","title":"The Problem: Implicit Simulators","text":"<p>Many scientific models (biology, physics, cosmology, neuroscience, etc.) are implicit simulators:</p> <ul> <li>You can simulate data \\(x \\sim p(x|\\theta)\\)</li> <li>But you cannot evaluate the likelihood \\(p(x|\\theta)\\) or its gradient</li> </ul> <p>This is called Simulation-Based Inference (SBI).</p> <p>If you want to do maximum likelihood estimation (MLE), you need the Fisher score:</p> \\[ \\nabla_\\theta \\log p(x|\\theta) \\] <p>But this derivative is unavailable because the likelihood is unknown.</p>"},{"location":"score_matching/SM-fisher-score-matching/#main-idea-local-fisher-score-matching","title":"Main Idea: Local Fisher Score Matching","text":"<p>The authors propose Direct Fisher Score Estimation via a new method called Local Fisher Score Matching (FSM).</p> <p>FSM directly estimates the Fisher score using only:</p> <ul> <li>Samples from a local region around the current parameter \\(\\theta_t\\)</li> <li>A simple linear regression model</li> </ul> <p>No likelihoods, no densities, no gradients of the simulator are needed.</p> <p>This enables a gradient-based MLE method in fully likelihood-free settings.</p> <p>The method works sequentially:</p> <ol> <li>At parameter iterate \\(\\theta_t\\), draw nearby parameters from a Gaussian</li> <li>Simulate data at those parameters</li> <li>Fit a local surrogate model \\(S_W(x) \\approx \\nabla_\\theta \\log p(x|\\theta_t)\\)</li> <li>Use this surrogate to take a gradient step in \\(\\theta\\)</li> </ol>"},{"location":"score_matching/SM-fisher-score-matching/#why-score-matching","title":"Why Score Matching?","text":"<p>Score matching is a classical method for training energy-based models when the normalizing constant is intractable.</p> <p>The ordinary score matching objective is:</p> \\[ \\mathbb{E}_{x \\sim p_D} \\left[ \\frac{1}{2} |s_\\theta(x) - \\nabla_x \\log p_D(x)|^2 \\right] \\] <p>But FSM adapts the idea in a novel way:</p> Approach Differentiate w.r.t. Estimates Original score matching Data \\(x\\) Stein score \\(\\nabla_x \\log p_\\theta(x)\\) Fisher score matching Parameters \\(\\theta\\) Fisher score \\(\\nabla_\\theta \\log p_\\theta(x)\\) <p>This is non-standard and the main conceptual innovation of the paper.</p>"},{"location":"score_matching/SM-fisher-score-matching/#background-score-matching-section-21","title":"Background: Score Matching (Section 2.1)","text":"<p>Score matching solves density estimation without computing the normalizing constant \\(Z_\\theta\\).</p>"},{"location":"score_matching/SM-fisher-score-matching/#energy-based-model-ebm","title":"Energy-Based Model (EBM)","text":"\\[ p_\\theta(x) = \\frac{\\exp(-E_\\theta(x))}{Z_\\theta} \\] <p>The score is:</p> \\[ s_\\theta(x) = \\nabla_x \\log p_\\theta(x) = -\\nabla_x E_\\theta(x) \\]"},{"location":"score_matching/SM-fisher-score-matching/#the-explicit-score-matching-loss","title":"The explicit score matching loss","text":"\\[ L_{\\text{ESM}} = \\mathbb{E}_{x \\sim p_D} \\left[ \\frac{1}{2}|s_\\theta(x)|^2 + \\mathrm{tr}(\\nabla_x s_\\theta(x)) \\right] \\] <p>No need to compute \\(Z_\\theta\\)! But it still requires computing Jacobians/Hessians, which may be expensive.</p> <p>This background is crucial because FSM uses a parameter-space analogue of this trick.</p>"},{"location":"score_matching/SM-fisher-score-matching/#how-fsm-modifies-score-matching","title":"How FSM Modifies Score Matching","text":"<p>FSM wants to estimate:</p> \\[ g(x; \\theta_t) := \\nabla_\\theta \\log p(x|\\theta)\\big|_{\\theta=\\theta_t} \\] <p>But this is unknown.</p> <p>So the authors define a joint distribution over data and parameters:</p> <ul> <li>Sample parameters locally: \\(\\theta \\sim q(\\theta|\\theta_t) = \\mathcal{N}(\\theta_t, \\sigma^2 I)\\)</li> <li>For each sampled \\(\\theta\\), simulate data: \\(x \\sim p(x|\\theta)\\)</li> </ul> <p>This gives a simple joint distribution:</p> \\[ p(x, \\theta | \\theta_t) = p(x|\\theta) \\cdot q(\\theta|\\theta_t) \\] <p>Then define the local score-matching objective:</p> \\[ J(W; \\theta_t) = \\mathbb{E}_{x,\\theta} \\left[ |S_W(x) - \\nabla_\\theta \\log p(x|\\theta)|^2 \\right] \\tag{1} \\] <p>Here \\(S_W(x)\\) is a surrogate model for the Fisher score.</p> <p>But \\(\\nabla_\\theta \\log p(x|\\theta)\\) is unknown! So how do we optimize (1)?</p>"},{"location":"score_matching/SM-fisher-score-matching/#trick-1-integration-by-parts-removes-the-likelihood-term","title":"Trick #1: Integration by Parts Removes the Likelihood Term","text":"<p>(Section 3.1, Theorem 3.1)</p> <p>Through an integration-by-parts identity very similar to original score matching, the intractable term disappears:</p> \\[ J(W; \\theta_t) = \\mathbb{E} \\left[ |S_W(x)|^2 + 2 S_W(x)^\\top \\nabla_\\theta \\log q(\\theta|\\theta_t) \\right] \\tag{2} \\] <p>This is remarkable:</p> <ul> <li>The likelihood gradient vanishes entirely</li> <li>Only the proposal distribution's gradient remains:</li> </ul> \\[ \\nabla_\\theta \\log q(\\theta|\\theta_t) = -\\frac{1}{\\sigma^2}(\\theta - \\theta_t) \\] <p>Thus the entire objective is computable by simulation.</p>"},{"location":"score_matching/SM-fisher-score-matching/#what-is-the-optimal-solution-of-fsm","title":"What Is the Optimal Solution of FSM?","text":"<p>(Theorem 3.2)</p> \\[ S^*(x; \\theta_t) = \\mathbb{E}_{\\theta \\sim p(\\theta|x,\\theta_t)} \\left[ \\nabla_\\theta \\log p(x|\\theta) \\right] \\] <p>This is the Bayes estimator of the score under the local posterior:</p> \\[ p(\\theta|x,\\theta_t) \\propto p(x|\\theta) \\cdot q(\\theta|\\theta_t) \\] <p>Intuition:</p> <ul> <li>You can't estimate the true score at a single point \\(\\theta_t\\) because you never see data exactly at that point</li> <li>So FSM estimates a locally smoothed version of the Fisher score</li> </ul> <p>This becomes important in Section 5.</p>"},{"location":"score_matching/SM-fisher-score-matching/#trick-2-fsm-gradient-of-a-gaussian-smoothed-likelihood","title":"Trick #2: FSM = Gradient of a Gaussian-Smoothed Likelihood","text":"<p>(Section 5.1, Theorem 5.1)</p> <p>FSM is exactly the gradient of the smoothed likelihood:</p> \\[ \\tilde{\\ell}(\\theta_t; x) = \\log \\int p(x|\\theta) \\cdot q(\\theta|\\theta_t) \\, d\\theta \\] <p>And:</p> \\[ S^*(x; \\theta_t) = \\nabla_{\\theta_t} \\tilde{\\ell}(\\theta_t; x) \\] <p>Hence the algorithm is performing:</p> <p>Gradient descent on a locally smoothed likelihood, not on the raw likelihood.</p> <p>This explains:</p> <ul> <li>Robustness to non-smooth likelihoods</li> <li>Ability to escape flat regions</li> <li>Improved stability vs. finite-difference estimators</li> </ul>"},{"location":"score_matching/SM-fisher-score-matching/#practical-parameterization-linear-score-model","title":"Practical Parameterization: Linear Score Model","text":"<p>The authors choose:</p> \\[ S_W(x) = W^\\top x \\] <p>leading to a closed-form linear regression solution:</p> \\[ \\hat{W} = -\\left(\\sum_j G_j\\right)^{-1} \\sum_j \\sum_k x_{j,k} \\cdot \\nabla_\\theta \\log q(\\theta_j|\\theta_t)^\\top \\] <p>This converts FSM into an extremely efficient method.</p>"},{"location":"score_matching/SM-fisher-score-matching/#full-fsm-mle-algorithm","title":"Full FSM-MLE Algorithm","text":"<p>At iteration \\(t\\):</p> <ol> <li> <p>Sample parameters: \\(\\theta_j \\sim \\mathcal{N}(\\theta_t, \\sigma^2 I)\\)</p> </li> <li> <p>Simulate data: \\(x_{j,k} \\sim p(x|\\theta_j)\\)</p> </li> <li> <p>Fit linear model \\(S_{\\hat{W}}(x)\\) by solving the FSM least-squares problem</p> </li> <li> <p>Estimate gradient of smoothed log-likelihood:</p> </li> </ol> \\[ \\widehat{\\nabla_\\theta \\ell(\\theta_t)} = \\sum_{i=1}^N S_{\\hat{W}}(x_i) \\] <ol> <li>Update parameters using SGD or Adam</li> </ol>"},{"location":"score_matching/SM-fisher-score-matching/#why-this-works-intuition","title":"Why This Works (Intuition)","text":"<p>FSM pulls off something surprising:</p> <ul> <li>It never evaluates \\(p(x|\\theta)\\)</li> <li>It never computes \\(\\nabla_\\theta \\log p(x|\\theta)\\)</li> <li>It never estimates likelihoods like KDE-based methods do</li> </ul> <p>Yet it performs a gradient-based maximum likelihood optimization.</p> <p>The key is the joint sampling over \\((x, \\theta)\\) and score-matching identity that replaces the intractable terms with derivatives of a simple Gaussian proposal.</p>"},{"location":"score_matching/SM-fisher-score-matching/#understanding-the-bias-smoothing-effect-section-52","title":"Understanding the Bias / Smoothing Effect (Section 5.2)","text":"<p>If \\(\\sigma\\) (local proposal width) is too small:</p> <ul> <li>Variance explodes (like finite differences)</li> <li>Estimator becomes unstable</li> </ul> <p>If \\(\\sigma\\) is too large:</p> <ul> <li>Bias grows (you oversmooth the likelihood)</li> </ul> <p>Theorem 5.2 shows:</p> \\[ \\text{Bias} \\le L \\sqrt{d} \\cdot \\sigma \\cdot \\mathbb{E}[R(x)] \\] <p>This formalizes the bias\u2013variance tradeoff.</p>"},{"location":"score_matching/SM-fisher-score-matching/#summary-of-core-contributions","title":"Summary of Core Contributions","text":"<ol> <li>Novel local Fisher score matching objective</li> <li>Likelihood-free derivation using integration-by-parts</li> <li>Closed-form linear surrogate model</li> <li>Equivalence to Gaussian smoothing gradient estimator</li> <li>Strong theoretical properties</li> <li>Bias bounds</li> <li>Convergence via averaged SGD</li> <li>Asymptotic normality of estimator</li> <li>Superior empirical performance</li> <li>Over KDE + SPSA</li> <li>Over Neural Likelihood Estimators</li> <li>In high-dimensional SBI tasks</li> </ol>"},{"location":"score_matching/SM-fisher-score-matching/#connection-to-ebms","title":"Connection to EBMs","text":"<p>Fisher score matching connects directly to the EBM training problem:</p> <ul> <li>EBM challenge: The MLE gradient requires \\(\\mathbb{E}_{p_\\theta}[\\nabla_\\theta E_\\theta(x')]\\) (see EBM MLE Gradient Derivation)</li> <li>FSM solution: Bypass the likelihood entirely by estimating the Fisher score directly from simulations</li> </ul> <p>Both approaches share the core insight: use integration-by-parts to eliminate intractable terms.</p>"},{"location":"score_matching/SM-fisher-score-matching/#references","title":"References","text":"<ul> <li>Khoo et al. (2025). Direct Fisher Score Estimation for Likelihood Maximization</li> <li>Hyv\u00e4rinen (2005). Estimation of Non-Normalized Statistical Models by Score Matching</li> </ul>"},{"location":"score_matching/sm-01-overview/","title":"Score Matching: Overview","text":"<p>Score matching learns the shape of a data distribution by learning how probability mass locally flows, without ever computing or normalizing the probability itself.</p>"},{"location":"score_matching/sm-01-overview/#0-why-score-matching-is-the-conceptual-hinge","title":"0. Why Score Matching is the Conceptual Hinge","text":"<p>Score matching sits at the conceptual crossroads between VAEs and diffusion/energy-based models. Understanding why requires seeing what each approach optimizes:</p>"},{"location":"score_matching/sm-01-overview/#the-generative-modeling-landscape","title":"The Generative Modeling Landscape","text":"Approach What it learns Core object Key limitation VAEs \\(p_\\theta(x \\mid z)\\) and \\(q_\\phi(z \\mid x)\\) Latent representation \\(z\\) KL-to-prior tension, blurry samples Score Matching \\(\\nabla_x \\log p(x)\\) Gradient field in data space No explicit latent, slow sampling Diffusion Models Multi-scale score functions Denoising trajectory Expensive iteration EBMs \\(E_\\theta(x)\\) (energy) Unnormalized density Intractable partition function"},{"location":"score_matching/sm-01-overview/#the-key-insight","title":"The Key Insight","text":"<p>VAEs ask: \"What latent code \\(z\\) explains this data point \\(x\\)?\"</p> <p>Score matching asks: \"Which direction does probability increase from here?\"</p> <p>This shift\u2014from latent inference to gradient estimation\u2014is profound:</p> <ol> <li>No encoder needed: Score models work directly in data space</li> <li>No KL divergence: No tension between reconstruction and prior matching</li> <li>No normalization constant: The score \\(\\nabla_x \\log p(x)\\) cancels \\(Z\\)</li> </ol>"},{"location":"score_matching/sm-01-overview/#why-hinge","title":"Why \"Hinge\"?","text":"<p>Score matching is the hinge because:</p> <ul> <li>It inherits the probabilistic foundation from likelihood-based models (like VAEs)</li> <li>It enables diffusion models by providing the gradient field for sampling</li> <li>It connects to energy-based models since \\(s(x) = -\\nabla_x E(x)\\)</li> </ul> <p>In other words:</p> \\[ \\text{VAE} \\xrightarrow{\\text{drop encoder, learn gradient}} \\text{Score Matching} \\xrightarrow{\\text{multi-scale + Langevin}} \\text{Diffusion} \\]"},{"location":"score_matching/sm-01-overview/#1-notation","title":"1. Notation","text":"Symbol Meaning \\(x \\in \\mathbb{R}^d\\) Observed data (image, gene expression, etc.) \\(p_{\\text{data}}(x)\\) Unknown true data distribution \\(p_\\theta(x)\\) Model distribution we want to learn \\(\\log p_\\theta(x)\\) Log-density (unknown up to normalization) \\(s_\\theta(x)\\) Score function: \\(\\nabla_x \\log p_\\theta(x)\\)"},{"location":"score_matching/sm-01-overview/#critical-distinction","title":"Critical Distinction","text":"<ul> <li>VAEs differentiate with respect to parameters \\(\\theta\\)</li> <li>Score matching differentiates with respect to data \\(x\\)</li> </ul> <p>This is a deep conceptual shift.</p>"},{"location":"score_matching/sm-01-overview/#2-what-is-the-score","title":"2. What is the \"Score\"?","text":"<p>If you stand at a point \\(x\\) in data space:</p> <ul> <li>The score tells you which direction probability increases fastest</li> <li>It points \"uphill\" toward regions of higher density</li> <li>Its magnitude tells you how steep that increase is</li> </ul> <p>\"The score is a local vector field that tells us, at each point in space, which way the data distribution wants to pull samples.\"</p> <p>No probabilities yet. Just directions.</p>"},{"location":"score_matching/sm-01-overview/#3-the-core-problem-score-matching-solves","title":"3. The Core Problem Score Matching Solves","text":"<p>Maximum likelihood wants to minimize:</p> \\[ \\mathbb{E}_{x \\sim p_{\\text{data}}}\\left[-\\log p_\\theta(x)\\right] \\] <p>But for energy-based models:</p> \\[ \\log p_\\theta(x) = -E_\\theta(x) - \\log Z_\\theta \\] <p>The partition function \\(Z_\\theta = \\int e^{-E_\\theta(x)} dx\\) is usually intractable.</p> <p>Score matching says:</p> <p>\"If we can't compute the height of the landscape, let's learn its slope.\"</p> <p>Taking the gradient with respect to \\(x\\):</p> \\[ \\nabla_x \\log p_\\theta(x) = -\\nabla_x E_\\theta(x) \\] <p>The \\(\\log Z_\\theta\\) term vanishes because it doesn't depend on \\(x\\).</p>"},{"location":"score_matching/sm-01-overview/#4-the-original-score-matching-objective-hyvarinen-2005","title":"4. The Original Score Matching Objective (Hyv\u00e4rinen 2005)","text":""},{"location":"score_matching/sm-01-overview/#objective","title":"Objective","text":"\\[ \\mathcal{L}_{\\text{SM}}(\\theta) = \\mathbb{E}_{x \\sim p_{\\text{data}}} \\left[ \\frac{1}{2} \\left\\| \\nabla_x \\log p_\\theta(x) - \\nabla_x \\log p_{\\text{data}}(x) \\right\\|^2 \\right] \\]"},{"location":"score_matching/sm-01-overview/#in-words","title":"In Words","text":"<p>\"We want the gradient of the model's log-density to match the gradient of the true data log-density, on average over real data.\"</p> <p>But there's a problem: we don't know \\(\\nabla_x \\log p_{\\text{data}}(x)\\).</p>"},{"location":"score_matching/sm-01-overview/#5-the-clever-trick-integration-by-parts","title":"5. The Clever Trick: Integration by Parts","text":"<p>Hyv\u00e4rinen showed that after integration by parts, the objective becomes:</p> \\[ \\mathcal{L}_{\\text{SM}}(\\theta) = \\mathbb{E}_{x \\sim p_{\\text{data}}} \\left[ \\frac{1}{2} \\|s_\\theta(x)\\|^2 + \\text{tr}\\left(\\nabla_x s_\\theta(x)\\right) \\right] \\] <p>where:</p> <ul> <li>\\(s_\\theta(x) = \\nabla_x \\log p_\\theta(x)\\) is the model's score</li> <li>\\(\\text{tr}(\\nabla_x s_\\theta(x))\\) is the divergence (sum of diagonal Hessian elements)</li> </ul>"},{"location":"score_matching/sm-01-overview/#the-miracle","title":"The Miracle","text":"<p>\"We can train a model to produce a vector field without ever knowing the true data density.\"</p> <p>The unknown \\(\\nabla_x \\log p_{\\text{data}}(x)\\) disappears from the objective.</p>"},{"location":"score_matching/sm-01-overview/#6-why-classical-score-matching-is-fragile","title":"6. Why Classical Score Matching is Fragile","text":"<p>The divergence term:</p> <ul> <li>Requires second derivatives (Hessian diagonal)</li> <li>Is numerically unstable in high dimensions</li> <li>Has cubic complexity in naive implementations</li> </ul> <p>This is why classical score matching was beautiful but impractical for deep learning.</p>"},{"location":"score_matching/sm-01-overview/#7-denoising-score-matching-dsm-the-modern-approach","title":"7. Denoising Score Matching (DSM) \u2014 The Modern Approach","text":""},{"location":"score_matching/sm-01-overview/#key-idea","title":"Key Idea","text":"<p>\"Instead of learning the score of the data distribution directly, learn the score of noisy data distributions, which are smoother and easier.\"</p>"},{"location":"score_matching/sm-01-overview/#setup","title":"Setup","text":"<p>Add Gaussian noise:</p> \\[ \\tilde{x} = x + \\sigma \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I) \\]"},{"location":"score_matching/sm-01-overview/#dsm-objective","title":"DSM Objective","text":"\\[ \\mathcal{L}_{\\text{DSM}}(\\theta) = \\mathbb{E}_{x, \\epsilon} \\left[ \\left\\| s_\\theta(\\tilde{x}) + \\frac{\\tilde{x} - x}{\\sigma^2} \\right\\|^2 \\right] \\] <p>Or equivalently (in terms of \\(\\epsilon\\)):</p> \\[ \\mathcal{L}_{\\text{DSM}}(\\theta) = \\mathbb{E}_{x, \\epsilon} \\left[ \\left\\| s_\\theta(\\tilde{x}) + \\frac{\\epsilon}{\\sigma} \\right\\|^2 \\right] \\]"},{"location":"score_matching/sm-01-overview/#in-words_1","title":"In Words","text":"<p>\"We corrupt a real data point with Gaussian noise. We then train a network to predict the direction that points back toward the original clean data. That direction is exactly the score of the noisy data distribution.\"</p>"},{"location":"score_matching/sm-01-overview/#8-why-dsm-works","title":"8. Why DSM Works","text":"<p>For Gaussian corruption, the conditional score is known analytically:</p> \\[ \\nabla_{\\tilde{x}} \\log p(\\tilde{x} \\mid x) = -\\frac{\\tilde{x} - x}{\\sigma^2} = -\\frac{\\epsilon}{\\sigma} \\] <p>So DSM trains:</p> \\[ s_\\theta(\\tilde{x}) \\approx \\nabla_{\\tilde{x}} \\log p_\\sigma(\\tilde{x}) \\] <p>\"The model learns how noise should be removed, infinitesimally.\"</p>"},{"location":"score_matching/sm-01-overview/#9-from-dsm-to-diffusion-models","title":"9. From DSM to Diffusion Models","text":"<p>If you train many noise levels \\(\\sigma_1 &gt; \\sigma_2 &gt; \\cdots &gt; \\sigma_T\\):</p> Noise Level What the Score Captures High \\(\\sigma\\) Coarse global structure Low \\(\\sigma\\) Fine local details <p>Sampling then becomes:</p> <ol> <li>Start from pure noise \\(x_T \\sim \\mathcal{N}(0, I)\\)</li> <li>Repeatedly move along the score field (gradient ascent on log-density)</li> <li>Add small noise (Langevin dynamics)</li> </ol> <p>This is exactly diffusion / score-based generative modeling.</p> <p>Diffusion is not a new idea\u2014it is multi-scale score matching + numerical integration.</p>"},{"location":"score_matching/sm-01-overview/#10-comparison-vaes-vs-score-matching","title":"10. Comparison: VAEs vs Score Matching","text":""},{"location":"score_matching/sm-01-overview/#what-vaes-do","title":"What VAEs Do","text":"<ul> <li>Learn a latent-variable model \\(p_\\theta(x, z) = p(z) p_\\theta(x \\mid z)\\)</li> <li>Optimize likelihood lower bound (ELBO)</li> <li>Require a prior \\(p(z)\\) and inference model \\(q_\\phi(z \\mid x)\\)</li> <li>Enforce global latent geometry</li> </ul>"},{"location":"score_matching/sm-01-overview/#what-score-matching-does","title":"What Score Matching Does","text":"<ul> <li>Learn a vector field in data space</li> <li>Never compute likelihood directly</li> <li>No latent variables required</li> <li>No global coordinate system imposed</li> </ul>"},{"location":"score_matching/sm-01-overview/#trade-offs","title":"Trade-offs","text":"Aspect VAE Score Matching Sample quality Often blurry High fidelity Latent space Explicit, interpretable None Sampling speed Fast (single forward pass) Slow (iterative) Training stability KL collapse issues More stable Controllability Easy via latent manipulation Harder (guidance needed)"},{"location":"score_matching/sm-01-overview/#11-when-to-use-each-approach","title":"11. When to Use Each Approach","text":""},{"location":"score_matching/sm-01-overview/#use-score-matching-diffusion-when","title":"Use Score Matching / Diffusion When","text":"<ul> <li>Sample fidelity matters more than speed</li> <li>Data is high-dimensional and complex (images, audio)</li> <li>You don't need fast inference or explicit latents</li> </ul>"},{"location":"score_matching/sm-01-overview/#use-vaes-when","title":"Use VAEs When","text":"<ul> <li>You need explicit latent representations (world models, planning)</li> <li>Fast sampling is required</li> <li>Interpretability and controllability matter</li> <li>Downstream tasks need embeddings</li> </ul>"},{"location":"score_matching/sm-01-overview/#the-complementary-view","title":"The Complementary View","text":"<p>VAEs and score models are complementary, not rivals.</p> <p>Modern architectures often combine them (e.g., latent diffusion models use a VAE encoder + diffusion in latent space).</p>"},{"location":"score_matching/sm-01-overview/#12-key-takeaway","title":"12. Key Takeaway","text":"<p>VAEs learn where probability mass is. Score matching learns how probability mass flows.</p>"},{"location":"score_matching/sm-01-overview/#13-next-steps-on-the-roadmap","title":"13. Next Steps on the Roadmap","text":"<ol> <li>Langevin Dynamics: How scores generate samples via gradient-based MCMC</li> <li>Diffusion Forward\u2013Reverse Processes: The full SDE/ODE framework</li> <li>Guidance and Conditioning: Classifier-free guidance, conditional generation</li> <li>Latent Diffusion: Combining VAE compression with diffusion sampling</li> </ol>"},{"location":"score_matching/sm-01-overview/#references","title":"References","text":"<ul> <li>Hyv\u00e4rinen, A. (2005). Estimation of Non-Normalized Statistical Models by Score Matching. JMLR.</li> <li>Vincent, P. (2011). A Connection Between Score Matching and Denoising Autoencoders. Neural Computation.</li> <li>Song, Y. &amp; Ermon, S. (2019). Generative Modeling by Estimating Gradients of the Data Distribution. NeurIPS.</li> <li>Song, Y. et al. (2021). Score-Based Generative Modeling through Stochastic Differential Equations. ICLR.</li> </ul>"},{"location":"score_matching/sm-02-energy/","title":"Energy Functions in Generative Modeling","text":"<p>The energy function is the unifying language of generative models\u2014from Boltzmann machines to diffusion models to JEPA.</p>"},{"location":"score_matching/sm-02-energy/#0-motivation-why-energy","title":"0. Motivation: Why Energy?","text":"<p>The concept of \"energy\" comes from statistical physics, where systems naturally settle into low-energy states. This intuition transfers beautifully to machine learning:</p> <ul> <li>Low energy = high probability (likely configurations)</li> <li>High energy = low probability (unlikely configurations)</li> </ul> <p>Energy-based thinking provides a unified framework for understanding:</p> Model Family How Energy Appears Boltzmann Machines Explicit energy function over binary states Score Matching Score = negative gradient of energy Diffusion Models Denoising = following energy gradients Contrastive Learning Energy difference between positive/negative pairs JEPA Energy measures prediction error in latent space"},{"location":"score_matching/sm-02-energy/#1-notation-clarification-mathbbecdot-vs-e_thetax","title":"1. Notation Clarification: \\(\\mathbb{E}[\\cdot]\\) vs \\(E_\\theta(x)\\)","text":"<p>A common source of confusion: both use the letter \"E\" but mean completely different things.</p>"},{"location":"score_matching/sm-02-energy/#expectation-mathbbecdot","title":"Expectation: \\(\\mathbb{E}[\\cdot]\\)","text":"\\[ \\mathbb{E}_{x \\sim p(x)}[f(x)] = \\int f(x) \\, p(x) \\, dx \\] <p>\"The average value of \\(f(x)\\) when \\(x\\) is drawn from distribution \\(p(x)\\).\"</p>"},{"location":"score_matching/sm-02-energy/#energy-function-e_thetax","title":"Energy Function: \\(E_\\theta(x)\\)","text":"\\[ E_\\theta(x) : \\mathbb{R}^d \\to \\mathbb{R} \\] <p>\"A scalar-valued function that assigns an 'energy' (unnormalized negative log-probability) to each configuration \\(x\\).\"</p> <p>Key rule:</p> Symbol Meaning Type \\(\\mathbb{E}[\\cdot]\\) Expectation (average) Operator \\(E_\\theta(x)\\) Energy function Scalar function <p>They are completely unrelated despite sharing a letter.</p>"},{"location":"score_matching/sm-02-energy/#2-the-boltzmann-distribution","title":"2. The Boltzmann Distribution","text":"<p>In an energy-based model, probability is defined via the Boltzmann distribution:</p> \\[ p_\\theta(x) = \\frac{e^{-E_\\theta(x)}}{Z_\\theta} \\] <p>where:</p> <ul> <li>\\(E_\\theta(x)\\): energy of configuration \\(x\\)</li> <li>\\(Z_\\theta\\): partition function (normalization constant)</li> </ul>"},{"location":"score_matching/sm-02-energy/#intuition","title":"Intuition","text":"Energy Probability Interpretation Low \\(E_\\theta(x)\\) High \\(p_\\theta(x)\\) Likely, stable configuration High \\(E_\\theta(x)\\) Low \\(p_\\theta(x)\\) Unlikely, unstable configuration <p>The exponential ensures:</p> <ul> <li>Probabilities are always positive</li> <li>Small energy differences create large probability ratios</li> </ul>"},{"location":"score_matching/sm-02-energy/#3-the-partition-function-problem","title":"3. The Partition Function Problem","text":""},{"location":"score_matching/sm-02-energy/#definition","title":"Definition","text":"\\[ Z_\\theta = \\int e^{-E_\\theta(x)} \\, dx \\] <p>This integral sums the unnormalized probability mass over all possible configurations.</p>"},{"location":"score_matching/sm-02-energy/#why-its-intractable","title":"Why It's Intractable","text":"<p>For high-dimensional \\(x\\) (images, gene expression vectors):</p> <ul> <li>The integral is over \\(\\mathbb{R}^d\\) where \\(d\\) can be thousands or millions</li> <li>No closed-form solution exists for neural network energies</li> <li>Monte Carlo estimation requires samples from \\(p_\\theta(x)\\)\u2014a chicken-and-egg problem</li> </ul> <p>This is the fundamental computational bottleneck of energy-based models.</p>"},{"location":"score_matching/sm-02-energy/#4-from-energy-to-log-probability","title":"4. From Energy to Log-Probability","text":"<p>Starting from the Boltzmann distribution:</p> \\[ p_\\theta(x) = \\frac{e^{-E_\\theta(x)}}{Z_\\theta} \\] <p>Take the logarithm:</p> \\[ \\log p_\\theta(x) = \\log e^{-E_\\theta(x)} - \\log Z_\\theta \\] \\[ \\log p_\\theta(x) = -E_\\theta(x) - \\log Z_\\theta \\] <p>This is the key equation connecting energy to log-probability.</p>"},{"location":"score_matching/sm-02-energy/#5-the-score-cancels-the-partition-function","title":"5. The Score Cancels the Partition Function","text":"<p>Here's the crucial insight that makes score matching work.</p> <p>The score is defined as:</p> \\[ s(x) = \\nabla_x \\log p_\\theta(x) \\] <p>Substituting our expression for log-probability:</p> \\[ \\nabla_x \\log p_\\theta(x) = \\nabla_x \\left( -E_\\theta(x) - \\log Z_\\theta \\right) \\] \\[ = -\\nabla_x E_\\theta(x) - \\nabla_x \\log Z_\\theta \\] <p>But \\(Z_\\theta\\) is a constant with respect to \\(x\\) (it integrates over all \\(x\\)), so:</p> \\[ \\nabla_x \\log Z_\\theta = 0 \\] <p>Therefore:</p> \\[ \\boxed{s_\\theta(x) = \\nabla_x \\log p_\\theta(x) = -\\nabla_x E_\\theta(x)} \\]"},{"location":"score_matching/sm-02-energy/#the-miracle","title":"The Miracle","text":"<p>\"The score is just the negative gradient of the energy. The intractable partition function disappears completely.\"</p> <p>This is why score matching works\u2014we never need to compute \\(Z_\\theta\\).</p>"},{"location":"score_matching/sm-02-energy/#6-energy-landscapes-a-visual-intuition","title":"6. Energy Landscapes: A Visual Intuition","text":"<p>Think of \\(E_\\theta(x)\\) as defining a landscape over data space:</p> <pre><code>Energy\n  ^\n  |     /\\\n  |    /  \\      /\\\n  |   /    \\    /  \\\n  |  /      \\  /    \\\n  | /        \\/      \\\n  +-------------------&gt; x\n       valleys = data modes (low energy, high probability)\n       peaks = unlikely regions (high energy, low probability)\n</code></pre> <ul> <li>Score vectors point downhill (toward lower energy / higher probability)</li> <li>Sampling = rolling a ball downhill with some noise (Langevin dynamics)</li> <li>Training = shaping the landscape so valleys align with data</li> </ul>"},{"location":"score_matching/sm-02-energy/#7-historical-context-from-physics-to-ml","title":"7. Historical Context: From Physics to ML","text":"<p>The energy-based view has deep roots:</p> Era Development 1900s Boltzmann: Statistical mechanics, partition functions 1980s Hopfield networks, Boltzmann machines 2000s Contrastive divergence, RBMs, deep belief networks 2010s Score matching (Hyv\u00e4rinen), noise-contrastive estimation 2020s Diffusion models, energy-based priors, JEPA <p>The energy perspective keeps returning because it's mathematically natural for describing probability without normalization.</p>"},{"location":"score_matching/sm-02-energy/#8-foreshadowing-energy-in-modern-generative-models","title":"8. Foreshadowing: Energy in Modern Generative Models","text":""},{"location":"score_matching/sm-02-energy/#81-diffusion-models","title":"8.1 Diffusion Models","text":"<p>Diffusion models are implicitly energy-based. The denoising network learns:</p> \\[ \\epsilon_\\theta(x_t, t) \\approx -\\sigma_t \\nabla_{x_t} \\log p_t(x_t) \\] <p>This is exactly the score! The connection:</p> \\[ \\text{Denoising direction} = \\text{Score} = -\\nabla E \\] <p>Diffusion sampling is gradient descent on a time-varying energy landscape.</p>"},{"location":"score_matching/sm-02-energy/#82-energy-based-models-modern","title":"8.2 Energy-Based Models (Modern)","text":"<p>Recent work trains EBMs directly using:</p> <ul> <li>Contrastive divergence: Compare real vs. model samples</li> <li>Score matching: Avoid partition function entirely</li> <li>Noise-contrastive estimation: Classify real vs. noise</li> </ul> <p>EBMs are making a comeback for:</p> <ul> <li>Composable generation (add energies = combine concepts)</li> <li>Out-of-distribution detection (high energy = anomaly)</li> <li>Hybrid models (EBM prior + VAE decoder)</li> </ul>"},{"location":"score_matching/sm-02-energy/#83-jepa-joint-embedding-predictive-architecture","title":"8.3 JEPA (Joint Embedding Predictive Architecture)","text":"<p>Yann LeCun's JEPA uses energy in a fundamentally different way:</p> \\[ E(x, y) = \\| s_y - \\text{Predictor}(s_x) \\|^2 \\] <p>where \\(s_x, s_y\\) are learned representations.</p> <p>Key differences from generative EBMs:</p> Aspect Generative EBM JEPA Space Data space \\(x\\) Latent space \\(s\\) Goal Model \\(p(x)\\) Learn representations Energy meaning Unnormalized log-prob Prediction error Sampling Generate new \\(x\\) Not the goal <p>JEPA's insight: You don't need to model pixel-level probability to learn useful representations.</p>"},{"location":"score_matching/sm-02-energy/#84-contrastive-learning","title":"8.4 Contrastive Learning","text":"<p>InfoNCE and similar objectives are energy-based:</p> \\[ \\mathcal{L} = -\\log \\frac{e^{-E(x, x^+)}}{e^{-E(x, x^+)} + \\sum_j e^{-E(x, x^-_j)}} \\] <ul> <li>Low energy for positive pairs (similar)</li> <li>High energy for negative pairs (dissimilar)</li> </ul>"},{"location":"score_matching/sm-02-energy/#9-summary-the-energy-perspective","title":"9. Summary: The Energy Perspective","text":"Concept Definition Role \\(E_\\theta(x)\\) Energy function Unnormalized negative log-probability \\(Z_\\theta\\) Partition function Normalization constant (intractable) \\(p_\\theta(x)\\) Probability \\(\\propto e^{-E_\\theta(x)}\\) \\(s_\\theta(x)\\) Score \\(-\\nabla_x E_\\theta(x)\\) (no \\(Z\\)!)"},{"location":"score_matching/sm-02-energy/#the-core-insight","title":"The Core Insight","text":"<p>Energy-based thinking lets us work with unnormalized probabilities. Score matching lets us learn without ever computing the normalization.</p>"},{"location":"score_matching/sm-02-energy/#10-next-steps-on-the-roadmap","title":"10. Next Steps on the Roadmap","text":"<ol> <li>Langevin Dynamics: How to sample by following score/energy gradients</li> <li>Contrastive Divergence: Training EBMs with MCMC</li> <li>Diffusion as Energy: The SDE/ODE perspective</li> <li>JEPA Deep Dive: Energy in representation learning</li> </ol>"},{"location":"score_matching/sm-02-energy/#references","title":"References","text":"<ul> <li>Hopfield, J. (1982). Neural Networks and Physical Systems with Emergent Collective Computational Abilities. PNAS.</li> <li>Hinton, G. (2002). Training Products of Experts by Minimizing Contrastive Divergence. Neural Computation.</li> <li>Hyv\u00e4rinen, A. (2005). Estimation of Non-Normalized Statistical Models by Score Matching. JMLR.</li> <li>LeCun, Y. et al. (2006). A Tutorial on Energy-Based Learning. In Predicting Structured Data.</li> <li>Song, Y. &amp; Ermon, S. (2019). Generative Modeling by Estimating Gradients of the Data Distribution. NeurIPS.</li> <li>LeCun, Y. (2022). A Path Towards Autonomous Machine Intelligence. OpenReview.</li> </ul>"}]}