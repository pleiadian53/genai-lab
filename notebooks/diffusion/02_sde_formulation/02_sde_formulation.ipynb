{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDE Formulation of Diffusion Models: Interactive Tutorial\n",
    "\n",
    "This notebook implements the concepts from the companion theory document [`README.md`](./README.md).\n",
    "\n",
    "**Learning objectives:**\n",
    "1. Visualize Brownian motion and understand its properties\n",
    "2. Simulate forward SDEs (data → noise)\n",
    "3. Implement score matching training\n",
    "4. Sample from reverse SDEs (noise → data)\n",
    "5. Compare VP-SDE and VE-SDE\n",
    "\n",
    "**Prerequisites:**\n",
    "- Basic probability (Gaussian distributions)\n",
    "- PyTorch fundamentals\n",
    "- Understanding of DDPM (see `01_ddpm_basics.ipynb`)\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import integrate\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 4)\n",
    "\n",
    "# Random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Brownian Motion\n",
    "\n",
    "Before diving into SDEs, let's visualize Brownian motion $w(t)$ and understand its key properties:\n",
    "\n",
    "1. $w(0) = 0$\n",
    "2. Independent increments\n",
    "3. $w(t+\\Delta t) - w(t) \\sim \\mathcal{N}(0, \\Delta t)$\n",
    "4. Continuous but nowhere differentiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_brownian_motion(T=1.0, num_steps=1000, num_paths=5):\n",
    "    \"\"\"Simulate Brownian motion paths.\n",
    "    \n",
    "    Args:\n",
    "        T: Total time\n",
    "        num_steps: Number of discrete steps\n",
    "        num_paths: Number of paths to simulate\n",
    "    \n",
    "    Returns:\n",
    "        t: Time points\n",
    "        w: Brownian motion paths [num_paths, num_steps]\n",
    "    \"\"\"\n",
    "    dt = T / num_steps\n",
    "    t = np.linspace(0, T, num_steps)\n",
    "    \n",
    "    # Generate increments: dw ~ N(0, dt)\n",
    "    dw = np.random.randn(num_paths, num_steps) * np.sqrt(dt)\n",
    "    \n",
    "    # Cumulative sum to get w(t)\n",
    "    w = np.cumsum(dw, axis=1)\n",
    "    \n",
    "    # Ensure w(0) = 0\n",
    "    w = np.concatenate([np.zeros((num_paths, 1)), w], axis=1)\n",
    "    t = np.concatenate([[0], t])\n",
    "    \n",
    "    return t, w\n",
    "\n",
    "# Simulate and visualize\n",
    "t, w = simulate_brownian_motion(T=1.0, num_steps=1000, num_paths=10)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot 1: Multiple paths\n",
    "for i in range(10):\n",
    "    axes[0].plot(t, w[i], alpha=0.6, linewidth=0.8)\n",
    "axes[0].set_xlabel('Time t')\n",
    "axes[0].set_ylabel('w(t)')\n",
    "axes[0].set_title('Brownian Motion Paths')\n",
    "axes[0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Plot 2: Distribution at different times\n",
    "times = [0.25, 0.5, 0.75, 1.0]\n",
    "for time in times:\n",
    "    idx = int(time * 1000)\n",
    "    axes[1].hist(w[:, idx], bins=30, alpha=0.5, label=f't={time}', density=True)\n",
    "axes[1].set_xlabel('w(t)')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].set_title('Distribution at Different Times')\n",
    "axes[1].legend()\n",
    "\n",
    "# Plot 3: Variance over time (should be linear)\n",
    "variance = np.var(w, axis=0)\n",
    "axes[2].plot(t, variance, label='Empirical variance')\n",
    "axes[2].plot(t, t, 'r--', label='Theoretical: Var[w(t)] = t')\n",
    "axes[2].set_xlabel('Time t')\n",
    "axes[2].set_ylabel('Variance')\n",
    "axes[2].set_title('Variance Growth (Linear in Time)')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Key property: Variance at t=1.0 is {variance[-1]:.3f} (should be ≈ 1.0)\")\n",
    "print(f\"Standard deviation scales as √t: std(w(0.25)) = {np.std(w[:, 250]):.3f} ≈ {np.sqrt(0.25):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Forward SDE: Data → Noise\n",
    "\n",
    "Let's implement the forward SDE that corrupts data:\n",
    "\n",
    "$$\n",
    "dx = f(x,t)\\,dt + g(t)\\,dw(t)\n",
    "$$\n",
    "\n",
    "We'll focus on **VP-SDE** (Variance-Preserving):\n",
    "\n",
    "$$\n",
    "dx = -\\frac{1}{2}\\beta(t) x\\,dt + \\sqrt{\\beta(t)}\\,dw(t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPSDE:\n",
    "    \"\"\"Variance-Preserving SDE.\"\"\"\n",
    "    \n",
    "    def __init__(self, beta_min=0.1, beta_max=20.0, T=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            beta_min: Minimum noise level\n",
    "            beta_max: Maximum noise level\n",
    "            T: Total diffusion time\n",
    "        \"\"\"\n",
    "        self.beta_min = beta_min\n",
    "        self.beta_max = beta_max\n",
    "        self.T = T\n",
    "    \n",
    "    def beta(self, t):\n",
    "        \"\"\"Linear noise schedule.\"\"\"\n",
    "        return self.beta_min + (self.beta_max - self.beta_min) * t / self.T\n",
    "    \n",
    "    def drift(self, x, t):\n",
    "        \"\"\"Drift coefficient: f(x,t) = -0.5 * beta(t) * x\"\"\"\n",
    "        return -0.5 * self.beta(t) * x\n",
    "    \n",
    "    def diffusion(self, t):\n",
    "        \"\"\"Diffusion coefficient: g(t) = sqrt(beta(t))\"\"\"\n",
    "        return np.sqrt(self.beta(t))\n",
    "    \n",
    "    def marginal_prob(self, x0, t):\n",
    "        \"\"\"Compute mean and std of p_t(x | x_0).\n",
    "        \n",
    "        For VP-SDE:\n",
    "        mean = sqrt(alpha_bar_t) * x_0\n",
    "        std = sqrt(1 - alpha_bar_t)\n",
    "        \"\"\"\n",
    "        # Compute alpha_bar_t = exp(-0.5 * integral_0^t beta(s) ds)\n",
    "        log_alpha_bar = -0.25 * t**2 * (self.beta_max - self.beta_min) / self.T - 0.5 * t * self.beta_min\n",
    "        alpha_bar = np.exp(log_alpha_bar)\n",
    "        \n",
    "        mean = np.sqrt(alpha_bar) * x0\n",
    "        std = np.sqrt(1 - alpha_bar)\n",
    "        \n",
    "        return mean, std\n",
    "    \n",
    "    def sample_from_marginal(self, x0, t):\n",
    "        \"\"\"Sample x_t ~ p_t(x | x_0).\"\"\"\n",
    "        mean, std = self.marginal_prob(x0, t)\n",
    "        noise = np.random.randn(*x0.shape)\n",
    "        return mean + std * noise, noise\n",
    "\n",
    "# Test the SDE\n",
    "sde = VPSDE(beta_min=0.1, beta_max=20.0, T=1.0)\n",
    "\n",
    "# Visualize beta(t)\n",
    "t_vals = np.linspace(0, 1, 100)\n",
    "beta_vals = [sde.beta(t) for t in t_vals]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(t_vals, beta_vals)\n",
    "plt.xlabel('Time t')\n",
    "plt.ylabel('β(t)')\n",
    "plt.title('Noise Schedule')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Visualize alpha_bar(t)\n",
    "alpha_bar_vals = [np.exp(-0.25 * t**2 * (sde.beta_max - sde.beta_min) - 0.5 * t * sde.beta_min) for t in t_vals]\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(t_vals, alpha_bar_vals)\n",
    "plt.xlabel('Time t')\n",
    "plt.ylabel('ᾱ(t)')\n",
    "plt.title('Signal Decay')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Forward Diffusion on 2D Data\n",
    "\n",
    "Let's see how the SDE corrupts a simple 2D distribution (Swiss roll)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_swiss_roll(n_samples=1000):\n",
    "    \"\"\"Generate 2D Swiss roll dataset.\"\"\"\n",
    "    theta = np.sqrt(np.random.rand(n_samples)) * 3 * np.pi\n",
    "    x = theta * np.cos(theta)\n",
    "    y = theta * np.sin(theta)\n",
    "    data = np.stack([x, y], axis=1) / 10.0  # Scale down\n",
    "    return data\n",
    "\n",
    "# Generate data\n",
    "x0 = generate_swiss_roll(n_samples=2000)\n",
    "\n",
    "# Apply forward diffusion at different times\n",
    "times = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, t in enumerate(times):\n",
    "    if t == 0:\n",
    "        xt = x0\n",
    "    else:\n",
    "        xt, _ = sde.sample_from_marginal(x0, t)\n",
    "    \n",
    "    axes[i].scatter(xt[:, 0], xt[:, 1], alpha=0.3, s=1)\n",
    "    axes[i].set_xlim(-3, 3)\n",
    "    axes[i].set_ylim(-3, 3)\n",
    "    axes[i].set_title(f't = {t:.1f}')\n",
    "    axes[i].set_aspect('equal')\n",
    "    \n",
    "    # Show mean and std\n",
    "    mean, std = sde.marginal_prob(x0, t)\n",
    "    axes[i].text(0.05, 0.95, f'std={std:.3f}', \n",
    "                transform=axes[i].transAxes, \n",
    "                verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.suptitle('Forward Diffusion: Swiss Roll → Gaussian Noise', fontsize=14, y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation: As t increases, the structure dissolves into isotropic Gaussian noise.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Score Function and Training\n",
    "\n",
    "The **score function** is the gradient of the log-density:\n",
    "\n",
    "$$\n",
    "s(x, t) = \\nabla_x \\log p_t(x)\n",
    "$$\n",
    "\n",
    "For training, we use **denoising score matching**. The conditional score has a closed form:\n",
    "\n",
    "$$\n",
    "\\nabla_x \\log p_t(x_t | x_0) = -\\frac{x_t - \\text{mean}(t)}{\\text{std}^2(t)} = -\\frac{\\varepsilon}{\\text{std}(t)}\n",
    "$$\n",
    "\n",
    "where $\\varepsilon$ is the noise we added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleScoreNetwork(nn.Module):\n",
    "    \"\"\"Simple MLP score network for 2D data.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dim=2, hidden_dim=128, time_dim=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Time embedding (sinusoidal)\n",
    "        self.time_dim = time_dim\n",
    "        \n",
    "        # Network\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(data_dim + time_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, data_dim),\n",
    "        )\n",
    "    \n",
    "    def time_embedding(self, t):\n",
    "        \"\"\"Sinusoidal time embedding.\"\"\"\n",
    "        half_dim = self.time_dim // 2\n",
    "        emb = np.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "        return emb\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        \"\"\"Predict score: s(x, t) ≈ ∇_x log p_t(x).\n",
    "        \n",
    "        Args:\n",
    "            x: Data [batch_size, data_dim]\n",
    "            t: Time [batch_size]\n",
    "        \n",
    "        Returns:\n",
    "            score: [batch_size, data_dim]\n",
    "        \"\"\"\n",
    "        t_emb = self.time_embedding(t)\n",
    "        h = torch.cat([x, t_emb], dim=-1)\n",
    "        return self.net(h)\n",
    "\n",
    "# Test the network\n",
    "model = SimpleScoreNetwork(data_dim=2, hidden_dim=128).to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "x_test = torch.randn(16, 2).to(device)\n",
    "t_test = torch.rand(16).to(device)\n",
    "score_test = model(x_test, t_test)\n",
    "print(f\"Input shape: {x_test.shape}\")\n",
    "print(f\"Output shape: {score_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "Train the score network using denoising score matching:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\mathbb{E}_{t, x_0, \\varepsilon} \\left[ \\lambda(t) \\left\\| s_\\theta(x_t, t) + \\frac{\\varepsilon}{\\sigma(t)} \\right\\|^2 \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_score_network(model, data, sde, num_epochs=1000, batch_size=256, lr=1e-3):\n",
    "    \"\"\"Train score network using denoising score matching.\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    \n",
    "    data_tensor = torch.FloatTensor(data).to(device)\n",
    "    n_samples = len(data)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
    "        # Sample batch\n",
    "        idx = np.random.choice(n_samples, batch_size, replace=True)\n",
    "        x0 = data_tensor[idx]\n",
    "        \n",
    "        # Sample time uniformly\n",
    "        t = torch.rand(batch_size, device=device) * sde.T\n",
    "        \n",
    "        # Sample noisy data\n",
    "        mean_np, std_np = sde.marginal_prob(x0.cpu().numpy(), t.cpu().numpy())\n",
    "        mean = torch.FloatTensor(mean_np).to(device)\n",
    "        std = torch.FloatTensor(std_np).to(device)\n",
    "        \n",
    "        noise = torch.randn_like(x0)\n",
    "        xt = mean + std[:, None] * noise\n",
    "        \n",
    "        # Target score: -noise / std\n",
    "        target_score = -noise / std[:, None]\n",
    "        \n",
    "        # Predict score\n",
    "        pred_score = model(xt, t)\n",
    "        \n",
    "        # Loss (weighted by std^2)\n",
    "        loss = torch.mean((pred_score - target_score) ** 2 * (std[:, None] ** 2))\n",
    "        \n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Train the model\n",
    "losses = train_score_network(\n",
    "    model=model,\n",
    "    data=x0,\n",
    "    sde=sde,\n",
    "    num_epochs=500,  # Increase for better results\n",
    "    batch_size=256,\n",
    "    lr=1e-3,\n",
    ")\n",
    "\n",
    "# Plot training curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Score Matching Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sampling: Reverse SDE\n",
    "\n",
    "Generate samples by solving the reverse-time SDE:\n",
    "\n",
    "$$\n",
    "dx = \\left[f(x,t) - g(t)^2 s_\\theta(x,t)\\right] dt + g(t)\\,d\\bar{w}(t)\n",
    "$$\n",
    "\n",
    "We'll use the **Euler-Maruyama method** to discretize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_reverse_sde(model, sde, n_samples=1000, num_steps=500):\n",
    "    \"\"\"Sample from reverse SDE.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained score network\n",
    "        sde: Forward SDE\n",
    "        n_samples: Number of samples to generate\n",
    "        num_steps: Number of discretization steps\n",
    "    \n",
    "    Returns:\n",
    "        samples: Generated samples [n_samples, data_dim]\n",
    "        trajectory: Full trajectory [num_steps, n_samples, data_dim]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Start from noise\n",
    "    x = torch.randn(n_samples, 2, device=device)\n",
    "    \n",
    "    dt = -sde.T / num_steps\n",
    "    trajectory = [x.cpu().numpy()]\n",
    "    \n",
    "    for i in tqdm(range(num_steps), desc=\"Sampling\", leave=False):\n",
    "        t = sde.T - i * (-dt)\n",
    "        t_batch = torch.ones(n_samples, device=device) * t\n",
    "        \n",
    "        # Predict score\n",
    "        score = model(x, t_batch)\n",
    "        \n",
    "        # Drift: f(x,t) - g(t)^2 * score\n",
    "        drift = sde.drift(x.cpu().numpy(), t)\n",
    "        drift = torch.FloatTensor(drift).to(device)\n",
    "        g_t = sde.diffusion(t)\n",
    "        drift = drift - (g_t ** 2) * score\n",
    "        \n",
    "        # Diffusion: g(t) * dw\n",
    "        noise = torch.randn_like(x)\n",
    "        diffusion = g_t * noise * np.sqrt(-dt)\n",
    "        \n",
    "        # Update\n",
    "        x = x + drift * dt + diffusion\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            trajectory.append(x.cpu().numpy())\n",
    "    \n",
    "    return x.cpu().numpy(), np.array(trajectory)\n",
    "\n",
    "# Generate samples\n",
    "samples, trajectory = sample_reverse_sde(model, sde, n_samples=2000, num_steps=500)\n",
    "\n",
    "print(f\"Generated {samples.shape[0]} samples\")\n",
    "print(f\"Trajectory has {len(trajectory)} snapshots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Generated Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare real vs generated\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].scatter(x0[:, 0], x0[:, 1], alpha=0.3, s=1, label='Real data')\n",
    "axes[0].set_xlim(-3, 3)\n",
    "axes[0].set_ylim(-3, 3)\n",
    "axes[0].set_title('Real Data (Swiss Roll)')\n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].scatter(samples[:, 0], samples[:, 1], alpha=0.3, s=1, label='Generated', color='orange')\n",
    "axes[1].set_xlim(-3, 3)\n",
    "axes[1].set_ylim(-3, 3)\n",
    "axes[1].set_title('Generated Samples (via Reverse SDE)')\n",
    "axes[1].set_aspect('equal')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Reverse Diffusion Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show snapshots of reverse process\n",
    "num_snapshots = min(6, len(trajectory))\n",
    "indices = np.linspace(0, len(trajectory)-1, num_snapshots, dtype=int)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    xt = trajectory[idx]\n",
    "    t_val = sde.T * (1 - idx / len(trajectory))\n",
    "    \n",
    "    axes[i].scatter(xt[:, 0], xt[:, 1], alpha=0.3, s=1)\n",
    "    axes[i].set_xlim(-3, 3)\n",
    "    axes[i].set_ylim(-3, 3)\n",
    "    axes[i].set_title(f't = {t_val:.3f}')\n",
    "    axes[i].set_aspect('equal')\n",
    "\n",
    "plt.suptitle('Reverse Diffusion: Noise → Data', fontsize=14, y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Probability Flow ODE (Deterministic Sampling)\n",
    "\n",
    "Instead of the stochastic reverse SDE, we can use the **probability flow ODE**:\n",
    "\n",
    "$$\n",
    "\\frac{dx}{dt} = f(x,t) - \\frac{1}{2}g(t)^2 s_\\theta(x,t)\n",
    "$$\n",
    "\n",
    "This generates samples **deterministically** (like DDIM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_probability_flow_ode(model, sde, n_samples=1000, num_steps=100):\n",
    "    \"\"\"Sample using probability flow ODE (deterministic).\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    x = torch.randn(n_samples, 2, device=device)\n",
    "    dt = -sde.T / num_steps\n",
    "    \n",
    "    for i in tqdm(range(num_steps), desc=\"ODE Sampling\", leave=False):\n",
    "        t = sde.T - i * (-dt)\n",
    "        t_batch = torch.ones(n_samples, device=device) * t\n",
    "        \n",
    "        score = model(x, t_batch)\n",
    "        \n",
    "        # ODE drift: f(x,t) - 0.5 * g(t)^2 * score\n",
    "        drift = sde.drift(x.cpu().numpy(), t)\n",
    "        drift = torch.FloatTensor(drift).to(device)\n",
    "        g_t = sde.diffusion(t)\n",
    "        drift = drift - 0.5 * (g_t ** 2) * score\n",
    "        \n",
    "        # Update (no noise!)\n",
    "        x = x + drift * dt\n",
    "    \n",
    "    return x.cpu().numpy()\n",
    "\n",
    "# Generate samples with ODE\n",
    "samples_ode = sample_probability_flow_ode(model, sde, n_samples=2000, num_steps=100)\n",
    "\n",
    "# Compare SDE vs ODE sampling\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].scatter(x0[:, 0], x0[:, 1], alpha=0.3, s=1)\n",
    "axes[0].set_title('Real Data')\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "axes[1].scatter(samples[:, 0], samples[:, 1], alpha=0.3, s=1, color='orange')\n",
    "axes[1].set_title('SDE Sampling (Stochastic)')\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "axes[2].scatter(samples_ode[:, 0], samples_ode[:, 1], alpha=0.3, s=1, color='green')\n",
    "axes[2].set_title('ODE Sampling (Deterministic)')\n",
    "axes[2].set_aspect('equal')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlim(-3, 3)\n",
    "    ax.set_ylim(-3, 3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: ODE sampling is faster (fewer steps) but may be less diverse.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've implemented the SDE framework for diffusion models:\n",
    "\n",
    "1. ✅ **Brownian motion**: Visualized continuous random walks\n",
    "2. ✅ **Forward SDE**: Corrupted data with VP-SDE\n",
    "3. ✅ **Score matching**: Trained neural network to predict scores\n",
    "4. ✅ **Reverse SDE**: Generated samples by solving reverse-time SDE\n",
    "5. ✅ **Probability flow ODE**: Deterministic sampling alternative\n",
    "\n",
    "**Key takeaways:**\n",
    "- SDEs provide a **continuous-time** view of diffusion\n",
    "- Only the **score function** needs to be learned\n",
    "- **Reverse SDE** (stochastic) vs **ODE** (deterministic) sampling\n",
    "- VP-SDE is the continuous version of DDPM\n",
    "\n",
    "**Next steps:**\n",
    "- Apply to high-dimensional data (images, gene expression)\n",
    "- Implement conditional generation\n",
    "- Study scPPDM (latent-space VP-SDE for drug response)\n",
    "\n",
    "See [`README.md`](./README.md) for detailed theory and mathematical derivations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
