{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Models for Gene Expression Data\n",
    "\n",
    "This notebook extends the SDE-based diffusion framework to generate **realistic gene expression data**.\n",
    "\n",
    "**Motivation:**\n",
    "- Companies like Synthesize Bio (GEM-1), Insilico Medicine (Precious3GPT), and scGPT are building generative models for gene expression\n",
    "- Applications: drug target discovery, clinical trial acceleration, in-silico perturbation experiments\n",
    "\n",
    "**Learning objectives:**\n",
    "1. Understand challenges of applying diffusion to gene expression data\n",
    "2. Implement latent diffusion for high-dimensional biological data\n",
    "3. Add conditional generation (cell type, tissue, disease)\n",
    "4. Evaluate generated samples with biological metrics\n",
    "\n",
    "**Prerequisites:** `02_sde_formulation.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sys.path.insert(0, str(Path('../../../src').resolve()))\n",
    "\n",
    "from genailab.diffusion import VPSDE, train_score_network, sample_reverse_sde\n",
    "from genailab.data import ToyBulkDataset\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Synthetic Gene Expression Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ToyBulkDataset(n=5000, n_genes=500, n_tissues=5, n_diseases=3, n_batches=4, seed=42)\n",
    "print(f\"Dataset: {len(dataset)} samples, {dataset.n_genes} genes\")\n",
    "print(f\"Conditions: {list(dataset.cond.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "pca = PCA(n_components=2)\n",
    "x_pca = pca.fit_transform(dataset.x.numpy())\n",
    "scatter = axes[0].scatter(x_pca[:, 0], x_pca[:, 1], c=dataset.cond['tissue'].numpy(), cmap='tab10', alpha=0.5, s=5)\n",
    "axes[0].set_title('PCA by Tissue')\n",
    "plt.colorbar(scatter, ax=axes[0])\n",
    "axes[1].hist(dataset.x.numpy().flatten(), bins=50, density=True)\n",
    "axes[1].set_title('Expression Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Latent Diffusion Approach\n",
    "\n",
    "Key insight: Run diffusion in a learned latent space (like Stable Diffusion, scPPDM).\n",
    "\n",
    "```\n",
    "Genes (500) → VAE Encoder → Latent (32) → Diffusion → VAE Decoder → Genes (500)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneVAE(nn.Module):\n",
    "    def __init__(self, n_genes, latent_dim=32, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = nn.Sequential(nn.Linear(n_genes, hidden_dim), nn.LayerNorm(hidden_dim), nn.SiLU(),\n",
    "                                     nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim), nn.SiLU())\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.decoder = nn.Sequential(nn.Linear(latent_dim, hidden_dim), nn.LayerNorm(hidden_dim), nn.SiLU(),\n",
    "                                     nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim), nn.SiLU(),\n",
    "                                     nn.Linear(hidden_dim, n_genes))\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = mu + torch.randn_like(mu) * torch.exp(0.5 * logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train VAE\n",
    "latent_dim = 32\n",
    "vae = GeneVAE(n_genes=dataset.n_genes, latent_dim=latent_dim).to(device)\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "data = dataset.x.to(device)\n",
    "\n",
    "for epoch in tqdm(range(2000), desc=\"Training VAE\"):\n",
    "    idx = np.random.choice(len(data), 128)\n",
    "    x = data[idx]\n",
    "    recon, mu, logvar = vae(x)\n",
    "    loss = F.mse_loss(recon, x) + 0.01 * (-0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp()))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get latent representations\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    mu, _ = vae.encode(data)\n",
    "    latent_data = mu.cpu().numpy()\n",
    "print(f\"Latent shape: {latent_data.shape} (compression: {dataset.n_genes/latent_dim:.0f}x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train diffusion in latent space\n",
    "from genailab.diffusion import SimpleScoreNetwork\n",
    "\n",
    "score_net = SimpleScoreNetwork(data_dim=latent_dim, hidden_dim=256, num_layers=4).to(device)\n",
    "sde = VPSDE(beta_min=0.1, beta_max=20.0, T=1.0)\n",
    "\n",
    "losses = train_score_network(score_net, latent_data, sde, num_epochs=5000, batch_size=128, lr=1e-3, device=device)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Latent Diffusion Training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples\n",
    "latent_samples, _ = sample_reverse_sde(score_net, sde, n_samples=1000, num_steps=500, data_dim=latent_dim, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    gene_samples = vae.decode(torch.FloatTensor(latent_samples).to(device)).cpu().numpy()\n",
    "\n",
    "print(f\"Generated {gene_samples.shape[0]} samples with {gene_samples.shape[1]} genes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Distribution\n",
    "axes[0].hist(dataset.x.numpy().flatten(), bins=50, density=True, alpha=0.5, label='Real')\n",
    "axes[0].hist(gene_samples.flatten(), bins=50, density=True, alpha=0.5, label='Generated')\n",
    "axes[0].legend()\n",
    "axes[0].set_title('Expression Distribution')\n",
    "\n",
    "# Gene means\n",
    "real_means = dataset.x.numpy().mean(axis=0)\n",
    "gen_means = gene_samples.mean(axis=0)\n",
    "axes[1].scatter(real_means, gen_means, alpha=0.3, s=5)\n",
    "axes[1].plot([-2, 2], [-2, 2], 'r--')\n",
    "corr = np.corrcoef(real_means, gen_means)[0, 1]\n",
    "axes[1].set_title(f'Gene Means (r={corr:.3f})')\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "real_pca = pca.fit_transform(dataset.x.numpy())\n",
    "gen_pca = pca.transform(gene_samples)\n",
    "axes[2].scatter(real_pca[:, 0], real_pca[:, 1], alpha=0.3, s=5, label='Real')\n",
    "axes[2].scatter(gen_pca[:, 0], gen_pca[:, 1], alpha=0.3, s=5, label='Generated')\n",
    "axes[2].legend()\n",
    "axes[2].set_title('PCA Overlay')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summary & Next Steps\n",
    "\n",
    "**What we learned:**\n",
    "- Latent diffusion is more efficient than direct diffusion for high-dimensional data\n",
    "- VAE provides a compressed, structured representation\n",
    "- Generated samples capture gene-gene correlations\n",
    "\n",
    "**Next steps:**\n",
    "1. Add conditional generation (tissue, disease)\n",
    "2. Apply to real single-cell data (PBMC3k)\n",
    "3. Evaluate with biological metrics (pathway enrichment, DE analysis)\n",
    "4. Connect to scPPDM for perturbation prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genailab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
