{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Models for Gene Expression Data\n",
    "\n",
    "This notebook extends the SDE-based diffusion framework to generate **realistic gene expression data**.\n",
    "\n",
    "**Motivation:**\n",
    "- Companies like Synthesize Bio (GEM-1), Insilico Medicine (Precious3GPT), and scGPT are building generative models for gene expression\n",
    "- Applications: drug target discovery, clinical trial acceleration, in-silico perturbation experiments\n",
    "\n",
    "**Learning objectives:**\n",
    "1. Understand challenges of applying diffusion to gene expression data\n",
    "2. Implement latent diffusion for high-dimensional biological data\n",
    "3. Add conditional generation (cell type, tissue, disease)\n",
    "4. Evaluate generated samples with biological metrics\n",
    "\n",
    "**Prerequisites:** `02_sde_formulation.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sys.path.insert(0, str(Path('../../../src').resolve()))\n",
    "\n",
    "from genailab.diffusion import VPSDE, train_score_network, sample_reverse_sde\n",
    "from genailab.data import ToyBulkDataset\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Synthetic Gene Expression Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ToyBulkDataset(n=5000, n_genes=500, n_tissues=5, n_diseases=3, n_batches=4, seed=42)\n",
    "print(f\"Dataset: {len(dataset)} samples, {dataset.n_genes} genes\")\n",
    "print(f\"Conditions: {list(dataset.cond.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "pca = PCA(n_components=2)\n",
    "x_pca = pca.fit_transform(dataset.x.numpy())\n",
    "scatter = axes[0].scatter(x_pca[:, 0], x_pca[:, 1], c=dataset.cond['tissue'].numpy(), cmap='tab10', alpha=0.5, s=5)\n",
    "axes[0].set_title('PCA by Tissue')\n",
    "plt.colorbar(scatter, ax=axes[0])\n",
    "axes[1].hist(dataset.x.numpy().flatten(), bins=50, density=True)\n",
    "axes[1].set_title('Expression Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Latent Diffusion Approach\n",
    "\n",
    "Key insight: Run diffusion in a learned latent space (like Stable Diffusion, scPPDM).\n",
    "\n",
    "```\n",
    "Genes (500) → VAE Encoder → Latent (32) → Diffusion → VAE Decoder → Genes (500)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneVAE(nn.Module):\n",
    "    def __init__(self, n_genes, latent_dim=32, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = nn.Sequential(nn.Linear(n_genes, hidden_dim), nn.LayerNorm(hidden_dim), nn.SiLU(),\n",
    "                                     nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim), nn.SiLU())\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.decoder = nn.Sequential(nn.Linear(latent_dim, hidden_dim), nn.LayerNorm(hidden_dim), nn.SiLU(),\n",
    "                                     nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim), nn.SiLU(),\n",
    "                                     nn.Linear(hidden_dim, n_genes))\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = mu + torch.randn_like(mu) * torch.exp(0.5 * logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train VAE\n",
    "latent_dim = 32\n",
    "vae = GeneVAE(n_genes=dataset.n_genes, latent_dim=latent_dim).to(device)\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "data = dataset.x.to(device)\n",
    "\n",
    "for epoch in tqdm(range(2000), desc=\"Training VAE\"):\n",
    "    idx = np.random.choice(len(data), 128)\n",
    "    x = data[idx]\n",
    "    recon, mu, logvar = vae(x)\n",
    "    loss = F.mse_loss(recon, x) + 0.01 * (-0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp()))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get latent representations\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    mu, _ = vae.encode(data)\n",
    "    latent_data = mu.cpu().numpy()\n",
    "print(f\"Latent shape: {latent_data.shape} (compression: {dataset.n_genes/latent_dim:.0f}x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train diffusion in latent space\n",
    "from genailab.diffusion import SimpleScoreNetwork\n",
    "\n",
    "score_net = SimpleScoreNetwork(data_dim=latent_dim, hidden_dim=256, num_layers=4).to(device)\n",
    "sde = VPSDE(beta_min=0.1, beta_max=20.0, T=1.0)\n",
    "\n",
    "losses = train_score_network(score_net, latent_data, sde, num_epochs=5000, batch_size=128, lr=1e-3, device=device)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Latent Diffusion Training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples\n",
    "latent_samples, _ = sample_reverse_sde(score_net, sde, n_samples=1000, num_steps=500, data_dim=latent_dim, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    gene_samples = vae.decode(torch.FloatTensor(latent_samples).to(device)).cpu().numpy()\n",
    "\n",
    "print(f\"Generated {gene_samples.shape[0]} samples with {gene_samples.shape[1]} genes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Distribution\n",
    "axes[0].hist(dataset.x.numpy().flatten(), bins=50, density=True, alpha=0.5, label='Real')\n",
    "axes[0].hist(gene_samples.flatten(), bins=50, density=True, alpha=0.5, label='Generated')\n",
    "axes[0].legend()\n",
    "axes[0].set_title('Expression Distribution')\n",
    "\n",
    "# Gene means\n",
    "real_means = dataset.x.numpy().mean(axis=0)\n",
    "gen_means = gene_samples.mean(axis=0)\n",
    "axes[1].scatter(real_means, gen_means, alpha=0.3, s=5)\n",
    "axes[1].plot([-2, 2], [-2, 2], 'r--')\n",
    "corr = np.corrcoef(real_means, gen_means)[0, 1]\n",
    "axes[1].set_title(f'Gene Means (r={corr:.3f})')\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "real_pca = pca.fit_transform(dataset.x.numpy())\n",
    "gen_pca = pca.transform(gene_samples)\n",
    "axes[2].scatter(real_pca[:, 0], real_pca[:, 1], alpha=0.3, s=5, label='Real')\n",
    "axes[2].scatter(gen_pca[:, 0], gen_pca[:, 1], alpha=0.3, s=5, label='Generated')\n",
    "axes[2].legend()\n",
    "axes[2].set_title('PCA Overlay')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handling Count Data: The Core Challenge\n",
    "\n",
    "The approach above uses MSE loss and Gaussian outputs, which works for **log-normalized** expression data. But real gene expression is **count data**:\n",
    "\n",
    "- **UMI counts** (scRNA-seq): integers with many zeros\n",
    "- **TPM/FPKM** (bulk RNA-seq): continuous but count-derived\n",
    "- **Heavy-tailed**: few highly expressed genes, many low/zero\n",
    "\n",
    "**Problem**: Adding Gaussian noise to counts doesn't have clear biological meaning.\n",
    "\n",
    "**Solution**: Use count-aware decoders (Negative Binomial, Zero-Inflated NB) that output distribution parameters instead of point estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import count-aware decoders and losses from genailab\n",
    "from genailab.model.decoders import NegativeBinomialDecoder, ZINBDecoder\n",
    "from genailab.objectives.losses import nb_loss, zinb_loss, elbo_loss_nb, elbo_loss_zinb\n",
    "\n",
    "print(\"Available count-aware components:\")\n",
    "print(\"  Decoders: NegativeBinomialDecoder, ZINBDecoder\")\n",
    "print(\"  Losses: nb_loss, zinb_loss, elbo_loss_nb, elbo_loss_zinb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 VAE with Negative Binomial Decoder\n",
    "\n",
    "The key insight: **run diffusion in continuous latent space, but decode to count distributions**.\n",
    "\n",
    "```\n",
    "Counts → Encoder → z (continuous) → Diffusion → z' → NB Decoder → NB(μ, θ) → Sample counts\n",
    "```\n",
    "\n",
    "The NB decoder outputs:\n",
    "- **μ (mu)**: Expected count per gene\n",
    "- **θ (theta)**: Dispersion parameter (inverse overdispersion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneVAE_NB(nn.Module):\n",
    "    \"\"\"VAE with Negative Binomial decoder for count data.\n",
    "    \n",
    "    Architecture:\n",
    "        Encoder: expression → latent (mu, logvar)\n",
    "        Decoder: latent → NB parameters (mu, theta)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_genes, latent_dim=32, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.n_genes = n_genes\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder (same as before)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_genes, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # NB Decoder: outputs rate parameters\n",
    "        self.decoder_net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        self.rho_head = nn.Linear(hidden_dim, n_genes)  # Rate (before library scaling)\n",
    "        \n",
    "        # Gene-specific dispersion (learned parameter)\n",
    "        self.log_theta = nn.Parameter(torch.zeros(n_genes))\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode expression to latent distribution.\"\"\"\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Sample z from q(z|x) using reparameterization trick.\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z, library_size=None):\n",
    "        \"\"\"Decode latent to NB parameters.\n",
    "        \n",
    "        Returns:\n",
    "            mu: Expected counts (n_samples, n_genes)\n",
    "            theta: Dispersion (n_genes,) broadcast to (n_samples, n_genes)\n",
    "        \"\"\"\n",
    "        h = self.decoder_net(z)\n",
    "        \n",
    "        # Rate: softmax ensures non-negative and sums to 1\n",
    "        rho = F.softmax(self.rho_head(h), dim=-1)\n",
    "        \n",
    "        # Scale by library size (total counts per sample)\n",
    "        if library_size is not None:\n",
    "            if library_size.dim() == 1:\n",
    "                library_size = library_size.unsqueeze(-1)\n",
    "            mu = rho * library_size\n",
    "        else:\n",
    "            # Default: assume library size = n_genes (normalized)\n",
    "            mu = rho * self.n_genes\n",
    "        \n",
    "        # Dispersion: exp to ensure positive\n",
    "        theta = torch.exp(self.log_theta).unsqueeze(0).expand(z.shape[0], -1)\n",
    "        \n",
    "        return mu, theta\n",
    "    \n",
    "    def forward(self, x, library_size=None):\n",
    "        \"\"\"Full forward pass.\"\"\"\n",
    "        # Encode\n",
    "        enc_mu, enc_logvar = self.encode(x)\n",
    "        z = self.reparameterize(enc_mu, enc_logvar)\n",
    "        \n",
    "        # Decode to NB parameters\n",
    "        dec_mu, dec_theta = self.decode(z, library_size)\n",
    "        \n",
    "        return dec_mu, dec_theta, enc_mu, enc_logvar\n",
    "\n",
    "print(\"GeneVAE_NB defined with Negative Binomial decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic count data for demonstration\n",
    "# (Our ToyBulkDataset uses log-normalized data; let's create count-like data)\n",
    "\n",
    "def generate_synthetic_counts(n_samples=5000, n_genes=500, seed=42):\n",
    "    \"\"\"Generate synthetic count data with NB-like properties.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Base expression rates (log-normal distributed)\n",
    "    log_base_rates = np.random.normal(2, 2, n_genes)\n",
    "    base_rates = np.exp(log_base_rates)\n",
    "    \n",
    "    # Sample-specific library sizes (total counts)\n",
    "    library_sizes = np.random.lognormal(10, 0.5, n_samples)\n",
    "    \n",
    "    # Gene-specific dispersion (smaller = more overdispersion)\n",
    "    dispersions = np.random.uniform(0.1, 10, n_genes)\n",
    "    \n",
    "    # Generate counts using Negative Binomial\n",
    "    counts = np.zeros((n_samples, n_genes))\n",
    "    for i in range(n_samples):\n",
    "        # Rates for this sample\n",
    "        rates = base_rates * (library_sizes[i] / base_rates.sum())\n",
    "        for j in range(n_genes):\n",
    "            # NB parameterization: mean=mu, var=mu + mu^2/theta\n",
    "            mu = rates[j]\n",
    "            theta = dispersions[j]\n",
    "            # Convert to scipy's NB parameterization\n",
    "            p = theta / (theta + mu)\n",
    "            counts[i, j] = np.random.negative_binomial(theta, p) if p < 1 else 0\n",
    "    \n",
    "    return counts.astype(np.float32), library_sizes.astype(np.float32)\n",
    "\n",
    "# Generate count data\n",
    "count_data, library_sizes = generate_synthetic_counts(n_samples=5000, n_genes=500)\n",
    "print(f\"Count data shape: {count_data.shape}\")\n",
    "print(f\"Count range: [{count_data.min():.0f}, {count_data.max():.0f}]\")\n",
    "print(f\"Sparsity (zeros): {(count_data == 0).mean()*100:.1f}%\")\n",
    "print(f\"Library sizes: mean={library_sizes.mean():.0f}, std={library_sizes.std():.0f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].hist(count_data.flatten(), bins=50, density=True, log=True)\n",
    "axes[0].set_xlabel('Count')\n",
    "axes[0].set_ylabel('Density (log)')\n",
    "axes[0].set_title('Count Distribution (heavy-tailed)')\n",
    "\n",
    "axes[1].hist(np.log1p(count_data).flatten(), bins=50, density=True)\n",
    "axes[1].set_xlabel('log1p(Count)')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].set_title('Log-transformed Distribution')\n",
    "\n",
    "axes[2].hist(library_sizes, bins=30, density=True)\n",
    "axes[2].set_xlabel('Library Size')\n",
    "axes[2].set_ylabel('Density')\n",
    "axes[2].set_title('Library Size Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train VAE with NB decoder on count data\n",
    "# Note: We log-transform input for encoder stability, but use NB loss on original counts\n",
    "\n",
    "# Prepare data\n",
    "count_tensor = torch.FloatTensor(count_data).to(device)\n",
    "library_tensor = torch.FloatTensor(library_sizes).to(device)\n",
    "\n",
    "# Log-transform for encoder input (standard practice in scVI, scGen)\n",
    "log_counts = torch.log1p(count_tensor)\n",
    "\n",
    "# Initialize model\n",
    "vae_nb = GeneVAE_NB(n_genes=500, latent_dim=32, hidden_dim=256).to(device)\n",
    "optimizer_nb = torch.optim.Adam(vae_nb.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "losses_nb = []\n",
    "for epoch in tqdm(range(3000), desc=\"Training VAE-NB\"):\n",
    "    idx = np.random.choice(len(count_tensor), 128)\n",
    "    x_counts = count_tensor[idx]\n",
    "    x_log = log_counts[idx]\n",
    "    lib_size = library_tensor[idx]\n",
    "    \n",
    "    # Forward pass (encode log-transformed, decode to NB params)\n",
    "    dec_mu, dec_theta, enc_mu, enc_logvar = vae_nb(x_log, lib_size)\n",
    "    \n",
    "    # Loss: NB reconstruction + KL divergence\n",
    "    loss, loss_dict = elbo_loss_nb(\n",
    "        x=x_counts,           # Original counts for NB loss\n",
    "        mu=dec_mu,            # Predicted mean\n",
    "        theta=dec_theta,      # Predicted dispersion\n",
    "        enc_mu=enc_mu,        # Encoder mean\n",
    "        enc_logvar=enc_logvar,# Encoder logvar\n",
    "        beta=0.01             # KL weight (low to avoid posterior collapse)\n",
    "    )\n",
    "    \n",
    "    optimizer_nb.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_nb.step()\n",
    "    \n",
    "    losses_nb.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}, \"\n",
    "              f\"Recon: {loss_dict['recon'].item():.4f}, KL: {loss_dict['kl'].item():.4f}\")\n",
    "\n",
    "# Plot training curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses_nb)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('VAE-NB Training (NB Reconstruction Loss)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Latent Diffusion with NB Decoder\n",
    "\n",
    "Now we combine the trained VAE-NB with diffusion in latent space:\n",
    "\n",
    "1. **Extract latent representations** from VAE-NB encoder\n",
    "2. **Train diffusion** in the continuous latent space\n",
    "3. **Sample**: noise → diffusion → latent → NB decoder → sample from NB distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract latent representations from VAE-NB\n",
    "vae_nb.eval()\n",
    "with torch.no_grad():\n",
    "    enc_mu, _ = vae_nb.encode(log_counts)\n",
    "    latent_data_nb = enc_mu.cpu().numpy()\n",
    "\n",
    "print(f\"Latent shape: {latent_data_nb.shape}\")\n",
    "print(f\"Latent range: [{latent_data_nb.min():.2f}, {latent_data_nb.max():.2f}]\")\n",
    "\n",
    "# Step 2: Train diffusion in latent space\n",
    "score_net_nb = SimpleScoreNetwork(data_dim=32, hidden_dim=256, num_layers=4).to(device)\n",
    "sde_nb = VPSDE(beta_min=0.1, beta_max=20.0, T=1.0)\n",
    "\n",
    "losses_diff_nb = train_score_network(\n",
    "    score_net_nb, latent_data_nb, sde_nb, \n",
    "    num_epochs=5000, batch_size=128, lr=1e-3, device=device\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses_diff_nb)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Latent Diffusion Training (for NB-VAE)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Generate samples using latent diffusion + NB decoder\n",
    "\n",
    "# Sample latent vectors from diffusion\n",
    "latent_samples_nb, _ = sample_reverse_sde(\n",
    "    score_net_nb, sde_nb, \n",
    "    n_samples=500, num_steps=500, data_dim=32, device=device\n",
    ")\n",
    "\n",
    "# Decode to NB parameters\n",
    "vae_nb.eval()\n",
    "with torch.no_grad():\n",
    "    z_tensor = torch.FloatTensor(latent_samples_nb).to(device)\n",
    "    # Use median library size for generation\n",
    "    gen_lib_size = torch.full((500,), library_sizes.mean(), device=device)\n",
    "    gen_mu, gen_theta = vae_nb.decode(z_tensor, gen_lib_size)\n",
    "\n",
    "# Sample from NB distribution to get actual counts\n",
    "def sample_from_nb(mu, theta):\n",
    "    \"\"\"Sample counts from Negative Binomial distribution.\"\"\"\n",
    "    mu_np = mu.cpu().numpy()\n",
    "    theta_np = theta.cpu().numpy()\n",
    "    \n",
    "    # NB parameterization: p = theta / (theta + mu)\n",
    "    p = theta_np / (theta_np + mu_np + 1e-8)\n",
    "    p = np.clip(p, 1e-8, 1 - 1e-8)\n",
    "    \n",
    "    # Sample\n",
    "    samples = np.zeros_like(mu_np)\n",
    "    for i in range(mu_np.shape[0]):\n",
    "        for j in range(mu_np.shape[1]):\n",
    "            if p[i, j] < 1:\n",
    "                samples[i, j] = np.random.negative_binomial(theta_np[i, j], p[i, j])\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Generate count samples\n",
    "gen_counts = sample_from_nb(gen_mu, gen_theta)\n",
    "print(f\"Generated {gen_counts.shape[0]} count samples\")\n",
    "print(f\"Count range: [{gen_counts.min():.0f}, {gen_counts.max():.0f}]\")\n",
    "print(f\"Sparsity (zeros): {(gen_counts == 0).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate: Compare real vs generated count distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Row 1: Count distributions\n",
    "axes[0, 0].hist(count_data.flatten(), bins=50, density=True, alpha=0.5, label='Real', log=True)\n",
    "axes[0, 0].hist(gen_counts.flatten(), bins=50, density=True, alpha=0.5, label='Generated', log=True)\n",
    "axes[0, 0].set_xlabel('Count')\n",
    "axes[0, 0].set_ylabel('Density (log)')\n",
    "axes[0, 0].set_title('Count Distribution')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Log-transformed comparison\n",
    "axes[0, 1].hist(np.log1p(count_data).flatten(), bins=50, density=True, alpha=0.5, label='Real')\n",
    "axes[0, 1].hist(np.log1p(gen_counts).flatten(), bins=50, density=True, alpha=0.5, label='Generated')\n",
    "axes[0, 1].set_xlabel('log1p(Count)')\n",
    "axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].set_title('Log-transformed Distribution')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Gene means correlation\n",
    "real_means = count_data.mean(axis=0)\n",
    "gen_means = gen_counts.mean(axis=0)\n",
    "axes[0, 2].scatter(real_means, gen_means, alpha=0.3, s=5)\n",
    "max_val = max(real_means.max(), gen_means.max())\n",
    "axes[0, 2].plot([0, max_val], [0, max_val], 'r--')\n",
    "corr = np.corrcoef(real_means, gen_means)[0, 1]\n",
    "axes[0, 2].set_xlabel('Real Gene Mean')\n",
    "axes[0, 2].set_ylabel('Generated Gene Mean')\n",
    "axes[0, 2].set_title(f'Gene Means (r={corr:.3f})')\n",
    "\n",
    "# Row 2: More detailed comparisons\n",
    "# Gene variances\n",
    "real_vars = count_data.var(axis=0)\n",
    "gen_vars = gen_counts.var(axis=0)\n",
    "axes[1, 0].scatter(np.log1p(real_vars), np.log1p(gen_vars), alpha=0.3, s=5)\n",
    "max_var = max(np.log1p(real_vars).max(), np.log1p(gen_vars).max())\n",
    "axes[1, 0].plot([0, max_var], [0, max_var], 'r--')\n",
    "corr_var = np.corrcoef(real_vars, gen_vars)[0, 1]\n",
    "axes[1, 0].set_xlabel('Real Gene Variance (log)')\n",
    "axes[1, 0].set_ylabel('Generated Gene Variance (log)')\n",
    "axes[1, 0].set_title(f'Gene Variances (r={corr_var:.3f})')\n",
    "\n",
    "# Sparsity per gene\n",
    "real_sparsity = (count_data == 0).mean(axis=0)\n",
    "gen_sparsity = (gen_counts == 0).mean(axis=0)\n",
    "axes[1, 1].scatter(real_sparsity, gen_sparsity, alpha=0.3, s=5)\n",
    "axes[1, 1].plot([0, 1], [0, 1], 'r--')\n",
    "corr_sparse = np.corrcoef(real_sparsity, gen_sparsity)[0, 1]\n",
    "axes[1, 1].set_xlabel('Real Sparsity')\n",
    "axes[1, 1].set_ylabel('Generated Sparsity')\n",
    "axes[1, 1].set_title(f'Gene Sparsity (r={corr_sparse:.3f})')\n",
    "\n",
    "# PCA comparison\n",
    "pca = PCA(n_components=2)\n",
    "real_pca = pca.fit_transform(np.log1p(count_data[:500]))  # Subsample for speed\n",
    "gen_pca = pca.transform(np.log1p(gen_counts))\n",
    "axes[1, 2].scatter(real_pca[:, 0], real_pca[:, 1], alpha=0.3, s=5, label='Real')\n",
    "axes[1, 2].scatter(gen_pca[:, 0], gen_pca[:, 1], alpha=0.3, s=5, label='Generated')\n",
    "axes[1, 2].set_xlabel('PC1')\n",
    "axes[1, 2].set_ylabel('PC2')\n",
    "axes[1, 2].set_title('PCA Overlay (log-transformed)')\n",
    "axes[1, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Latent Diffusion + NB Decoder: Real vs Generated Counts', y=1.02, fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(f\"  Real counts - Mean: {count_data.mean():.1f}, Sparsity: {(count_data==0).mean()*100:.1f}%\")\n",
    "print(f\"  Generated   - Mean: {gen_counts.mean():.1f}, Sparsity: {(gen_counts==0).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary & Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "**The Count Data Challenge:**\n",
    "- Gene expression is fundamentally count data (UMI counts, TPM)\n",
    "- Standard diffusion assumes continuous data with Gaussian noise\n",
    "- Adding noise to counts doesn't have clear biological meaning\n",
    "\n",
    "**Solutions Implemented:**\n",
    "\n",
    "| Approach | How It Works | Pros | Cons |\n",
    "|----------|--------------|------|------|\n",
    "| **Latent Diffusion** | Diffusion in VAE latent space | Well-defined, flexible | Requires VAE training |\n",
    "| **NB Decoder** | Output NB(μ, θ) parameters | Proper count model | More complex |\n",
    "| **ZINB Decoder** | NB + dropout probability π | Handles sparsity | Even more complex |\n",
    "\n",
    "**The Recommended Pipeline:**\n",
    "```\n",
    "Counts → log1p → Encoder → z (continuous) → Diffusion → z' → NB Decoder → NB(μ,θ) → Sample\n",
    "```\n",
    "\n",
    "### Implementation in genai-lab\n",
    "\n",
    "- `src/genailab/model/decoders.py`: `NegativeBinomialDecoder`, `ZINBDecoder`\n",
    "- `src/genailab/objectives/losses.py`: `nb_loss()`, `zinb_loss()`, `elbo_loss_nb()`, `elbo_loss_zinb()`\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Add conditioning** - Condition on tissue, disease, perturbation\n",
    "2. **Apply to real data** - PBMC3k, GTEx, scPerturb\n",
    "3. **Implement ZINB** - For sparse scRNA-seq with dropout\n",
    "4. **Benchmark** - Compare with scVI, scGen on standard tasks\n",
    "5. **Connect to scPPDM** - Full perturbation prediction pipeline\n",
    "\n",
    "### References\n",
    "\n",
    "- Lopez et al. (2018) - \"Deep generative modeling for single-cell transcriptomics\" (scVI)\n",
    "- Lotfollahi et al. (2020) - \"scGen predicts single-cell perturbation responses\"\n",
    "- See also: `docs/incubation/generative-ai-for-gene-expression-prediction.md`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genailab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
